<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no">
<meta name="author" content="MingmingYe">


    
    


<meta property="og:type" content="website">
<meta property="og:title" content="Stay hungry, Stay foolish.">
<meta property="og:url" content="http://mmyblog.cn/index.html">
<meta property="og:site_name" content="Stay hungry, Stay foolish.">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Stay hungry, Stay foolish.">

<link rel="apple-touch-icon" href="/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="Stay hungry, Stay foolish." type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">


    <style> .article { opacity: 0;} </style>


<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>Stay hungry, Stay foolish.</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: true
    }
</script>


    <script>
        yiliaConfig.jquery_ui = [true, "//cdn.bootcss.com/jqueryui/1.10.4/jquery-ui.min.js", "//cdn.bootcss.com/jqueryui/1.10.4/css/jquery-ui.min.css"];
    </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






</head></html>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/deep.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">MingmingYe</a></h1>
        </hgroup>

        
        <p class="header-subtitle">当你的才华撑不起你的野心时，只有静下心来好好学习！纵使命运注定是个打酱油的，也要打一瓶与别人不一样的酱油！</p>
        

        
            <form id="search-form">
            <input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false" />
            <i class="fa fa-times" onclick="resetSearch()"></i>
            </form>
            <div id="local-search-result"></div>
            <p class='no-result'>No results found <i class='fa fa-spinner fa-pulse'></i></p>
        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:878759487@qq.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AUC/">AUC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention/">Attention</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/">BERT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BLUE/">BLUE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Beam-search/">Beam search</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CRF/">CRF</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ConvNet/">ConvNet</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ELMo/">ELMo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GPT/">GPT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GRU/">GRU</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gradient-Clipping/">Gradient Clipping</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LR/">LR</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LSTM/">LSTM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linear/">Linear</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Parsing/">Parsing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PyTorch/">PyTorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/QA/">QA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN-LSTM/">RNN/LSTM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ROC/">ROC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recursive-Neural-Networks/">Recursive Neural Networks</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQuAD-BiDAF/">SQuAD-BiDAF</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Seq2Seq/">Seq2Seq</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TorchText/">TorchText</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/">Transformer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer-XL/">Transformer-XL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/XLNet/">XLNet</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/boosting/">boosting</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cbow/">cbow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hierarchical-softmax/">hierarchical softmax</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/inference/">inference</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jiaba/">jiaba</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jieba/">jieba</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mumpy/">mumpy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/negative-sampling/">negative sampling</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pandas/">pandas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pyspark/">pyspark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/seaborn/">seaborn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/skip-gram/">skip-gram</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/softmax/">softmax</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/word-embedding/">word-embedding</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/wxBot/">wxBot</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/中文分词/">中文分词</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/优化方法/">优化方法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/决策树/">决策树</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/图神经网络/">图神经网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/微积分/">微积分</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/朴素贝叶斯/">朴素贝叶斯</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/概率/">概率</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/模型调优/">模型调优</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/正则表达式/">正则表达式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习速查表/">深度学习速查表</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/特征工程/">特征工程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/线性代数/">线性代数</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/统计/">统计</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/聊天机器人/">聊天机器人</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/聚类/">聚类</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/语言模型/">语言模型</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/贝叶斯分类器/">贝叶斯分类器</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/降维/">降维</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/集成学习/">集成学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/面试/">面试</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://mmyblog.cn/">mmy</a>
                    
                    </div>
                </section>
                

                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">MingmingYe</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/deep.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">MingmingYe</a></h1>
            </hgroup>
            
            <p class="header-subtitle">当你的才华撑不起你的野心时，只有静下心来好好学习！纵使命运注定是个打酱油的，也要打一瓶与别人不一样的酱油！</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:878759487@qq.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap">
  
    <article id="post-扩展内容" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/06/09/扩展内容/" class="article-date">
      <time datetime="2020-06-09T00:51:03.000Z" itemprop="datePublished">2020-06-09</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/09/扩展内容/">扩展内容</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="结构化预测"><a href="#结构化预测" class="headerlink" title="结构化预测"></a><a href="https://shimo.im/docs/t9hjVGDx33vyTDjD" target="_blank" rel="noopener">结构化预测</a></h3><ul>
<li>隐马尔科夫模型 <a href="http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf" target="_blank" rel="noopener">http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf</a></li>
<li>最大熵与词性标注</li>
<li>条件随机场</li>
</ul>
<h3 id="中文分词-Chinese-Word-Segmentation"><a href="#中文分词-Chinese-Word-Segmentation" class="headerlink" title="中文分词 Chinese Word Segmentation"></a>中文分词 Chinese Word Segmentation</h3><ul>
<li><a href="https://www.aclweb.org/anthology/D18-1529.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D18-1529.pdf</a></li>
</ul>
<h3 id="Parsing与Recursive-Neural-Networks"><a href="#Parsing与Recursive-Neural-Networks" class="headerlink" title="Parsing与Recursive Neural Networks"></a>Parsing与Recursive Neural Networks</h3><ul>
<li><a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture18-TreeRNNs.pdf" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture18-TreeRNNs.pdf</a></li>
<li><a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture05-dep-parsing.pdf" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture05-dep-parsing.pdf</a></li>
</ul>
<h3 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h3><ul>
<li>GCN: <a href="https://arxiv.org/pdf/1609.02907.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1609.02907.pdf</a></li>
<li>GCN for relational graph: <a href="https://arxiv.org/pdf/1703.06103.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1703.06103.pdf</a></li>
</ul>
<h3 id="Data-to-Text-文本生成"><a href="#Data-to-Text-文本生成" class="headerlink" title="Data to Text 文本生成"></a>Data to Text 文本生成</h3><ul>
<li>GCN生成文本 <a href="https://arxiv.org/pdf/1810.09995v1.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1810.09995v1.pdf</a></li>
</ul>
<h3 id="知识图谱相关问题"><a href="#知识图谱相关问题" class="headerlink" title="知识图谱相关问题"></a><a href="https://shimo.im/docs/9pwCHPwXxcGHRrxh" target="_blank" rel="noopener">知识图谱相关问题</a></h3>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Parsing/">Parsing</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Recursive-Neural-Networks/">Recursive Neural Networks</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/中文分词/">中文分词</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/图神经网络/">图神经网络</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-XLNet" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/06/09/XLNet/" class="article-date">
      <time datetime="2020-06-09T00:33:08.000Z" itemprop="datePublished">2020-06-09</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/09/XLNet/">XLNet</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context"><a href="#Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context" class="headerlink" title="Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"></a>Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</h2><p><a href="https://arxiv.org/pdf/1901.02860.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.02860.pdf</a></p>
<p>相较于传统transformer decoder，引入两个新模块</p>
<ul>
<li>segment-level recurrence mechanism</li>
</ul>
<p><img src="https://uploader.shimo.im/f/DpNe30kuahkbOeW5.png!thumbnail" alt="img"></p>
<ul>
<li><p>a novel positional encoding scheme</p>
</li>
<li><p>考虑我们在attention机制中如何使用positional encoding</p>
</li>
</ul>
<p>(E_{x_i}^T+U_i^T)W_q^TW_kE_{x_j}U_j</p>
<p><img src="https://uploader.shimo.im/f/5zNU9yZQtQMClNiY.png!thumbnail" alt="img"></p>
<ul>
<li><p>R他们采用的是transformer当中的positional encoding</p>
</li>
<li><p>u和v是需要训练的模型参数</p>
</li>
</ul>
<p>最终Transformer XL模型</p>
<p><img src="https://uploader.shimo.im/f/Nm1uk49MIjUys1aK.png!thumbnail" alt="img"></p>
<p>代码</p>
<p><a href="https://github.com/kimiyoung/transformer-xl" target="_blank" rel="noopener">https://github.com/kimiyoung/transformer-xl</a></p>
<h2 id="XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding"><a href="#XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding" class="headerlink" title="XLNet: Generalized Autoregressive Pretraining for Language Understanding"></a>XLNet: Generalized Autoregressive Pretraining for Language Understanding</h2><p><a href="https://arxiv.org/pdf/1906.08237.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1906.08237.pdf</a></p>
<p>背景知识</p>
<ul>
<li><p>自回归语言模型（Autoregressive Language Model）：采用从左往右或从右往左的语言模型，根据上文预测下文。</p>
</li>
<li><p>缺点：只利用了预测单词左边或右边的信息，无法同时利用两边的信息。ELMo在一定程度上解决了这个问题。</p>
</li>
<li><p><img src="https://uploader.shimo.im/f/cpfGbeRfzf8c1ga8.png!thumbnail" alt="img"></p>
</li>
<li><p>自编码模型（Denoising Auto Encoder, DAE）：在输入中随机mask一些单词，利用上下文来预测被mask掉的单词。BERT采用了这一思路。</p>
</li>
<li><p><img src="https://uploader.shimo.im/f/za1FnG3zHdsbm5gD.png!thumbnail" alt="img"></p>
</li>
</ul>
<p>两个模型的问题</p>
<p><img src="https://uploader.shimo.im/f/A1rO6rAR1nAQqqvu.png!thumbnail" alt="img"></p>
<p>XLNet的目标是融合以上两种模型的优点，解决它们各自存在的问题。</p>
<p>XLNet模型：Permutation Language Modeling</p>
<p><img src="https://uploader.shimo.im/f/LdaKeEgG8XwH3iNj.png!thumbnail" alt="img"></p>
<p>Two-Stream Self-Attention</p>
<p><img src="https://uploader.shimo.im/f/TdQVsxOeYMoakBW0.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/iLMqF1WinQI6wOsW.png!thumbnail" alt="img"></p>
<p>参考资料</p>
<p><a href="https://zhuanlan.zhihu.com/p/70257427" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/70257427</a></p>
<p>代码</p>
<p><a href="https://github.com/zihangdai/xlnet" target="_blank" rel="noopener">https://github.com/zihangdai/xlnet</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/word-embedding/">word-embedding</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-PyTorch" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/06/08/PyTorch/" class="article-date">
      <time datetime="2020-06-08T11:12:06.000Z" itemprop="datePublished">2020-06-08</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/08/PyTorch/">PyTorch</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="什么是PyTorch"><a href="#什么是PyTorch" class="headerlink" title="什么是PyTorch?"></a>什么是PyTorch?</h1><p>PyTorch是一个基于Python的科学计算库，它有以下特点:</p>
<ul>
<li>类似于NumPy，但是它可以使用GPU</li>
<li>可以用它定义深度学习模型，可以灵活地进行深度学习模型的训练和使用</li>
</ul>
<h2 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h2><p>Tensor类似与NumPy的ndarray，唯一的区别是Tensor可以在GPU上加速运算。</p>
<p>In [2]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br></pre></td></tr></table></figure>

<p>构造一个未初始化的5x3矩阵:</p>
<p>In [4]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(5,3)</span><br><span class="line">x</span><br></pre></td></tr></table></figure>

<p>Out[4]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.0000e+00, -8.5899e+09,  0.0000e+00],</span><br><span class="line">        [-8.5899e+09,         nan,  0.0000e+00],</span><br><span class="line">        [ 2.7002e-06,  1.8119e+02,  1.2141e+01],</span><br><span class="line">        [ 7.8503e+02,  6.7504e-07,  6.5200e-10],</span><br><span class="line">        [ 2.9537e-06,  1.7186e-04,         nan]])</span><br></pre></td></tr></table></figure>

<p>构建一个随机初始化的矩阵:</p>
<p>In [5]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(5,3)</span><br><span class="line">x</span><br></pre></td></tr></table></figure>

<p>Out[5]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.4628, 0.7432, 0.9785],</span><br><span class="line">        [0.2068, 0.4441, 0.9176],</span><br><span class="line">        [0.1027, 0.5275, 0.3884],</span><br><span class="line">        [0.9380, 0.2113, 0.2839],</span><br><span class="line">        [0.0094, 0.4001, 0.6483]])</span><br></pre></td></tr></table></figure>

<p>构建一个全部为0，类型为long的矩阵:</p>
<p>In [8]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(5,3,dtype=torch.long)</span><br><span class="line">x</span><br></pre></td></tr></table></figure>

<p>Out[8]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0, 0, 0],</span><br><span class="line">        [0, 0, 0],</span><br><span class="line">        [0, 0, 0],</span><br><span class="line">        [0, 0, 0],</span><br><span class="line">        [0, 0, 0]])</span><br></pre></td></tr></table></figure>

<p>In [11]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(5,3).long()</span><br><span class="line">x.dtype</span><br></pre></td></tr></table></figure>

<p>Out[11]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.int64</span><br></pre></td></tr></table></figure>

<p>从数据直接直接构建tensor:</p>
<p>In [12]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([5.5,3])</span><br><span class="line">x</span><br></pre></td></tr></table></figure>

<p>Out[12]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([5.5000, 3.0000])</span><br></pre></td></tr></table></figure>

<p>也可以从一个已有的tensor构建一个tensor。这些方法会重用原来tensor的特征，例如，数据类型，除非提供新的数据。</p>
<p>In [16]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = x.new_ones(5,3, dtype=torch.double)</span><br><span class="line">x</span><br></pre></td></tr></table></figure>

<p>Out[16]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]], dtype=torch.float64)</span><br></pre></td></tr></table></figure>

<p>In [17]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn_like(x, dtype=torch.float)</span><br><span class="line">x</span><br></pre></td></tr></table></figure>

<p>Out[17]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.2411, -0.3961, -0.9206],</span><br><span class="line">        [-0.0508,  0.2653,  0.4685],</span><br><span class="line">        [ 0.5368, -0.3606, -0.0073],</span><br><span class="line">        [ 0.3383,  0.6826,  1.7368],</span><br><span class="line">        [-0.0811, -0.6957, -0.4566]])</span><br></pre></td></tr></table></figure>

<p>得到tensor的形状:</p>
<p>In [20]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure>

<p>Out[20]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([5, 3])</span><br></pre></td></tr></table></figure>

<h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><p><code>torch.Size</code> 返回的是一个tuple</p>
<p>Operations</p>
<p>有很多种tensor运算。我们先介绍加法运算。</p>
<p>In [21]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = torch.rand(5,3)</span><br><span class="line">y</span><br></pre></td></tr></table></figure>

<p>Out[21]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.9456, 0.3996, 0.1981],</span><br><span class="line">        [0.8728, 0.7097, 0.3721],</span><br><span class="line">        [0.7489, 0.9502, 0.6241],</span><br><span class="line">        [0.5176, 0.0200, 0.5130],</span><br><span class="line">        [0.3552, 0.2710, 0.7392]])</span><br></pre></td></tr></table></figure>

<p>In [23]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x + y</span><br></pre></td></tr></table></figure>

<p>Out[23]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.1866,  0.0035, -0.7225],</span><br><span class="line">        [ 0.8220,  0.9750,  0.8406],</span><br><span class="line">        [ 1.2857,  0.5896,  0.6168],</span><br><span class="line">        [ 0.8559,  0.7026,  2.2498],</span><br><span class="line">        [ 0.2741, -0.4248,  0.2826]])</span><br></pre></td></tr></table></figure>

<p>另一种着加法的写法</p>
<p>In [24]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.add(x, y)</span><br></pre></td></tr></table></figure>

<p>Out[24]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.1866,  0.0035, -0.7225],</span><br><span class="line">        [ 0.8220,  0.9750,  0.8406],</span><br><span class="line">        [ 1.2857,  0.5896,  0.6168],</span><br><span class="line">        [ 0.8559,  0.7026,  2.2498],</span><br><span class="line">        [ 0.2741, -0.4248,  0.2826]])</span><br></pre></td></tr></table></figure>

<p>加法：把输出作为一个变量</p>
<p>In [26]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">result = torch.empty(5,3)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line"># result = x + y</span><br><span class="line">result</span><br></pre></td></tr></table></figure>

<p>Out[26]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.1866,  0.0035, -0.7225],</span><br><span class="line">        [ 0.8220,  0.9750,  0.8406],</span><br><span class="line">        [ 1.2857,  0.5896,  0.6168],</span><br><span class="line">        [ 0.8559,  0.7026,  2.2498],</span><br><span class="line">        [ 0.2741, -0.4248,  0.2826]])</span><br></pre></td></tr></table></figure>

<p>in-place加法</p>
<p>In [28]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.add_(x)</span><br><span class="line">y</span><br></pre></td></tr></table></figure>

<p>Out[28]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.1866,  0.0035, -0.7225],</span><br><span class="line">        [ 0.8220,  0.9750,  0.8406],</span><br><span class="line">        [ 1.2857,  0.5896,  0.6168],</span><br><span class="line">        [ 0.8559,  0.7026,  2.2498],</span><br><span class="line">        [ 0.2741, -0.4248,  0.2826]])</span><br></pre></td></tr></table></figure>

<h4 id="注意-1"><a href="#注意-1" class="headerlink" title="注意"></a>注意</h4><p>任何in-place的运算都会以<code>_</code>结尾。 举例来说：<code>x.copy_(y)</code>, <code>x.t_()</code>, 会改变 <code>x</code>。</p>
<p>各种类似NumPy的indexing都可以在PyTorch tensor上面使用。</p>
<p>In [31]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[1:, 1:]</span><br></pre></td></tr></table></figure>

<p>Out[31]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.2653,  0.4685],</span><br><span class="line">        [-0.3606, -0.0073],</span><br><span class="line">        [ 0.6826,  1.7368],</span><br><span class="line">        [-0.6957, -0.4566]])</span><br></pre></td></tr></table></figure>

<p>Resizing: 如果你希望resize/reshape一个tensor，可以使用<code>torch.view</code>：</p>
<p>In [39]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(4,4)</span><br><span class="line">y = x.view(16)</span><br><span class="line">z = x.view(-1,8)</span><br><span class="line">z</span><br></pre></td></tr></table></figure>

<p>Out[39]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.5683,  1.3885, -2.0829, -0.7613, -1.9115,  0.3732, -0.2055, -1.2300],</span><br><span class="line">        [-0.2612, -0.4682, -1.0596,  0.7447,  0.7603, -0.4281,  0.5495,  0.1025]])</span><br></pre></td></tr></table></figure>

<p>如果你有一个只有一个元素的tensor，使用<code>.item()</code>方法可以把里面的value变成Python数值。</p>
<p>In [40]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(1)</span><br><span class="line">x</span><br></pre></td></tr></table></figure>

<p>Out[40]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([-1.1493])</span><br></pre></td></tr></table></figure>

<p>In [44]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.item()</span><br></pre></td></tr></table></figure>

<p>Out[44]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-1.1493233442306519</span><br></pre></td></tr></table></figure>

<p>In [48]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z.transpose(1,0)</span><br></pre></td></tr></table></figure>

<p>Out[48]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.5683, -0.2612],</span><br><span class="line">        [ 1.3885, -0.4682],</span><br><span class="line">        [-2.0829, -1.0596],</span><br><span class="line">        [-0.7613,  0.7447],</span><br><span class="line">        [-1.9115,  0.7603],</span><br><span class="line">        [ 0.3732, -0.4281],</span><br><span class="line">        [-0.2055,  0.5495],</span><br><span class="line">        [-1.2300,  0.1025]])</span><br></pre></td></tr></table></figure>

<p><strong>更多阅读</strong></p>
<p>各种Tensor operations, 包括transposing, indexing, slicing, mathematical operations, linear algebra, random numbers在<code>&lt;https://pytorch.org/docs/torch&gt;</code>.</p>
<h2 id="Numpy和Tensor之间的转化"><a href="#Numpy和Tensor之间的转化" class="headerlink" title="Numpy和Tensor之间的转化"></a>Numpy和Tensor之间的转化</h2><p>在Torch Tensor和NumPy array之间相互转化非常容易。</p>
<p>Torch Tensor和NumPy array会共享内存，所以改变其中一项也会改变另一项。</p>
<p>把Torch Tensor转变成NumPy Array</p>
<p>In [49]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(5)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>

<p>Out[49]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1., 1., 1., 1., 1.])</span><br></pre></td></tr></table></figure>

<p>In [50]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = a.numpy()</span><br><span class="line">b</span><br></pre></td></tr></table></figure>

<p>Out[50]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([1., 1., 1., 1., 1.], dtype=float32)</span><br></pre></td></tr></table></figure>

<p>改变numpy array里面的值。</p>
<p>In [51]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b[1] = 2</span><br><span class="line">b</span><br></pre></td></tr></table></figure>

<p>Out[51]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([1., 2., 1., 1., 1.], dtype=float32)</span><br></pre></td></tr></table></figure>

<p>In [52]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a</span><br></pre></td></tr></table></figure>

<p>Out[52]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1., 2., 1., 1., 1.])</span><br></pre></td></tr></table></figure>

<p>把NumPy ndarray转成Torch Tensor</p>
<p>In [54]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br></pre></td></tr></table></figure>

<p>In [55]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = np.ones(5)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, 1, out=a)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2. 2. 2. 2. 2.]</span><br></pre></td></tr></table></figure>

<p>In [56]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b</span><br></pre></td></tr></table></figure>

<p>Out[56]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span><br></pre></td></tr></table></figure>

<p>所有CPU上的Tensor都支持转成numpy或者从numpy转成Tensor。</p>
<h2 id="CUDA-Tensors"><a href="#CUDA-Tensors" class="headerlink" title="CUDA Tensors"></a>CUDA Tensors</h2><p>使用<code>.to</code>方法，Tensor可以被移动到别的device上。</p>
<p>In [60]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line">    y = torch.ones_like(x, device=device)</span><br><span class="line">    x = x.to(device)</span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))</span><br></pre></td></tr></table></figure>

<p>Out[60]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">False</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.to(<span class="string">"cpu"</span>).data.numpy()</span><br><span class="line">y.cpu().data.numpy()</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = model.cuda()</span><br></pre></td></tr></table></figure>

<h2 id="热身-用numpy实现两层神经网络"><a href="#热身-用numpy实现两层神经网络" class="headerlink" title="热身: 用numpy实现两层神经网络"></a>热身: 用numpy实现两层神经网络</h2><p>一个全连接ReLU神经网络，一个隐藏层，没有bias。用来从x预测y，使用L2 Loss。</p>
<ul>
<li>ℎ=𝑊1𝑋h=W1X</li>
<li>𝑎=𝑚𝑎𝑥(0,ℎ)a=max(0,h)</li>
<li>𝑦ℎ𝑎𝑡=𝑊2𝑎yhat=W2a</li>
</ul>
<p>这一实现完全使用numpy来计算前向神经网络，loss，和反向传播。</p>
<ul>
<li>forward pass</li>
<li>loss</li>
<li>backward pass</li>
</ul>
<p>numpy ndarray是一个普通的n维array。它不知道任何关于深度学习或者梯度(gradient)的知识，也不知道计算图(computation graph)，只是一种用来计算数学运算的数据结构。</p>
<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机创建一些训练数据</span></span><br><span class="line">x = np.random.randn(N, D_in)</span><br><span class="line">y = np.random.randn(N, D_out)</span><br><span class="line"></span><br><span class="line">w1 = np.random.randn(D_in, H)</span><br><span class="line">w2 = np.random.randn(H, D_out)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    h = x.dot(w1) <span class="comment"># N * H</span></span><br><span class="line">    h_relu = np.maximum(h, <span class="number">0</span>) <span class="comment"># N * H</span></span><br><span class="line">    y_pred = h_relu.dot(w2) <span class="comment"># N * D_out</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute loss</span></span><br><span class="line">    loss = np.square(y_pred - y).sum()</span><br><span class="line">    print(it, loss)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    <span class="comment"># compute the gradient</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</span><br><span class="line">    grad_h = grad_h_relu.copy()</span><br><span class="line">    grad_h[h&lt;<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.T.dot(grad_h)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update weights of w1 and w2</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure>

<h2 id="PyTorch-Tensors"><a href="#PyTorch-Tensors" class="headerlink" title="PyTorch: Tensors"></a>PyTorch: Tensors</h2><p>这次我们使用PyTorch tensors来创建前向神经网络，计算损失，以及反向传播。</p>
<p>一个PyTorch Tensor很像一个numpy的ndarray。但是它和numpy ndarray最大的区别是，PyTorch Tensor可以在CPU或者GPU上运算。如果想要在GPU上运算，就需要把Tensor换成cuda类型。</p>
<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机创建一些训练数据</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line">w1 = torch.randn(D_in, H)</span><br><span class="line">w2 = torch.randn(H, D_out)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    h = x.mm(w1) <span class="comment"># N * H</span></span><br><span class="line">    h_relu = h.clamp(min=<span class="number">0</span>) <span class="comment"># N * H</span></span><br><span class="line">    y_pred = h_relu.mm(w2) <span class="comment"># N * D_out</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum().item()</span><br><span class="line">    print(it, loss)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    <span class="comment"># compute the gradient</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.t().mm(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.mm(w2.t())</span><br><span class="line">    grad_h = grad_h_relu.clone()</span><br><span class="line">    grad_h[h&lt;<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.t().mm(grad_h)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update weights of w1 and w2</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure>

<p>简单的autograd</p>
<p>In [72]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">1.</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w = torch.tensor(<span class="number">2.</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = w*x + b <span class="comment"># y = 2*1+3</span></span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># dy / dw = x</span></span><br><span class="line">print(w.grad)</span><br><span class="line">print(x.grad)</span><br><span class="line">print(b.grad)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor(1.)</span><br><span class="line">tensor(2.)</span><br><span class="line">tensor(1.)</span><br></pre></td></tr></table></figure>

<h2 id="PyTorch-Tensor和autograd"><a href="#PyTorch-Tensor和autograd" class="headerlink" title="PyTorch: Tensor和autograd"></a>PyTorch: Tensor和autograd</h2><p>PyTorch的一个重要功能就是autograd，也就是说只要定义了forward pass(前向神经网络)，计算了loss之后，PyTorch可以自动求导计算模型所有参数的梯度。</p>
<p>一个PyTorch的Tensor表示计算图中的一个节点。如果<code>x</code>是一个Tensor并且<code>x.requires_grad=True</code>那么<code>x.grad</code>是另一个储存着<code>x</code>当前梯度(相对于一个scalar，常常是loss)的向量。</p>
<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机创建一些训练数据</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line">w1 = torch.randn(D_in, H, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn(H, D_out, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = x.mm(w1).clamp(min=<span class="number">0</span>).mm(w2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum() <span class="comment"># computation graph</span></span><br><span class="line">    print(it, loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update weights of w1 and w2</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        w1 -= learning_rate * w1.grad</span><br><span class="line">        w2 -= learning_rate * w2.grad</span><br><span class="line">        w1.grad.zero_()</span><br><span class="line">        w2.grad.zero_()</span><br></pre></td></tr></table></figure>

<h2 id="PyTorch-nn"><a href="#PyTorch-nn" class="headerlink" title="PyTorch: nn"></a>PyTorch: nn</h2><p>这次我们使用PyTorch中nn这个库来构建网络。 用PyTorch autograd来构建计算图和计算gradients， 然后PyTorch会帮我们自动计算gradient。</p>
<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机创建一些训练数据</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H, bias=<span class="literal">False</span>), <span class="comment"># w_1 * x + b_1</span></span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out, bias=<span class="literal">False</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">torch.nn.init.normal_(model[<span class="number">0</span>].weight)</span><br><span class="line">torch.nn.init.normal_(model[<span class="number">2</span>].weight)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model = model.cuda()</span></span><br><span class="line"></span><br><span class="line">loss_fn = nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(x) <span class="comment"># model.forward() </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y) <span class="comment"># computation graph</span></span><br><span class="line">    print(it, loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update weights of w1 and w2</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters(): <span class="comment"># param (tensor, grad)</span></span><br><span class="line">            param -= learning_rate * param.grad</span><br><span class="line">            </span><br><span class="line">    model.zero_grad()</span><br></pre></td></tr></table></figure>

<p>In [113]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model[0].weight</span><br></pre></td></tr></table></figure>

<p>Out[113]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[-0.0218,  0.0212,  0.0243,  ...,  0.0230,  0.0247,  0.0168],</span><br><span class="line">        [-0.0144,  0.0177, -0.0221,  ...,  0.0161,  0.0098, -0.0172],</span><br><span class="line">        [ 0.0086, -0.0122, -0.0298,  ..., -0.0236, -0.0187,  0.0295],</span><br><span class="line">        ...,</span><br><span class="line">        [ 0.0266, -0.0008, -0.0141,  ...,  0.0018,  0.0319, -0.0129],</span><br><span class="line">        [ 0.0296, -0.0005,  0.0115,  ...,  0.0141, -0.0088, -0.0106],</span><br><span class="line">        [ 0.0289, -0.0077,  0.0239,  ..., -0.0166, -0.0156, -0.0235]],</span><br><span class="line">       requires_grad=True)</span><br></pre></td></tr></table></figure>

<h2 id="PyTorch-optim"><a href="#PyTorch-optim" class="headerlink" title="PyTorch: optim"></a>PyTorch: optim</h2><p>这一次我们不再手动更新模型的weights,而是使用optim这个包来帮助我们更新参数。 optim这个package提供了各种不同的模型优化方法，包括SGD+momentum, RMSProp, Adam等等。</p>
<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机创建一些训练数据</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H, bias=<span class="literal">False</span>), <span class="comment"># w_1 * x + b_1</span></span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out, bias=<span class="literal">False</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">torch.nn.init.normal_(model[<span class="number">0</span>].weight)</span><br><span class="line">torch.nn.init.normal_(model[<span class="number">2</span>].weight)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model = model.cuda()</span></span><br><span class="line"></span><br><span class="line">loss_fn = nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line"><span class="comment"># learning_rate = 1e-4</span></span><br><span class="line"><span class="comment"># optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span></span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(x) <span class="comment"># model.forward() </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y) <span class="comment"># computation graph</span></span><br><span class="line">    print(it, loss.item())</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update model parameters</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<h2 id="PyTorch-自定义-nn-Modules"><a href="#PyTorch-自定义-nn-Modules" class="headerlink" title="PyTorch: 自定义 nn Modules"></a>PyTorch: 自定义 nn Modules</h2><p>我们可以定义一个模型，这个模型继承自nn.Module类。如果需要定义一个比Sequential模型更加复杂的模型，就需要定义nn.Module模型。</p>
<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机创建一些训练数据</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></span><br><span class="line">        super(TwoLayerNet, self).__init__()</span><br><span class="line">        <span class="comment"># define the model architecture</span></span><br><span class="line">        self.linear1 = torch.nn.Linear(D_in, H, bias=<span class="literal">False</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(H, D_out, bias=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        y_pred = self.linear2(self.linear1(x).clamp(min=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line">model = TwoLayerNet(D_in, H, D_out)</span><br><span class="line">loss_fn = nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(x) <span class="comment"># model.forward() </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y) <span class="comment"># computation graph</span></span><br><span class="line">    print(it, loss.item())</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update model parameters</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PyTorch/">PyTorch</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-朴素贝叶斯" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/06/08/朴素贝叶斯/" class="article-date">
      <time datetime="2020-06-08T10:24:41.000Z" itemprop="datePublished">2020-06-08</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/08/朴素贝叶斯/">朴素贝叶斯</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h1><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>贝叶斯方法是一个历史悠久，有着坚实的理论基础的方法，同时处理很多问题时直接而又高效，很多高级自然语言处理模型也可以从它演化而来。因此，学习贝叶斯方法，是研究自然语言处理问题的一个非常好的切入口。</p>
<h2 id="2-贝叶斯公式"><a href="#2-贝叶斯公式" class="headerlink" title="2. 贝叶斯公式"></a>2. 贝叶斯公式</h2><p>贝叶斯公式就一行：</p>
<blockquote>
<p>$$<br>P(Y|X)=P(X|Y)P(Y)/P(X)<br>$$</p>
</blockquote>
<p>而它其实是由以下的联合概率公式推导出来：<br>$$<br>P(Y,X)=P(Y|X)P(X)=P(X|Y)P(Y)<br>$$<br>其中P(Y)叫做先验概率，P(Y|X)叫做后验概率，P(Y,X)叫做联合概率。</p>
<p>没了，贝叶斯最核心的公式就这么些。</p>
<h2 id="3-用机器学习的视角理解贝叶斯公式"><a href="#3-用机器学习的视角理解贝叶斯公式" class="headerlink" title="3. 用机器学习的视角理解贝叶斯公式"></a>3. 用机器学习的视角理解贝叶斯公式</h2><p>在机器学习的视角下，我们把X理解成<strong>“具有某特征”</strong>，把Y理解成<strong>“类别标签”</strong>(一般机器学习为题中都是<code>X=&gt;特征</code>, <code>Y=&gt;结果</code>对吧)。在最简单的二分类问题(<code>是</code>与<code>否</code>判定)下，我们将Y理解成<strong>“属于某类</strong>”的标签。于是贝叶斯公式就变形成了下面的样子:</p>
<blockquote>
<p>P(“属于某类”|“具有某特征”)=P(“具有某特征”|“属于某类”)P(“属于某类”)P(“具有某特征”)</p>
</blockquote>
<p>我们简化解释一下上述公式：</p>
<blockquote>
<p>P(“属于某类”|“具有某特征”)=在已知某样本“具有某特征”的条件下，该样本“属于某类”的概率。所以叫做<strong>『后验概率』</strong>。<br>P(“具有某特征”|“属于某类”)=在已知某样本“属于某类”的条件下，该样本“具有某特征”的概率。<br>P(“属于某类”)=（在未知某样本具有该“具有某特征”的条件下，）该样本“属于某类”的概率。所以叫做<strong>『先验概率』</strong>。<br>P(“具有某特征”)=(在未知某样本“属于某类”的条件下，)该样本“具有某特征”的概率。</p>
</blockquote>
<p>而我们二分类问题的最终目的就是要<strong>判断P(“属于某类”|“具有某特征”)是否大于1/2</strong>就够了。贝叶斯方法把计算<strong>“具有某特征的条件下属于某类”</strong>的概率转换成需要计算<strong>“属于某类的条件下具有某特征”</strong>的概率，而后者获取方法就简单多了，我们只需要找到一些包含已知特征标签的样本，即可进行训练。而样本的类别标签都是明确的，所以贝叶斯方法在机器学习里属于有监督学习方法。</p>
<p>这里再补充一下，一般<strong>『先验概率』、『后验概率』是相对</strong>出现的，比如P(Y)与P(Y|X)是关于Y的先验概率与后验概率，P(X)与P(X|Y)是关于X的先验概率与后验概率。</p>
<h2 id="4-垃圾邮件识别"><a href="#4-垃圾邮件识别" class="headerlink" title="4. 垃圾邮件识别"></a>4. 垃圾邮件识别</h2><p>举个例子好啦，我们现在要对邮件进行分类，识别垃圾邮件和普通邮件，如果我们选择使用朴素贝叶斯分类器，那目标就是<strong>判断P(“垃圾邮件”|“具有某特征”)是否大于1/2</strong>。现在假设我们有垃圾邮件和正常邮件各1万封作为训练集。需要判断以下这个邮件是否属于垃圾邮件：</p>
<blockquote>
<p>“我司可办理正规发票（保真）17%增值税发票点数优惠！”</p>
</blockquote>
<p>也就是<strong>判断概率P(“垃圾邮件”|“我司可办理正规发票（保真）17%增值税发票点数优惠！”)是否大于1/2</strong>。</p>
<p>咳咳，有木有发现，转换成的这个概率，计算的方法：就是写个计数器，然后+1 +1 +1统计出所有垃圾邮件和正常邮件中出现这句话的次数啊！！！好，具体点说：</p>
<blockquote>
<p>P(“垃圾邮件”|“我司可办理正规发票（保真）17%增值税发票点数优惠！”) =垃圾邮件中出现这句话的次数垃圾邮件中出现这句话的次数+正常邮件中出现这句话的次数</p>
</blockquote>
<h2 id="5-分词"><a href="#5-分词" class="headerlink" title="5. 分词"></a>5. 分词</h2><p>一个很悲哀但是很现实的结论： <strong>训练集是有限的，而句子的可能性则是无限的。所以覆盖所有句子可能性的训练集是不存在的。</strong></p>
<p>所以解决方法是？ <strong>句子的可能性无限，但是词语就那么些！！</strong>汉语常用字2500个，常用词语也就56000个(你终于明白小学语文老师的用心良苦了)。按人们的经验理解，两句话意思相近并不强求非得每个字、词语都一样。比如<strong>“我司可办理正规发票，17%增值税发票点数优惠！”</strong>，这句话就比之前那句话少了<strong>“（保真）”</strong>这个词，但是意思基本一样。如果把这些情况也考虑进来，那样本数量就会增加，这就方便我们计算了。</p>
<p>于是，我们可以不拿句子作为特征，而是拿句子里面的词语（组合）作为特征去考虑。比如<strong>“正规发票”</strong>可以作为一个单独的词语，<strong>“增值税”</strong>也可以作为一个单独的词语等等。</p>
<blockquote>
<p>句子<strong>“我司可办理正规发票，17%增值税发票点数优惠！”就可以变成（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)）</strong>。</p>
</blockquote>
<p>于是你接触到了中文NLP中，最最最重要的技术之一：<strong>分词</strong>！！！也就是<strong>把一整句话拆分成更细粒度的词语来进行表示</strong>。另外，分词之后<strong>去除标点符号、数字甚至无关成分(停用词)是特征预处理中的一项技术</strong>。</p>
<p><strong>中文分词是一个专门的技术领域(我不会告诉你某搜索引擎厂码砖工有专门做分词的！！！)，上过之前课程的同学都知道python有一个非常方便的分词工具jieba，假定我们已经完成分词工作：</strong></p>
<p>我们观察（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)，<strong>这可以理解成一个向量：向量的每一维度都表示着该特征词在文本中的特定位置存在。这种将特征拆分成更小的单元，依据这些更灵活、更细粒度的特征进行判断的思维方式，在自然语言处理与机器学习中都是非常常见又有效的。</strong></p>
<p>因此贝叶斯公式就变成了：</p>
<blockquote>
<p>P(“垃圾邮件”|（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)） =P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|”垃圾邮件”）P(“垃圾邮件”)P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”))</p>
<p>P(“正常邮件”|（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)） =P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|”正常邮件”）P(“正常邮件”)P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”))</p>
</blockquote>
<h2 id="6-条件独立假设"><a href="#6-条件独立假设" class="headerlink" title="6. 条件独立假设"></a>6. 条件独立假设</h2><p>下面我们马上会看到一个非常简单粗暴的假设。</p>
<p>概率P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|”垃圾邮件”）依旧不够好求，我们引进一个<strong>很朴素的近似</strong>。为了让公式显得更加紧凑，我们令字母S表示“垃圾邮件”,令字母H表示“正常邮件”。近似公式如下：</p>
<blockquote>
<p>P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|S）<br>=P(“我”|S）×P(“司”|S）×P(“可”|S）×P(“办理”|S）×P(“正规发票”|S） ×P(“保真”|S）×P(“增值税”|S）×P(“发票”|S）×P(“点数”|S）×P(“优惠”|S)</p>
</blockquote>
<p>这就是传说中的<strong>条件独立假设</strong>。基于“正常邮件”的条件独立假设的式子与上式类似，此处省去。接着，将条件独立假设代入上面两个相反事件的贝叶斯公式。</p>
<p>于是我们就只需要比较以下两个式子的大小：</p>
<blockquote>
<p>C=P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S) ×P(“保真”|S)P(“增值税”|S)P(“发票”|S)P(“点数”|S)P(“优惠”|S)P(“垃圾邮件”) C⎯⎯⎯⎯=P(“我”|H)P(“司”|H)P(“可”|H)P(“办理”|H)P(“正规发票”|H) ×P(“保真”|H)P(“增值税”|H)P(“发票”|H)P(“点数”|H)P(“优惠”|H)P(“正常邮件”)</p>
</blockquote>
<p>厉(wo)害(cao)！酱紫处理后<strong>式子中的每一项都特别好求</strong>！只需要<strong>分别统计各类邮件中该关键词出现的概率</strong>就可以了！！！比如：</p>
<blockquote>
<p>P(“发票”|S）=垃圾邮件中所有“发票”的次数垃圾邮件中所有词语的次数</p>
</blockquote>
<p>统计次数非常方便，而且样本数量足够大，算出来的概率比较接近真实。于是垃圾邮件识别的问题就可解了。</p>
<h2 id="7-朴素贝叶斯-Naive-Bayes-，“Naive”在何处？"><a href="#7-朴素贝叶斯-Naive-Bayes-，“Naive”在何处？" class="headerlink" title="7. 朴素贝叶斯(Naive Bayes)，“Naive”在何处？"></a>7. 朴素贝叶斯(Naive Bayes)，“Naive”在何处？</h2><p><strong>加上条件独立假设的贝叶斯方法就是朴素贝叶斯方法（Naive Bayes）。</strong> Naive的发音是“乃一污”，意思是“朴素的”、“幼稚的”、<strong>“蠢蠢的”</strong>。咳咳，也就是说，大神们取名说该方法是一种比较萌蠢的方法，为啥？</p>
<p>将句子（“我”,“司”,“可”,“办理”,“正规发票”) 中的 （“我”,“司”）与（“正规发票”）调换一下顺序，就变成了一个新的句子（“正规发票”,“可”,“办理”, “我”, “司”)。新句子与旧句子的意思完全不同。<strong>但由于乘法交换律，朴素贝叶斯方法中算出来二者的条件概率完全一样！</strong>计算过程如下：</p>
<blockquote>
<p>P(（“我”,“司”,“可”,“办理”,“正规发票”)|S) =P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S) =P(“正规发票”|S)P(“可”|S)P(“办理”|S)P(“我”|S)P(“司”|S） =P(（“正规发票”,“可”,“办理”,“我”,“司”)|S)</p>
</blockquote>
<p><strong>也就是说，在朴素贝叶斯眼里，“我司可办理正规发票”与“正规发票可办理我司”完全相同。朴素贝叶斯失去了词语之间的顺序信息。</strong>这就相当于把所有的词汇扔进到一个袋子里随便搅和，贝叶斯都认为它们一样。因此这种情况也称作<strong>词袋子模型(bag of words)</strong>。</p>
<p><img src="blob:file:///f3e451e8-1f8f-4c4c-b15f-25793dff88ca" alt="词袋子配图"></p>
<p>词袋子模型与人们的日常经验完全不同。比如，在条件独立假设的情况下，<strong>“武松打死了老虎”与“老虎打死了武松”被它认作一个意思了。</strong>恩，朴素贝叶斯就是这么单纯和直接，对比于其他分类器，好像是显得有那么点萌蠢。</p>
<h2 id="8-简单高效，吊丝逆袭"><a href="#8-简单高效，吊丝逆袭" class="headerlink" title="8. 简单高效，吊丝逆袭"></a>8. 简单高效，吊丝逆袭</h2><p>虽然说朴素贝叶斯方法萌蠢萌蠢的，但实践证明在垃圾邮件识别的应用还<strong>令人诧异地好</strong>。Paul Graham先生自己简单做了一个朴素贝叶斯分类器，<strong>“1000封垃圾邮件能够被过滤掉995封，并且没有一个误判”。</strong>（Paul Graham《黑客与画家》）</p>
<p>那个…效果为啥好呢？</p>
<p>“有人对此提出了一个理论解释，并且建立了什么时候朴素贝叶斯的效果能够等价于非朴素贝叶斯的充要条件，这个解释的核心就是：有些独立假设在各个分类之间的分布都是均匀的所以对于似然的相对大小不产生影响；即便不是如此，也有很大的可能性<strong>各个独立假设所产生的消极影响或积极影响互相抵消，最终导致结果受到的影响不大</strong>。具体的数学公式请参考<a href="http://www.cs.unb.ca/profs/hzhang/publications/FLAIRS04ZhangH.pdf" target="_blank" rel="noopener">这篇 paper</a>。”（刘未鹏《：平凡而又神奇的贝叶斯方法》）</p>
<p>恩，这个分类器中最简单直接看似萌蠢的小盆友『朴素贝叶斯』，实际上却是<strong>简单、实用、且强大</strong>的。</p>
<h2 id="9-处理重复词语的三种方式"><a href="#9-处理重复词语的三种方式" class="headerlink" title="9. 处理重复词语的三种方式"></a>9. 处理重复词语的三种方式</h2><p>我们<strong>之前的垃圾邮件向量（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)，其中每个词都不重复。</strong>而这在现实中其实很少见。因为如果文本长度增加，或者分词方法改变，<strong>必然会有许多词重复出现</strong>，因此需要对这种情况进行进一步探讨。比如以下这段邮件：</p>
<blockquote>
<p>“代开发票。增值税发票，正规发票。” 分词后为向量： （“代开”,“发票”,“增值税”,“发票”,“正规”,“发票”）</p>
</blockquote>
<p>其中“发票”重复了三次。</p>
<h3 id="9-1-多项式模型："><a href="#9-1-多项式模型：" class="headerlink" title="9.1 多项式模型："></a>9.1 多项式模型：</h3><p>如果我们考虑重复词语的情况，也就是说，<strong>重复的词语我们视为其出现多次</strong>，直接按条件独立假设的方式推导，则有</p>
<blockquote>
<p>P(（“代开”,“发票”,“增值税”,“发票”,“正规”,“发票”)|S） =P(“代开””|S)P(“发票”|S)P(“增值税”|S)P(“发票”|S)P(“正规”|S)P(“发票”|S）=P(“代开””|S)P3(“发票”|S)P(“增值税”|S)P(“正规”|S) <strong>注意这一项</strong>:P3(“发票”|S）。</p>
</blockquote>
<p>在统计计算P(“发票”|S）时，每个被统计的垃圾邮件样本中重复的词语也统计多次。</p>
<blockquote>
<p>P(“发票”|S）=每封垃圾邮件中出现“发票”的次数的总和每封垃圾邮件中所有词出现次数（计算重复次数）的总和</p>
</blockquote>
<p>你看这个多次出现的结果，出现在概率的指数/次方上，因此这样的模型叫作<strong>多项式模型</strong>。</p>
<h3 id="9-2-伯努利模型"><a href="#9-2-伯努利模型" class="headerlink" title="9.2 伯努利模型"></a>9.2 伯努利模型</h3><p>另一种更加简化的方法是<strong>将重复的词语都视为其只出现1次</strong>，</p>
<blockquote>
<p>P(（“代开”,“发票”,“增值税”,“发票”,“正规”,“发票”)|S） =P(“发票”|S)P(“代开””|S)P(“增值税”|S)P(“正规”|S）</p>
</blockquote>
<p>统计计算P(“词语”|S）时也是如此。</p>
<blockquote>
<p>P(“发票”|S）=出现“发票”的垃圾邮件的封数每封垃圾邮件中所有词出现次数（出现了只计算一次）的总和</p>
</blockquote>
<p>这样的模型叫作<strong>伯努利模型</strong>（又称为<strong>二项独立模型</strong>）。这种方式更加简化与方便。当然它丢失了词频的信息，因此效果可能会差一些。</p>
<h3 id="9-3-混合模型"><a href="#9-3-混合模型" class="headerlink" title="9.3 混合模型"></a>9.3 混合模型</h3><p>第三种方式是在计算句子概率时，不考虑重复词语出现的次数，但是在统计计算词语的概率P(“词语”|S）时，却考虑重复词语的出现次数，这样的模型可以叫作<strong>混合模型</strong>。</p>
<p>我们通过下图展示三种模型的关系。</p>
<p><img src="blob:file:///157f8870-f4d9-4b18-9922-0c1e7a18074b" alt="三种形态"></p>
<p>具体实践中采用那种模型，关键看具体的业务场景，一个简单经验是，<strong>对于垃圾邮件识别，混合模型更好些</strong>。</p>
<h2 id="10-去除停用词与选择关键词"><a href="#10-去除停用词与选择关键词" class="headerlink" title="10. 去除停用词与选择关键词"></a>10. 去除停用词与选择关键词</h2><p>我们继续观察<strong>（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)</strong> 这句话。其实，像<strong>“我”、“可”</strong>之类词其实非常中性，无论其是否出现在垃圾邮件中都无法帮助判断的有用信息。所以可以直接不考虑这些典型的词。这些无助于我们分类的词语叫作<strong>“停用词”（Stop Words）</strong>。这样可以<strong>减少我们训练模型、判断分类的时间</strong>。 于是之前的句子就变成了<strong>（“司”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)</strong> 。</p>
<p>我们进一步分析。以人类的经验，其实<strong>“正规发票”、“发票”</strong>这类的词如果出现的话，邮件作为垃圾邮件的概率非常大，可以作为我们区分垃圾邮件的<strong>“关键词”</strong>。而像<strong>“司”、“办理”、“优惠”</strong>这类的词则有点鸡肋，可能有助于分类，但又不那么强烈。如果想省事做个简单的分类器的话，则可以直接采用“关键词”进行统计与判断，剩下的词就可以先不管了。于是之前的垃圾邮件句子就变成了<strong>（“正规发票”,“发票”)</strong> 。这样就更加减少了我们训练模型、判断分类的时间，速度非常快。</p>
<p><strong>“停用词”和“关键词”一般都可以提前靠人工经验指定</strong>。不同的“停用词”和“关键词”训练出来的分类器的效果也会有些差异。</p>
<h2 id="11-浅谈平滑技术"><a href="#11-浅谈平滑技术" class="headerlink" title="11. 浅谈平滑技术"></a>11. 浅谈平滑技术</h2><p>我们来说个问题(中文NLP里问题超级多，哭瞎T_T)，比如在计算以下独立条件假设的概率：</p>
<blockquote>
<p>P(（“我”,“司”,“可”,“办理”,“正规发票”)|S) =P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S）</p>
</blockquote>
<p>我们扫描一下训练集，发现<strong>“正规发票”这个词从出现过！！！*，于是P(“正规发票”|S）=0…问题严重了，整个概率都变成0了！！！朴素贝叶斯方法面对一堆0，很凄惨地失效了…更残酷的是</strong>这种情况其实很常见<strong>，因为哪怕训练集再大，也可能有覆盖不到的词语。本质上还是</strong>样本数量太少，不满足大数定律，计算出来的概率失真**。为了解决这样的问题，一种分析思路就是直接不考虑这样的词语，但这种方法就相当于默认给P(“正规发票”|S）赋值为1。其实效果不太好，大量的统计信息给浪费掉了。我们进一步分析，既然可以默认赋值为1，为什么不能默认赋值为一个很小的数？这就是平滑技术的基本思路，依旧保持着一贯的作风，<code>朴实/土</code>但是<code>直接而有效</code>。</p>
<p>对于伯努利模型，P(“正规发票”|S）的一种平滑算法是：</p>
<blockquote>
<p>P(“正规发票”|S）=出现“正规发票”的垃圾邮件的封数+1每封垃圾邮件中所有词出现次数（出现了只计算一次）的总和+2</p>
</blockquote>
<p>对于多项式模型，P(“正规发票”| S）的一种平滑算法是：</p>
<blockquote>
<p>P(“发票”|S）=每封垃圾邮件中出现“发票”的次数的总和+1每封垃圾邮件中所有词出现次数（计算重复次数）的总和+被统计的词表的词语数量</p>
</blockquote>
<p>说起来，平滑技术的种类其实非常多，有兴趣的话回头我们专门拉个专题讲讲好了。这里只提一点，就是所有的<strong>平滑技术都是给未出现在训练集中的词语一个估计的概率，而相应地调低其他已经出现的词语的概率</strong>。</p>
<p>平滑技术是因为数据集太小而产生的现实需求。<strong>如果数据集足够大，平滑技术对结果的影响将会变小。</strong></p>
<h2 id="12-内容小结"><a href="#12-内容小结" class="headerlink" title="12. 内容小结"></a>12. 内容小结</h2><p>我们找了个最简单常见的例子：垃圾邮件识别，说明了一下朴素贝叶斯进行文本分类的思路过程。基本思路是先区分好训练集与测试集，对文本集合进行分词、去除标点符号等特征预处理的操作，然后使用条件独立假设，将原概率转换成词概率乘积，再进行后续的处理。</p>
<blockquote>
<p>贝叶斯公式 + 条件独立假设 = 朴素贝叶斯方法</p>
</blockquote>
<p>基于对重复词语在训练阶段与判断（测试）阶段的三种不同处理方式，我们相应的有伯努利模型、多项式模型和混合模型。在训练阶段，如果样本集合太小导致某些词语并未出现，我们可以采用平滑技术对其概率给一个估计值。而且并不是所有的词语都需要统计，我们可以按相应的“停用词”和“关键词”对模型进行进一步简化，提高训练和判断速度。</p>
<h2 id="13-为什么不直接匹配关键词来识别垃圾邮件？"><a href="#13-为什么不直接匹配关键词来识别垃圾邮件？" class="headerlink" title="13. 为什么不直接匹配关键词来识别垃圾邮件？"></a>13. 为什么不直接匹配关键词来识别垃圾邮件？</h2><p>有同学可能会问：“何必费这么大劲算那么多词的概率？直接看邮件中有没有‘代开发票’、‘转售发票’之类的关键词不就得了？如果关键词比较多就认为是垃圾邮件呗。”</p>
<p>其实关键词匹配的方法如果有效的话真不必用朴素贝叶斯。毕竟这种方法简单嘛，<strong>就是一个字符串匹配</strong>。从历史来看，之前没有贝叶斯方法的时候主要也是用关键词匹配。<strong>但是这种方法准确率太低</strong>。我们在工作项目中也尝试过用关键词匹配的方法去进行文本分类，发现大量误报。感觉就像扔到垃圾箱的邮件99%都是正常的！这样的效果不忍直视。而加一个朴素贝叶斯方法就可能把误报率拉低近一个数量级，体验好得不要不要的。</p>
<p><strong>另一个原因是词语会随着时间不断变化</strong>。发垃圾邮件的人也不傻，当他们发现自己的邮件被大量屏蔽之后，也会考虑采用新的方式，<strong>如变换文字、词语、句式、颜色等方式来绕过反垃圾邮件系统</strong>。比如对于垃圾邮件“我司可办理正规发票，17%增值税发票点数优惠”,他们采用火星文：<strong>“涐司岢办理㊣規髮票，17%增値稅髮票嚸數優蕙”</strong>，那么字符串匹配的方法又要重新找出这些火星文，一个一个找出关键词，重新写一些匹配规则。更可怕的是，这些规则可能相互之间的耦合关系异常复杂，要把它们梳理清楚又是大一个数量级的工作量。等这些规则失效了又要手动更新新的规则……<strong>无穷无尽猫鼠游戏最终会把猫给累死</strong>。</p>
<p>而朴素贝叶斯方法却显示出无比的优势。因为它是<strong>基于统计方法</strong>的，只要训练样本中有更新的垃圾邮件的新词语，哪怕它们是火星文，<strong>都能自动地把哪些更敏感的词语（如“髮”、“㊣”等）给凸显出来，并根据统计意义上的敏感性给他们分配适当的权重</strong> ，这样就不需要什么人工了，非常省事。<strong>你只需要时不时地拿一些最新的样本扔到训练集中，重新训练一次即可</strong>。</p>
<p>小补充一下，对于火星文、同音字等替代语言，一般的分词技术可能会分得不准，最终可能只把一个一个字给分出来，成为“分字”。效果可能不会太好。也可以用过n-gram之类的语言模型，拿到最常见短语。当然，对于英文等天生自带空格来间隔单词的语言，分词则不是什么问题，使用朴素贝叶斯方法将会更加顺畅。</p>
<h2 id="14-实际工程的tricks"><a href="#14-实际工程的tricks" class="headerlink" title="14.实际工程的tricks"></a>14.实际工程的tricks</h2><p>应用朴素贝叶斯方法的过程中，一些tricks能显著帮助工程解决问题。我们毕竟经验有限，无法将它们全都罗列出来，只能就所知的一点点经验与大家分享，欢迎批评指正。</p>
<h3 id="14-1-trick1：取对数"><a href="#14-1-trick1：取对数" class="headerlink" title="14.1 trick1：取对数"></a>14.1 trick1：取对数</h3><p>我们提到用来识别垃圾邮件的方法是比较以下两个概率的大小（字母S表示“垃圾邮件”,字母H表示“正常邮件”）：</p>
<blockquote>
<p>C=P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S)</p>
<p>×P(“保真”|S)P(“增值税”|S)P(“发票”|S)P(“点数”|S)P(“优惠”|S)P(“垃圾邮件”)</p>
<p>C⎯⎯⎯⎯=P(“我”|H)P(“司”|H)P(“可”|H)P(“办理”|H)P(“正规发票”|H)</p>
<p>×P(“保真”|H)P(“增值税”|H)P(“发票”|H)P(“点数”|H)P(“优惠”|H)P(“正常邮件”)</p>
</blockquote>
<p>但这里进行了<strong>很多乘法运算，计算的时间开销比较大</strong>。尤其是对于篇幅比较长的邮件，几万个数相乘起来还是非常花时间的。如果能<strong>把这些乘法变成加法则方便得多</strong>。刚好数学中的对数函数log就可以实现这样的功能。两边同时取对数（本文统一取底数为2），则上面的公式变为：</p>
<blockquote>
<p>logC=logP(“我”|S)+logP(“司”|S)+logP(“可”|S)+logP(“办理”|S)+logP(“正规发票”|S)</p>
<p>+logP(“保真”|S)+logP(“增值税”|S)+logP(“发票”|S)+logP(“点数”|S)+logP(“优惠”|S)+logP(“垃圾邮件”)</p>
<p>logC⎯⎯⎯⎯=logP(“我”|H)+logP(“司”|H)+logP(“可”|H)+logP(“办理”|H)+logP(“正规发票”|H)</p>
<p>+logP(“保真”|H)+logP(“增值税”|H)+logP(“发票”|H)+logP(“点数”|H)+logP(“优惠”|H)+logP(“正常邮件”)</p>
</blockquote>
<p>有同学可能要叫了：“做对数运算岂不会也很花时间？”的确如此，但是可以在训练阶段直接计算 logP ，然后把他们存在一张大的hash表里。<strong>在判断的时候直接提取hash表中已经计算好的对数概率，然后相加即可。这样使得判断所需要的计算时间被转移到了训练阶段</strong>，实时运行的时候速度就比之前快得多，这可不止几个数量级的提升。</p>
<h3 id="14-2-trick2：转换为权重"><a href="#14-2-trick2：转换为权重" class="headerlink" title="14.2 trick2：转换为权重"></a>14.2 trick2：转换为权重</h3><p>对于二分类，我们还可以继续提高判断的速度。既然要比较logC 和logC⎯⎯⎯⎯ 的大小，那就可以直接将上下两式相减，并继续化简：</p>
<blockquote>
<p>logCC⎯⎯⎯⎯⎯=logP(“我”|S)P(“我”|H)+logP(“司”|S)P(“司”|H)+logP(“可”|S)P(“可”|H)+logP(“办理”|S)P(“办理”|H)+logP(“正规发票”|S)P(“正规发票”|H)</p>
<p>+logP(“保真”|S)P(“保真”|H)+logP(“增值税”|S)P(“增值税”|H)+logP(“发票”|S)P(“发票”|H)+logP(“点数”|S)P(“点数”|H)+logP(“优惠”|S)P(“优惠”|H)+logP(“正常邮件”|S)P(“正常邮件”)</p>
</blockquote>
<p><strong>logCC⎯⎯⎯⎯⎯ 如果大于0则属于垃圾邮件。我们可以把其中每一项作为其对应词语的权重</strong>，比如logP(“发票”|S)P(“发票”|H) 就可以作为词语“发票”的权重，权重越大就越说明“发票”更可能是与“垃圾邮件”相关的特征。<strong>这样可以根据权重的大小来评估和筛选显著的特征，比如关键词。而这些权重值可以直接提前计算好而存在hash表中</strong> 。判断的时候直接将权重求和即可。</p>
<p>关键词hash表的样子如下，左列是权重，右列是其对应的词语，权重越高的说明越“关键”：</p>
<p><img src="blob:file:///6f29d16a-1075-45cb-9c7e-206aefcf4e4a" alt="hash"></p>
<h3 id="14-3-trick3：选取topk的关键词"><a href="#14-3-trick3：选取topk的关键词" class="headerlink" title="14.3 trick3：选取topk的关键词"></a>14.3 trick3：选取topk的关键词</h3><p>前文说过可以通过提前选取关键词来提高判断的速度。有一种方法可以省略提前选取关键词的步骤，<strong>就是直接选取一段文本中权重最高的K个词语，将其权重进行加和</strong>。比如Paul Graham 在《黑客与画家》中是选取邮件中权重最高的15个词语计算的。</p>
<p>通过权重hash表可知，如果是所有词语的权重，则权重有正有负。如果只选择权重最高的K个词语，则它们的权重基本都是正的。所以就不能像之前那样判断logCC⎯⎯⎯⎯⎯ 是否大于0来区分邮件了。而这<strong>需要依靠经验选定一个正数的阈值（门槛值）</strong> ，依据logCC⎯⎯⎯⎯⎯ 与该门槛值的大小来识别垃圾邮件。</p>
<p>如下图所示，蓝色点代表垃圾邮件，绿色点代表正常邮件，横坐标为计算出来的logCC⎯⎯⎯⎯⎯ 值，中间的红线代表阈值。</p>
<p><img src="blob:file:///fa290902-0e88-4f3b-8a65-79c442c05c05" alt="权重"></p>
<h3 id="14-4-trick4：分割样本"><a href="#14-4-trick4：分割样本" class="headerlink" title="14.4 trick4：分割样本"></a>14.4 trick4：分割样本</h3><p>选取topk个词语的方法对于篇幅变动不大的邮件样本比较有效。但是对篇幅过大或者过小的邮件则会有判断误差。</p>
<p>比如这个垃圾邮件的例子：（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)。分词出了10个词语，其中有“正规发票”、“发票”2个关键词。关键词的密度还是蛮大的，应该算是敏感邮件。但因为采用最高15个词语的权重求和，并且相应的阈值是基于15个词的情况有效，可能算出来的结果还小于之前的阈值，这就造成漏判了。</p>
<p>类似的，如果一封税务主题的邮件有1000个词语，其中只有“正规发票”、“发票”、“避税方法”3个权重比较大的词语，它们只是在正文表述中顺带提到的内容。关键词的密度被较长的篇幅稀释了，应该算是正常邮件。但是却被阈值判断成敏感邮件，造成误判了。</p>
<p><strong>这两种情况都说明topk关键词的方法需要考虑篇幅的影响</strong>。这里有许多种处理方式，<strong>它们的基本思想都是选取词语的个数及对应的阈值要与篇幅的大小成正比</strong>，本文只介绍其中一种方方法：</p>
<ul>
<li>对于长篇幅邮件，按一定的大小，比如每500字，将其分割成小的文本段落，再对小文本段落采用topk关键词的方法。只要其中有一个小文本段落超过阈值就判断整封邮件是垃圾邮件。</li>
<li>对于超短篇幅邮件，比如50字，可以按篇幅与标准比较篇幅的比例来选取topk，以确定应该匹配关键词语的个数。比如选取 50500×15≈2 个词语进行匹配，相应的阈值可以是之前阈值的 215 。以此来判断则更合理。</li>
</ul>
<h3 id="14-5-trick5：位置权重"><a href="#14-5-trick5：位置权重" class="headerlink" title="14.5 trick5：位置权重"></a>14.5 trick5：位置权重</h3><p>到目前为止，我们对词语权重求和的过程都没有考虑邮件篇章结构的因素。比如“正规发票”如果出现在标题中应该比它出现在正文中对判断整个邮件的影响更大；而出现在段首句中又比其出现在段落正文中对判断整个邮件的影响更大。<strong>所以可以根据词语出现的位置，对其权重再乘以一个放大系数，以扩大其对整封邮件的影响，提高识别准确度</strong>。</p>
<p>比如一封邮件其标题是“正规发票”（假设标题的放大系数为2），段首句是“发票”,“点数”,“优惠”（假设段首的放大系数为1.5），剩下的句子是（“我”,“司”,“可”,“办理”,“保真”）。则计算logCC⎯⎯⎯⎯⎯ 时的公式就可以调整为：</p>
<blockquote>
<p>logCC⎯⎯⎯⎯⎯=2×logP(“正规发票”|S)P(“正规发票”|H)+1.5×logP(“发票”|S)P(“发票”|H)+1.5×logP(“点数”|S)P(“点数”|H)+1.5×logP(“优惠”|S)P(“优惠”|H)</p>
<p>+logP(“我”|S)P(“我”|H)+logP(“司”|S)P(“司”|H)+logP(“可”|S)P(“可”|H)+logP(“办理”|S)P(“办理”|H)+logP(“保真”|S)P(“保真”|H)+logP(“正常邮件”|S)P(“正常邮件”)</p>
</blockquote>
<h3 id="14-6-trick6：蜜罐"><a href="#14-6-trick6：蜜罐" class="headerlink" title="14.6 trick6：蜜罐"></a>14.6 trick6：蜜罐</h3><p>我们通过辛辛苦苦的统计与计算，好不容易得到了不同词语的权重。然而这并不是一劳永逸的。我们我们之前交代过，<strong>词语及其权重会随着时间不断变化，需要时不时地用最新的样本来训练以更新词语及其权重</strong>。</p>
<p>而搜集最新垃圾邮件有一个技巧，就是随便注册一些邮箱，然后将它们公布在各大论坛上。接下来就坐等一个月，到时候收到的邮件就绝大部分都是垃圾邮件了（好奸诈）。再找一些正常的邮件，基本就能够训练了。这些用于自动搜集垃圾邮件的邮箱叫做“蜜罐”。<strong>“蜜罐”是网络安全领域常用的手段，因其原理类似诱捕昆虫的装有蜜的罐子而得名</strong>。比如杀毒软件公司会利用蜜罐来监视或获得计算机网络中的病毒样本、攻击行为等。</p>
<h2 id="15-贝叶斯方法的思维方式"><a href="#15-贝叶斯方法的思维方式" class="headerlink" title="15. 贝叶斯方法的思维方式"></a>15. 贝叶斯方法的思维方式</h2><p>讲了这么多tricks，但这些手段都是建立在贝叶斯方法基础之上的。因此有必要探讨一下贝叶斯方法的思维方式，以便更好地应用这种方法解决实际问题。</p>
<h3 id="15-1-逆概问题"><a href="#15-1-逆概问题" class="headerlink" title="15.1 逆概问题"></a>15.1 逆概问题</h3><p>我们重新看一眼贝叶斯公式：</p>
<blockquote>
<p>P(Y|X)=P(X|Y)P(Y)P(X)</p>
</blockquote>
<p>先不考虑先验概率P(Y)与P(X)，观察两个后验概率P(Y|X)与P(X|Y)，可见贝叶斯公式能够揭示<strong>两个相反方向的条件概率之间的转换关系</strong>。</p>
<p>从贝叶斯公式的发现历史来看，其就是为了处理所谓“逆概”问题而诞生的。比如P(Y|X) 不能通过直接观测来得到结果，而P(X|Y) 却容易通过直接观测得到结果，就可以通过贝叶斯公式<strong>从间接地观测对象去推断不可直接观测的对象的情况</strong>。</p>
<p>好吧，我们说人话。基于邮件的文本内容判断其属于垃圾邮件的概率不好求（不可通过直接观测、统计得到），但是基于已经搜集好的垃圾邮件样本，去统计（直接观测）其文本内部各个词语的概率却非常方便。这就可以用贝叶斯方法。</p>
<p>引申一步，基于样本特征去判断其所属标签的概率不好求，但是基于已经搜集好的打上标签的样本（有监督），却可以直接统计属于同一标签的样本内部各个特征的概率分布。因此贝叶斯方法的理论视角适用于一切分类问题的求解。</p>
<h3 id="15-2-处理多分类问题"><a href="#15-2-处理多分类问题" class="headerlink" title="15.2 处理多分类问题"></a>15.2 处理多分类问题</h3><p>前面我们一直在探讨二分类（判断题）问题，现在可以引申到多分类（单选题）问题了。</p>
<p>还是用邮件分类的例子，这是现在不只要判断垃圾邮件，还要将正常邮件细分为私人邮件、工作邮件。现在有这3类邮件各1万封作为样本。需要训练出一个贝叶斯分类器。这里依次用Y1,Y2,Y3表示这三类邮件，用X表示被判断的邮件。套用贝叶斯公式有：</p>
<blockquote>
<p>P(Y1|X)=P(X|Y1)P(Y1)P(X)</p>
<p>P(Y2|X)=P(X|Y2)P(Y2)P(X)</p>
<p>P(Y3|X)=P(X|Y3)P(Y3)P(X)</p>
</blockquote>
<p>通过比较3个概率值的大小即可得到X所属的分类。发现三个式子的分母P(X) 一样，比较大小时可以忽略不计，于是就可以用下面这一个式子表达上面3式：</p>
<blockquote>
<p>P(Yi|X)∝P(X|Yi)P(Yi)；i=1,2,3</p>
</blockquote>
<p>其中 ∝ 表示“正比于”。而P(X|Yi) 则有个特别高逼格的名字叫做“<strong>似然函数</strong>”。我们上大学的时候也被这个名字搞得晕晕乎乎的，其实它也是个概率，直接理解成<strong>“P(Yi|X) 的逆反条件概率”</strong> 就方便了。</p>
<p>这里只是以垃圾邮件3分类问题举了个例子，<strong>对于任意多分类的问题都可以用这样的思路去理解。比如新闻分类、情感喜怒哀乐分类等等</strong>。</p>
<h3 id="15-3-先验概率的问题"><a href="#15-3-先验概率的问题" class="headerlink" title="15.3 先验概率的问题"></a>15.3 先验概率的问题</h3><p>在垃圾邮件的例子中，先验概率都相等，P(Y1)=P(Y2)=P(Y3)=10000/30000=1/3，所以上面是式子又可以进一步化简：</p>
<blockquote>
<p>P(Yi|X)∝P(X|Yi)；i=1,2,3</p>
</blockquote>
<p>只需比较右边式子（也就是“似然函数”）的大小就可以了。这种方法就是传说中的<strong>最大似然法</strong>:不考虑先验概率而直接比较似然函数。</p>
<p>关于选出最佳分类Yi是否要考虑先验概率P(Yi)的问题，曾经在频率学派和贝叶斯学派之间产生了激烈的教派冲突。统计学家（频率学派）说：我们让数据自己说话。言下之意就是要摒弃先验概率。而贝叶斯学派支持者则说：数据会有各种各样的偏差，而一个<strong>靠谱的先验概率</strong>则可以对这些随机噪音做到健壮。对此有兴趣的同学可以找更多资料进行了解，本文在此不做更多的引申，只基于垃圾邮件识别的例子进行探讨。</p>
<p>比如我们在采集垃圾邮件样本的时候，不小心delete掉了一半的数据，就剩下5000封邮件。则计算出来的先验概率为:</p>
<blockquote>
<p>P(Y1)=5000/25000=1/5，</p>
<p>P(Y2)=P(Y3)=10000/25000=2/5</p>
</blockquote>
<p>如果还用贝叶斯方法,就要在似然函数后面乘上先验概率。比如之前用最大似然法算出Y1 垃圾邮件的概率大，但是因为P(Y1)特别小，用贝叶斯方法得出的结果是Y2 私人邮件的概率大。那相信哪个呢？其实，我们删掉了部分带标签的样本，从计算结果看P(Y1)，P(Y2)，P(Y3)的概率分布变化了，但实际上<strong>这三个类别的真实分布应该是一个客观的状态，不应该因为我们的计算方法而发生变化</strong>。所以是我们计算出来的先验概率失真，应该放弃这样计算出来的先验概率，而用最大似然法。但即便我们不删掉一半垃圾邮件，这三类邮件的分布就真的是1:1:1那样平均吗？那也未必。<strong>我们只是按1:1:1这样的方式进行了抽样而已，真正在邮箱里收到的这三类邮件的分布可能并不是这样</strong>。也就是说，<strong>在我们对于先验概率一无所知时，只能假设每种猜测的先验概率是均等的（其实这也是人类经验的结果），这个时候就只有用最大似然了</strong>。在现实运用过程中如果发现最大似然法有偏差，可以考虑对不同的似然函数设定一些系数或者阈值，使其接近真实情况。</p>
<p>但是，<strong>如果我们有足够的自信，训练集中这三类的样本分布的确很接近真实的情况，这时就应该用贝叶斯方法</strong>。难怪前面的贝叶斯学派强调的是“靠谱的先验概率”。所以说<strong>贝叶斯学派的适用范围更广，关键要先验概率靠谱，而频率学派有效的前提也是他们的先验概率同样是经验统计的结果</strong>。</p>
<h2 id="16-朴素-贝叶斯方法的常见应用"><a href="#16-朴素-贝叶斯方法的常见应用" class="headerlink" title="16. (朴素)贝叶斯方法的常见应用"></a>16. (朴素)贝叶斯方法的常见应用</h2><p>说了这么多理论的问题，咱们就可以探讨一下(朴素)贝叶斯方法在自然语言处理中的一些常见应用了。以下只是从原理上进行探讨，对于具体的技术细节顾及不多。</p>
<h3 id="16-1-褒贬分析"><a href="#16-1-褒贬分析" class="headerlink" title="16.1 褒贬分析"></a>16.1 褒贬分析</h3><p>一个比较常见的应用场景是情感褒贬分析。比如你要统计微博上人们对一个新上映电影的褒贬程度评价：好片还是烂片。但是一条一条地看微博是根本看不过来，只能用自动化的方法。我们可以有一个很粗略的思路：</p>
<ul>
<li>首先是用爬虫将微博上提到这个电影名字的微博全都抓取下来，比如有10万条。</li>
<li>然后用训练好的朴素贝叶斯分类器分别判断这些微博对电影是好评还是差评。</li>
<li>最后统计出这些好评的影评占所有样本中的比例，就能形成微博网友对这个电影综合评价的大致估计。</li>
</ul>
<p>接下来的核心问题就是训练出一个靠谱的分类器。首先需要有打好标签的文本。这个好找，豆瓣影评上就有大量网友对之前电影的评价，并且对电影进行1星到5星的评价。我们可以认为3星以上的评论都是好评，3星以下的评论都是差评。这样就分别得到了好评差评两类的语料样本。剩下就可以用朴素贝叶斯方法进行训练了。基本思路如下：</p>
<ul>
<li>训练与测试样本：豆瓣影评的网友评论，用爬虫抓取下100万条。</li>
<li>标签：3星以上的是好评，3星以下的是差评。</li>
<li>特征：豆瓣评论分词后的词语。一个简单的方法是只选择其中的形容词，网上有大量的情绪词库可以为我们所用。</li>
<li>然后再用常规的朴素贝叶斯方法进行训练。</li>
</ul>
<p>但是由于自然语言的特点，在提取特征的过程当中，有一些tricks需要注意：</p>
<ul>
<li><strong>对否定句进行特别的处理</strong>。比如这句话“我不是很喜欢部电影，因为它让我开心不起来。”其中两个形容词“喜欢”、“开心”都是褒义词，但是因为句子的否定句，所以整体是贬义的。有一种比较简单粗暴的处理方式，就是<strong>“对否定词（“不”、“非”、“没”等）与句尾标点之间的所有形容词都采用其否定形式”</strong> 。则这句话中提取出来的形容词就应该是“不喜欢”和“不开心”。</li>
<li>一般说来，最相关的情感词在一些文本片段中仅仅出现一次，词频模型起得作用有限，甚至是负作用，<strong>则使用伯努利模型代替多项式模型</strong>。这种情况在微博这样的小篇幅文本中似乎不太明显，但是在博客、空间、论坛之类允许长篇幅文本出现的平台中需要注意。</li>
<li>其实，副词对情感的评价有一定影响。“不很喜欢”与“很不喜欢”的程度就有很大差异。但如果是朴素贝叶斯方法的话比较难处理这样的情况。我们可以考虑用语言模型或者加入词性标注的信息进行综合判断。这些内容我们将在之后的文章进行探讨。</li>
</ul>
<p>当然经过以上的处理，情感分析还是会有一部分误判。这里涉及到许多问题，都是情感分析的难点：</p>
<ul>
<li><strong>情绪表达的含蓄微妙</strong>：“导演你出来，我保证不打死你。”你让机器怎么判断是褒还是贬？</li>
<li><strong>转折性表达</strong>：“我非常喜欢这些大牌演员，非常崇拜这个导演，非常赞赏这个剧本，非常欣赏他们的预告片，我甚至为了这部影片整整期待了一年，最后进了电影院发现这是个噩梦。” 五个褒义的形容词、副词对一个不那么贬义的词。机器自然判断成褒义，但这句话是妥妥的贬义。</li>
</ul>
<h3 id="16-2-拼写纠错"><a href="#16-2-拼写纠错" class="headerlink" title="16.2 拼写纠错"></a>16.2 拼写纠错</h3><p>拼写纠错本质上也是一个分类问题。但按照错误类型不同，又分为两种情况：</p>
<ul>
<li>非词错误（Non-word Errors）：指那些拼写错误后的词本身就不合法，如将“wifi”写成“wify”；</li>
<li>真词错误（Real-word Errors）：指那些拼写错误后的词仍然是合法的情况，如将“wifi”写成“wife”。</li>
</ul>
<p>真词错误复杂一些，我们将在接下来的文章中进行探讨。而对于非词错误，就可以直接采用贝叶斯方法，其基本思路如下：</p>
<ul>
<li>标签：通过计算错误词语的最小编辑距离（之前咱们提到过的），获取最相似的候选词，每个候选词作为一个分类。</li>
<li>特征：拼写错误的词本身。因为它就一个特征，所以没有什么条件独立性假设、朴素贝叶斯啥的。它就是纯而又纯的贝叶斯方法。</li>
<li>判别公式:</li>
</ul>
<blockquote>
<p>P(候选词i|错误词)∝P(错误词|候选词i)P(候选词i)；i=1,2,3,…</p>
</blockquote>
<ul>
<li>训练样本1：该场景下的正常用词语料库，用于计算P(候选词i)。</li>
</ul>
<blockquote>
<p>P(候选词i)=候选词出现的次数所有词出现的次数</p>
</blockquote>
<ul>
<li>训练样本2：该场景下错误词与正确词对应关系的语料库，用于计算P(错误词|候选词i)</li>
</ul>
<blockquote>
<p>P(错误词|候选词i)=候选词被拼写成该“错误词”的次数候选词出现的次数</p>
</blockquote>
<p>由于自然语言的特点，有一些tricks需要注意：</p>
<ul>
<li>据统计，80%的拼写错误编辑距离为1，几乎所有的拼写错误编辑距离小于等于2。我们<strong>只选择编辑距离为1或2的词作为候选词，这样就可以减少大量不必要的计算</strong>。</li>
<li>由于我们只选择编辑距离为1或2的词，其差别只是一两个字母级别差别。因此计算似然函数的时候，<strong>可以只统计字母层面的编辑错误，这样搜集的样本更多，更满足大数定律，也更简单</strong>。对于编辑距离为1的似然函数计算公式可以进化为：</li>
</ul>
<blockquote>
<p>P(错误词|候选词i)=⎧⎩⎨⎪⎪⎪⎪⎪⎪字母“xy”被拼写成“y”的次数字母“xy”出现的次数,字母“x”被拼写成“xy”的次数字母“x”出现的次数,字母“x”被拼写成“y”的次数字母“x”出现的次数,字母“xy”被拼写成“yx的次数字母“xy”出现的次数,</p>
</blockquote>
<ul>
<li><strong>键盘上临近的按键更容易拼写错误，据此可以对上面这个条件概率进行加权</strong>。</li>
</ul>
<p><img src="blob:file:///3ecb40bd-0f8b-4742-ac50-e0912d5c6cc0" alt="\[键盘\]"></p>
<h2 id="17-内容小结"><a href="#17-内容小结" class="headerlink" title="17. 内容小结"></a>17. 内容小结</h2><p>从前面大家基本可以看出，工程应用不同于学术理论，有许多tricks需要考虑，而理论本质就是翻来倒去折腾贝叶斯公式，都快玩出花来了。</p>
<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/朴素贝叶斯/">朴素贝叶斯</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-GPT模型" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/05/30/GPT模型/" class="article-date">
      <time datetime="2020-05-30T00:30:42.000Z" itemprop="datePublished">2020-05-30</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/30/GPT模型/">GPT模型</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>文本分类：</p>
<p>数据集</p>
<p>THUCNews 数据集子集,链接: <a href="https://pan.baidu.com/s/1hugrfRu" target="_blank" rel="noopener">https://pan.baidu.com/s/1hugrfRu</a> 密码: qfud</p>
<p>Language Understanding</p>
<ul>
<li><p>intent classification</p>
</li>
<li><p>dialogue state tracking</p>
</li>
<li><p>sentiment classification</p>
</li>
</ul>
<p>Language Generation</p>
<ul>
<li>information, structured, sentiment –&gt; language</li>
</ul>
<h1 id="必读论文"><a href="#必读论文" class="headerlink" title="必读论文"></a>必读论文</h1><h1 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h1><p>Radford et. al., Improving Language Understanding by Generative Pre-Training</p>
<p><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</a></p>
<p>这篇文章推出了generative pre-training + discriminative fine-tuning的方法，后来也被BERT沿用。task-aware input transformation也是BERT借用的一个点。当年这篇文章刚出来的时候刷榜一波，不过离BERT太近，导致后来大家都不怎么关心这篇文章了。</p>
<p><s>  a b c d e f </s></p>
<p>|        |  |  |  |  |  |</p>
<p>a       b c d e f  </p>
<p>1 0 0 0 0 0 0</p>
<p>1 1 0 0 0 0 0</p>
<p>1 1 1 0 0 0 0</p>
<p>1 1 1 1 0 0 0</p>
<p>1 1 1 1 1 0 0</p>
<p>1 1 1 1 1 1 0</p>
<h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><p>语言模型objective</p>
<p><img src="https://uploader.shimo.im/f/jUXNKYwxjuUim5hM.png!thumbnail" alt="img"></p>
<p>Transformer Decoder</p>
<p><img src="https://uploader.shimo.im/f/2VVdkCN84SM8h2NA.png!thumbnail" alt="img"></p>
<p>训练使用BooksCorpus数据集，7000本书。</p>
<p>模型参数：</p>
<ul>
<li><p>12层transformer decoder</p>
</li>
<li><p>768 hidden states, 12 attention heads</p>
</li>
<li><p>FFN层有3072维度inner states</p>
</li>
</ul>
<h2 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine tuning"></a>Fine tuning</h2><p>使用最后一层最后一个token的representation来做task specific的模型fine tuning</p>
<p><img src="https://uploader.shimo.im/f/sMzH5VF5BcQsk8pY.png!thumbnail" alt="img"></p>
<p>依然使用Log loss</p>
<p><img src="https://uploader.shimo.im/f/odJ6a19y3swDAYcI.png!thumbnail" alt="img"></p>
<p>作者发现在fine tuning的时候继续使用语言模型的loss也有好处</p>
<p><img src="https://uploader.shimo.im/f/z2GX6ss3yL8CrvaN.png!thumbnail" alt="img"></p>
<h2 id="Task-specific-Input-Transformations"><a href="#Task-specific-Input-Transformations" class="headerlink" title="Task-specific Input Transformations"></a>Task-specific Input Transformations</h2><p>四种问题有四种不同的文本表示方法</p>
<p><img src="https://uploader.shimo.im/f/e3Ep9QOmlzI2YmM8.png!thumbnail" alt="img"></p>
<p>Natural Language Inference</p>
<ul>
<li><p>判断两句话的关系，entailment 承接关系，contradiction 矛盾关系，neutral 中立关系</p>
</li>
<li><p>在几个NLI任务上都有不小的提升</p>
</li>
</ul>
<p>Question Answering and Common Sense Reasoning</p>
<p>Semantic Similarity 语义相似度</p>
<ul>
<li><p>Microsoft Paraphrase Corpus</p>
</li>
<li><p>Quora Question Pairs</p>
</li>
</ul>
<p>分类问题 </p>
<ul>
<li><p>Corpus of Lingustic Acceptability，判断一句话的语法是不是正确。</p>
</li>
<li><p>Stanford Sentiment Treebank 情感分类</p>
</li>
</ul>
<h1 id="GPT2"><a href="#GPT2" class="headerlink" title="GPT2"></a>GPT2</h1><p>Radford et. al., <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener">Language Models are Unsupervised Multitask Learners</a></p>
<p><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener">https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</a></p>
<p>比GPT更大的训练数据集</p>
<ul>
<li>Common Crawl来自网页爬取，删除了Wikipedia，总共40GB数据。</li>
</ul>
<p>evaluation任务</p>
<ul>
<li><p>The Winograd Schema Challenge</p>
</li>
<li><p>LAMBADA dataset <a href="https://arxiv.org/pdf/1606.06031.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1606.06031.pdf</a></p>
</li>
</ul>
<p>关于文本生成</p>
<p><a href="https://arxiv.org/pdf/1904.09751.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.09751.pdf</a></p>
<h2 id="代码解读"><a href="#代码解读" class="headerlink" title="代码解读"></a>代码解读</h2><p>我对代码添加了一些注释</p>
<p><a href="https://github.com/ZeweiChu/gpt-2/blob/master/src/model.py" target="_blank" rel="noopener">https://github.com/ZeweiChu/gpt-2/blob/master/src/model.py</a></p>
<p>huggingface代码</p>
<p><a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_gpt2.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_gpt2.py</a></p>
<p>自动检测</p>
<p>def attention_mask(nd, ns, *, dtype):</p>
<p>​    “””1’s in the lower triangle, counting from the lower right corner.</p>
<p>​    左下角的三角形都是1，其余是0，用于生成mask。</p>
<p>​    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn’t produce garbage on TPUs.</p>
<p>​    “””</p>
<p>​    i = tf.range(nd)[:,None]</p>
<p>​    j = tf.range(ns)</p>
<p>​    m = i &gt;= j - ns + nd</p>
<p>​    return tf.cast(m, dtype)</p>
<p>0 0 0 0 0</p>
<p>1 1 1 1 1</p>
<p>2 2 2 2 2</p>
<p>3 3 3 3 3</p>
<p>4 4 4 4 4</p>
<p>0 1 2 3 4 </p>
<p>1 0 0 0 0</p>
<p>1 1 0 0 0</p>
<p>1 1 1 0 0</p>
<p>1 1 1 1 0</p>
<p>1 1 1 1 1</p>
<p>w00 w01-inf w02-inf w03-inf w04-inf</p>
<p>w10 w11 w12-inf w13-inf w14-inf</p>
<p>w20 w21 w22 w23-inf w24-inf</p>
<p>w30 w31 w32 w33 w34-inf</p>
<p>w40 w41 w42 w43 w44</p>
<p>阅读GPT2代码：<a href="https://github.com/ZeweiChu/gpt-2/blob/master/src/model.py" target="_blank" rel="noopener">https://github.com/ZeweiChu/gpt-2/blob/master/src/model.py</a></p>
<h1 id="Google-T5-Transformer预训练模型大总结"><a href="#Google-T5-Transformer预训练模型大总结" class="headerlink" title="Google T5: Transformer预训练模型大总结"></a>Google T5: Transformer预训练模型大总结</h1><p>论文地址：<a href="https://arxiv.org/pdf/1910.10683.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1910.10683.pdf</a></p>
<p>代码+预训练模型：<a href="https://github.com/google-research/text-to-text-transfer-transformer" target="_blank" rel="noopener">https://github.com/google-research/text-to-text-transfer-transformer</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GPT/">GPT</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-BERT系列预训练模型" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/05/30/BERT系列预训练模型/" class="article-date">
      <time datetime="2020-05-30T00:29:27.000Z" itemprop="datePublished">2020-05-30</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/30/BERT系列预训练模型/">BERT系列预训练模型</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>BERT：Masked Language Modeling预训练模型</p>
<p>论文地址：<a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1810.04805.pdf</a></p>
<p>中文翻译：<a href="https://zhuanlan.zhihu.com/p/59775981" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/59775981</a></p>
<p> Language modeling预训练任务</p>
<h2 id="Masked-Language-Model"><a href="#Masked-Language-Model" class="headerlink" title="Masked Language Model"></a>Masked Language Model</h2><p>完形填空</p>
<p>I study at Julyedu . </p>
<p>80% I study at [MASK] . </p>
<p>10% I study at Apple . </p>
<p>10% I study at Julyedu . </p>
<p>[CLS] I study at [MASK] .  [SEP] I love [MASK] language processing . [SEP]</p>
<p>–&gt; transformer encoder</p>
<p>o1, o2, o3, o4, o5, …., o_n</p>
<p>o5 –&gt; Julyedu  cross entropy</p>
<p>o10 –&gt; natural cross entropy</p>
<p>o1 –&gt; True cross entropy</p>
<p>BERT说：“我要用 transformer 的 encoders”</p>
<p>Ernie不屑道：“呵呵，你不能像Bi-Lstm一样考虑文章”</p>
<p>BERT自信回答道：“我们会用masks”</p>
<blockquote>
<p>解释一下Mask：</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>语言模型会根据前面单词来预测下一个单词，但是self-attention的注意力只会放在自己身上，那么这样100%预测到自己，毫无意义，所以用Mask，把需要预测的词给挡住。</p>
</blockquote>
<p>如下图：</p>
<p><img src="https://uploader.shimo.im/f/jvcJ8SPeBEwszR8M.png!thumbnail" alt="img"></p>
<h2 id="Two-sentence-Tasks"><a href="#Two-sentence-Tasks" class="headerlink" title="Two-sentence Tasks"></a>Two-sentence Tasks</h2><p>我们回顾一下OpenAI transformer处理不同任务的输入转换，你会发现在某些任务上我们需要2个句子作为输入，并做一些更为智能的判断，比如是否相似，比如 给出一个维基百科的内容作为输入，同时在放入一条针对该条目的问题，那么我们的算法模型能够处理这个问题吗？</p>
<p>为了使BERT更好的处理2个句子之间的关系，预训练的过程还有一个额外的任务：给定2个句子（A和B）,A与B是否相似？（0或者1）</p>
<h2 id="特殊NLP任务"><a href="#特殊NLP任务" class="headerlink" title="特殊NLP任务"></a>特殊NLP任务</h2><p>BERT的论文为我们介绍了几种BERT可以处理的NLP任务：</p>
<ol>
<li><p>短文本相似 </p>
</li>
<li><p>文本分类</p>
</li>
<li><p>QA机器人</p>
</li>
<li><p>语义标注</p>
</li>
</ol>
<p><img src="https://uploader.shimo.im/f/yKFxOevBvMQXvjnv.png!thumbnail" alt="img"></p>
<h2 id="BERT用做特征提取"><a href="#BERT用做特征提取" class="headerlink" title="BERT用做特征提取"></a>BERT用做特征提取</h2><p>微调方法并不是使用BERT的唯一方法，就像ELMo一样，你可以使用预选训练好的BERT来创建语境化词嵌入。然后你可以将这些嵌入提供给现有的模型。</p>
<p><img src="https://uploader.shimo.im/f/uKUkG73gELQGry4L.png!thumbnail" alt="img"></p>
<p>哪个向量最适合作为上下文嵌入？ 我认为这取决于任务。 本文考察了六种选择（与微调模型相比，得分为96.4）：</p>
<p><img src="https://uploader.shimo.im/f/bfpUyWE9YCEP9IU2.png!thumbnail" alt="img"></p>
<ul>
<li><p>Feature Extraction：特征提取</p>
</li>
<li><p>Finetune：微调</p>
</li>
</ul>
<h1 id="如何使用BERT"><a href="#如何使用BERT" class="headerlink" title="如何使用BERT"></a>如何使用BERT</h1><h2 id="BERT源码"><a href="#BERT源码" class="headerlink" title="BERT源码"></a>BERT源码</h2><p>查看<a href="https://github.com/google-research/bert" target="_blank" rel="noopener">BERT仓库</a>中的代码：</p>
<ol>
<li><p>该模型在modeling.py（BertModel类）中构建，与vanilla Transformer编码器完全相同。</p>
</li>
<li><p>run_classifier.py是微调过程的一个示例。它还构建了监督模型的分类层。如果要构建自己的分类器，请查看该文件中的create_model()方法。</p>
</li>
<li><p>可以下载几种预先训练的模型。涵盖102种语言的多语言模型，这些语言都是在维基百科的数据基础上训练而成的。</p>
</li>
<li><p>BERT不会将单词视为tokens。相反，它注重WordPieces。 tokenization.py是将你的单词转换为适合BERT的wordPieces的tokensizer。</p>
</li>
</ol>
<p>可以查看BERT的PyTorch实现 (<a href="https://github.com/huggingface/transformers)。" target="_blank" rel="noopener">https://github.com/huggingface/transformers)。</a> </p>
<ul>
<li><p>modeling: <a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py</a></p>
</li>
<li><p>BertEmbedding: wordpiece embedding + position embedding + token type embedding</p>
</li>
<li><p>BertSelfAttnetion: query, key, value的变换</p>
</li>
<li><p>BertSelfOutput: </p>
</li>
<li><p>BertIntermediate</p>
</li>
<li><p>BertOutput</p>
</li>
<li><p>BertForSequenceClassification</p>
</li>
<li><p>configuration: <a href="https://github.com/huggingface/transformers/blob/master/transformers/configuration_bert.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/transformers/configuration_bert.py</a></p>
</li>
<li><p>tokenization: <a href="https://github.com/huggingface/transformers/blob/master/transformers/tokenization_bert.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/transformers/tokenization_bert.py</a></p>
</li>
<li><p>DataProcessor: <a href="https://github.com/huggingface/transformers/blob/master/transformers/data/processors/glue.py#L194" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/transformers/data/processors/glue.py#L194</a></p>
</li>
</ul>
<h2 id="BERT模型的使用"><a href="#BERT模型的使用" class="headerlink" title="BERT模型的使用"></a>BERT模型的使用</h2><ul>
<li>文本分类：<a href="https://github.com/huggingface/transformers/blob/master/examples/run_glue.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/examples/run_glue.py</a></li>
</ul>
<h1 id="BERT升级版"><a href="#BERT升级版" class="headerlink" title="BERT升级版"></a>BERT升级版</h1><h2 id="RoBERTa：更强大的BERT"><a href="#RoBERTa：更强大的BERT" class="headerlink" title="RoBERTa：更强大的BERT"></a>RoBERTa：更强大的BERT</h2><p>论文地址：<a href="https://arxiv.org/pdf/1907.11692.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1907.11692.pdf</a></p>
<ul>
<li><p>加大训练数据 16GB -&gt; 160GB，更大的batch size，训练时间加长</p>
</li>
<li><p>不需要NSP Loss: natural inference </p>
</li>
<li><p>使用更长的训练 Sequence</p>
</li>
<li><p>Static vs. Dynamic Masking </p>
</li>
<li><p>模型训练成本在6万美金以上（估算）</p>
</li>
</ul>
<h2 id="ALBERT：参数更少的BERT"><a href="#ALBERT：参数更少的BERT" class="headerlink" title="ALBERT：参数更少的BERT"></a>ALBERT：参数更少的BERT</h2><p>论文地址：<a href="https://arxiv.org/pdf/1909.11942.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1909.11942.pdf</a></p>
<ul>
<li><p>一个轻量级的BERT模型</p>
</li>
<li><p>核心思想：</p>
</li>
<li><p>共享层与层之间的参数 （减少模型参数）</p>
</li>
<li><p>增加单层向量维度</p>
</li>
</ul>
<h2 id="DistilBERT：轻量版BERT"><a href="#DistilBERT：轻量版BERT" class="headerlink" title="DistilBERT：轻量版BERT"></a>DistilBERT：轻量版BERT</h2><p>MNIST</p>
<p>0, 1, 2, 3, …, 9</p>
<p>logits: [0.1, 0.6, …, 0.01] q</p>
<p><strong>label: 2 [0, 0, 1, …, 0] p</strong></p>
<p>loss: cross entropy loss -\sum_{i=1}^10 p_i*log q_i</p>
<p>loss: -log q_{label}</p>
<p>训练一个Student network，mimic the behavior of the teacher network</p>
<p>teacher network: [0.1, 0.6, …, 0.01] t</p>
<p><strong>student network</strong>: [s_1, s_2, .., s_10]</p>
<p>cross entropy loss: -sum_{i=1}^10 t_i * log s_i</p>
<p>4, 7</p>
<p><a href="https://arxiv.org/pdf/1910.01108.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1910.01108.pdf</a></p>
<ul>
<li><p>MLM, NSP</p>
</li>
<li><p>MLM: cross entropy loss: -\sum_{i=1}^k p_i log (q_i) = - log (q_{label})</p>
</li>
<li><p>teacher (MLM) = distribution</p>
</li>
<li><p>student: 学习distribution: -\sum_{i=1}^k p_teacher_i log (q_student_i)</p>
</li>
</ul>
<p>Patient Distillation</p>
<p><a href="https://arxiv.org/abs/1908.09355" target="_blank" rel="noopener">https://arxiv.org/abs/1908.09355</a></p>
<p><img src="https://uploader.shimo.im/f/FtKDArmN5UoEwpsF.png!thumbnail" alt="img"></p>
<h1 id="参考阅读资料"><a href="#参考阅读资料" class="headerlink" title="参考阅读资料"></a>参考阅读资料</h1><h3 id="BERT-Distillation"><a href="#BERT-Distillation" class="headerlink" title="BERT Distillation"></a>BERT Distillation</h3><p>对于BERT模型压缩感兴趣的同学可以参考以下资料</p>
<ul>
<li>Patient Knowledge Distillation for BERT Model Compression  <a href="https://www.aclweb.org/anthology/D19-1441.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D19-1441.pdf</a></li>
</ul>
<p>关于BERT模型压缩的一套方法</p>
<h3 id="ELECTRA"><a href="#ELECTRA" class="headerlink" title="ELECTRA"></a>ELECTRA</h3><p><a href="https://openreview.net/pdf?id=r1xMH1BtvB" target="_blank" rel="noopener">https://openreview.net/pdf?id=r1xMH1BtvB</a></p>
<p>使用GAN训练BERT模型</p>
<p><img src="https://uploader.shimo.im/f/PJ9RGb3HpgIA4WYN.png!thumbnail" alt="img"></p>
<ul>
<li><p>Generator针对[MASK]位置生成单词，Discriminator判断这些单词是由Generator (从[MASK]) 生成的还是原本就存在的。</p>
</li>
<li><p>Discriminator被用于downstream task finetuning。</p>
</li>
</ul>
<h3 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h3><p>我在上一期NLP就业班中介绍了XLNet，不过由于近些日子BERT的各种加强版层出不穷，XLNet显得并不特别突出。感兴趣的同学可以参考上一期的课件：<a href="https://shimo.im/docs/PHqcpWtYjJjW3yH3" target="_blank" rel="noopener">https://shimo.im/docs/PHqcpWtYjJjW3yH3</a></p>
<p>XLNet的代码和预训练模型也可以在Huggingface的版本中找到。</p>
<h3 id="NLP预训练模型串讲"><a href="#NLP预训练模型串讲" class="headerlink" title="NLP预训练模型串讲"></a>NLP预训练模型串讲</h3><p>我之前在七月在线的公开课中使用的PPT</p>
<p>NLP预训练模型.pdf1.9MB</p>
<h3 id="参考阅读：The-Illustrated-BERT-ELMo-and-co"><a href="#参考阅读：The-Illustrated-BERT-ELMo-and-co" class="headerlink" title="参考阅读：The Illustrated BERT, ELMo, and co."></a>参考阅读：The Illustrated BERT, ELMo, and co.</h3><p><a href="https://shimo.im/docs/Y6q3gX8yGGjpWqXx" target="_blank" rel="noopener">https://shimo.im/docs/Y6q3gX8yGGjpWqXx</a></p>
<ul>
<li><p>阅读BertSelfAttention代码 <a href="https://github.com/huggingface/transformers/blob/master/transformers/modeling_bert.py#L190" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/transformers/modeling_bert.py#L190</a></p>
</li>
<li><p>阅读run_glue.py <a href="https://github.com/huggingface/transformers/blob/master/examples/run_glue.py#L152" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/examples/run_glue.py#L152</a></p>
</li>
<li><p>阅读BertForSequenceClassification <a href="https://github.com/huggingface/transformers/blob/master/transformers/modeling_bert.py#L970" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/transformers/modeling_bert.py#L970</a></p>
</li>
<li><p>阅读glue.py <a href="https://github.com/huggingface/transformers/blob/master/transformers/data/processors/glue.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/transformers/data/processors/glue.py</a> 用来做文本预处理</p>
</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BERT/">BERT</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-阅读理解" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/05/19/阅读理解/" class="article-date">
      <time datetime="2020-05-19T00:45:58.000Z" itemprop="datePublished">2020-05-19</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/19/阅读理解/">阅读理解</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>NLP当中的阅读理解(Reading Comprehension, Question Answering)任务主要是以下形式：给定一些背景知识，主要是一篇文章，有时候也可能是一些结构化的知识图谱，然后回答与该背景知识的相关问题。</p>
<p>常见的问题和答案形式有：</p>
<ul>
<li><p>完形填空：在文章中给定一个空位和一些候选答案，我们需要把一个候选答案填充进去。</p>
</li>
<li><p>简答题：给定一篇文章和一个问题，我们需要从文章中去找答案，且这个答案一定在文章中出现过。SQuAD</p>
</li>
<li><p>选择题：给定一篇文章，一个问题和一些候选答案，选择一个正确答案。</p>
</li>
</ul>
<p>还有一些在上述问答任务基础上的拓展情况，例如有的任务需要在多篇文章的基础上作答，有的QA任务需要我们自己来推理和撰写答案 (open domain)，无法直接从文中找到答案。</p>
<p>整个QA领域的发展主要都是依靠一些数据集的提出和解决来推动的。往往是有人制作了一个数据集和一个QA任务，然后大家开始比赛谁能更好地解决它。</p>
<p>我认为最好的学习方法是去了解这些QA数据集（<a href="http://nlpprogress.com/english/question_answering.html），针对自己感兴趣的数据集去寻找相应的解决方案，在这个过程中了解QA的解决方法。将来如果在实际的应用场景中遇到类似的QA任务（例如一些客服机器人等），我们就可以寻找到比较相关的QA数据集，使用在这些数据集上最好的解决方案来解决自己的任务。" target="_blank" rel="noopener">http://nlpprogress.com/english/question_answering.html），针对自己感兴趣的数据集去寻找相应的解决方案，在这个过程中了解QA的解决方法。将来如果在实际的应用场景中遇到类似的QA任务（例如一些客服机器人等），我们就可以寻找到比较相关的QA数据集，使用在这些数据集上最好的解决方案来解决自己的任务。</a></p>
<h2 id><a href="#" class="headerlink" title=" "></a> </h2><h1 id="一些有名的阅读理解数据集和模型"><a href="#一些有名的阅读理解数据集和模型" class="headerlink" title="一些有名的阅读理解数据集和模型"></a>一些有名的阅读理解数据集和模型</h1><h2 id="SQuAD-1-0-2-0"><a href="#SQuAD-1-0-2-0" class="headerlink" title="SQuAD 1.0/2.0"></a>SQuAD 1.0/2.0</h2><p>从文章中找答案</p>
<p><a href="https://aclweb.org/anthology/D16-1264" target="_blank" rel="noopener">https://aclweb.org/anthology/D16-1264</a></p>
<p><img src="https://uploader.shimo.im/f/yqtsScSwls8OkJWs.png!thumbnail" alt="img"></p>
<h3 id="BiDAF模型"><a href="#BiDAF模型" class="headerlink" title="BiDAF模型"></a>BiDAF模型</h3><p>Bi-Directional Attention Fflow for Machine Comprehension</p>
<p><a href="https://arxiv.org/pdf/1611.01603.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1611.01603.pdf</a></p>
<p>2017年的模型，用于解决SQuAD之类的问题。后来的很多模型都参考了该模型的设计思想</p>
<p><img src="https://uploader.shimo.im/f/hL8lQitMAxMtYJ5w.png!thumbnail" alt="img"></p>
<p>预测： start_pos, end_pos</p>
<p>其他相关模型</p>
<p>Document Reader (single model)</p>
<p>r-net (single model)</p>
<p>QANet (single)</p>
<p><a href="https://github.com/allenai/bi-att-flow" target="_blank" rel="noopener">https://github.com/allenai/bi-att-flow</a></p>
<p>MCTest</p>
<p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/MCTest_EMNLP2013.pdf" target="_blank" rel="noopener">https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/MCTest_EMNLP2013.pdf</a></p>
<h3 id="BERT模型"><a href="#BERT模型" class="headerlink" title="BERT模型"></a>BERT模型</h3><p>模型 <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1810.04805.pdf</a></p>
<p>代码 <a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1402" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1402</a></p>
<p><a href="https://github.com/huggingface/transformers/blob/master/examples/run_squad.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/examples/run_squad.py</a></p>
<h2 id="CNN-Daily-Mail"><a href="#CNN-Daily-Mail" class="headerlink" title="CNN/Daily Mail"></a>CNN/Daily Mail</h2><p>完形填空类问题</p>
<p>Teaching Machines to Read and Comprehend</p>
<p><a href="https://arxiv.org/pdf/1506.03340.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.03340.pdf</a></p>
<p><img src="https://uploader.shimo.im/f/ppAcqx7DjtM3486H.png!thumbnail" alt="img"></p>
<h3 id="Attention-Sum模型"><a href="#Attention-Sum模型" class="headerlink" title="Attention Sum模型"></a>Attention Sum模型</h3><p><a href="https://arxiv.org/pdf/1603.01547.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.01547.pdf</a></p>
<p>Gated Attention Sum模型是该模型的一个拓展形式</p>
<p><a href="https://arxiv.org/abs/1606.01549" target="_blank" rel="noopener">https://arxiv.org/abs/1606.01549</a></p>
<h3 id="陈丹琦在CNN-Daily-Mail上的工作"><a href="#陈丹琦在CNN-Daily-Mail上的工作" class="headerlink" title="陈丹琦在CNN/Daily Mail上的工作"></a>陈丹琦在CNN/Daily Mail上的工作</h3><p>A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task</p>
<p><a href="https://www.aclweb.org/anthology/P16-1223" target="_blank" rel="noopener">https://www.aclweb.org/anthology/P16-1223</a></p>
<p><img src="https://uploader.shimo.im/f/BKipbLYDzic4bWbc.png!thumbnail" alt="img"></p>
<p>顺便介绍一下，<a href="https://www.cs.princeton.edu/~danqic/" target="_blank" rel="noopener">陈丹琦</a>在QA领域做了很多工作，对QA感兴趣的同学可以参考她的博士论文。</p>
<p><a href="https://www.cs.princeton.edu/~danqic/" target="_blank" rel="noopener">https://www.cs.princeton.edu/~danqic/</a></p>
<p><a href="https://www.cs.princeton.edu/~danqic/papers/thesis.pdf" target="_blank" rel="noopener">https://www.cs.princeton.edu/~danqic/papers/thesis.pdf</a></p>
<p><a href="https://arxiv.org/pdf/1506.03340.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.03340.pdf</a></p>
<p><a href="https://arxiv.org/pdf/1603.01547.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.01547.pdf</a></p>
<p><a href="http://www.cs.cmu.edu/~bdhingra/papers/ga_reader.pdf" target="_blank" rel="noopener">http://www.cs.cmu.edu/~bdhingra/papers/ga_reader.pdf</a></p>
<p><a href="https://arxiv.org/pdf/1611.07954.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1611.07954.pdf</a></p>
<h2 id="RACE数据集"><a href="#RACE数据集" class="headerlink" title="RACE数据集"></a>RACE数据集</h2><p>RACE: Large-scale ReAding Comprehension Dataset From Examinations</p>
<p><a href="https://arxiv.org/pdf/1704.04683.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1704.04683.pdf</a></p>
<p>RACE数据集来自中国的中高考英语阅读理解题。</p>
<p>代码：</p>
<p><a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1204" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1204</a></p>
<p><a href="https://github.com/huggingface/transformers/blob/master/examples/utils_multiple_choice.py#L36" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/examples/utils_multiple_choice.py#L36</a></p>
<p><a href="https://github.com/huggingface/transformers/blob/master/examples/run_multiple_choice.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/examples/run_multiple_choice.py</a></p>
<p>SWAG</p>
<p>后来出现了很多的不同方向的QA问题</p>
<ul>
<li><p>基于多文本的、长文章的问答</p>
</li>
<li><p>narrative qa <a href="https://arxiv.org/pdf/1712.07040.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1712.07040.pdf</a></p>
</li>
<li><p>基于维基百科，结合文本搜索系统的问答</p>
</li>
<li><p>Dr QA <a href="https://arxiv.org/pdf/1704.00051.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1704.00051.pdf</a></p>
</li>
<li><p>基于聊天记录的问答 </p>
</li>
<li><p>QuAC : Question Answering in Context <a href="https://arxiv.org/pdf/1808.07036.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1808.07036.pdf</a></p>
</li>
</ul>
<p>参考以下链接</p>
<ul>
<li><p><a href="https://github.com/karthikncode/nlp-datasets#question-answering" target="_blank" rel="noopener">https://github.com/karthikncode/nlp-datasets#question-answering</a></p>
</li>
<li><p><a href="http://nlpprogress.com/english/question_answering.html" target="_blank" rel="noopener">http://nlpprogress.com/english/question_answering.html</a></p>
</li>
</ul>
<h2 id="基于多文本的QA任务"><a href="#基于多文本的QA任务" class="headerlink" title="基于多文本的QA任务"></a>基于多文本的QA任务</h2><p>HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering</p>
<p><a href="https://www.aclweb.org/anthology/D18-1259" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D18-1259</a></p>
<p>HotpotQA的主要特点是，这是一个基于多文本的QA任务。给定一系列的文章和一个问题，我们需要给出该问题的答案，并且回答我们是从哪些相关的句子中得到问题的答案的。</p>
<p>已经公开的可参考论文</p>
<p>Hierarchical Graph Network for Multi-hop Question Answering <a href="https://arxiv.org/pdf/1911.00484.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1911.00484.pdf</a></p>
<p>Select, Answer and Explain: Interpretable Multi-hop Reading Comprehension over Multiple Documents <a href="https://arxiv.org/pdf/1911.03631.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1911.03631.pdf</a></p>
<h2 id="CoQA-基于对话的问答数据集"><a href="#CoQA-基于对话的问答数据集" class="headerlink" title="CoQA 基于对话的问答数据集"></a>CoQA 基于对话的问答数据集</h2><p>leaderboard <a href="https://stanfordnlp.github.io/coqa/" target="_blank" rel="noopener">https://stanfordnlp.github.io/coqa/</a></p>
<p>论文 <a href="https://arxiv.org/abs/1808.07042" target="_blank" rel="noopener">https://arxiv.org/abs/1808.07042</a></p>
<p>搜狗有一个BERT模型的实现</p>
<p><a href="https://github.com/sogou/SMRCToolkit" target="_blank" rel="noopener">https://github.com/sogou/SMRCToolkit</a></p>
<p>这位同学也实现了一些模型</p>
<p><a href="https://github.com/jayelm/dialog-qa" target="_blank" rel="noopener">https://github.com/jayelm/dialog-qa</a></p>
<h2 id="中文数据集"><a href="#中文数据集" class="headerlink" title="中文数据集"></a>中文数据集</h2><h3 id="法研杯-阅读理解数据集"><a href="#法研杯-阅读理解数据集" class="headerlink" title="法研杯 阅读理解数据集"></a>法研杯 阅读理解数据集</h3><p><a href="http://cail.cipsc.org.cn/" target="_blank" rel="noopener">http://cail.cipsc.org.cn/</a></p>
<h3 id="讯飞杯-中文阅读理解评测"><a href="#讯飞杯-中文阅读理解评测" class="headerlink" title="讯飞杯 中文阅读理解评测"></a>讯飞杯 中文阅读理解评测</h3><p><a href="https://hfl-rc.github.io/cmrc2017/" target="_blank" rel="noopener">https://hfl-rc.github.io/cmrc2017/</a></p>
<p><a href="https://hfl-rc.github.io/cmrc2018/" target="_blank" rel="noopener">https://hfl-rc.github.io/cmrc2018/</a></p>
<p><a href="https://hfl-rc.github.io/cmrc2019/" target="_blank" rel="noopener">https://hfl-rc.github.io/cmrc2019/</a></p>
<p>同学们可以找到这三次比赛的数据集和相应的表现最好的模型代码进行学习。</p>
<p>KBQA</p>
<p><a href="http://tcci.ccf.org.cn/conference/2018/papers/EV51.pdf" target="_blank" rel="noopener">http://tcci.ccf.org.cn/conference/2018/papers/EV51.pdf</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/QA/">QA</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-Transformer模型解读" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/05/18/Transformer模型解读/" class="article-date">
      <time datetime="2020-05-18T00:28:15.000Z" itemprop="datePublished">2020-05-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/18/Transformer模型解读/">Transformer模型解读</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>contextualized word vectors</p>
<p>RNN, LSTM</p>
<p>RNN(I study at Julyedu.) –&gt; RNN(I)-&gt;h1, RNN(study, h1)-&gt;h2, RNN(at, h2)-&gt;h3. </p>
<p>Encoder. 我可以同时观看全局信息。</p>
<p>query, keys, values</p>
<p>q1, q2, .., q5</p>
<p>k1, k2, k3, k4, k5</p>
<p>score(q, k1), score(q, k2), …, score(q, k5)</p>
<p>v1, v2, v3, v4, v5</p>
<p>\sum_{i=1}^5 func(score_i) v_i</p>
<p>dot(a, b)</p>
<p>mean</p>
<p>var(dot(a, b))</p>
<p>dot(a, b) = a1<em>b1 + a2</em>b2. …. </p>
<p>E(dot(a, b)) = n * E(ai*bi)</p>
<p>var(dot(a, b)) = E(dot(a, b)^2) - E(dot(a, b))^2</p>
<p>affine transformation</p>
<p>WX+b</p>
<p>Attention(Q, K, V ) = softmax(QKT √ dk )V</p>
<p>Q : seq_len, hid_size</p>
<p>K^T:  hid_size, seq_len</p>
<p>V: seq_len, hid_size</p>
<p>QK^T : seq_len, seq_len</p>
<p>QK^T V: seq_len, hid_size</p>
<p>[emb_w(x), emb_p(i)]W –&gt; </p>
<p>近两年来，NLP领域的模型研究已经被transformer模型以及它的各种变种给占领了。Transformer模型的火爆有很多原因，例如：</p>
<ul>
<li><p>模型简单易懂，encoder和decoder模块高度相似且通用</p>
</li>
<li><p>（encoder）容易并行，模型训练速度快</p>
</li>
<li><p>效果拔群，在NMT等领域都取得了state-of-the-art的效果</p>
</li>
</ul>
<p>论文地址</p>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a> </li>
</ul>
<p>下面的文章翻译自</p>
<ul>
<li><p><a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">The Illustrated Transformer</a></p>
</li>
<li><p><a href="https://blog.csdn.net/yujianmin1990/article/details/85221271" target="_blank" rel="noopener">中文翻译</a></p>
</li>
</ul>
<p>高屋建瓴地说，Transformer模型拿到一个序列，用来生成另一个序列。</p>
<p><img src="https://uploader.shimo.im/f/vkvOEopS6TMPw0SL.png!thumbnail" alt="img"></p>
<p>打开这个黑箱，我们会看到其中包含了两个部分，encoders和decoders。</p>
<p><img src="https://uploader.shimo.im/f/hraPVC4iek06oDwt.png!thumbnail" alt="img"></p>
<p>其中encoders和decoders都是两个堆叠架构。一层一层同质的结构堆叠到一起，组成了编码器和解码器。</p>
<p><img src="https://uploader.shimo.im/f/WFbnFyb8peoeJuXW.png!thumbnail" alt="img"></p>
<p>首先我们打开每个encoder来参观一下其中包含的内容：</p>
<p><img src="https://uploader.shimo.im/f/c7oNzYNSIoceXYFZ.png!thumbnail" alt="img"></p>
<p>每一个encoder都包含了一个自注意力（self-attention）层和一个Feed Forward Neural Network。</p>
<p>encoder的输入首先会经过一个self-attention层。self-attention的作用是让每个单词可以看到自己和其他单词的关系，并且将自己转换成一个与所有单词相关的，<strong>focus在自己身上的词向量(?)</strong>。</p>
<p>self-attention之后的输出会再经过一层feed-forward神经网络。每个位置的输出被同样的feed-forward network处理。</p>
<p>decoder也有同样的self-attention和feed-forward结构，但是在这两层之间还有一层encoder-decoder attention层，帮助decoder关注到某一些特别需要关注的encoder位置。</p>
<h2 id="Tensor的变化"><a href="#Tensor的变化" class="headerlink" title="Tensor的变化"></a>Tensor的变化</h2><p><img src="https://uploader.shimo.im/f/Hmbb5V4mEJkBYpFS.png!thumbnail" alt="img"></p>
<h2 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h2><p>下面我们来详细解读一下编码器的工作。</p>
<p><img src="https://uploader.shimo.im/f/MzJmdqVJiSUz4DT9.png!thumbnail" alt="img"></p>
<h3 id="Self-Attention机制"><a href="#Self-Attention机制" class="headerlink" title="Self-Attention机制"></a>Self-Attention机制</h3><p>我们考虑用Transformer模型翻译下面这一句话：</p>
<p>“The animal didn’t cross the street because it was too tired”。</p>
<p>当我们翻译到 it 的时候，我们知道 it 指代的是 animal 而不是 street。所以，如果有办法可以让 it 对应位置的 embedding 适当包含 animal 的信息，就会非常有用。self-attention的出现就是为了完成这一任务。</p>
<p>如下图所示，self attnetion会让单词 it 和 某些单词发生比较强的联系，得到比较搞的attention分数。</p>
<p><img src="https://uploader.shimo.im/f/UsNXjO1OpN0usuAg.png!thumbnail" alt="img"></p>
<p>weight(The) = softmax(v(it) * v(The) / \sqrt(d))</p>
<p>weight(The) = softmx(Query(It) * Key(The) / \sqrt(d))</p>
<p>\sum_{word} weight(word) * Value(word)</p>
<h3 id="Self-attention的细节"><a href="#Self-attention的细节" class="headerlink" title="Self-attention的细节"></a>Self-attention的细节</h3><p>为了实现 self-attention，每个输入的位置需要产生三个向量，分别是 <strong>Query 向量，Key 向量和 Value 向量</strong>。这些向量都是由输入 embedding 通过三个 matrices （也就是线性变化）产生的。</p>
<p>注意到在Transformer架构中，这些新的向量比原来的输入向量要小，原来的向量是512维，转变后的三个向量都是64维。</p>
<p><img src="https://uploader.shimo.im/f/MAqlj67rbPYBI7Ad.png!thumbnail" alt="img"></p>
<p>第二步是<strong>计算分数</strong>。当我们在用self-attention encode某个位置上的某个单词的时候，我们希望知道这个单词对应的句子上其他单词的分数。其他单词所得到的分数表示了当我们encode当前单词的时候，应该放多少的关注度在其余的每个单词上。又或者说，其他单词和我当前的单词有多大的相关性或者相似性。</p>
<p>在transformer模型中，这个分数是由query vector和key vector做点积（dot product）所得的结果。所以说，当我们在对第一个单词做self-attention处理的时候，第一个单词的分数是q_1和k_1的点积，第二个分数是q_1和k_2的分数。</p>
<p><img src="https://uploader.shimo.im/f/kW9cJM4TjTc9xtMV.png!thumbnail" alt="img"></p>
<p>第三步和第四步是将这些分数除以8。8这个数字是64的开方，也就是key vector的维度的开方。据说这么做可以稳定模型的gradient。然后我们将这些分数传入softmax层产生一些符合概率分布的probability scores。</p>
<p><img src="https://uploader.shimo.im/f/6kTtVymp0XgZCDh0.png!thumbnail" alt="img"></p>
<p>softmax = exp(x_i) / sum exp(x_i)</p>
<p>这些分数就表示了在处理当前单词的时候我们应该分配多少的关注度给其他单词。</p>
<p>第五步是将每个value vector乘以它们各自的attention score。第六步是把这些weighted value vectors相加，成为当前单词的vector表示。</p>
<p><img src="https://uploader.shimo.im/f/FrqMNrQrlo0tLBgV.png!thumbnail" alt="img"></p>
<p>得到了self-attention生成的词向量之后，我们就可以将它们传入feed-forward network了。</p>
<h3 id="Self-Attention中的矩阵运算"><a href="#Self-Attention中的矩阵运算" class="headerlink" title="Self-Attention中的矩阵运算"></a>Self-Attention中的矩阵运算</h3><p>首先，我们要对每一个词向量计算Query, Key和Value矩阵。我们把句子中的每个词向量拼接到一起变成一个矩阵X，然后乘以不同的矩阵做线性变换（WQ, WK, WV）。</p>
<p><img src="https://uploader.shimo.im/f/xRsGTXMRHTQsNPiL.png!thumbnail" alt="img"></p>
<p>然后我们就用矩阵乘法实现上面介绍过的Self-Attention机制了。</p>
<p><img src="https://uploader.shimo.im/f/S1IEPFyGeMUTWMBk.png!thumbnail" alt="img"></p>
<h3 id="Multi-headed-attention"><a href="#Multi-headed-attention" class="headerlink" title="Multi-headed attention"></a>Multi-headed attention</h3><p>在论文当中，每个embedding vector并不止产生一个key, value, query vectors，而是产生若干组这样的vectors，称之为”multi-headed” attention。这么做有几个好处：</p>
<ul>
<li><p>k: key, q: query, v: value</p>
</li>
<li><p>模型有更强的能力产生不同的attention机制，focus在不同的单词上。</p>
</li>
<li><p>attention layer有多个不同的”representation space”。</p>
</li>
</ul>
<p><img src="https://uploader.shimo.im/f/vQX0sIYIoqUNYO4J.png!thumbnail" alt="img"></p>
<p>每个attention head最终都产生了一个matrix表示这个句子中的所有词向量。在transformer模型中，我们产生了八个matrices。我们知道self attention之后就是一个feed-forward network。那么我们是否需要做8次feed-forward network运算呢？事实上是不用的。我们只需要将这8个matrices拼接到一起，然后做一次前向神经网络的运算就可以了。</p>
<p><img src="https://uploader.shimo.im/f/E4AxOnUs2JgGJ0bW.png!thumbnail" alt="img"></p>
<p>综合起来，我们可以用下面一张图表示Self-Attention模块所做的事情。</p>
<p><img src="https://uploader.shimo.im/f/YmfWxTGsc48tTbfi.png!thumbnail" alt="img"></p>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>thinking machine</p>
<p>w_1, w_2</p>
<p>p_1, p_2</p>
<p>positional_embedding = nn.Embedding(512, 300)</p>
<p>w_1 + p_1, w_2 + p_2, w_3 + p_3, …, w_n + p_n</p>
<p>到目前为止，我们的模型完全没有考虑单词的顺序。即使我们将句子中单词的顺序完全打乱，对于transformer这个模型来说，并没有什么区别。为了加入句子中单词的顺序信息，我们引入一个概念叫做positional encoding。</p>
<p><img src="https://uploader.shimo.im/f/1F6bv1ngvE4hEp99.png!thumbnail" alt="img"></p>
<p>如果我们假设输入的embedding是4个维度的，那么他们的position encodings大概长下面这样。</p>
<p><img src="https://uploader.shimo.im/f/1p4K2IclsvwWGw0Z.png!thumbnail" alt="img"></p>
<p>下面这张图的每一行表示一个positional encoding vector。第一行表示第一个单词的positional encoding，以此类推。每一行都有512个-1到1之间的数字。我们用颜色标记了这些vectors。</p>
<p><img src="https://uploader.shimo.im/f/HMQy3lipFooyu8rO.png!thumbnail" alt="img"></p>
<h3 id="Residuals"><a href="#Residuals" class="headerlink" title="Residuals"></a>Residuals</h3><p>另外一个细节是，encoder中的每一层都包含了一个residual connection和layer-normalization。如下图所示。</p>
<p><img src="https://uploader.shimo.im/f/1qIGhKLQLYkHahSn.png!thumbnail" alt="img"></p>
<p>下面这张图是更详细的vector表示。</p>
<p><img src="https://uploader.shimo.im/f/ivgMtxCc8CsI7lAF.png!thumbnail" alt="img"></p>
<p>decoder也是同样的架构。如果我们把encoder和decoder放到一起，他们就长这样。</p>
<p><img src="https://uploader.shimo.im/f/TumXWzLQ6XMjJneZ.png!thumbnail" alt="img"></p>
<h2 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h2><p>encoder最后一层会输出attention vectors K和V。K和V会被decoder用作解码的原材料。</p>
<p><img src="https://uploader.shimo.im/f/3AgIt6lqzgADLwuf.png!thumbnail" alt="img"></p>
<p>在解码的过程中，解码器每一步会输出一个token。一直循环往复，直到它输出了一个特殊的end of sequence token，表示解码结束了。</p>
<p><img src="https://uploader.shimo.im/f/ai444UV6eQ4E0f6O.png!thumbnail" alt="img"></p>
<p>decoder的self attention机制与encoder稍有不同。在decoder当中，self attention层只能看到之前已经解码的文字。我们只需要把当前输出位置之后的单词全都mask掉（softmax层之前全都设置成-inf）即可。</p>
<p>softmax(Q matmul K^T / sqrt(d)) matmul V</p>
<p>weights = Q matmul K^T: [seq_len, seq_len]</p>
<p>Masked Self Attention</p>
<p>q, k (<strong>100, 24</strong>, 35 - inf, 88 - inf, -55 - inf) –&gt; softmax –&gt; (0.9, 0.1, 0, 0, 0)</p>
<p>attention_mask</p>
<p>0, -inf, -inf, -inf</p>
<p>0, 0, -inf, -inf</p>
<p>0, 0, 0, -inf </p>
<p>0, 0, 0, 0</p>
<p>softmax(weights - attention_mask, -1)</p>
<p>训练</p>
<p>QKV, 并行训练</p>
<p>预测</p>
<p>一个单词一个单词解码</p>
<p>Encoder-Decoder Attention层和普通的multiheaded self-attention一样，除了它的Queries完全来自下面的decoder层，然后Key和Value来自encoder的输出向量。</p>
<p>batch_size * seq_length * hidden_size </p>
<p>padding_mask</p>
<p>tgt_mask</p>
<h3 id="最后的线性层和softmax层"><a href="#最后的线性层和softmax层" class="headerlink" title="最后的线性层和softmax层"></a>最后的线性层和softmax层</h3><p>解码器最后输出浮点向量，如何将它转成词？这是最后的线性层和softmax层的主要工作。</p>
<p>线性层是个简单的全连接层，将解码器的最后输出映射到一个非常大的logits向量上。假设模型已知有1万个单词（输出的词表）从训练集中学习得到。那么，logits向量就有1万维，每个值表示是某个词的可能倾向值。</p>
<p>softmax层将这些分数转换成概率值（都是正值，且加和为1），最高值对应的维上的词就是这一步的输出单词。</p>
<p><img src="https://uploader.shimo.im/f/7ffWFIfMqOgtsK22.png!thumbnail" alt="img"></p>
<h2 id="模型的训练"><a href="#模型的训练" class="headerlink" title="模型的训练"></a>模型的训练</h2><p>现在我们已经了解了一个训练完毕的Transformer的前向过程，顺道看下训练的概念也是非常有用的。在训练时，模型将经历上述的前向过程，当我们在标记训练集上训练时，可以对比预测输出与实际输出。为了可视化，假设输出一共只有6个单词（“a”, “am”, “i”, “thanks”, “student”, “”）</p>
<p><img src="https://uploader.shimo.im/f/FNgjBBm5gbUGYgbs.png!thumbnail" alt="img"></p>
<p>模型的词表是在训练之前的预处理中生成的</p>
<p>一旦定义了词表，我们就能够构造一个同维度的向量来表示每个单词，比如one-hot编码，下面举例编码“am”。</p>
<p><img src="https://uploader.shimo.im/f/feV2TQAHPF0z3Rr2.png!thumbnail" alt="img"></p>
<p>举例采用one-hot编码输出词表</p>
<p>下面让我们讨论下模型的loss损失，在训练过程中用来优化的指标，指导学习得到一个非常准确的模型。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>我们用一个简单的例子来示范训练，比如翻译“merci”为“thanks”。那意味着输出的概率分布指向单词“thanks”，但是由于模型未训练是随机初始化的，不太可能就是期望的输出。</p>
<p><img src="https://uploader.shimo.im/f/aWNgQPklQh8odGQP.png!thumbnail" alt="img"></p>
<p>由于模型参数是随机初始化的，未训练的模型输出随机值。我们可以对比真实输出，然后利用误差后传调整模型权重，使得输出更接近与真实输出。如何对比两个概率分布呢？简单采用 <a href="https://colah.github.io/posts/2015-09-Visual-Information/" target="_blank" rel="noopener">cross-entropy</a>或者<a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained" target="_blank" rel="noopener">Kullback-Leibler divergence</a>中的一种。鉴于这是个极其简单的例子，更真实的情况是，使用一个句子作为输入。比如，输入是“je suis étudiant”，期望输出是“i am a student”。在这个例子下，我们期望模型输出连续的概率分布满足如下条件：</p>
<ol>
<li><p>每个概率分布都与词表同维度</p>
</li>
<li><p>第一个概率分布对“i”具有最高的预测概率值。</p>
</li>
<li><p>第二个概率分布对“am”具有最高的预测概率值。</p>
</li>
<li><p>一直到第五个输出指向””标记。</p>
</li>
</ol>
<p><img src="https://uploader.shimo.im/f/rAnz8qY0eHgt2OAe.png!thumbnail" alt="img"></p>
<p>对一个句子而言，训练模型的目标概率分布</p>
<p>在足够大的训练集上训练足够时间之后，我们期望产生的概率分布如下所示：</p>
<p><img src="https://uploader.shimo.im/f/IyKk2fNcC3k4tgBt.png!thumbnail" alt="img"></p>
<p>训练好之后，模型的输出是我们期望的翻译。当然，这并不意味着这一过程是来自训练集。注意，每个位置都能有值，即便与输出近乎无关，这也是softmax对训练有帮助的地方。现在，因为模型每步只产生一组输出，假设模型选择最高概率，扔掉其他的部分，这是种产生预测结果的方法，叫做greedy 解码。另外一种方法是beam search，每一步仅保留最头部高概率的两个输出，根据这俩输出再预测下一步，再保留头部高概率的两个输出，重复直到预测结束</p>
<h2 id="更多资料"><a href="#更多资料" class="headerlink" title="更多资料"></a>更多资料</h2><ul>
<li><p><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a> </p>
</li>
<li><p>Transformer博客文章 <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank" rel="noopener">Transformer: A Novel Neural Network Architecture for Language Understanding</a></p>
</li>
<li><p><a href="https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html" target="_blank" rel="noopener">Tensor2Tensor announcement</a>.</p>
</li>
<li><p><a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb" target="_blank" rel="noopener">Jupyter Notebook provided as part of the Tensor2Tensor repo</a></p>
</li>
<li><p><a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener">Tensor2Tensor repo</a>.</p>
</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/">Transformer</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-Transformer-XL" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/05/16/Transformer-XL/" class="article-date">
      <time datetime="2020-05-16T00:34:49.000Z" itemprop="datePublished">2020-05-16</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/16/Transformer-XL/">Transformer-XL</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context"><a href="#Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context" class="headerlink" title="Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"></a>Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</h2><p><a href="https://arxiv.org/pdf/1901.02860.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.02860.pdf</a></p>
<p>相较于传统transformer decoder，引入两个新模块</p>
<ul>
<li>segment-level recurrence mechanism</li>
</ul>
<p><img src="https://uploader.shimo.im/f/DpNe30kuahkbOeW5.png!thumbnail" alt="img"></p>
<ul>
<li><p>a novel positional encoding scheme</p>
</li>
<li><p>考虑我们在attention机制中如何使用positional encoding</p>
</li>
</ul>
<p>(E_{x_i}^T+U_i^T)W_q^TW_kE_{x_j}U_j</p>
<p><img src="https://uploader.shimo.im/f/5zNU9yZQtQMClNiY.png!thumbnail" alt="img"></p>
<ul>
<li><p>R他们采用的是transformer当中的positional encoding</p>
</li>
<li><p>u和v是需要训练的模型参数</p>
</li>
</ul>
<p>最终Transformer XL模型</p>
<p><img src="https://uploader.shimo.im/f/Nm1uk49MIjUys1aK.png!thumbnail" alt="img"></p>
<p>代码</p>
<p><a href="https://github.com/kimiyoung/transformer-xl" target="_blank" rel="noopener">https://github.com/kimiyoung/transformer-xl</a></p>
<h2 id="XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding"><a href="#XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding" class="headerlink" title="XLNet: Generalized Autoregressive Pretraining for Language Understanding"></a>XLNet: Generalized Autoregressive Pretraining for Language Understanding</h2><p><a href="https://arxiv.org/pdf/1906.08237.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1906.08237.pdf</a></p>
<p>背景知识</p>
<ul>
<li><p>自回归语言模型（Autoregressive Language Model）：采用从左往右或从右往左的语言模型，根据上文预测下文。</p>
</li>
<li><p>缺点：只利用了预测单词左边或右边的信息，无法同时利用两边的信息。ELMo在一定程度上解决了这个问题。</p>
</li>
<li><p><img src="https://uploader.shimo.im/f/cpfGbeRfzf8c1ga8.png!thumbnail" alt="img"></p>
</li>
<li><p>自编码模型（Denoising Auto Encoder, DAE）：在输入中随机mask一些单词，利用上下文来预测被mask掉的单词。BERT采用了这一思路。</p>
</li>
<li><p><img src="https://uploader.shimo.im/f/za1FnG3zHdsbm5gD.png!thumbnail" alt="img"></p>
</li>
</ul>
<p>两个模型的问题</p>
<p><img src="https://uploader.shimo.im/f/A1rO6rAR1nAQqqvu.png!thumbnail" alt="img"></p>
<p>XLNet的目标是融合以上两种模型的优点，解决它们各自存在的问题。</p>
<p>XLNet模型：Permutation Language Modeling</p>
<p><img src="https://uploader.shimo.im/f/LdaKeEgG8XwH3iNj.png!thumbnail" alt="img"></p>
<p>Two-Stream Self-Attention</p>
<p><img src="https://uploader.shimo.im/f/TdQVsxOeYMoakBW0.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/iLMqF1WinQI6wOsW.png!thumbnail" alt="img"></p>
<p>参考资料</p>
<p><a href="https://zhuanlan.zhihu.com/p/70257427" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/70257427</a></p>
<p>代码</p>
<p><a href="https://github.com/zihangdai/xlnet" target="_blank" rel="noopener">https://github.com/zihangdai/xlnet</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer-XL/">Transformer-XL</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/XLNet/">XLNet</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-英文书籍word级别的文本生成代码注释" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/05/12/英文书籍word级别的文本生成代码注释/" class="article-date">
      <time datetime="2020-05-12T10:52:06.000Z" itemprop="datePublished">2020-05-12</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/12/英文书籍word级别的文本生成代码注释/">英文书籍word级别的文本生成代码注释</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p><strong>先看丘吉尔的人物传记char级别的文本生成</strong></p>
<p>举个小小的例子，来看看LSTM是怎么玩的</p>
<p>我们这里不再用char级别，我们用word级别来做。我们这里的文本预测就是，给了前面的单词以后，下一个单词是谁？</p>
<p>比如，hello from the other, 给出 side</p>
<p>第一步，一样，先导入各种库</p>
<h3 id="导入数据并分词"><a href="#导入数据并分词" class="headerlink" title="导入数据并分词"></a>导入数据并分词</h3><p>In [1]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dropout</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> LSTM</span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ModelCheckpoint</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> gensim.models.word2vec <span class="keyword">import</span> Word2Vec</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Using TensorFlow backend.</span><br></pre></td></tr></table></figure>

<p>In [8]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行资源充足的可以试试下面的代码</span></span><br><span class="line"><span class="comment"># raw_text = ''</span></span><br><span class="line"><span class="comment"># for file in os.listdir("./input/"):</span></span><br><span class="line"><span class="comment">#     # os.listdir列出路径下的所有文件的名字</span></span><br><span class="line"><span class="comment">#     if file.endswith(".txt"): # 取出后缀.txt的文件</span></span><br><span class="line"><span class="comment">#         raw_text += open("./input/"+file, errors='ignore').read() + '\n\n'</span></span><br><span class="line">raw_text = open(<span class="string">'./input/Winston_Churchil.txt'</span>).read()</span><br><span class="line"><span class="comment"># 我们仍用丘吉尔的语料生成文本</span></span><br><span class="line">raw_text = raw_text.lower()</span><br><span class="line">sentensor = nltk.data.load(<span class="string">'tokenizers/punkt/english.pickle'</span>)   </span><br><span class="line"><span class="comment"># 加载英文的划分句子的模型</span></span><br><span class="line">sents = sentensor.tokenize(raw_text)</span><br><span class="line"><span class="comment"># .tokenize对一段文本进行分句，分成各个句子组成的列表。详解看下这个博客，蛮有意思的</span></span><br><span class="line"><span class="comment"># https://blog.csdn.net/ustbbsy/article/details/80053307</span></span><br><span class="line">print(sents[:<span class="number">2</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&apos;\ufeffproject gutenberg’s real soldiers of fortune, by richard harding davis\n\nthis ebook is for the use of anyone anywhere at no cost and with\nalmost no restrictions whatsoever.&apos;, &apos;you may copy it, give it away or\nre-use it under the terms of the project gutenberg license included\nwith this ebook or online at www.gutenberg.org\n\n\ntitle: real soldiers of fortune\n\nauthor: richard harding davis\n\nposting date: february 22, 2009 [ebook #3029]\nlast updated: september 26, 2016\n\nlanguage: english\n\ncharacter set encoding: utf-8\n\n*** start of this project gutenberg ebook real soldiers of fortune ***\n\n\n\n\nproduced by david reed, and ronald j. wilson\n\n\n\n\n\nreal soldiers of fortune\n\n\nby richard harding davis\n\n\n\n\n\nmajor-general henry ronald douglas maciver\n\nany sunny afternoon, on fifth avenue, or at night in the _table d’hote_\nrestaurants of university place, you may meet the soldier of fortune who\nof all his brothers in arms now living is the most remarkable.&apos;]</span><br></pre></td></tr></table></figure>

<p>In [9]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">corpus = []</span><br><span class="line"><span class="keyword">for</span> sen <span class="keyword">in</span> sents: <span class="comment"># 针对每个句子，再次进行分词。</span></span><br><span class="line">    corpus.append(nltk.word_tokenize(sen))</span><br><span class="line"></span><br><span class="line">print(len(corpus))</span><br><span class="line">print(corpus[:<span class="number">2</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1792</span><br><span class="line">[[&apos;\ufeffproject&apos;, &apos;gutenberg&apos;, &apos;’&apos;, &apos;s&apos;, &apos;real&apos;, &apos;soldiers&apos;, &apos;of&apos;, &apos;fortune&apos;, &apos;,&apos;, &apos;by&apos;, &apos;richard&apos;, &apos;harding&apos;, &apos;davis&apos;, &apos;this&apos;, &apos;ebook&apos;, &apos;is&apos;, &apos;for&apos;, &apos;the&apos;, &apos;use&apos;, &apos;of&apos;, &apos;anyone&apos;, &apos;anywhere&apos;, &apos;at&apos;, &apos;no&apos;, &apos;cost&apos;, &apos;and&apos;, &apos;with&apos;, &apos;almost&apos;, &apos;no&apos;, &apos;restrictions&apos;, &apos;whatsoever&apos;, &apos;.&apos;], [&apos;you&apos;, &apos;may&apos;, &apos;copy&apos;, &apos;it&apos;, &apos;,&apos;, &apos;give&apos;, &apos;it&apos;, &apos;away&apos;, &apos;or&apos;, &apos;re-use&apos;, &apos;it&apos;, &apos;under&apos;, &apos;the&apos;, &apos;terms&apos;, &apos;of&apos;, &apos;the&apos;, &apos;project&apos;, &apos;gutenberg&apos;, &apos;license&apos;, &apos;included&apos;, &apos;with&apos;, &apos;this&apos;, &apos;ebook&apos;, &apos;or&apos;, &apos;online&apos;, &apos;at&apos;, &apos;www.gutenberg.org&apos;, &apos;title&apos;, &apos;:&apos;, &apos;real&apos;, &apos;soldiers&apos;, &apos;of&apos;, &apos;fortune&apos;, &apos;author&apos;, &apos;:&apos;, &apos;richard&apos;, &apos;harding&apos;, &apos;davis&apos;, &apos;posting&apos;, &apos;date&apos;, &apos;:&apos;, &apos;february&apos;, &apos;22&apos;, &apos;,&apos;, &apos;2009&apos;, &apos;[&apos;, &apos;ebook&apos;, &apos;#&apos;, &apos;3029&apos;, &apos;]&apos;, &apos;last&apos;, &apos;updated&apos;, &apos;:&apos;, &apos;september&apos;, &apos;26&apos;, &apos;,&apos;, &apos;2016&apos;, &apos;language&apos;, &apos;:&apos;, &apos;english&apos;, &apos;character&apos;, &apos;set&apos;, &apos;encoding&apos;, &apos;:&apos;, &apos;utf-8&apos;, &apos;***&apos;, &apos;start&apos;, &apos;of&apos;, &apos;this&apos;, &apos;project&apos;, &apos;gutenberg&apos;, &apos;ebook&apos;, &apos;real&apos;, &apos;soldiers&apos;, &apos;of&apos;, &apos;fortune&apos;, &apos;***&apos;, &apos;produced&apos;, &apos;by&apos;, &apos;david&apos;, &apos;reed&apos;, &apos;,&apos;, &apos;and&apos;, &apos;ronald&apos;, &apos;j.&apos;, &apos;wilson&apos;, &apos;real&apos;, &apos;soldiers&apos;, &apos;of&apos;, &apos;fortune&apos;, &apos;by&apos;, &apos;richard&apos;, &apos;harding&apos;, &apos;davis&apos;, &apos;major-general&apos;, &apos;henry&apos;, &apos;ronald&apos;, &apos;douglas&apos;, &apos;maciver&apos;, &apos;any&apos;, &apos;sunny&apos;, &apos;afternoon&apos;, &apos;,&apos;, &apos;on&apos;, &apos;fifth&apos;, &apos;avenue&apos;, &apos;,&apos;, &apos;or&apos;, &apos;at&apos;, &apos;night&apos;, &apos;in&apos;, &apos;the&apos;, &apos;_table&apos;, &apos;d&apos;, &apos;’&apos;, &apos;hote_&apos;, &apos;restaurants&apos;, &apos;of&apos;, &apos;university&apos;, &apos;place&apos;, &apos;,&apos;, &apos;you&apos;, &apos;may&apos;, &apos;meet&apos;, &apos;the&apos;, &apos;soldier&apos;, &apos;of&apos;, &apos;fortune&apos;, &apos;who&apos;, &apos;of&apos;, &apos;all&apos;, &apos;his&apos;, &apos;brothers&apos;, &apos;in&apos;, &apos;arms&apos;, &apos;now&apos;, &apos;living&apos;, &apos;is&apos;, &apos;the&apos;, &apos;most&apos;, &apos;remarkable&apos;, &apos;.&apos;]]</span><br></pre></td></tr></table></figure>

<h1 id="word2vec生成词向量"><a href="#word2vec生成词向量" class="headerlink" title="word2vec生成词向量"></a>word2vec生成词向量</h1><p>In [45]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">w2v_model = Word2Vec(corpus, size=<span class="number">128</span>, window=<span class="number">5</span>, min_count=<span class="number">2</span>, workers=<span class="number">4</span>)</span><br><span class="line"><span class="comment"># Word2Vec()参数看这个博客：https://www.cnblogs.com/pinard/p/7278324.html</span></span><br><span class="line"><span class="comment"># size：词向量的维度</span></span><br><span class="line"><span class="comment"># window：即词向量上下文最大距离，window越大，则和某一词较远的词也会产生上下文关系。默认值为5。</span></span><br><span class="line"><span class="comment"># min_count：需要计算词向量的最小词频。这个值可以去掉一些很生僻的低频词，默认是5。如果是小语料，可以调低这个值。</span></span><br><span class="line"><span class="comment"># workers：用于控制训练的并行数。</span></span><br><span class="line"></span><br><span class="line">print(w2v_model[<span class="string">'office'</span>][:<span class="number">20</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[-0.03379476 -0.22743131 -0.17660786 -0.00957653 -0.10752155 -0.14298159</span><br><span class="line">  0.02914934 -0.08970737 -0.15872304 -0.05246524 -0.00084796 -0.05634443</span><br><span class="line"> -0.1461402   0.03880814 -0.12331649 -0.06511988 -0.08555544 -0.2300725</span><br><span class="line"> -0.0083805   0.02204316]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).</span><br></pre></td></tr></table></figure>

<h3 id="构造训练集"><a href="#构造训练集" class="headerlink" title="构造训练集"></a>构造训练集</h3><p>In [46]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">raw_input = [item <span class="keyword">for</span> sublist <span class="keyword">in</span> corpus <span class="keyword">for</span> item <span class="keyword">in</span> sublist]</span><br><span class="line">print(len(raw_input)) <span class="comment"># 原始语料库里的词语总数</span></span><br><span class="line">text_stream = []</span><br><span class="line">vocab = w2v_model.wv.vocab <span class="comment"># 查看w2v_model生成的词向量</span></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> raw_input:</span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> vocab:</span><br><span class="line">        text_stream.append(word)</span><br><span class="line">print(len(text_stream))  </span><br><span class="line"><span class="comment"># 查看去掉低频词后的总的词数，因为min_count把低频词去掉了</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">55562</span><br><span class="line">51876</span><br></pre></td></tr></table></figure>

<p>In [47]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 处理方式同char级别的文本生成</span></span><br><span class="line">seq_length = <span class="number">10</span> </span><br><span class="line">x = []</span><br><span class="line">y = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(text_stream) - seq_length):</span><br><span class="line">    given = text_stream[i:i + seq_length]</span><br><span class="line">    predict = text_stream[i + seq_length]</span><br><span class="line">    x.append([w2v_model[word] <span class="keyword">for</span> word <span class="keyword">in</span> given])</span><br><span class="line">    y.append(w2v_model[predict])</span><br><span class="line"></span><br><span class="line">x = np.reshape(x, (<span class="number">-1</span>, seq_length, <span class="number">128</span>))</span><br><span class="line">y = np.reshape(y, (<span class="number">-1</span>,<span class="number">128</span>))</span><br><span class="line">print(x.shape)</span><br><span class="line">print(y.shape)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).</span><br><span class="line">  </span><br><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).</span><br><span class="line">  if __name__ == &apos;__main__&apos;:</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(51866, 10, 128)</span><br><span class="line">(51866, 128)</span><br></pre></td></tr></table></figure>

<h3 id="构建和训练模型"><a href="#构建和训练模型" class="headerlink" title="构建和训练模型"></a>构建和训练模型</h3><p>In [53]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(LSTM(<span class="number">256</span>, input_shape=(seq_length, <span class="number">128</span>),dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.2</span>))</span><br><span class="line"><span class="comment"># 第一个dropout是x和hidden之间的dropout</span></span><br><span class="line"><span class="comment"># 第二个recurrent_dropout，这里我理解为是横向不同时刻隐藏层之间的dropout</span></span><br><span class="line">model.add(Dropout(<span class="number">0.2</span>)) <span class="comment"># 第三个，这里我理解为纵向层与层之间的dropout</span></span><br><span class="line">model.add(Dense(<span class="number">128</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.compile(loss=<span class="string">'mse'</span>, optimizer=<span class="string">'adam'</span>)</span><br><span class="line"><span class="comment"># 损失用的均方差损失，优化器adam</span></span><br></pre></td></tr></table></figure>

<p>In [54]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(x, y, nb_epoch=<span class="number">10</span>, batch_size=<span class="number">4096</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.</span><br><span class="line">  &quot;&quot;&quot;Entry point for launching an IPython kernel.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/10</span><br><span class="line">51866/51866 [==============================] - 28s 539us/step - loss: 0.3177</span><br><span class="line">Epoch 2/10</span><br><span class="line">51866/51866 [==============================] - 28s 542us/step - loss: 0.1405</span><br><span class="line">Epoch 3/10</span><br><span class="line">51866/51866 [==============================] - 29s 560us/step - loss: 0.1329</span><br><span class="line">Epoch 4/10</span><br><span class="line">51866/51866 [==============================] - 30s 584us/step - loss: 0.1318</span><br><span class="line">Epoch 5/10</span><br><span class="line">51866/51866 [==============================] - 28s 548us/step - loss: 0.1313</span><br><span class="line">Epoch 6/10</span><br><span class="line">51866/51866 [==============================] - 30s 574us/step - loss: 0.1309</span><br><span class="line">Epoch 7/10</span><br><span class="line">51866/51866 [==============================] - 30s 570us/step - loss: 0.1306</span><br><span class="line">Epoch 8/10</span><br><span class="line">51866/51866 [==============================] - 29s 551us/step - loss: 0.1303</span><br><span class="line">Epoch 9/10</span><br><span class="line">51866/51866 [==============================] - 27s 524us/step - loss: 0.1299</span><br><span class="line">Epoch 10/10</span><br><span class="line">51866/51866 [==============================] - 27s 512us/step - loss: 0.1296</span><br></pre></td></tr></table></figure>

<p>Out[54]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;keras.callbacks.History at 0x1a32c9a2b0&gt;</span><br></pre></td></tr></table></figure>

<h3 id="预测模型"><a href="#预测模型" class="headerlink" title="预测模型"></a>预测模型</h3><p>In [55]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 代码注释同丘吉尔的人物传记char级别的文本生成</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_next</span><span class="params">(input_array)</span>:</span></span><br><span class="line">    x = np.reshape(input_array, (<span class="number">-1</span>,seq_length,<span class="number">128</span>))</span><br><span class="line">    y = model.predict(x)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">string_to_index</span><span class="params">(raw_input)</span>:</span></span><br><span class="line">    raw_input = raw_input.lower()</span><br><span class="line">    input_stream = nltk.word_tokenize(raw_input)</span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> input_stream[(len(input_stream)-seq_length):]:</span><br><span class="line">        res.append(w2v_model[word])</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">y_to_word</span><span class="params">(y)</span>:</span></span><br><span class="line">    word = w2v_model.most_similar(positive=y, topn=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> word</span><br></pre></td></tr></table></figure>

<p>In [56]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_article</span><span class="params">(init, rounds=<span class="number">30</span>)</span>:</span></span><br><span class="line">    in_string = init.lower()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(rounds):</span><br><span class="line">        n = y_to_word(predict_next(string_to_index(in_string)))</span><br><span class="line">        in_string += <span class="string">' '</span> + n[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> in_string</span><br></pre></td></tr></table></figure>

<p>In [58]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">init = <span class="string">'His object in coming to New York was to engage officers for that service. He came at an  moment'</span></span><br><span class="line">article = generate_article(init)</span><br><span class="line">print(article) <span class="comment"># 语料库较小，可以看到重复了</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).</span><br><span class="line">  if sys.path[0] == &apos;&apos;:</span><br><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).</span><br><span class="line">  app.launch_new_instance()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">his object in coming to new york was to engage officers for that service. he came at an  moment battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LSTM/">LSTM</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</div>
      <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2019-2020 MingmingYe
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>

    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>