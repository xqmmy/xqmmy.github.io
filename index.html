<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no">
<meta name="author" content="MingmingYe">


    
    


<meta property="og:type" content="website">
<meta property="og:title" content="Stay hungry, Stay foolish.">
<meta property="og:url" content="http://mmyblog.cn/index.html">
<meta property="og:site_name" content="Stay hungry, Stay foolish.">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Stay hungry, Stay foolish.">

<link rel="apple-touch-icon" href="/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="Stay hungry, Stay foolish." type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">


    <style> .article { opacity: 0;} </style>


<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>Stay hungry, Stay foolish.</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: true
    }
</script>


    <script>
        yiliaConfig.jquery_ui = [true, "//cdn.bootcss.com/jqueryui/1.10.4/jquery-ui.min.js", "//cdn.bootcss.com/jqueryui/1.10.4/css/jquery-ui.min.css"];
    </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






</head></html>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/deep.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">MingmingYe</a></h1>
        </hgroup>

        
        <p class="header-subtitle">当你的才华撑不起你的野心时，只有静下心来好好学习！纵使命运注定是个打酱油的，也要打一瓶与别人不一样的酱油！</p>
        

        
            <form id="search-form">
            <input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false" />
            <i class="fa fa-times" onclick="resetSearch()"></i>
            </form>
            <div id="local-search-result"></div>
            <p class='no-result'>No results found <i class='fa fa-spinner fa-pulse'></i></p>
        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:878759487@qq.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AUC/">AUC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LR/">LR</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ROC/">ROC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/boosting/">boosting</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cbow/">cbow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hierarchical-softmax/">hierarchical softmax</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mumpy/">mumpy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/negative-sampling/">negative sampling</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pandas/">pandas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pyspark/">pyspark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/seaborn/">seaborn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/skip-gram/">skip-gram</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/softmax/">softmax</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/优化方法/">优化方法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/决策树/">决策树</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/微积分/">微积分</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/概率/">概率</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/模型调优/">模型调优</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习速查表/">深度学习速查表</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/特征工程/">特征工程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/线性代数/">线性代数</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/统计/">统计</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/聚类/">聚类</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/贝叶斯分类器/">贝叶斯分类器</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/降维/">降维</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/集成学习/">集成学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/面试/">面试</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://mmyblog.cn/">mmy</a>
                    
                    </div>
                </section>
                

                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">MingmingYe</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/deep.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">MingmingYe</a></h1>
            </hgroup>
            
            <p class="header-subtitle">当你的才华撑不起你的野心时，只有静下心来好好学习！纵使命运注定是个打酱油的，也要打一瓶与别人不一样的酱油！</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:878759487@qq.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap">
  
    <article id="post-word2vec" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/04/24/word2vec/" class="article-date">
      <time datetime="2020-04-24T05:29:22.000Z" itemprop="datePublished">2020-04-24</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/24/word2vec/">word2vec</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h4 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h4><ul>
<li>有一个很大的词表库</li>
<li>在词表中的每个词都可以通过向量表征</li>
<li>有一个中心词c，有一个输出词o</li>
<li>用词c和o的相似度来计算他们之间同时出现的概率</li>
<li>调整这个词向量来获得最大输出概率</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/word2vec/">word2vec</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cbow/">cbow</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hierarchical-softmax/">hierarchical softmax</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/negative-sampling/">negative sampling</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/skip-gram/">skip-gram</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-特征工程与模型调优" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/04/20/特征工程与模型调优/" class="article-date">
      <time datetime="2020-04-20T06:17:41.000Z" itemprop="datePublished">2020-04-20</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/20/特征工程与模型调优/">特征工程与模型调优</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="机器学习特征工程"><a href="#机器学习特征工程" class="headerlink" title="机器学习特征工程"></a>机器学习特征工程</h2><h3 id="机器学习流程与概念"><a href="#机器学习流程与概念" class="headerlink" title="机器学习流程与概念"></a>机器学习流程与概念</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBncGV.jpg" alt></p>
<h3 id="机器学习建模流程"><a href="#机器学习建模流程" class="headerlink" title="机器学习建模流程"></a>机器学习建模流程</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBn2xU.png" alt></p>
<h3 id="机器学习特征工程一览"><a href="#机器学习特征工程一览" class="headerlink" title="机器学习特征工程一览"></a>机器学习特征工程一览</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBnOMD.jpg" alt></p>
<h3 id="机器学习特征工程介绍"><a href="#机器学习特征工程介绍" class="headerlink" title="机器学习特征工程介绍"></a>机器学习特征工程介绍</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBnjqH.jpg" alt></p>
<h3 id="特征清洗"><a href="#特征清洗" class="headerlink" title="特征清洗"></a>特征清洗</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBumon.jpg" alt><br><img src="https://s1.ax1x.com/2020/04/24/JBuKJ0.jpg" alt><br><img src="https://s1.ax1x.com/2020/04/24/JBu3yF.jpg" alt></p>
<h3 id="数值型数据上的特征工程"><a href="#数值型数据上的特征工程" class="headerlink" title="数值型数据上的特征工程"></a>数值型数据上的特征工程</h3><p>数值型数据通常以标量的形式表示数据，描述观测值、记录或者测量值。本文的数值型数据是指连续型数据而不是离散型数据，表示不同类目的数据就是后者。数值型数据也可以用向量来表示，向量的每个值或分量代表一个特征。整数和浮点数是连续型数值数据中最常见也是最常使用的数值型数据类型。即使数值型数据可以直接输入到机器学习模型中，你仍需要在建模前设计与场景、问题和领域相关的特征。因此仍需要特征工程。让我们利用 python 来看看在数值型数据上做特征工程的一些策略。我们首先加载下面一些必要的依赖（通常在 <a href="http://jupyter.org/" target="_blank" rel="noopener"><strong>Jupyter</strong> </a> botebook 上）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">&gt;</span><br><span class="line">&gt; <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">&gt;</span><br><span class="line">&gt; <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">&gt;</span><br><span class="line">&gt; <span class="keyword">import</span> scipy.stats <span class="keyword">as</span> spstats</span><br><span class="line">&gt;</span><br><span class="line">&gt; %matplotlib inline</span><br></pre></td></tr></table></figure>

<p>原始度量</p>
<p>正如我们先前提到的，根据上下文和数据的格式，原始数值型数据通常可直接输入到机器学习模型中。原始的度量方法通常用数值型变量来直接表示为特征，而不需要任何形式的变换或特征工程。通常这些特征可以表示一些值或总数。让我们加载四个数据集之一的 <a href="https://www.kaggle.com/abcsds/pokemon/data" target="_blank" rel="noopener">Pokemon </a>数据集，该数据集也在 <a href="https://www.kaggle.com/abcsds/pokemon/data" target="_blank" rel="noopener">Kaggle </a>上公布了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">poke_df = pd.read_csv(<span class="string">'datasets/Pokemon.csv'</span>, encoding=<span class="string">'utf-8'</span>) </span><br><span class="line"></span><br><span class="line">poke_df.head()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f55514768e.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="我们的Pokemon数据集截图"><a href="#我们的Pokemon数据集截图" class="headerlink" title="我们的Pokemon数据集截图"></a>我们的Pokemon数据集截图</h5><p>Pokemon 是一个大型多媒体游戏，包含了各种口袋妖怪（Pokemon）角色。简而言之，你可以认为他们是带有超能力的动物！这些数据集由这些口袋妖怪角色构成，每个角色带有各种统计信息。</p>
<h4 id="数值"><a href="#数值" class="headerlink" title="数值"></a>数值</h4><p>如果你仔细地观察上图中这些数据，你会看到几个代表数值型原始值的属性，它可以被直接使用。下面的这行代码挑出了其中一些重点特征。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poke_df[[&apos;HP&apos;, &apos;Attack&apos;, &apos;Defense&apos;]].head()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f557552811.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="带（连续型）数值数据的特征"><a href="#带（连续型）数值数据的特征" class="headerlink" title="带（连续型）数值数据的特征"></a>带（连续型）数值数据的特征</h5><p>这样，你可以直接将这些属性作为特征，如上图所示。这些特征包括 Pokemon 的 HP（血量），Attack（攻击）和 Defense（防御）状态。事实上，我们也可以基于这些字段计算出一些基本的统计量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poke_df[[&apos;HP&apos;, &apos;Attack&apos;, &apos;Defense&apos;]].describe()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f559f61c14.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p><strong>数值特征形式的基本描述性统计量</strong></p>
<p>这样你就对特征中的统计量如总数、平均值、标准差和四分位数有了一个很好的印象。</p>
<h4 id="记数"><a href="#记数" class="headerlink" title="记数"></a>记数</h4><p>原始度量的另一种形式包括代表频率、总数或特征属性发生次数的特征。让我们看看 <a href="https://labrosa.ee.columbia.edu/millionsong/" target="_blank" rel="noopener"><strong>millionsong</strong></a> <strong>数据集</strong>中的一个例子，其描述了某一歌曲被各种用户收听的总数或频数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">popsong_df = pd.read_csv(&apos;datasets/song_views.csv&apos;,encoding=&apos;utf-8&apos;)</span><br><span class="line"></span><br><span class="line">popsong_df.head(10)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f55bf6176f.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="数值特征形式的歌曲收听总数"><a href="#数值特征形式的歌曲收听总数" class="headerlink" title="数值特征形式的歌曲收听总数"></a>数值特征形式的歌曲收听总数</h5><p>根据这张截图，显而易见 listen_count 字段可以直接作为基于数值型特征的频数或总数。</p>
<h4 id="二值化"><a href="#二值化" class="headerlink" title="二值化"></a>二值化</h4><p>基于要解决的问题构建模型时，通常原始频数或总数可能与此不相关。比如如果我要建立一个推荐系统用来推荐歌曲，我只希望知道一个人是否感兴趣或是否听过某歌曲。我不需要知道一首歌被听过的次数，因为我更关心的是一个人所听过的各种各样的歌曲。在这个例子中，二值化的特征比基于计数的特征更合适。我们二值化 listen_count 字段如下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; watched = np.array(popsong_df[&apos;listen_count&apos;])</span><br><span class="line">&gt;</span><br><span class="line">&gt; watched[watched &gt;= 1] = 1</span><br><span class="line">&gt;</span><br><span class="line">&gt; popsong_df[&apos;watched&apos;] = watched</span><br></pre></td></tr></table></figure>

<p>你也可以使用 scikit-learn 中 preprocessing 模块的 Binarizer 类来执行同样的任务，而不一定使用 numpy 数组。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import Binarizer</span><br><span class="line"></span><br><span class="line">bn = Binarizer(threshold=0.9)</span><br><span class="line"></span><br><span class="line">pd_watched =bn.transform([popsong_df[&apos;listen_count&apos;]])[0]</span><br><span class="line"></span><br><span class="line">popsong_df[&apos;pd_watched&apos;] = pd_watched</span><br><span class="line"></span><br><span class="line">popsong_df.head(11)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56505e8ff.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="歌曲收听总数的二值化结构"><a href="#歌曲收听总数的二值化结构" class="headerlink" title="歌曲收听总数的二值化结构"></a>歌曲收听总数的二值化结构</h5><p>你可以从上面的截图中清楚地看到，两个方法得到了相同的结果。因此我们得到了一个二值化的特征来表示一首歌是否被每个用户听过，并且可以在相关的模型中使用它。</p>
<h4 id="数据舍入"><a href="#数据舍入" class="headerlink" title="数据舍入"></a>数据舍入</h4><p>处理连续型数值属性如比例或百分比时，我们通常不需要高精度的原始数值。因此通常有必要将这些高精度的百分比舍入为整数型数值。这些整数可以直接作为原始数值甚至分类型特征（基于离散类的）使用。让我们试着将这个观念应用到一个虚拟数据集上，该数据集描述了库存项和他们的流行度百分比。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">items_popularity =pd.read_csv(<span class="string">'datasets/item_popularity.csv'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">items_popularity[<span class="string">'popularity_scale_10'</span>] = np.array(np.round((items_popularity[<span class="string">'pop_percent'</span>] * <span class="number">10</span>)),dtype=<span class="string">'int'</span>)</span><br><span class="line"></span><br><span class="line">items_popularity[<span class="string">'popularity_scale_100'</span>] = np.array(np.round((items_popularity[<span class="string">'pop_percent'</span>] * <span class="number">100</span>)),dtype=<span class="string">'int'</span>)</span><br><span class="line"></span><br><span class="line">items_popularity</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f566e30ad2.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="不同尺度下流行度舍入结果"><a href="#不同尺度下流行度舍入结果" class="headerlink" title="不同尺度下流行度舍入结果"></a>不同尺度下流行度舍入结果</h5><p>基于上面的输出，你可能猜到我们试了两种不同的舍入方式。这些特征表明项目流行度的特征现在既有 1-10 的尺度也有 1-100 的尺度。基于这个场景或问题你可以使用这些值同时作为数值型或分类型特征。</p>
<h4 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h4><p>高级机器学习模型通常会对作为输入特征变量函数的输出响应建模（离散类别或连续数值）。例如，一个简单的线性回归方程可以表示为</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56ab22fb7.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>其中输入特征用变量表示为</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56c69ac66.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>权重或系数可以分别表示为</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56de74ee7.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>目标是预测响应 <strong>*y*</strong>.</p>
<p>在这个例子中，仅仅根据单个的、分离的输入特征，这个简单的线性模型描述了输出与输入之间的关系。</p>
<p>然而，在一些真实场景中，有必要试着捕获这些输入特征集一部分的特征变量之间的相关性。上述带有相关特征的线性回归方程的展开式可以简单表示为</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f5701419ee.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>此处特征可表示为</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f57162d4f7.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>表示了相关特征。现在让我们试着在 Pokemon 数据集上设计一些相关特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">atk_def = poke_df[[<span class="string">'Attack'</span>, <span class="string">'Defense'</span>]]</span><br><span class="line"></span><br><span class="line">atk_def.head()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f572bad2cc.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>从输出数据框中，我们可以看到我们有两个数值型（连续的）特征，Attack 和 Defence。现在我们可以利用 scikit-learn 建立二度特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">pf = PolynomialFeatures(degree=<span class="number">2</span>,</span><br><span class="line"></span><br><span class="line">interaction_only=<span class="literal">False</span>,include_bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">res = pf.fit_transform(atk_def)</span><br><span class="line"></span><br><span class="line">res</span><br><span class="line"></span><br><span class="line">**Output**</span><br><span class="line"></span><br><span class="line">**------**</span><br><span class="line"></span><br><span class="line">array([[ <span class="number">49.</span>, <span class="number">49.</span>, <span class="number">2401.</span>, <span class="number">2401.</span>, <span class="number">2401.</span>],</span><br><span class="line"></span><br><span class="line">  [ <span class="number">62.</span>, <span class="number">63.</span>, <span class="number">3844.</span>, <span class="number">3906.</span>, <span class="number">3969.</span>],</span><br><span class="line"></span><br><span class="line">  [ <span class="number">82.</span>, <span class="number">83.</span>, <span class="number">6724.</span>, <span class="number">6806.</span>, <span class="number">6889.</span>],</span><br><span class="line"></span><br><span class="line">  ...,</span><br><span class="line"></span><br><span class="line">  [ <span class="number">110.</span>, <span class="number">60.</span>, <span class="number">12100.</span>, <span class="number">6600.</span>, <span class="number">3600.</span>],</span><br><span class="line"></span><br><span class="line">  [ <span class="number">160.</span>, <span class="number">60.</span>, <span class="number">25600.</span>, <span class="number">9600.</span>, <span class="number">3600.</span>],</span><br><span class="line"></span><br><span class="line">[ <span class="number">110.</span>, <span class="number">120.</span>, <span class="number">12100.</span>, <span class="number">13200.</span>, <span class="number">14400.</span>]])</span><br></pre></td></tr></table></figure>

<p>上面的特征矩阵一共描述了 5 个特征，其中包括新的相关特征。我们可以看到上述矩阵中每个特征的度，如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.DataFrame(pf.powers_, columns=[<span class="string">'Attack_degree'</span>,<span class="string">'Defense_degree'</span>])</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f575a65683.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>基于这个输出，现在我们可以通过每个特征的度知道它实际上代表什么。在此基础上，现在我们可以对每个特征进行命名如下。这仅仅是为了便于理解，你可以给这些特征取更好的、容易使用和简单的名字。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">intr_features = pd.DataFrame(res, columns=[<span class="string">'Attack'</span>,<span class="string">'Defense'</span>,<span class="string">'Attack^2'</span>,<span class="string">'Attack x Defense'</span>,<span class="string">'Defense^2'</span>])</span><br><span class="line"></span><br><span class="line">intr_features.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f576e91376.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="数值型特征及其相关特征"><a href="#数值型特征及其相关特征" class="headerlink" title="数值型特征及其相关特征"></a>数值型特征及其相关特征</h5><p>因此上述数据代表了我们原始的特征以及它们的相关特征。</p>
<h4 id="分区间处理数据"><a href="#分区间处理数据" class="headerlink" title="分区间处理数据"></a>分区间处理数据</h4><p>处理原始、连续的数值型特征问题通常会导致这些特征值的分布被破坏。这表明有些值经常出现而另一些值出现非常少。除此之外，另一个问题是这些特征的值的变化范围。比如某个音乐视频的观看总数会非常大（<a href="https://www.youtube.com/watch?v=kJQP7kiw5Fk" target="_blank" rel="noopener">Despacito</a>，说你呢）而一些值会非常小。直接使用这些特征会产生很多问题，反而会影响模型表现。因此出现了处理这些问题的技巧，包括分区间法和变换。</p>
<p>分区间（Bining），也叫做量化，用于将连续型数值特征转换为离散型特征（类别）。可以认为这些离散值或数字是类别或原始的连续型数值被分区间或分组之后的数目。每个不同的区间大小代表某种密度，因此一个特定范围的连续型数值会落在里面。对数据做分区间的具体技巧包括等宽分区间以及自适应分区间。我们使用从 <a href="https://github.com/freeCodeCamp/2016-new-coder-survey" target="_blank" rel="noopener">2016 年 FreeCodeCamp 开发者和编码员调查报告</a>中抽取出来的一个子集中的数据，来讨论各种针对编码员和软件开发者的属性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df =pd.read_csv(<span class="string">'datasets/fcc_2016_coder_survey_subset.csv'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'EmploymentField'</span>, <span class="string">'Age'</span>,<span class="string">'Income'</span>]].head()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f578e01139.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="来自FCC编码员调查数据集的样本属性"><a href="#来自FCC编码员调查数据集的样本属性" class="headerlink" title="来自FCC编码员调查数据集的样本属性"></a>来自FCC编码员调查数据集的样本属性</h5><p>对于每个参加调查的编码员或开发者，ID.x 变量基本上是一个唯一的标识符而其他字段是可自我解释的。</p>
<h4 id="等宽分区间"><a href="#等宽分区间" class="headerlink" title="等宽分区间"></a>等宽分区间</h4><p>就像名字表明的那样，在等宽分区间方法中，每个区间都是固定宽度的，通常可以预先分析数据进行定义。基于一些领域知识、规则或约束，每个区间有个预先固定的值的范围，只有处于范围内的数值才被分配到该区间。基于数据舍入操作的分区间是一种方式，你可以使用数据舍入操作来对原始值进行分区间，我们前面已经讲过。</p>
<p>现在我们分析编码员调查报告数据集的 Age 特征并看看它的分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Age'</span>].hist(color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Age Histogram'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Age'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f57b05846b.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="描述开发者年龄分布的直方图"><a href="#描述开发者年龄分布的直方图" class="headerlink" title="描述开发者年龄分布的直方图"></a>描述开发者年龄分布的直方图</h5><p>上面的直方图表明，如预期那样，开发者年龄分布仿佛往左侧倾斜（上年纪的开发者偏少）。现在我们根据下面的模式，将这些原始年龄值分配到特定的区间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Age Range: Bin</span><br><span class="line"></span><br><span class="line">\---------------</span><br><span class="line"></span><br><span class="line"><span class="number">0</span> - <span class="number">9</span> : <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="number">10</span> - <span class="number">19</span> : <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="number">20</span> - <span class="number">29</span> : <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="number">30</span> - <span class="number">39</span> : <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="number">40</span> - <span class="number">49</span> : <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="number">50</span> - <span class="number">59</span> : <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="number">60</span> - <span class="number">69</span> : <span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="meta">... </span><span class="keyword">and</span> so on</span><br></pre></td></tr></table></figure>

<p>我们可以简单地使用我们先前学习到的数据舍入部分知识，先将这些原始年龄值除以 10，然后通过 floor 函数对原始年龄数值进行截断。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df[<span class="string">'Age_bin_round'</span>] = np.array(np.floor(np.array(fcc_survey_df[<span class="string">'Age'</span>]) / <span class="number">10.</span>))</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>,<span class="string">'Age_bin_round'</span>]].iloc[<span class="number">1071</span>:<span class="number">1076</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f57d916a6f.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="通过舍入法分区间"><a href="#通过舍入法分区间" class="headerlink" title="通过舍入法分区间"></a>通过舍入法分区间</h5><p>你可以看到基于数据舍入操作的每个年龄对应的区间。但是如果我们需要更灵活的操作怎么办？如果我们想基于我们的规则或逻辑，确定或修改区间的宽度怎么办？基于常用范围的分区间方法将帮助我们完成这个。让我们来定义一些通用年龄段位，使用下面的方式来对开发者年龄分区间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Age Range : Bin</span><br><span class="line"></span><br><span class="line">\---------------</span><br><span class="line"></span><br><span class="line"><span class="number">0</span> - <span class="number">15</span> : <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="number">16</span> - <span class="number">30</span> : <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="number">31</span> - <span class="number">45</span> : <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="number">46</span> - <span class="number">60</span> : <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="number">61</span> - <span class="number">75</span> : <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="number">75</span> - <span class="number">100</span> : <span class="number">6</span></span><br></pre></td></tr></table></figure>

<p>基于这些常用的分区间方式，我们现在可以对每个开发者年龄值的区间打标签，我们将存储区间的范围和相应的标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">bin_ranges = [<span class="number">0</span>, <span class="number">15</span>, <span class="number">30</span>, <span class="number">45</span>, <span class="number">60</span>, <span class="number">75</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">bin_names = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Age_bin_custom_range'</span>] = pd.cut(np.array(fcc_survey_df[<span class="string">'Age'</span>]),bins=bin_ranges)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Age_bin_custom_label'</span>] = pd.cut(np.array(fcc_survey_df[<span class="string">'Age'</span>]),bins=bin_ranges, labels=bin_names)</span><br><span class="line"></span><br><span class="line">\<span class="comment"># view the binned features</span></span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Age_bin_round'</span>,<span class="string">'Age_bin_custom_range'</span>,<span class="string">'Age_bin_custom_label'</span>]].iloc[<span class="number">10</span>a71:<span class="number">1076</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58143c35f.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="开发者年龄的常用分区间方式"><a href="#开发者年龄的常用分区间方式" class="headerlink" title="开发者年龄的常用分区间方式"></a>开发者年龄的常用分区间方式</h5><h4 id="自适应分区间"><a href="#自适应分区间" class="headerlink" title="自适应分区间"></a>自适应分区间</h4><p>使用等宽分区间的不足之处在于，我们手动决定了区间的值范围，而由于落在某个区间中的数据点或值的数目是不均匀的，因此可能会得到不规则的区间。一些区间中的数据可能会非常的密集，一些区间会非常稀疏甚至是空的！自适应分区间方法是一个更安全的策略，在这些场景中，我们让数据自己说话！这样，我们使用数据分布来决定区间的范围。</p>
<p>基于分位数的分区间方法是自适应分箱方法中一个很好的技巧。量化对于特定值或切点有助于将特定数值域的连续值分布划分为离散的互相挨着的区间。因此 q 分位数有助于将数值属性划分为 q 个相等的部分。关于量化比较流行的例子包括 2 分位数，也叫中值，将数据分布划分为2个相等的区间；4 分位数，也简称分位数，它将数据划分为 4 个相等的区间；以及 10 分位数，也叫十分位数，创建 10 个相等宽度的区间，现在让我们看看开发者数据集的 Income 字段的数据分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>].hist(bins=<span class="number">30</span>, color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Income Histogram'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Developer Income'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f583631eff.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p><strong>描述开发者收入分布的直方图</strong></p>
<p>上述的分布描述了一个在收入上右歪斜的分布，少数人赚更多的钱，多数人赚更少的钱。让我们基于自适应分箱方式做一个 4-分位数或分位数。我们可以很容易地得到如下的分位数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">quantile_list = [<span class="number">0</span>, <span class="number">.25</span>, <span class="number">.5</span>, <span class="number">.75</span>, <span class="number">1.</span>]</span><br><span class="line"></span><br><span class="line">quantiles =</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>].quantile(quantile_list)</span><br><span class="line"></span><br><span class="line">quantiles</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**Output**</span><br><span class="line"></span><br><span class="line">**------**</span><br><span class="line"></span><br><span class="line"><span class="number">0.00</span> <span class="number">6000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">0.25</span> <span class="number">20000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">0.50</span> <span class="number">37000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">0.75</span> <span class="number">60000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">1.00</span> <span class="number">200000.0</span></span><br><span class="line"></span><br><span class="line">Name: Income, dtype: float64</span><br></pre></td></tr></table></figure>

<p>现在让我们在原始的分布直方图中可视化下这些分位数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>].hist(bins=<span class="number">30</span>, color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> quantile <span class="keyword">in</span> quantiles:</span><br><span class="line"></span><br><span class="line">qvl = plt.axvline(quantile, color=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">ax.legend([qvl], [<span class="string">'Quantiles'</span>], fontsize=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Income Histogram with Quantiles'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Developer Income'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f5853f1a2c.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="带分位数形式描述开发者收入分布的直方图"><a href="#带分位数形式描述开发者收入分布的直方图" class="headerlink" title="带分位数形式描述开发者收入分布的直方图"></a>带分位数形式描述开发者收入分布的直方图</h5><p>上面描述的分布中红色线代表了分位数值和我们潜在的区间。让我们利用这些知识来构建我们基于分区间策略的分位数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">quantile_labels = [<span class="string">'0-25Q'</span>, <span class="string">'25-50Q'</span>, <span class="string">'50-75Q'</span>, <span class="string">'75-100Q'</span>]</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_quantile_range'</span>] = pd.qcut(</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>],q=quantile_list)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_quantile_label'</span>] = pd.qcut(</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>],q=quantile_list,labels=quantile_labels)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Income'</span>,<span class="string">'Income_quantile_range'</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">'Income_quantile_label'</span>]].iloc[<span class="number">4</span>:<span class="number">9</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f586dbd8f4.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="基于分位数的开发者收入的区间范围和标签"><a href="#基于分位数的开发者收入的区间范围和标签" class="headerlink" title="基于分位数的开发者收入的区间范围和标签"></a>基于分位数的开发者收入的区间范围和标签</h5><p>通过这个例子，你应该对如何做基于分位数的自适应分区间法有了一个很好的认识。一个需要重点记住的是，分区间的结果是离散值类型的分类特征，当你在模型中使用分类数据之前，可能需要额外的特征工程相关步骤。我们将在接下来的部分简要地讲述分类数据的特征工程技巧。</p>
<h4 id="统计变换"><a href="#统计变换" class="headerlink" title="统计变换"></a>统计变换</h4><p>我们讨论下先前简单提到过的数据分布倾斜的负面影响。现在我们可以考虑另一个特征工程技巧，即利用统计或数学变换。我们试试看 Log 变换和 Box-Cox 变换。这两种变换函数都属于幂变换函数簇，通常用来创建单调的数据变换。它们的主要作用在于它能帮助稳定方差，始终保持分布接近于正态分布并使得数据与分布的平均值无关。</p>
<h4 id="Log变换"><a href="#Log变换" class="headerlink" title="Log变换"></a>Log变换</h4><p>log 变换属于幂变换函数簇。该函数用数学表达式表示为</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f588a0f6a5.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>读为以 b 为底 x 的对数等于 y。这可以变换为</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f589e77242.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>表示以b为底指数必须达到多少才等于x。自然对数使用 b=e，e=2.71828，通常叫作欧拉常数。你可以使用通常在十进制系统中使用的 b=10 作为底数。</p>
<p><strong>当应用于倾斜分布时 Log 变换是很有用的，因为他们倾向于拉伸那些落在较低的幅度范围内自变量值的范围，倾向于压缩或减少更高幅度范围内的自变量值的范围</strong>。从而使得倾斜分布尽可能的接近正态分布。让我们对先前使用的开发者数据集的 Income 特征上使用log变换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df[<span class="string">'Income_log'</span>] = np.log((<span class="number">1</span>+fcc_survey_df[<span class="string">'Income'</span>]))</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Income'</span>,<span class="string">'Income_log'</span>]].iloc[<span class="number">4</span>:<span class="number">9</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58b3ed249.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="开发者收入log变换后结构"><a href="#开发者收入log变换后结构" class="headerlink" title="开发者收入log变换后结构"></a>开发者收入log变换后结构</h5><p>Income_log 字段描述了经过 log 变换后的特征。现在让我们来看看字段变换后数据的分布。</p>
<p>基于上面的图，我们可以清楚地看到与先前倾斜分布相比，该分布更加像正态分布或高斯分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">income_log_mean =np.round(np.mean(fcc_survey_df[<span class="string">'Income_log'</span>]), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_log'</span>].hist(bins=<span class="number">30</span>,color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">plt.axvline(income_log_mean, color=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Income Histogram after Log Transform'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Developer Income (log scale)'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.text(<span class="number">11.5</span>, <span class="number">450</span>, <span class="string">r'$\mu$='</span>+str(income_log_mean),fontsize=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58cdaf02a.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p><strong>经过log变换后描述开发者收入分布的直方图</strong></p>
<h4 id="Box-Cox变换"><a href="#Box-Cox变换" class="headerlink" title="Box-Cox变换"></a>Box-Cox变换</h4><p>Box-Cox 变换是另一个流行的幂变换函数簇中的一个函数。该函数有一个前提条件，即数值型值必须先变换为正数（与 log 变换所要求的一样）。万一出现数值是负的，使用一个常数对数值进行偏移是有帮助的。数学上，Box-Cox 变换函数可以表示如下。</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58e556c08.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>生成的变换后的输出y是输入 x 和变换参数的函数；当 λ=0 时，该变换就是自然对数 log 变换，前面我们已经提到过了。λ 的最佳取值通常由最大似然或最大对数似然确定。现在让我们在开发者数据集的收入特征上应用 Box-Cox 变换。首先我们从数据分布中移除非零值得到最佳的值，结果如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">income = np.array(fcc_survey_df[<span class="string">'Income'</span>])</span><br><span class="line"></span><br><span class="line">income_clean = income[~np.isnan(income)]</span><br><span class="line"></span><br><span class="line">l, opt_lambda = spstats.boxcox(income_clean)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Optimal lambda value:'</span>, opt_lambda)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**Output**</span><br><span class="line"></span><br><span class="line">**------**</span><br><span class="line"></span><br><span class="line">Optimal <span class="keyword">lambda</span> value: <span class="number">0.117991239456</span></span><br></pre></td></tr></table></figure>

<p>现在我们得到了最佳的值，让我们在取值为 0 和 λ（最佳取值 λ ）时使用 Box-Cox 变换对开发者收入特征进行变换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df[<span class="string">'Income_boxcox_lambda_0'</span>] = spstats.boxcox((<span class="number">1</span>+fcc_survey_df[<span class="string">'Income'</span>]),lmbda=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_boxcox_lambda_opt'</span>] = spstats.boxcox(fcc_survey_df[<span class="string">'Income'</span>],lmbda=opt_lambda)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Income'</span>, <span class="string">'Income_log'</span>,<span class="string">'Income_boxcox_lambda_0'</span>,<span class="string">'Income_boxcox_lambda_opt'</span>]].iloc[<span class="number">4</span>:<span class="number">9</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58fd7fd5e.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="经过-Box-Cox-变换后开发者的收入分布"><a href="#经过-Box-Cox-变换后开发者的收入分布" class="headerlink" title="经过 Box-Cox 变换后开发者的收入分布"></a>经过 Box-Cox 变换后开发者的收入分布</h5><p>变换后的特征在上述数据框中描述了。就像我们期望的那样，Income_log 和 Income_boxcox_lamba_0具有相同的取值。让我们看看经过最佳λ变换后 Income 特征的分布。</p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;income_boxcox_mean = np.round(np.mean(fcc_survey_df[<span class="string">'Income_boxcox_lambda_opt'</span>]),<span class="number">2</span>)</span><br><span class="line">&gt; </span><br><span class="line">&gt;fig, ax = plt.subplots()</span><br><span class="line">&gt; </span><br><span class="line">&gt;fcc_survey_df[<span class="string">'Income_boxcox_lambda_opt'</span>].hist(bins=<span class="number">30</span>,  color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>, grid=<span class="literal">False</span>)</span><br><span class="line">&gt;    plt.axvline(income_boxcox_mean, color=<span class="string">'r'</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.set_title(<span class="string">'Developer Income Histogram after Box–Cox Transform'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.set_xlabel(<span class="string">'Developer Income (Box–Cox transform)'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.text(<span class="number">24</span>, <span class="number">450</span>, <span class="string">r'$\mu$='</span>+str(income_boxcox_mean),fontsize=<span class="number">10</span>)       </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f591679bfb.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p><strong>经过Box-Cox变换后描述开发者收入分布的直方图</strong></p>
<p> 分布看起来更像是正态分布，与我们经过 log 变换后的分布相似。</p>
<h3 id="类别型数据上的特征工程"><a href="#类别型数据上的特征工程" class="headerlink" title="类别型数据上的特征工程"></a>类别型数据上的特征工程</h3><p>在深入研究特征工程之前，让我们先了解一下分类数据。通常，在<strong>自然界中可分类的任意数据属性都是离散值，这意味着它们属于某一特定的有限类别</strong>。在模型预测的属性或者变量（通常被称为<strong>响应变量 response variables</strong>）中，这些也经常被称为类别或者标签。这些离散值在自然界中可以是文本或者数字（甚至是诸如图像这样的非结构化数据）。分类数据有两大类——<strong>定类（Nominal）和定序（Ordinal）</strong>。</p>
<p>在任意定类分类数据属性中，这些属性值之间<strong>没有顺序的概念</strong>。如下图所示，举个简单的例子，天气分类。我们可以看到，在这个特定的场景中，主要有六个大类，而这些类之间没有任何顺序上的关系（刮风天并不总是发生在晴天之前，并且也不能说比晴天来的更小或者更大）</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af6bc87b4e.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>将天气作为分类属性</p>
<p>与天气相类似的属性还有很多，比如电影、音乐、电子游戏、国家、食物和美食类型等等，这些都属于定类分类属性。</p>
<p>定序分类的属性值则存在着一定的顺序意义或概念。例如，下图中的字母标识了衬衫的大小。显而易见的是，当我们考虑衬衫的时候，它的“大小”属性是很重要的（S 码比 M 码来的小，而 M 码又小于 L 码等等）。</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af6d3b83ac.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>衬衫大小作为定序分类属性</p>
<p>鞋号、受教育水平和公司职位则是定序分类属性的一些其它例子。既然已经对分类数据有了一个大致的理解之后，接下来我们来看看一些特征工程的策略。</p>
<p>在接受像文本标签这样复杂的分类数据类型问题上，各种机器学习框架均已取得了许多的进步。通常，特征工程中的任意标准工作流都涉及将这些分类值转换为数值标签的某种形式，然后对这些值应用一些<strong>编码方案</strong>。我们将在开始之前导入必要的工具包。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<h4 id="定类属性转换-LabelEncoding"><a href="#定类属性转换-LabelEncoding" class="headerlink" title="定类属性转换(LabelEncoding)"></a>定类属性转换(LabelEncoding)</h4><p><strong>定类属性由离散的分类值组成，它们没有先后顺序概念</strong>。这里的思想是将这些属性转换成更具代表性的数值格式，这样可以很容易被下游的代码和流水线所理解。我们来看一个关于视频游戏销售的新数据集。这个数据集也可以在 <a href="https://www.kaggle.com/gregorut/videogamesales" target="_blank" rel="noopener">Kaggle</a> 和我的 <a href="https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/notebooks/Ch04_Feature_Engineering_and_Selection" target="_blank" rel="noopener">GitHub</a> 仓库中找到。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vg_df = pd.read_csv(<span class="string">'datasets/vgsales.csv'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">vg_df[[<span class="string">'Name'</span>, <span class="string">'Platform'</span>, <span class="string">'Year'</span>, <span class="string">'Genre'</span>, <span class="string">'Publisher'</span>]].iloc[<span class="number">1</span>:<span class="number">7</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af756b687d.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>游戏销售数据</p>
<p>让我们首先专注于上面数据框中“视频游戏风格（Genre）”属性。显而易见的是，这是一个类似于“发行商（Publisher）”和“平台（Platform）”属性一样的定类分类属性。我们可以很容易得到一个独特的视频游戏风格列表，如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">genres = np.unique(vg_df[<span class="string">'Genre'</span>])</span><br><span class="line"></span><br><span class="line">genres</span><br><span class="line"></span><br><span class="line">Output</span><br><span class="line"></span><br><span class="line">\------</span><br><span class="line"></span><br><span class="line">array([<span class="string">'Action'</span>, <span class="string">'Adventure'</span>, <span class="string">'Fighting'</span>, <span class="string">'Misc'</span>, <span class="string">'Platform'</span>, <span class="string">'Puzzle'</span>, <span class="string">'Racing'</span>, <span class="string">'Role-Playing'</span>, <span class="string">'Shooter'</span>, <span class="string">'Simulation'</span>, <span class="string">'Sports'</span>, <span class="string">'Strategy'</span>], dtype=object)</span><br></pre></td></tr></table></figure>

<p>输出结果表明，我们有 12 种不同的视频游戏风格。我们现在可以生成一个标签编码方法，即利用 scikit-learn 将每个类别映射到一个数值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line">gle = LabelEncoder()</span><br><span class="line"></span><br><span class="line">genre_labels = gle.fit_transform(vg_df[<span class="string">'Genre'</span>])</span><br><span class="line"></span><br><span class="line">genre_mappings = &#123;index: label <span class="keyword">for</span> index, label <span class="keyword">in</span> enumerate(gle.classes_)&#125;</span><br><span class="line"></span><br><span class="line">genre_mappings</span><br><span class="line"></span><br><span class="line">Output</span><br><span class="line"></span><br><span class="line">\------</span><br><span class="line"></span><br><span class="line">&#123;<span class="number">0</span>: <span class="string">'Action'</span>, <span class="number">1</span>: <span class="string">'Adventure'</span>, <span class="number">2</span>: <span class="string">'Fighting'</span>, <span class="number">3</span>: <span class="string">'Misc'</span>, <span class="number">4</span>: <span class="string">'Platform'</span>, <span class="number">5</span>: <span class="string">'Puzzle'</span>, <span class="number">6</span>: <span class="string">'Racing'</span>, <span class="number">7</span>: <span class="string">'Role-Playing'</span>, <span class="number">8</span>: <span class="string">'Shooter'</span>, <span class="number">9</span>: <span class="string">'Simulation'</span>, <span class="number">10</span>: <span class="string">'Sports'</span>, <span class="number">11</span>: <span class="string">'Strategy'</span>&#125;</span><br></pre></td></tr></table></figure>

<p>因此，在 <em>LabelEncoder</em> 类的实例对象 <em>gle</em> 的帮助下生成了一个映射方案，成功地将每个风格属性映射到一个数值。转换后的标签存储在 <em>genre_labels</em> 中，该变量允许我们将其写回数据表中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vg_df[<span class="string">'GenreLabel'</span>] = genre_labels</span><br><span class="line"></span><br><span class="line">vg_df[[<span class="string">'Name'</span>, <span class="string">'Platform'</span>, <span class="string">'Year'</span>, <span class="string">'Genre'</span>, <span class="string">'GenreLabel'</span>]].iloc[<span class="number">1</span>:<span class="number">7</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af8164e6db.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>视频游戏风格及其编码标签</p>
<p>如果你打算将它们用作预测的响应变量，那么这些标签通常可以直接用于诸如 sikit-learn 这样的框架。但是如前所述，我们还需要额外的编码步骤才能将它们用作特征。</p>
<h4 id="定序属性编码"><a href="#定序属性编码" class="headerlink" title="定序属性编码"></a>定序属性编码</h4><p><strong>定序属性是一种带有先后顺序概念的分类属性</strong>。这里我将以本系列文章第一部分所使用的<a href="https://www.kaggle.com/abcsds/pokemon/data" target="_blank" rel="noopener">神奇宝贝数据集</a>进行说明。让我们先专注于 「世代（Generation）」 属性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; poke_df = pd.read_csv(<span class="string">'datasets/Pokemon.csv'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; poke_df = poke_df.sample(random_state=<span class="number">1</span>, frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; np.unique(poke_df[<span class="string">'Generation'</span>])</span><br><span class="line">&gt;</span><br><span class="line">&gt; Output</span><br><span class="line">&gt;</span><br><span class="line">&gt; \------</span><br><span class="line">&gt;</span><br><span class="line">&gt; array([<span class="string">'Gen 1'</span>, <span class="string">'Gen 2'</span>, <span class="string">'Gen 3'</span>, <span class="string">'Gen 4'</span>, <span class="string">'Gen 5'</span>, <span class="string">'Gen 6'</span>], dtype=object)</span><br></pre></td></tr></table></figure>

<p>根据上面的输出，我们可以看到一共有 6 代，并且每个神奇宝贝通常属于视频游戏的特定世代（依据发布顺序），而且电视系列也遵循了相似的时间线。这个属性通常是定序的（需要相关的领域知识才能理解），因为属于第一代的大多数神奇宝贝在第二代的视频游戏或者电视节目中也会被更早地引入。神奇宝贝的粉丝们可以看下下图，然后记住每一代中一些比较受欢迎的神奇宝贝（不同的粉丝可能有不同的看法）。</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af8f58f535.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>基于不同类型和世代选出的一些受欢迎的神奇宝贝</p>
<p>因此，它们之间存在着先后顺序。一般来说，没有通用的模块或者函数可以根据这些顺序自动将这些特征转换和映射到数值表示。因此，我们可以使用自定义的编码\映射方案。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">gen_ord_map = &#123;<span class="string">'Gen 1'</span>: <span class="number">1</span>, <span class="string">'Gen 2'</span>: <span class="number">2</span>, <span class="string">'Gen 3'</span>: <span class="number">3</span>, <span class="string">'Gen 4'</span>: <span class="number">4</span>, <span class="string">'Gen 5'</span>: <span class="number">5</span>, <span class="string">'Gen 6'</span>: <span class="number">6</span>&#125; </span><br><span class="line"></span><br><span class="line">poke_df[<span class="string">'GenerationLabel'</span>] = poke_df[<span class="string">'Generation'</span>].map(gen_ord_map)</span><br><span class="line"></span><br><span class="line">poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'GenerationLabel'</span>]].iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af95f94dc8.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>神奇宝贝世代编码</p>
<p>从上面的代码中可以看出，来自 <em>pandas</em> 库的 <em>map(…)</em> 函数在转换这种定序特征的时候非常有用。</p>
<h4 id="编码分类属性–独热编码方案（One-hot-Encoding-Scheme）"><a href="#编码分类属性–独热编码方案（One-hot-Encoding-Scheme）" class="headerlink" title="编码分类属性–独热编码方案（One-hot Encoding Scheme）"></a>编码分类属性–独热编码方案（One-hot Encoding Scheme）</h4><p>如果你还记得我们之前提到过的内容，通常对分类数据进行特征工程就涉及到一个转换过程，我们在前一部分描述了一个转换过程，还有一个强制编码过程，我们应用特定的编码方案为特定的每个类别创建虚拟变量或特征分类属性。</p>
<p>你可能想知道，我们刚刚在上一节说到将类别转换为数字标签，为什么现在我们又需要这个？原因很简单。考虑到视频游戏风格，如果我们直接将 <em>GenereLabel</em> 作为属性特征提供给机器学习模型，则模型会认为它是一个连续的数值特征，从而认为值 10 （体育）要大于值 6 （赛车），然而事实上这种信息是毫无意义的，因为<em>体育类型</em>显然并不大于或者小于<em>赛车类型</em>，这些不同值或者类别无法直接进行比较。因此我们需要另一套编码方案层，它要能为每个属性的所有不同类别中的每个唯一值或类别创建虚拟特征。</p>
<p>考虑到任意具有 m 个标签的分类属性（变换之后）的数字表示，独热编码方案将该属性编码或变换成 m 个二进制特征向量（向量中的每一维的值只能为 0 或 1）。那么在这个分类特征中每个属性值都被转换成一个 m 维的向量，其中只有某一维的值为 1。让我们来看看神奇宝贝数据集的一个子集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Legendary'</span>]].iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af9b37bf97.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>神奇宝贝数据集子集</p>
<p>这里关注的属性是神奇宝贝的「世代（Generation）」和「传奇（Legendary）」状态。第一步是根据之前学到的将这些属性转换为数值表示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder, LabelEncoder</span><br><span class="line"></span><br><span class="line">\<span class="comment"># transform and map pokemon generations</span></span><br><span class="line"></span><br><span class="line">gen_le = LabelEncoder()</span><br><span class="line"></span><br><span class="line">gen_labels = gen_le.fit_transform(poke_df[<span class="string">'Generation'</span>])</span><br><span class="line"></span><br><span class="line">poke_df[<span class="string">'Gen_Label'</span>] = gen_labels</span><br><span class="line"></span><br><span class="line">\<span class="comment"># transform and map pokemon legendary status</span></span><br><span class="line"></span><br><span class="line">leg_le = LabelEncoder()</span><br><span class="line"></span><br><span class="line">leg_labels = leg_le.fit_transform(poke_df[<span class="string">'Legendary'</span>])</span><br><span class="line"></span><br><span class="line">poke_df[<span class="string">'Lgnd_Label'</span>] = leg_labels</span><br><span class="line"></span><br><span class="line">poke_df_sub = poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>, <span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>]]</span><br><span class="line"></span><br><span class="line">poke_df_sub.iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afa18d27fc.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>转换后的标签属性</p>
<p><em>Gen_Label</em> 和 <em>Lgnd_Label</em> 特征描述了我们分类特征的数值表示。现在让我们在这些特征上应用独热编码方案。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encode generation labels using one-hot encoding scheme</span></span><br><span class="line"></span><br><span class="line">gen_ohe = OneHotEncoder()</span><br><span class="line"></span><br><span class="line">gen_feature_arr = gen_ohe.fit_transform(poke_df[[<span class="string">'Gen_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">gen_feature_labels = list(gen_le.classes_)</span><br><span class="line"></span><br><span class="line">gen_features = pd.DataFrame(gen_feature_arr, columns=gen_feature_labels)</span><br><span class="line"></span><br><span class="line">\<span class="comment"># encode legendary status labels using one-hot encoding scheme</span></span><br><span class="line"></span><br><span class="line">leg_ohe = OneHotEncoder()</span><br><span class="line"></span><br><span class="line">leg_feature_arr = leg_ohe.fit_transform(poke_df[[<span class="string">'Lgnd_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">leg_feature_labels = [<span class="string">'Legendary_'</span>+str(cls_label) <span class="keyword">for</span> cls_label <span class="keyword">in</span> leg_le.classes_]</span><br><span class="line"></span><br><span class="line">leg_features = pd.DataFrame(leg_feature_arr, columns=leg_feature_labels)</span><br></pre></td></tr></table></figure>

<p>通常来说，你可以使用 <em>fit_transform</em> 函数将两个特征一起编码（通过将两个特征的二维数组一起传递给函数，详情<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" target="_blank" rel="noopener">查看文档</a>）。但是我们分开编码每个特征，这样可以更易于理解。除此之外，我们还可以创建单独的数据表并相应地标记它们。现在让我们链接这些特征表（Feature frames）然后看看最终的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">poke_df_ohe = pd.concat([poke_df_sub, gen_features, leg_features], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">columns = sum([[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>], gen_feature_labels, [<span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>], leg_feature_labels], [])</span><br><span class="line"></span><br><span class="line">poke_df_ohe[columns].iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afab9940ae.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>神奇宝贝世代和传奇状态的独热编码特征</p>
<p>此时可以看到已经为「世代（Generation）」生成 6 个虚拟变量或者二进制特征，并为「传奇（Legendary）」生成了 2 个特征。这些特征数量是这些属性中不同类别的总数。<strong>某一类别的激活状态通过将对应的虚拟变量置 1 来表示</strong>，这从上面的数据表中可以非常明显地体现出来。</p>
<p>考虑你在训练数据上建立了这个编码方案，并建立了一些模型，现在你有了一些新的数据，这些数据必须在预测之前进行如下设计。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_poke_df = pd.DataFrame([[<span class="string">'PikaZoom'</span>, <span class="string">'Gen 3'</span>, <span class="literal">True</span>], [<span class="string">'CharMyToast'</span>, <span class="string">'Gen 4'</span>, <span class="literal">False</span>]], columns=[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Legendary'</span>])</span><br><span class="line"></span><br><span class="line">new_poke_df</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afaf0cb42e.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>新数据</p>
<p>你可以通过调用之前构建的 <em>LabelEncoder</em> 和 <em>OneHotEncoder</em> 对象的 <em>transform()</em> 方法来处理新数据。请记得我们的工作流程，首先我们要做转换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">new_gen_labels = gen_le.transform(new_poke_df[<span class="string">'Generation'</span>])</span><br><span class="line"></span><br><span class="line">new_poke_df[<span class="string">'Gen_Label'</span>] = new_gen_labels</span><br><span class="line"></span><br><span class="line">new_leg_labels = leg_le.transform(new_poke_df[<span class="string">'Legendary'</span>])</span><br><span class="line"></span><br><span class="line">new_poke_df[<span class="string">'Lgnd_Label'</span>] = new_leg_labels</span><br><span class="line"></span><br><span class="line">new_poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>, <span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>]]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afb428f0dc.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>转换之后的分类属性</p>
<p>在得到了数值标签之后，接下来让我们应用编码方案吧！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">new_gen_feature_arr = gen_ohe.transform(new_poke_df[[<span class="string">'Gen_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">new_gen_features = pd.DataFrame(new_gen_feature_arr, columns=gen_feature_labels)</span><br><span class="line"></span><br><span class="line">new_leg_feature_arr = leg_ohe.transform(new_poke_df[[<span class="string">'Lgnd_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">new_leg_features = pd.DataFrame(new_leg_feature_arr, columns=leg_feature_labels)</span><br><span class="line"></span><br><span class="line">new_poke_ohe = pd.concat([new_poke_df, new_gen_features, new_leg_features], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">columns = sum([[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>], gen_feature_labels, [<span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>], leg_feature_labels], [])</span><br><span class="line"></span><br><span class="line">new_poke_ohe[columns]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afb91bb3be.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>独热编码之后的分类属性</p>
<p>因此，通过利用 scikit-learn 强大的 API，我们可以很容易将编码方案应用于新数据。</p>
<p>你也可以通过利用来自 pandas 的 <em>to_dummies()</em> 函数轻松应用独热编码方案。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gen_onehot_features = pd.get_dummies(poke_df[<span class="string">'Generation'</span>])</span><br><span class="line"></span><br><span class="line">pd.concat([poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>]], gen_onehot_features], axis=<span class="number">1</span>).iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afbcc0ff2b.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>使用 pandas 实现的独热编码特征</p>
<p>上面的数据表描述了应用在「世代（Generation）」属性上的独热编码方案，结果与之前的一致。</p>
<h4 id="区间计数方案（Bin-counting-Scheme）"><a href="#区间计数方案（Bin-counting-Scheme）" class="headerlink" title="区间计数方案（Bin-counting Scheme）"></a>区间计数方案（Bin-counting Scheme）</h4><p>到目前为止，我们所讨论的编码方案在分类数据方面效果还不错，但是当任意特征的不同类别数量变得很大的时候，问题开始出现。对于具有 m 个不同标签的任意分类特征这点非常重要，你将得到 m 个独立的特征。这会很容易地增加特征集的大小，从而导致在时间、空间和内存方面出现存储问题或者模型训练问题。除此之外，我们还必须处理“<a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" target="_blank" rel="noopener">维度诅咒</a>”问题，通常指的是拥有大量的特征，却缺乏足够的代表性样本，然后模型的性能开始受到影响并导致过拟合。</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afd0459749.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>因此，我们需要针对那些可能具有非常多种类别的特征（如 IP 地址），研究其它分类数据特征工程方案。区间计数方案是处理具有多个类别的分类变量的有效方案。在这个方案中，我们使用<strong>基于概率的统计信息和在建模过程中所要预测的实际目标或者响应值</strong>，而不是使用实际的标签值进行编码。一个简单的例子是，基于过去的 IP 地址历史数据和 DDOS 攻击中所使用的历史数据，我们可以为任一 IP 地址会被 DDOS 攻击的可能性建立概率模型。使用这些信息，我们可以对输入特征进行编码，该输入特征描述了如果将来出现相同的 IP 地址，则引起 DDOS 攻击的概率值是多少。<strong>这个方案需要历史数据作为先决条件，并且要求数据非常详尽。</strong></p>
<h4 id="特征哈希方案"><a href="#特征哈希方案" class="headerlink" title="特征哈希方案"></a>特征哈希方案</h4><p>特征哈希方案（Feature Hashing Scheme）是处理大规模分类特征的另一个有用的特征工程方案。在该方案中，哈希函数通常与预设的编码特征的数量（作为预定义长度向量）一起使用，使得特征的哈希值被用作这个预定义向量中的索引，并且值也要做相应的更新。由于哈希函数将大量的值映射到一个小的有限集合中，因此<strong>多个不同值可能会创建相同的哈希</strong>，这一现象称为<strong>冲突</strong>。典型地，使用带符号的哈希函数，使得从哈希获得的值的符号被用作那些在适当的索引处存储在最终特征向量中的值的符号。这样能够确保实现较少的冲突和由于冲突导致的误差累积。</p>
<p>哈希方案适用于字符串、数字和其它结构（如向量）。你可以将哈希输出看作一个有限的 <em>b bins</em> 集合，以便于当将哈希函数应用于相同的值\类别时，哈希函数能根据哈希值将其分配到 <em>b bins</em> 中的同一个 bin（或者 bins 的子集）。我们可以预先定义 <em>b</em> 的值，它成为我们使用特征哈希方案编码的每个分类属性的编码特征向量的最终尺寸。</p>
<p>因此，即使我们有一个特征拥有超过 <strong>1000</strong> 个不同的类别，我们设置 <strong>b = 10</strong> 作为最终的特征向量长度，那么最终输出的特征将只有 10 个特征。而采用独热编码方案则有 1000 个二进制特征。我们来考虑下视频游戏数据集中的「风格（Genre）」属性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">unique_genres = np.unique(vg_df[[<span class="string">'Genre'</span>]])</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Total game genres:"</span>, len(unique_genres))</span><br><span class="line"></span><br><span class="line">print(unique_genres)</span><br><span class="line"></span><br><span class="line">Output</span><br><span class="line"></span><br><span class="line">\------</span><br><span class="line"></span><br><span class="line">Total game genres: <span class="number">12</span></span><br><span class="line"></span><br><span class="line">[<span class="string">'Action'</span> <span class="string">'Adventure'</span> <span class="string">'Fighting'</span> <span class="string">'Misc'</span> <span class="string">'Platform'</span> <span class="string">'Puzzle'</span> <span class="string">'Racing'</span> <span class="string">'Role-Playing'</span> <span class="string">'Shooter'</span> <span class="string">'Simulation'</span> <span class="string">'Sports'</span> <span class="string">'Strategy'</span>]</span><br></pre></td></tr></table></figure>

<p>我们可以看到，总共有 12 中风格的游戏。如果我们在“风格”特征中采用独热编码方案，则将得到 12 个二进制特征。而这次，我们将通过 scikit-learn 的 <em>FeatureHasher</em> 类来使用特征哈希方案，该类使用了一个有符号的 32 位版本的 <em>Murmurhash3</em> 哈希函数。在这种情况下，我们将预先定义最终的特征向量大小为 6。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> FeatureHasher</span><br><span class="line"></span><br><span class="line">fh = FeatureHasher(n_features=<span class="number">6</span>, input_type=<span class="string">'string'</span>)</span><br><span class="line"></span><br><span class="line">hashed_features = fh.fit_transform(vg_df[<span class="string">'Genre'</span>])</span><br><span class="line"></span><br><span class="line">hashed_features = hashed_features.toarray()pd.concat([vg_df[[<span class="string">'Name'</span>, <span class="string">'Genre'</span>]], pd.DataFrame(hashed_features)], axis=<span class="number">1</span>).iloc[<span class="number">1</span>:<span class="number">7</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afd62f2a51.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>风格属性的特征哈希</p>
<p>基于上述输出，「风格（Genre）」属性已经使用哈希方案编码成 6 个特征而不是 12 个。我们还可以看到，第 1 行和第 6 行表示相同风格的游戏「平台（Platform）」，而它们也被正确编码成了相同的特征向量。</p>
<h3 id="时间型"><a href="#时间型" class="headerlink" title="时间型"></a>时间型</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBQ8OS.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQrOU.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQcTJ.jpg" alt="avatar"></p>
<h3 id="文本型"><a href="#文本型" class="headerlink" title="文本型"></a>文本型</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBQhSx.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQIOO.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQzX8.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBlC7Q.jpg" alt="avatar"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/特征工程/">特征工程</a><a class="article-category-link" href="/categories/特征工程/模型调优/">模型调优</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/模型调优/">模型调优</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/特征工程/">特征工程</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-SVM" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/02/20/SVM/" class="article-date">
      <time datetime="2020-02-20T06:17:41.000Z" itemprop="datePublished">2020-02-20</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/20/SVM/">SVM</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <ul>
<li><a href="https://zhuanlan.zhihu.com/p/36332083" target="_blank" rel="noopener">攀登传统机器学习的珠峰-SVM (上)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/36379394" target="_blank" rel="noopener">攀登传统机器学习的珠峰-SVM (中)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/36535299" target="_blank" rel="noopener">攀登传统机器学习的珠峰-SVM (下)</a></li>
</ul>
<h2 id="机器学习中的SVM"><a href="#机器学习中的SVM" class="headerlink" title="机器学习中的SVM"></a>机器学习中的SVM</h2><p>支持向量机是一种经典的二分类模型，基本模型定义为特征空间中最大间隔的线性分类器，其学习的优化目标便是间隔最大化，因此支持向量机本身可以转化为一个凸二次规划求解的问题。</p>
<h3 id="函数间隔与几何间隔"><a href="#函数间隔与几何间隔" class="headerlink" title="函数间隔与几何间隔"></a>函数间隔与几何间隔</h3><p>对于二分类学习，假设现在的数据是线性可分的，这时分类学习最基本的想法就是找到一个合适的超平面，该超平面能够将不同类别的样本分开，类似二维平面使用ax+by+c=0来表示，超平面实际上表示的就是高维的平面，如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f6a2ec8a.png" alt="1.png"></p>
<p>对数据点进行划分时，易知：当超平面距离与它最近的数据点的间隔越大，分类的鲁棒性越好，即当新的数据点加入时，超平面对这些点的适应性最强，出错的可能性最小。因此需要让所选择的超平面能够最大化这个间隔Gap（如下图所示）， 常用的间隔定义有两种，一种称之为函数间隔，一种为几何间隔，下面将分别介绍这两种间隔，并对SVM为什么会选用几何间隔做了一些阐述。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f6a06d5a.png" alt="2.png"></p>
<h4 id="函数间隔"><a href="#函数间隔" class="headerlink" title="函数间隔"></a>函数间隔</h4><p>在超平面w’x+b=0确定的情况下，|w’x<em>+b|能够代表点x*距离超平面的远近，易知：当w’x</em>+b&gt;0时，表示x<em>在超平面的一侧（正类，类标为1），而当w’x</em>+b&lt;0时，则表示x<em>在超平面的另外一侧（负类，类别为-1），因此（w’x</em>+b）y* 的正负性恰能表示数据点x<em>是否被分类正确。于是便引出了*</em>函数间隔**的定义（functional margin）:</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f690a14b.png" alt="3.png"></p>
<p>而超平面（w,b）关于所有样本点（Xi，Yi）的函数间隔最小值则为超平面在训练数据集T上的函数间隔：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f690ac26.png" alt="4.png"></p>
<p>可以看出：这样定义的函数间隔在处理SVM上会有问题，当超平面的两个参数w和b同比例改变时，函数间隔也会跟着改变，但是实际上超平面还是原来的超平面，并没有变化。例如：w1x1+w2x2+w3x3+b=0其实等价于2w1x1+2w2x2+2w3x3+2b=0，但计算的函数间隔却翻了一倍。从而引出了能真正度量点到超平面距离的概念–几何间隔（geometrical margin）。</p>
<h4 id="几何间隔"><a href="#几何间隔" class="headerlink" title="几何间隔"></a>几何间隔</h4><p><strong>几何间隔</strong>代表的则是数据点到超平面的真实距离，对于超平面w’x+b=0，w代表的是该超平面的法向量，设x<em>为超平面外一点x在法向量w方向上的投影点，x与超平面的距离为r，则有x</em>=x-r(w/||w||)，又x<em>在超平面上，即w’x</em>+b=0，代入即可得：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f697d499.png" alt="5.png"></p>
<p>为了得到r的绝对值，令r呈上其对应的类别y，即可得到几何间隔的定义：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f696fd10.png" alt="6.png"></p>
<p>从上述函数间隔与几何间隔的定义可以看出：实质上函数间隔就是|w’x+b|，而几何间隔就是点到超平面的距离。</p>
<h3 id="最大间隔与支持向量"><a href="#最大间隔与支持向量" class="headerlink" title="最大间隔与支持向量"></a>最大间隔与支持向量</h3><p>通过前面的分析可知：函数间隔不适合用来最大化间隔，因此这里我们要找的最大间隔指的是几何间隔，于是最大间隔分类器的目标函数定义为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f69af163.png" alt="7.png"></p>
<p>一般地，我们令r^为1（这样做的目的是为了方便推导和目标函数的优化），从而上述目标函数转化为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f697bb1d.png" alt="8.png"></p>
<p>对于y(w’x+b)=1的数据点，即下图中位于w’x+b=1或w’x+b=-1上的数据点，我们称之为<strong>支持向量</strong>（support vector），易知：对于所有的支持向量，它们恰好满足y<em>(w’x</em>+b)=1，而所有不是支持向量的点，有y<em>(w’x</em>+b)&gt;1。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f6a838c4.png" alt="9.png"></p>
<h3 id="从原始优化问题到对偶问题"><a href="#从原始优化问题到对偶问题" class="headerlink" title="从原始优化问题到对偶问题"></a>从原始优化问题到对偶问题</h3><p>对于上述得到的目标函数，求1/||w||的最大值相当于求||w||^2的最小值，因此很容易将原来的目标函数转化为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f6978cbb.png" alt="10.png"></p>
<p>即变为了一个带约束的凸二次规划问题，按书上所说可以使用现成的优化计算包（QP优化包）求解，但由于SVM的特殊性，一般我们将原问题变换为它的<strong>对偶问题</strong>，接着再对其对偶问题进行求解。为什么通过对偶问题进行求解，有下面两个原因：</p>
<pre><code>* 一是因为使用对偶问题更容易求解；
* 二是因为通过对偶问题求解出现了向量内积的形式，从而能更加自然地引出核函数。</code></pre><p>对偶问题，顾名思义，可以理解成优化等价的问题，更一般地，是将一个原始目标函数的最小化转化为它的对偶函数最大化的问题。对于当前的优化问题，首先我们写出它的朗格朗日函数：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f9332be7.png" alt="11.png"></p>
<p>上式很容易验证：当其中有一个约束条件不满足时，L的最大值为 ∞（只需令其对应的α为 ∞即可）；当所有约束条件都满足时，L的最大值为1/2||w||^2（此时令所有的α为0），因此实际上原问题等价于：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f93321c5.png" alt="12.png"></p>
<p>由于这个的求解问题不好做，因此一般我们将最小和最大的位置交换一下（需满足KKT条件） ，变成原问题的对偶问题：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f9330967.png" alt="13.png"></p>
<p>这样就将原问题的求最小变成了对偶问题求最大（用对偶这个词还是很形象），接下来便可以先求L对w和b的极小，再求L对α的极大。</p>
<p>（1）首先求L对w和b的极小，分别求L关于w和b的偏导，可以得出：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f9333e66.png" alt="14.png"></p>
<p>将上述结果代入L得到：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f935ae21.png" alt="15.png"></p>
<p>（2）接着L关于α极大求解α（通过SMO算法求解，此处不做深入）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f9338a9d.png" alt="16.png"></p>
<p>（3）最后便可以根据求解出的α，计算出w和b，从而得到分类超平面函数。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f93419ca.png" alt="17.png"></p>
<p>在对新的点进行预测时，实际上就是将数据点x*代入分类函数f(x)=w’x+b中，若f(x)&gt;0，则为正类，f(x)&lt;0，则为负类，根据前面推导得出的w与b，分类函数如下所示，此时便出现了上面所提到的内积形式。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f9353166.png" alt="18.png"></p>
<p>这里实际上只需计算新样本与支持向量的内积，因为对于非支持向量的数据点，其对应的拉格朗日乘子一定为0，根据最优化理论（K-T条件），对于不等式约束y(w’x+b)-1≥0，满足：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f933c947.png" alt="19.png">        </p>
<h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>由于上述的超平面只能解决线性可分的问题，对于线性不可分的问题，例如：异或问题，我们需要使用核函数将其进行推广。一般地，解决线性不可分问题时，常常采用<strong>映射</strong>的方式，将低维原始空间映射到高维特征空间，使得数据集在高维空间中变得线性可分，从而再使用线性学习器分类。如果原始空间为有限维，即属性数有限，那么总是存在一个高维特征空间使得样本线性可分。若∅代表一个映射，则在特征空间中的划分函数变为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f934303e.png" alt="20.png"></p>
<p>按照同样的方法，先写出新目标函数的拉格朗日函数，接着写出其对偶问题，求L关于w和b的极大，最后运用SOM求解α。可以得出：</p>
<p>（1）原对偶问题变为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730cc68b3b.png" alt="21.png"></p>
<p>（2）原分类函数变为：<br><img src="https://i.loli.net/2018/10/17/5bc730cc1b673.png" alt="22.png"></p>
<p>求解的过程中，只涉及到了高维特征空间中的内积运算，由于特征空间的维数可能会非常大，例如：若原始空间为二维，映射后的特征空间为5维，若原始空间为三维，映射后的特征空间将是19维，之后甚至可能出现无穷维，根本无法进行内积运算了，此时便引出了<strong>核函数</strong>（Kernel）的概念。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730cc49adc.png" alt="23.png"></p>
<p>因此，核函数可以直接计算隐式映射到高维特征空间后的向量内积，而不需要显式地写出映射后的结果，它虽然完成了将特征从低维到高维的转换，但最终却是在低维空间中完成向量内积计算，与高维特征空间中的计算等效<strong>（低维计算，高维表现）</strong>，从而避免了直接在高维空间无法计算的问题。引入核函数后，原来的对偶问题与分类函数则变为：</p>
<p>（1）对偶问题：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730cc173b2.png" alt="24.png"></p>
<p>（2）分类函数：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730cc05959.png" alt="25.png"></p>
<p>因此，在线性不可分问题中，核函数的选择成了支持向量机的最大变数，若选择了不合适的核函数，则意味着将样本映射到了一个不合适的特征空间，则极可能导致性能不佳。同时，核函数需要满足以下这个必要条件：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730ccc468c.png" alt="26.png"></p>
<p>由于核函数的构造十分困难，通常我们都是从一些常用的核函数中选择，下面列出了几种常用的核函数：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730ccc541a.png" alt="27.png"></p>
<h3 id="软间隔支持向量机"><a href="#软间隔支持向量机" class="headerlink" title="软间隔支持向量机"></a>软间隔支持向量机</h3><p>前面的讨论中，我们主要解决了两个问题：当数据线性可分时，直接使用最大间隔的超平面划分；当数据线性不可分时，则通过核函数将数据映射到高维特征空间，使之线性可分。然而在现实问题中，对于某些情形还是很难处理，例如数据中有<strong>噪声</strong>的情形，噪声数据（<strong>outlier</strong>）本身就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束，如果不要这些噪声数据还好，当加入这些outlier后导致划分超平面被挤歪了，如下图所示，对支持向量机的泛化性能造成很大的影响。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730ccce68e.png" alt="28.png"></p>
<p>为了解决这一问题，我们需要允许某一些数据点不满足约束，即可以在一定程度上偏移超平面，同时使得不满足约束的数据点尽可能少，这便引出了<strong>“软间隔”支持向量机</strong>的概念</p>
<pre><code>* 允许某些数据点不满足约束y(w&apos;x+b)≥1；
* 同时又使得不满足约束的样本尽可能少。</code></pre><p>这样优化目标变为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730cc6c9fe.png" alt="29.png"></p>
<p>如同阶跃函数，0/1损失函数虽然表示效果最好，但是数学性质不佳。因此常用其它函数作为“替代损失函数”。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730cc5e5a9.png" alt="30.png"></p>
<p>支持向量机中的损失函数为<strong>hinge损失</strong>，引入<strong>“松弛变量”</strong>，目标函数与约束条件可以写为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7317aa3411.png" alt="31.png"></p>
<p>其中C为一个参数，控制着目标函数与新引入正则项之间的权重，这样显然每个样本数据都有一个对应的松弛变量，用以表示该样本不满足约束的程度，将新的目标函数转化为拉格朗日函数得到：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7317a4c96e.png" alt="32.png"></p>
<p>按照与之前相同的方法，先让L求关于w，b以及松弛变量的极小，再使用SMO求出α，有：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7317a6dff2.png" alt="33.png"></p>
<p>将w代入L化简，便得到其对偶问题：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7317ab6646.png" alt="34.png"></p>
<p>将“软间隔”下产生的对偶问题与原对偶问题对比可以发现：新的对偶问题只是约束条件中的α多出了一个上限C，其它的完全相同，因此在引入核函数处理线性不可分问题时，便能使用与“硬间隔”支持向量机完全相同的方法。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SVM/">SVM</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-深度学习速查表" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/09/01/深度学习速查表/" class="article-date">
      <time datetime="2019-09-01T14:53:18.000Z" itemprop="datePublished">2019-09-01</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/01/深度学习速查表/">深度学习速查表</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>请参考</p>
<ul>
<li><a href="https://github.com/fengdu78/deeplearning_ai_books" target="_blank" rel="noopener">吴恩达老师的深度学习课程笔记及资源</a></li>
<li><a href="https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning" target="_blank" rel="noopener">CS229深度学习速查表</a></li>
<li><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks" target="_blank" rel="noopener">CS230深度学习速查表</a></li>
</ul>
<h2 id="深度学习各种模型与知识速查表"><a href="#深度学习各种模型与知识速查表" class="headerlink" title="深度学习各种模型与知识速查表"></a>深度学习各种模型与知识速查表</h2><p><img src="/blog_picture/dl-0001.jpg" alt="avatar"><br><img src="/blog_picture/dl-0002.jpg" alt="avatar"><br><img src="/blog_picture/dl-0003.jpg" alt="avatar"><br><img src="/blog_picture/dl-0004.jpg" alt="avatar"><br><img src="/blog_picture/dl-0005.jpg" alt="avatar"><br><img src="/blog_picture/dl-0006.jpg" alt="avatar"><br><img src="/blog_picture/dl-0007.jpg" alt="avatar"><br><img src="/blog_picture/dl-0008.jpg" alt="avatar"><br><img src="/blog_picture/dl-0009.jpg" alt="avatar"><br><img src="/blog_picture/dl-0010.jpg" alt="avatar"><br><img src="/blog_picture/dl-0011.jpg" alt="avatar"><br><img src="/blog_picture/dl-0012.jpg" alt="avatar"><br><img src="/blog_picture/dl-0013.jpg" alt="avatar"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习速查表/">深度学习速查表</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-模型调优" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/09/01/模型调优/" class="article-date">
      <time datetime="2019-09-01T14:52:00.000Z" itemprop="datePublished">2019-09-01</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/01/模型调优/">模型调优</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>参考阅读材料：</p>
<ul>
<li><a href="https://www.zhihu.com/question/34470160" target="_blank" rel="noopener">机器学习各种算法怎么调参?</a></li>
<li><a href="https://blog.csdn.net/mozhizun/article/details/60966354" target="_blank" rel="noopener">机器学习模型应用以及模型优化的一些思路</a></li>
<li><a href="https://blog.csdn.net/mozhizun/article/details/71438821" target="_blank" rel="noopener">机器学习模型优化中常见问题和解决思路</a></li>
<li><a href="https://blog.csdn.net/bitcarmanlee/article/details/71753056" target="_blank" rel="noopener">机器学习中模型优化不得不思考的几个问题</a></li>
<li><a href="https://www.jianshu.com/p/9fe84a7a5ba8" target="_blank" rel="noopener">机器学习中模型优化的两个问题</a></li>
</ul>
<h2 id="模型选择与调优"><a href="#模型选择与调优" class="headerlink" title="模型选择与调优"></a>模型选择与调优</h2><h3 id="不同模型的选择"><a href="#不同模型的选择" class="headerlink" title="不同模型的选择"></a>不同模型的选择</h3><p><img src="/blog_picture/model_tuning1.png" alt="avatar"></p>
<h3 id="模型超参数的选择"><a href="#模型超参数的选择" class="headerlink" title="模型超参数的选择"></a>模型超参数的选择</h3><p><img src="/blog_picture/model_tuning2.png" alt="avatar"></p>
<h3 id="评估方法-超参数产出方法"><a href="#评估方法-超参数产出方法" class="headerlink" title="评估方法+超参数产出方法"></a>评估方法+超参数产出方法</h3><p><img src="/blog_picture/model_tuning4.png" alt="avatar"><br><img src="/blog_picture/model_tuning5.png" alt="avatar"><br><img src="/blog_picture/model_tuning6.png" alt="avatar"></p>
<h3 id="模型状态"><a href="#模型状态" class="headerlink" title="模型状态"></a>模型状态</h3><p><img src="/blog_picture/model_tuning3.png" alt="avatar"><br><img src="/blog_picture/model_tuning7.png" alt="avatar"><br><img src="/blog_picture/model_tuning8.png" alt="avatar"><br><img src="/blog_picture/model_tuning9.png" alt="avatar"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/模型调优/">模型调优</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-集成学习与boosting模型" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/09/01/集成学习与boosting模型/" class="article-date">
      <time datetime="2019-09-01T14:44:32.000Z" itemprop="datePublished">2019-09-01</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/01/集成学习与boosting模型/">集成学习与boosting模型</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="机器学习中的集成学习"><a href="#机器学习中的集成学习" class="headerlink" title="机器学习中的集成学习"></a>机器学习中的集成学习</h2><p>顾名思义，集成学习（ensemble learning）指的是将多个学习器进行有效地结合，组建一个“学习器委员会”，其中每个学习器担任委员会成员并行使投票表决权，使得委员会最后的决定更能够四方造福普度众生<del>…</del>，即其泛化性能要能优于其中任何一个学习器。</p>
<h3 id="个体与集成"><a href="#个体与集成" class="headerlink" title="个体与集成"></a>个体与集成</h3><p>集成学习的基本结构为：先产生一组个体学习器，再使用某种策略将它们结合在一起。集成模型如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0c15683.png" alt="1.png"></p>
<p>在上图的集成模型中，若个体学习器都属于同一类别，例如都是决策树或都是神经网络，则称该集成为同质的（homogeneous）;若个体学习器包含多种类型的学习算法，例如既有决策树又有神经网络，则称该集成为异质的（heterogenous）。</p>
<blockquote>
<p><strong>同质集成</strong>：个体学习器称为“基学习器”（base learner），对应的学习算法为“基学习算法”（base learning algorithm）。</p>
</blockquote>
<blockquote>
<p><strong>异质集成</strong>：个体学习器称为“组件学习器”（component learner）或直称为“个体学习器”。</p>
</blockquote>
<p>上面我们已经提到要让集成起来的泛化性能比单个学习器都要好，虽说团结力量大但也有木桶短板理论调皮捣蛋，那如何做到呢？这就引出了集成学习的两个重要概念：<strong>准确性</strong>和<strong>多样性</strong>（diversity）。准确性指的是个体学习器不能太差，要有一定的准确度；多样性则是个体学习器之间的输出要具有差异性。通过下面的这三个例子可以很容易看出这一点，准确度较高，差异度也较高，可以较好地提升集成性能。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0d23e13.png" alt="2.png"></p>
<p>现在考虑二分类的简单情形，假设基分类器之间相互独立（能提供较高的差异度），且错误率相等为 ε，则可以将集成器的预测看做一个伯努利实验，易知当所有基分类器中不足一半预测正确的情况下，集成器预测错误，所以集成器的错误率可以计算为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0cce0bb.png" alt="3.png"></p>
<p>此时，集成器错误率随着基分类器的个数的增加呈指数下降，但前提是基分类器之间相互独立，在实际情形中显然是不可能的，假设训练有A和B两个分类器，对于某个测试样本，显然满足：P（A=1 | B=1）&gt; P（A=1），因为A和B为了解决相同的问题而训练，因此在预测新样本时存在着很大的联系。因此，<strong>个体学习器的“准确性”和“差异性”本身就是一对矛盾的变量</strong>，准确性高意味着牺牲多样性，所以产生“<strong>好而不同</strong>”的个体学习器正是集成学习研究的核心。现阶段有三种主流的集成学习方法：Boosting、Bagging以及随机森林（Random Forest），接下来将进行逐一介绍。</p>
<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>Boosting是一种串行的工作机制，即个体学习器的训练存在依赖关系，必须一步一步序列化进行。其基本思想是：增加前一个基学习器在训练训练过程中预测错误样本的权重，使得后续基学习器更加关注这些打标错误的训练样本，尽可能纠正这些错误，一直向下串行直至产生需要的T个基学习器，Boosting最终对这T个学习器进行加权结合，产生学习器委员会。</p>
<p>Boosting族算法最著名、使用最为广泛的就是AdaBoost，因此下面主要是对AdaBoost算法进行介绍。AdaBoost使用的是<strong>指数损失函数</strong>，因此AdaBoost的权值与样本分布的更新都是围绕着最小化指数损失函数进行的。看到这里回想一下之前的机器学习算法，<strong>不难发现机器学习的大部分带参模型只是改变了最优化目标中的损失函数</strong>：如果是Square loss，那就是最小二乘了；如果是Hinge Loss，那就是著名的SVM了；如果是log-Loss，那就是Logistic Regression了。</p>
<p>定义基学习器的集成为加权结合，则有：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0ca2ca5.png" alt="4.png"></p>
<p>AdaBoost算法的指数损失函数定义为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0d10461.png" alt="5.png"></p>
<p>具体说来，整个Adaboost 迭代算法分为3步：</p>
<ul>
<li>初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。</li>
<li>训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。</li>
<li>将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。</li>
</ul>
<p>整个AdaBoost的算法流程如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0d7c057.png" alt="6.png"></p>
<p>可以看出：<strong>AdaBoost的核心步骤就是计算基学习器权重和样本权重分布</strong>，那为何是上述的计算公式呢？这就涉及到了我们之前为什么说大部分带参机器学习算法只是改变了损失函数，就是因为<strong>大部分模型的参数都是通过最优化损失函数（可能还加个规则项）而计算（梯度下降，坐标下降等）得到</strong>，这里正是通过最优化指数损失函数从而得到这两个参数的计算公式，具体的推导过程此处不进行展开。</p>
<p>Boosting算法要求基学习器能对特定分布的数据进行学习，即每次都更新样本分布权重，这里书上提到了两种方法：“重赋权法”（re-weighting）和“重采样法”（re-sampling），书上的解释有些晦涩，这里进行展开一下：</p>
<blockquote>
<p><strong>重赋权法</strong> : 对每个样本附加一个权重，这时涉及到样本属性与标签的计算，都需要乘上一个权值。<br><strong>重采样法</strong> : 对于一些无法接受带权样本的及学习算法，适合用“重采样法”进行处理。方法大致过程是，根据各个样本的权重，对训练数据进行重采样，初始时样本权重一样，每个样本被采样到的概率一致，每次从N个原始的训练样本中按照权重有放回采样N个样本作为训练集，然后计算训练集错误率，然后调整权重，重复采样，集成多个基学习器。</p>
</blockquote>
<p>从偏差-方差分解来看：Boosting算法主要关注于降低偏差，每轮的迭代都关注于训练过程中预测错误的样本，将弱学习提升为强学习器。从AdaBoost的算法流程来看，标准的AdaBoost只适用于二分类问题。在此，当选为数据挖掘十大算法之一的AdaBoost介绍到这里，能够当选正是说明这个算法十分婀娜多姿，背后的数学证明和推导充分证明了这一点，限于篇幅不再继续展开。</p>
<h3 id="Bagging与Random-Forest"><a href="#Bagging与Random-Forest" class="headerlink" title="Bagging与Random Forest"></a>Bagging与Random Forest</h3><p>相比之下，Bagging与随机森林算法就简洁了许多，上面已经提到产生“好而不同”的个体学习器是集成学习研究的核心，即在保证基学习器准确性的同时增加基学习器之间的多样性。而这两种算法的基本思（tao）想（lu）都是通过“自助采样”的方法来增加多样性。</p>
<h4 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h4><p>Bagging是一种并行式的集成学习方法，即基学习器的训练之间没有前后顺序可以同时进行，Bagging使用“有放回”采样的方式选取训练集，对于包含m个样本的训练集，进行m次有放回的随机采样操作，从而得到m个样本的采样集，这样训练集中有接近36.8%的样本没有被采到。按照相同的方式重复进行，我们就可以采集到T个包含m个样本的数据集，从而训练出T个基学习器，最终对这T个基学习器的输出进行结合。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0ce62fc.png" alt="7.png"></p>
<p>Bagging算法的流程如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0d0e761.png" alt="8.png"></p>
<p>可以看出Bagging主要通过<strong>样本的扰动</strong>来增加基学习器之间的多样性，因此Bagging的基学习器应为那些对训练集十分敏感的不稳定学习算法，例如：神经网络与决策树等。从偏差-方差分解来看，Bagging算法主要关注于降低方差，即通过多次重复训练提高稳定性。不同于AdaBoost的是，Bagging可以十分简单地移植到多分类、回归等问题。总的说起来则是：<strong>AdaBoost关注于降低偏差，而Bagging关注于降低方差。</strong></p>
<h4 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h4><p>随机森林（Random Forest）是Bagging的一个拓展体，它的基学习器固定为决策树，多棵树也就组成了森林，而“随机”则在于选择划分属性的随机，随机森林在训练基学习器时，也采用有放回采样的方式添加样本扰动，同时它还引入了一种<strong>属性扰动</strong>，即在基决策树的训练过程中，在选择划分属性时，RF先从候选属性集中随机挑选出一个包含K个属性的子集，再从这个子集中选择最优划分属性，一般推荐K=log2（d）。</p>
<p>这样随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，从而进一步提升了基学习器之间的差异度。相比决策树的Bagging集成，随机森林的起始性能较差（由于属性扰动，基决策树的准确度有所下降），但随着基学习器数目的增多，随机森林往往会收敛到更低的泛化误差。同时不同于Bagging中决策树从所有属性集中选择最优划分属性，随机森林只在属性集的一个子集中选择划分属性，因此训练效率更高。<br><img src="../img/random-forest.png" alt><br><img src="https://i.loli.net/2018/10/18/5bc84d0d7a4fd.png" alt="9.png"></p>
<h3 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h3><p>结合策略指的是在训练好基学习器后，如何将这些基学习器的输出结合起来产生集成模型的最终输出，下面将介绍一些常用的结合策略：</p>
<h4 id="平均法（回归问题）"><a href="#平均法（回归问题）" class="headerlink" title="平均法（回归问题）"></a>平均法（回归问题）</h4><p><img src="https://i.loli.net/2018/10/18/5bc84d0d07983.png" alt="10.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de1b74ff.png" alt="11.png"></p>
<p>易知简单平均法是加权平均法的一种特例，加权平均法可以认为是集成学习研究的基本出发点。由于各个基学习器的权值在训练中得出，<strong>一般而言，在个体学习器性能相差较大时宜使用加权平均法，在个体学习器性能相差较小时宜使用简单平均法</strong>。</p>
<h4 id="投票法（分类问题）"><a href="#投票法（分类问题）" class="headerlink" title="投票法（分类问题）"></a>投票法（分类问题）</h4><p><img src="https://i.loli.net/2018/10/18/5bc84de2629c4.png" alt="12.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de25a74b.png" alt="13.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de1bacc4.png" alt="14.png"></p>
<p>绝对多数投票法（majority voting）提供了拒绝选项，这在可靠性要求很高的学习任务中是一个很好的机制。同时，对于分类任务，各个基学习器的输出值有两种类型，分别为类标记和类概率。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de2768c1.png" alt="15.png"></p>
<p>一些在产生类别标记的同时也生成置信度的学习器，置信度可转化为类概率使用，<strong>一般基于类概率进行结合往往比基于类标记进行结合的效果更好</strong>，需要注意的是对于异质集成，其类概率不能直接进行比较，此时需要将类概率转化为类标记输出，然后再投票。</p>
<h4 id="学习法"><a href="#学习法" class="headerlink" title="学习法"></a>学习法</h4><p>学习法是一种更高级的结合策略，即学习出一种“投票”的学习器，Stacking是学习法的典型代表。Stacking的基本思想是：首先训练出T个基学习器，对于一个样本它们会产生T个输出，将这T个基学习器的输出与该样本的真实标记作为新的样本，m个样本就会产生一个m<em>T的样本集，来训练一个新的“投票”学习器。投票学习器的输入属性与学习算法对Stacking集成的泛化性能有很大的影响，书中已经提到：*</em>投票学习器采用类概率作为输入属性，选用多响应线性回归（MLR）一般会产生较好的效果**。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de25cbaf.png" alt="16.png"></p>
<h4 id="多样性（diversity）"><a href="#多样性（diversity）" class="headerlink" title="多样性（diversity）"></a>多样性（diversity）</h4><p>在集成学习中，基学习器之间的多样性是影响集成器泛化性能的重要因素。因此增加多样性对于集成学习研究十分重要，一般的思路是在学习过程中引入随机性，常见的做法主要是对数据样本、输入属性、输出表示、算法参数进行扰动。</p>
<blockquote>
<p><strong>数据样本扰动</strong>，即利用具有差异的数据集来训练不同的基学习器。例如：有放回自助采样法，但此类做法只对那些不稳定学习算法十分有效，例如：决策树和神经网络等，训练集的稍微改变能导致学习器的显著变动。<br><strong>输入属性扰动</strong>，即随机选取原空间的一个子空间来训练基学习器。例如：随机森林，从初始属性集中抽取子集，再基于每个子集来训练基学习器。但若训练集只包含少量属性，则不宜使用属性扰动。<br><strong>输出表示扰动</strong>，此类做法可对训练样本的类标稍作变动，或对基学习器的输出进行转化。<br><strong>算法参数扰动</strong>，通过随机设置不同的参数，例如：神经网络中，随机初始化权重与随机设置隐含层节点数。</p>
</blockquote>
<h2 id="机器学习中的Boosting模型：GBDT-vs-Xgboost-vs-LightGBM"><a href="#机器学习中的Boosting模型：GBDT-vs-Xgboost-vs-LightGBM" class="headerlink" title="机器学习中的Boosting模型：GBDT vs Xgboost vs LightGBM"></a>机器学习中的Boosting模型：GBDT vs Xgboost vs LightGBM</h2><p>见资料<strong>GBDT_wepon.pdf</strong></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/boosting/">boosting</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/集成学习/">集成学习</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-聚类与降维" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/09/01/聚类与降维/" class="article-date">
      <time datetime="2019-09-01T14:40:06.000Z" itemprop="datePublished">2019-09-01</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/01/聚类与降维/">聚类与降维</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="机器学习中的聚类算法"><a href="#机器学习中的聚类算法" class="headerlink" title="机器学习中的聚类算法"></a>机器学习中的聚类算法</h2><p>聚类是一种经典的<strong>无监督学习</strong>方法，<strong>无监督学习的目标是通过对无标记训练样本的学习，发掘和揭示数据集本身潜在的结构与规律</strong>，即不依赖于训练数据集的类标记信息。聚类则是试图将数据集的样本划分为若干个互不相交的类簇，从而每个簇对应一个潜在的类别。</p>
<p>聚类直观上来说是将相似的样本聚在一起，从而形成一个<strong>类簇（cluster）</strong>。那首先的问题是如何来<strong>度量相似性</strong>（similarity measure）呢？这便是<strong>距离度量</strong>，在生活中我们说差别小则相似，对应到多维样本，每个样本可以对应于高维空间中的一个数据点，若它们的距离相近，我们便可以称它们相似。那接着如何来评价聚类结果的好坏呢？这便是<strong>性能度量</strong>，性能度量为评价聚类结果的好坏提供了一系列有效性指标。</p>
<h3 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h3><p>谈及距离度量，最熟悉的莫过于欧式距离了，从年头一直用到年尾的距离计算公式：即对应属性之间相减的平方和再开根号。度量距离还有其它的很多经典方法，通常它们需要满足一些基本性质：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed4c0390.png" alt="1.png"></p>
<p>最常用的距离度量方法是<strong>“闵可夫斯基距离”（Minkowski distance)</strong>：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed49e31f.png" alt="2.png"></p>
<p>当p=1时，闵可夫斯基距离即<strong>曼哈顿距离（Manhattan distance）</strong>：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed49c31f.png" alt="3.png"></p>
<p>当p=2时，闵可夫斯基距离即<strong>欧氏距离（Euclidean distance）</strong>：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed497613.png" alt="4.png"></p>
<p>我们知道属性分为两种：<strong>连续属性</strong>和<strong>离散属性</strong>（有限个取值）。对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；而对于离散值的属性，需要作下面进一步的处理：</p>
<blockquote>
<p>若属性值之间<strong>存在序关系</strong>，则可以将其转化为连续值，例如：身高属性“高”“中等”“矮”，可转化为{1, 0.5, 0}。<br>若属性值之间<strong>不存在序关系</strong>，则通常将其转化为向量的形式，例如：性别属性“男”“女”，可转化为{（1,0），（0,1）}。</p>
</blockquote>
<p>在进行距离度量时，易知<strong>连续属性和存在序关系的离散属性都可以直接参与计算</strong>，因为它们都可以反映一种程度，我们称其为“<strong>有序属性</strong>”；而对于不存在序关系的离散属性，我们称其为：“<strong>无序属性</strong>”，显然无序属性再使用闵可夫斯基距离就行不通了。</p>
<p><strong>对于无序属性，我们一般采用VDM进行距离的计算</strong>，例如：对于离散属性的两个取值a和b，定义：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed4e9560.png" alt="5.png"></p>
<p>于是，在计算两个样本之间的距离时，我们可以将闵可夫斯基距离和VDM混合在一起进行计算：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed507bc7.png" alt="6.png"></p>
<p>若我们定义的距离计算方法是用来度量相似性，例如下面将要讨论的聚类问题，即距离越小，相似性越大，反之距离越大，相似性越小。这时距离的度量方法并不一定需要满足前面所说的四个基本性质，这样的方法称为：<strong>非度量距离（non-metric distance）</strong>。</p>
<h3 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h3><p>由于聚类算法不依赖于样本的真实类标，就不能像监督学习的分类那般，通过计算分对分错（即精确度或错误率）来评价学习器的好坏或作为学习过程中的优化目标。一般聚类有两类性能度量指标：<strong>外部指标</strong>和<strong>内部指标</strong>。</p>
<h4 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h4><p>即将聚类结果与某个参考模型的结果进行比较，<strong>以参考模型的输出作为标准，来评价聚类好坏</strong>。假设聚类给出的结果为λ，参考模型给出的结果是λ*，则我们将样本进行两两配对，定义：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed59160e.png" alt="7.png"></p>
<p>显然a和b代表着聚类结果好坏的正能量，b和c则表示参考结果和聚类结果相矛盾，基于这四个值可以导出以下常用的外部评价指标：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed587438.png" alt="8.png"></p>
<h4 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h4><p>内部指标即不依赖任何外部模型，直接对聚类的结果进行评估，聚类的目的是想将那些相似的样本尽可能聚在一起，不相似的样本尽可能分开，直观来说：<strong>簇内高内聚紧紧抱团，簇间低耦合老死不相往来</strong>。定义：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed581852.png" alt="9.png"></p>
<p>基于上面的四个距离，可以导出下面这些常用的内部评价指标：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed582854.png" alt="10.png"></p>
<h3 id="原型聚类"><a href="#原型聚类" class="headerlink" title="原型聚类"></a>原型聚类</h3><p>原型聚类即“<strong>基于原型的聚类</strong>”（prototype-based clustering），原型表示模板的意思，就是通过参考一个模板向量或模板分布的方式来完成聚类的过程，常见的K-Means便是基于簇中心来实现聚类，混合高斯聚类则是基于簇分布来实现聚类。</p>
<h4 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h4><p>K-Means的思想十分简单，<strong>首先随机指定类中心，根据样本与类中心的远近划分类簇，接着重新计算类中心，迭代直至收敛</strong>。但是其中迭代的过程并不是主观地想象得出，事实上，若将样本的类别看做为“隐变量”（latent variable），类中心看作样本的分布参数，这一过程正是通过<strong>EM算法</strong>的两步走策略而计算出，其根本的目的是为了最小化平方误差函数E：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb82b5d3.png" alt="11.png"></p>
<p>K-Means的算法流程如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb9c0817.png" alt="12.png"></p>
<p><img src="../img/K-Means.png" alt></p>
<h4 id="高斯混合聚类"><a href="#高斯混合聚类" class="headerlink" title="高斯混合聚类"></a>高斯混合聚类</h4><p>现在可以看出K-Means与LVQ都试图以类中心作为原型指导聚类，高斯混合聚类则采用高斯分布来描述原型。现假设<strong>每个类簇中的样本都服从一个多维高斯分布，那么空间中的样本可以看作由k个多维高斯分布混合而成</strong>。</p>
<p>对于多维高斯分布，其概率密度函数如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb870d98.png" alt="14.png"></p>
<p>其中u表示均值向量，∑表示协方差矩阵，可以看出一个多维高斯分布完全由这两个参数所确定。接着定义高斯混合分布为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb876794.png" alt="15.png"></p>
<p>α称为混合系数，这样空间中样本的采集过程则可以抽象为：<strong>（1）先选择一个类簇（高斯分布），（2）再根据对应高斯分布的密度函数进行采样</strong>，这时候贝叶斯公式又能大展身手了：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb9191d9.png" alt="16.png"></p>
<p>此时只需要选择PM最大时的类簇并将该样本划分到其中，看到这里很容易发现：这和那个传说中的贝叶斯分类不是神似吗，都是通过贝叶斯公式展开，然后计算类先验概率和类条件概率。但遗憾的是：<strong>这里没有真实类标信息，对于类条件概率，并不能像贝叶斯分类那样通过最大似然法美好地计算出来</strong>，因为这里的样本可能属于所有的类簇，这里的似然函数变为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb871d4a.png" alt="17.png"></p>
<p>可以看出：简单的最大似然法根本无法求出所有的参数，这样PM也就没法计算。<strong>这里就要召唤出之前的EM大法，首先对高斯分布的参数及混合系数进行随机初始化，计算出各个PM（即γji，第i个样本属于j类），再最大化似然函数（即LL（D）分别对α、u和∑求偏导 ），对参数进行迭代更新</strong>。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb8a6f32.png" alt="18.png"></p>
<p>高斯混合聚类的算法流程如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb9c4fa4.png" alt="19.png"></p>
<h4 id="密度聚类"><a href="#密度聚类" class="headerlink" title="密度聚类"></a>密度聚类</h4><p>密度聚类则是基于密度的聚类，它从样本分布的角度来考察样本之间的可连接性，并基于可连接性（密度可达）不断拓展疆域（类簇）。其中最著名的便是<strong>DBSCAN</strong>算法，首先定义以下概念：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb9bd69c.png" alt="20.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc8509f8d619.png" alt="21.png"></p>
<p>简单来理解DBSCAN便是：<strong>找出一个核心对象所有密度可达的样本集合形成簇</strong>。首先从数据集中任选一个核心对象A，找出所有A密度可达的样本集合，将这些样本形成一个密度相连的类簇，直到所有的核心对象都遍历完。DBSCAN算法的流程如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc8509feb587.png" alt="22.png"></p>
<h4 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h4><p>层次聚类是一种基于树形结构的聚类方法，常用的是<strong>自底向上</strong>的结合策略（<strong>AGNES算法</strong>）。假设有N个待聚类的样本，其基本步骤是：</p>
<ul>
<li>1.初始化–&gt;把每个样本归为一类，计算每两个类之间的距离，也就是样本与样本之间的相似度；</li>
<li>2.寻找各个类之间最近的两个类，把他们归为一类（这样类的总数就少了一个）；</li>
<li>3.重新计算新生成的这个<strong>类与各个旧类之间的相似度</strong>；</li>
<li>4.重复2和3直到所有样本点都归为一类，结束。</li>
</ul>
<p>可以看出其中最关键的一步就是<strong>计算两个类簇的相似度</strong>，这里有多种度量方法：</p>
<pre><code>* 单链接（single-linkage）:取类间最小距离。</code></pre><p><img src="https://i.loli.net/2018/10/18/5bc8509ebb022.png" alt="23.png"></p>
<pre><code>* 全链接（complete-linkage）:取类间最大距离</code></pre><p><img src="https://i.loli.net/2018/10/18/5bc8509eb2b30.png" alt="24.png"></p>
<pre><code>* 均链接（average-linkage）:取类间两两的平均距离</code></pre><p><img src="https://i.loli.net/2018/10/18/5bc8509f089a7.png" alt="25.png"></p>
<p>很容易看出：<strong>单链接的包容性极强，稍微有点暧昧就当做是自己人了，全链接则是坚持到底，只要存在缺点就坚决不合并，均连接则是从全局出发顾全大局</strong>。层次聚类法的算法流程如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc8509f9d4a0.png" alt="26.png"><br><img src="/blog_picture/clustering.png" alt="avatar"></p>
<h2 id="机器学习中的PCA降维"><a href="#机器学习中的PCA降维" class="headerlink" title="机器学习中的PCA降维"></a>机器学习中的PCA降维</h2><p>资料from<a href="http://blog.codinglabs.org/articles/pca-tutorial.html" target="_blank" rel="noopener">PCA的数学原理</a></p>
<p><img src="/blog_picture/PCA_.jpg" alt="avatar"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/聚类/">聚类</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/降维/">降维</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-贝叶斯分类器" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/09/01/贝叶斯分类器/" class="article-date">
      <time datetime="2019-09-01T14:37:35.000Z" itemprop="datePublished">2019-09-01</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/01/贝叶斯分类器/">贝叶斯分类器</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>参考阅读材料：</p>
<ul>
<li><a href="https://blog.csdn.net/han_xiaoyang/article/details/50616559" target="_blank" rel="noopener">NLP系列(2)_用朴素贝叶斯进行文本分类(上)</a></li>
<li><a href="https://blog.csdn.net/han_xiaoyang/article/details/50629587" target="_blank" rel="noopener">NLP系列(3)_用朴素贝叶斯进行文本分类(下)</a></li>
</ul>
<p>朴素贝叶斯</p>
<ul>
<li>贝叶斯公式 + 条件独立假设</li>
<li>平滑算法</li>
</ul>
<h2 id="机器学习中的贝叶斯分类器"><a href="#机器学习中的贝叶斯分类器" class="headerlink" title="机器学习中的贝叶斯分类器"></a>机器学习中的贝叶斯分类器</h2><p>贝叶斯分类器是一种概率框架下的统计学习分类器，对分类任务而言，假设在相关概率都已知的情况下，贝叶斯分类器考虑如何基于这些概率为样本判定最优的类标。在开始介绍贝叶斯决策论之前，我们首先来回顾下概率论委员会常委–贝叶斯公式。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd7a2575.png" alt="1.png"></p>
<h3 id="贝叶斯决策论"><a href="#贝叶斯决策论" class="headerlink" title="贝叶斯决策论"></a>贝叶斯决策论</h3><p>若将上述定义中样本空间的划分Bi看做为类标，A看做为一个新的样本，则很容易将条件概率理解为样本A是类别Bi的概率。在机器学习训练模型的过程中，往往我们都试图去优化一个风险函数，因此在概率框架下我们也可以为贝叶斯定义“<strong>条件风险</strong>”（conditional risk）。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd15db94.png" alt="2.png"></p>
<p>我们的任务就是寻找一个判定准则最小化所有样本的条件风险总和，因此就有了<strong>贝叶斯判定准则</strong>（Bayes decision rule）:为最小化总体风险，只需在每个样本上选择那个使得条件风险最小的类标。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd308600.png" alt="3.png"></p>
<p>若损失函数λ取0-1损失，则有：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd37c502.png" alt="4.png"></p>
<p>即对于每个样本x，选择其后验概率P（c | x）最大所对应的类标，能使得总体风险函数最小，从而将原问题转化为估计后验概率P（c | x）。一般这里有两种策略来对后验概率进行估计：</p>
<pre><code>* 判别式模型：直接对 P（c | x）进行建模求解。例我们前面所介绍的决策树、神经网络、SVM都是属于判别式模型。
* 生成式模型：通过先对联合分布P（x,c）建模，从而进一步求解 P（c | x）。</code></pre><p>贝叶斯分类器就属于生成式模型，基于贝叶斯公式对后验概率P（c | x） 进行一项神奇的变换，巴拉拉能量…. P（c | x）变身：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd501ad3.png" alt="5.png"></p>
<p>对于给定的样本x，P（x）与类标无关，P（c）称为类先验概率，p（x | c ）称为类条件概率。这时估计后验概率P（c | x）就变成为估计类先验概率和类条件概率的问题。对于先验概率和后验概率，在看这章之前也是模糊了我好久，这里普及一下它们的基本概念。</p>
<pre><code>* 先验概率： 根据以往经验和分析得到的概率。
* 后验概率：后验概率是基于新的信息，修正原来的先验概率后所获得的更接近实际情况的概率估计。</code></pre><p>实际上先验概率就是在没有任何结果出来的情况下估计的概率，而后验概率则是在有一定依据后的重新估计，直观意义上后验概率就是条件概率。下面直接上Wiki上的一个例子，简单粗暴快速完事…</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd799610.png" alt="6.png"></p>
<p>回归正题，对于类先验概率P（c），p（c）就是样本空间中各类样本所占的比例，根据大数定理（当样本足够多时，频率趋于稳定等于其概率），这样当训练样本充足时，p(c)可以使用各类出现的频率来代替。因此只剩下类条件概率p（x | c ），它表达的意思是在类别c中出现x的概率，它涉及到属性的联合概率问题，若只有一个离散属性还好，当属性多时采用频率估计起来就十分困难，因此这里一般采用极大似然法进行估计。</p>
<h3 id="极大似然法"><a href="#极大似然法" class="headerlink" title="极大似然法"></a>极大似然法</h3><p>极大似然估计（Maximum Likelihood Estimation，简称MLE），是一种根据数据采样来估计概率分布的经典方法。常用的策略是先假定总体具有某种确定的概率分布，再基于训练样本对概率分布的参数进行估计。运用到类条件概率p（x | c ）中，假设p（x | c ）服从一个参数为θ的分布，问题就变为根据已知的训练样本来估计θ。极大似然法的核心思想就是：估计出的参数使得已知样本出现的概率最大，即使得训练数据的似然最大。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd70fb73.png" alt="7.png"></p>
<p>所以，贝叶斯分类器的训练过程就是参数估计。总结最大似然法估计参数的过程，一般分为以下四个步骤：</p>
<pre><code>* 1.写出似然函数；
* 2.对似然函数取对数，并整理；
* 3.求导数，令偏导数为0，得到似然方程组；
* 4.解似然方程组，得到所有参数即为所求。</code></pre><p>例如：假设样本属性都是连续值，p（x | c ）服从一个多维高斯分布，则通过MLE计算出的参数刚好分别为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd705729.png" alt="8.png"></p>
<p>上述结果看起来十分合乎实际，但是采用最大似然法估计参数的效果很大程度上依赖于作出的假设是否合理，是否符合潜在的真实数据分布。这就需要大量的经验知识，搞统计越来越值钱也是这个道理，大牛们掐指一算比我们搬砖几天更有效果。</p>
<h3 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h3><p>不难看出：原始的贝叶斯分类器最大的问题在于联合概率密度函数的估计，首先需要根据经验来假设联合概率分布，其次当属性很多时，训练样本往往覆盖不够，参数的估计会出现很大的偏差。为了避免这个问题，朴素贝叶斯分类器（naive Bayes classifier）采用了“属性条件独立性假设”，即样本数据的所有属性之间相互独立。这样类条件概率p（x | c ）可以改写为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd55e102.png" alt="9.png"></p>
<p>这样，为每个样本估计类条件概率变成为每个样本的每个属性估计类条件概率。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd6678cd.png" alt="10.png"></p>
<p>相比原始贝叶斯分类器，朴素贝叶斯分类器基于单个的属性计算类条件概率更加容易操作，需要注意的是：若某个属性值在训练集中和某个类别没有一起出现过，这样会抹掉其它的属性信息，因为该样本的类条件概率被计算为0。因此在估计概率值时，常常用进行平滑（smoothing）处理，拉普拉斯修正（Laplacian correction）就是其中的一种经典方法，具体计算方法如下：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fe54aaed.png" alt="11.png"></p>
<p>当训练集越大时，拉普拉斯修正引入的影响越来越小。对于贝叶斯分类器，模型的训练就是参数估计，因此可以事先将所有的概率储存好，当有新样本需要判定时，直接查表计算即可。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/贝叶斯分类器/">贝叶斯分类器</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-决策树与随机森林" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/09/01/决策树与随机森林/" class="article-date">
      <time datetime="2019-09-01T12:25:50.000Z" itemprop="datePublished">2019-09-01</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/01/决策树与随机森林/">决策树与随机森林</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="机器学习中的决策树模型"><a href="#机器学习中的决策树模型" class="headerlink" title="机器学习中的决策树模型"></a>机器学习中的决策树模型</h2><ul>
<li>① 树模型不用做scaling</li>
<li>② 树模型不太需要做离散化</li>
<li>③ 用Xgboost等工具库，是不需要做缺失值填充</li>
<li>④ 树模型是非线性模型，有非线性的表达能力</li>
</ul>
<h3 id="决策树基本概念"><a href="#决策树基本概念" class="headerlink" title="决策树基本概念"></a>决策树基本概念</h3><ul>
<li>决策时是一种树形结构，其中每个内部节点表示在一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别</li>
<li>决策树学习是以实例为基础的归纳学习</li>
<li>决策树学习采用的是自顶向下的归纳方法，其基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子节点处的熵值为0，此时每个叶子节点中的实例都属于同一类。</li>
</ul>
<p>顾名思义，决策树是基于树结构来进行决策的，，在网上看到一个例子十分有趣，放在这里正好合适。现想象一位捉急的母亲想要给自己的女娃介绍一个男朋友，于是有了下面的对话：</p>
<hr>
<pre><code>女儿：多大年纪了？
母亲：26。
女儿：长的帅不帅？
母亲：挺帅的。
女儿：收入高不？
母亲：不算很高，中等情况。
女儿：是公务员不？
母亲：是，在税务局上班呢。
女儿：那好，我去见见。</code></pre><hr>
<p>这个女孩的挑剔过程就是一个典型的决策树，即相当于通过年龄、长相、收入和是否公务员将男童鞋分为两个类别：见和不见。假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么使用下图就能很好地表示女孩的决策逻辑（即一颗决策树）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec84a77.png" alt="1.png"></p>
<p>在上图的决策树中，决策过程的每一次判定都是对某一属性的“测试”，决策最终结论则对应最终的判定结果。一般一颗决策树包含：一个根节点、若干个内部节点和若干个叶子节点，易知：</p>
<pre><code>* 每个非叶节点表示一个特征属性测试。
* 每个分支代表这个特征属性在某个值域上的输出。
* 每个叶子节点存放一个类别。
* 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。</code></pre><h3 id="决策树的构造"><a href="#决策树的构造" class="headerlink" title="决策树的构造"></a>决策树的构造</h3><p>决策树的构造是一个递归的过程，有三种情形会导致递归返回：(1) 当前结点包含的样本全属于同一类别，这时直接将该节点标记为叶节点，并设为相应的类别；(2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别；(3) 当前结点包含的样本集合为空，不能划分，这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。算法的基本流程如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ecc27fe.png" alt="2.png"></p>
<p>可以看出：决策树学习的关键在于如何选择划分属性，不同的划分属性得出不同的分支结构，从而影响整颗决策树的性能。属性划分的目标是让各个划分出来的子节点尽可能地“纯”，即属于同一类别。因此下面便是介绍量化纯度的具体方法，决策树最常用的算法有三种：ID3，C4.5和CART。##</p>
<h4 id="决策树学习算法的特点"><a href="#决策树学习算法的特点" class="headerlink" title="决策树学习算法的特点"></a>决策树学习算法的特点</h4><p>决策树学习算法最大的优点是，它可以自学习。在学习的过程中，不需要使用者了解过多背景知识，只需要对训练实例进行较好的标注，就能进行学习。显然，属于有监督学习。从一类无序、无规则的事物中推理出决策树表示分类的规则。</p>
<h4 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h4><p>ID3算法使用信息增益为准则来选择划分属性，“信息熵”(information entropy)是度量样本结合纯度的常用指标，假定当前样本集合D中第k类样本所占比例为pk，则样本集合D的信息熵定义为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec515a5.png" alt="3.png"></p>
<p>假定通过属性划分样本集D，产生了V个分支节点，v表示其中第v个分支节点，易知：分支节点包含的样本数越多，表示该分支节点的影响力越大。故可以计算出划分后相比原始数据集D获得的“信息增益”（information gain）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec3e067.png" alt="4.png"></p>
<p>Ent(D)划分前的信息增益 -划分后的信息增益</p>
<p>DV/D表示第V个分支的权重，样本越多越重要</p>
<p>信息增益越大，表示使用该属性划分样本集D的效果越好，因此ID3算法在递归过程中，每次选择最大信息增益的属性作为当前的划分属性。</p>
<h4 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h4><p>ID3算法存在一个问题，就是偏向于取值数目较多的属性，例如：如果存在一个唯一标识，这样样本集D将会被划分为|D|个分支，每个分支只有一个样本，这样划分后的信息熵为零，十分纯净，但是对分类毫无用处。因此C4.5算法使用了“增益率”（gain ratio）来选择划分属性，来避免这个问题带来的困扰。首先使用ID3算法计算出信息增益高于平均水平的候选属性，接着C4.5计算这些候选属性的增益率，增益率定义为：</p>
<p>启发式：先从候选划分属性中找出信息增益高于平均水平的，再从中选取增益率最高的</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec69647.png" alt="5.png"></p>
<h4 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h4><p>CART决策树使用“基尼指数”（Gini index）来选择划分属性，基尼指数反映的是从样本集D中随机抽取两个样本，其类别标记不一致的概率，因此Gini(D)越小越好，数据集D的纯度越高。基尼指数定义如下：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec5a2ff.png" alt="6.png"></p>
<p>进而，使用属性α划分后的基尼指数为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec62eaf.png" alt="7.png"></p>
<p>二分类视角看CART</p>
<ul>
<li>每一个产生分支的过程是一个二分类过程</li>
<li>这个过程叫作“决策树桩”</li>
<li>一棵CART是由许多决策树桩拼接起来的</li>
<li>决策树桩是只有一层的决策树</li>
</ul>
<h4 id="三种不同的决策树"><a href="#三种不同的决策树" class="headerlink" title="三种不同的决策树"></a>三种不同的决策树</h4><ul>
<li>ID3:取值多的属性，更容易使数据更纯，其信息增益更大；训练得到的是一棵庞大且深度浅的树：不合理</li>
<li>C4.5:采用信息增益率替代信息增益</li>
<li>CART：以基尼系数替代熵；最小化不纯度，而不是最大化信息增益</li>
</ul>
<h4 id="剪枝处理"><a href="#剪枝处理" class="headerlink" title="剪枝处理"></a>剪枝处理</h4><p>从决策树的构造流程中我们可以直观地看出：不管怎么样的训练集，决策树总是能很好地将各个类别分离开来，这时就会遇到之前提到过的问题：过拟合（overfitting），即太依赖于训练样本。剪枝（pruning）则是决策树算法对付过拟合的主要手段，剪枝的策略有两种如下：</p>
<pre><code>* 预剪枝（prepruning）：在构造的过程中先评估，再考虑是否分支。
* 后剪枝（post-pruning）：在构造好一颗完整的决策树后，自底向上，评估分支的必要性。</code></pre><p>评估指的是性能度量，即决策树的泛化性能。之前提到：可以使用测试集作为学习器泛化性能的近似，因此可以将数据集划分为训练集和测试集。预剪枝表示在构造数的过程中，对一个节点考虑是否分支时，首先计算决策树不分支时在测试集上的性能，再计算分支之后的性能，若分支对性能没有提升，则选择不分支（即剪枝）。后剪枝则表示在构造好一颗完整的决策树后，从最下面的节点开始，考虑该节点分支对模型的性能是否有提升，若无则剪枝，即将该节点标记为叶子节点，类别标记为其包含样本最多的类别。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec80d34.png" alt="8.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec9e330.png" alt="9.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec9d497.png" alt="10.png"></p>
<p>上图分别表示不剪枝处理的决策树、预剪枝决策树和后剪枝决策树。预剪枝处理使得决策树的很多分支被剪掉，因此大大降低了训练时间开销，同时降低了过拟合的风险，但另一方面由于剪枝同时剪掉了当前节点后续子节点的分支，因此预剪枝“贪心”的本质阻止了分支的展开，在一定程度上带来了欠拟合的风险。而后剪枝则通常保留了更多的分支，因此采用后剪枝策略的决策树性能往往优于预剪枝，但其自底向上遍历了所有节点，并计算性能，训练时间开销相比预剪枝大大提升。</p>
<h4 id="连续值与缺失值处理"><a href="#连续值与缺失值处理" class="headerlink" title="连续值与缺失值处理"></a>连续值与缺失值处理</h4><p>对于连续值的属性，若每个取值作为一个分支则显得不可行，因此需要进行离散化处理，常用的方法为二分法，基本思想为：给定样本集D与连续属性α，二分法试图找到一个划分点t将样本集D在属性α上分为≤t与＞t。</p>
<pre><code>* 首先将α的所有取值按升序排列，所有相邻属性的均值作为候选划分点（n-1个，n为α所有的取值数目）。
* 计算每一个划分点划分集合D（即划分为两个分支）后的信息增益。
* 选择最大信息增益的划分点作为最优划分点。</code></pre><p><img src="https://i.loli.net/2018/10/17/5bc72a0968fad.png" alt="11.png"></p>
<p>现实中常会遇到不完整的样本，即某些属性值缺失。有时若简单采取剔除，则会造成大量的信息浪费，因此在属性值缺失的情况下需要解决两个问题：（1）如何选择划分属性。（2）给定划分属性，若某样本在该属性上缺失值，如何划分到具体的分支上。假定为样本集中的每一个样本都赋予一个权重，根节点中的权重初始化为1，则定义：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72a098f3be.png" alt="12.png"></p>
<p>对于（1）：通过在样本集D中选取在属性α上没有缺失值的样本子集，计算在该样本子集上的信息增益，最终的信息增益等于该样本子集划分后信息增益乘以样本子集占样本集的比重。即：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72a096ccc3.png" alt="13.png"></p>
<p>对于（2）：若该样本子集在属性α上的值缺失，则将该样本以不同的权重（即每个分支所含样本比例）划入到所有分支节点中。该样本在分支节点中的权重变为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72a093ed3c.png" alt="14.png"></p>
<h4 id="Bootstraping"><a href="#Bootstraping" class="headerlink" title="Bootstraping"></a>Bootstraping</h4><p>称为自助法，它是一种有放回的抽样方法</p>
<p>####Bagging的策略</p>
<ul>
<li>bootstrap aggregation</li>
<li>从样本集中重采样(有重复的)选出n个样本</li>
<li>在所有属性上，对这n个样本建立分类器(ID3、C4.5、CART、SVM、Logistic回归等)</li>
<li>重复以上两步m次，即获得了m个分类器</li>
<li>将数据放在这m个分类器上，最后根据这m个分类器的投票结果，决定数据属于哪一类</li>
</ul>
<h4 id="OOB数据"><a href="#OOB数据" class="headerlink" title="OOB数据"></a>OOB数据</h4><p>可以发现，Bootstrap每次约有36.79%的样本不会出现在Bootstrap所采集的样本集合中，将未参与模型训练的数据称为袋外数据(out of bag)。它可以用于取代测试集用于误差估计。得到的模型参数是无偏估计。</p>
<h4 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h4><p>随机森林在bagging基础上做了修改</p>
<ul>
<li>从样本集中用bootstrap采样选出n个样本</li>
<li>从所有属性中随机选择k个属性，选择最佳分割属性作为节点建立CART决策树</li>
<li>重复以上两步m次，即建立m课CART决策树</li>
<li>这m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类</li>
</ul>
<h4 id="随机森林-bagging和决策树的关系"><a href="#随机森林-bagging和决策树的关系" class="headerlink" title="随机森林/bagging和决策树的关系"></a>随机森林/bagging和决策树的关系</h4><ul>
<li>当然可以使用决策树作为基本分类器</li>
<li>但也可以使用SVM、Logistics回归等其他分类，习惯上，这些分类器组成的“总分类器”，仍然叫做随机森林</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/决策树/">决策树</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-机器学习逻辑回归与softmax" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/08/29/机器学习逻辑回归与softmax/" class="article-date">
      <time datetime="2019-08-29T08:39:02.000Z" itemprop="datePublished">2019-08-29</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/29/机器学习逻辑回归与softmax/">机器学习逻辑回归与softmax</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="机器学习逻辑回归与softmax"><a href="#机器学习逻辑回归与softmax" class="headerlink" title="机器学习逻辑回归与softmax"></a>机器学习逻辑回归与softmax</h1><h2 id="机器学习中的线性模型"><a href="#机器学习中的线性模型" class="headerlink" title="机器学习中的线性模型"></a>机器学习中的线性模型</h2><p>谈及线性模型，其实我们很早就已经与它打过交道，还记得高中数学必修3课本中那个顽皮的“最小二乘法”吗？这就是线性模型的经典算法之一：根据给定的（x，y）点对，求出一条与这些点拟合效果最好的直线y=ax+b，之前我们利用下面的公式便可以计算出拟合直线的系数a,b（3.1中给出了具体的计算过程），从而对于一个新的x，可以预测它所对应的y值。前面我们提到：在机器学习的术语中，当预测值为连续值时，称为“回归问题”，离散值时为“分类问题”。本篇先从线性回归任务开始，接着讨论分类和多分类问题。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b068e48.png" alt="1.png"></p>
<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>线性回归问题就是试图学到一个线性模型尽可能准确地预测新样本的输出值，例如：通过历年的人口数据预测2017年人口数量。在这类问题中，往往我们会先得到一系列的有标记数据，例如：2000–&gt;13亿…2016–&gt;15亿，这时输入的属性只有一个，即年份；也有输入多属性的情形，假设我们预测一个人的收入，这时输入的属性值就不止一个了，例如：（学历，年龄，性别，颜值，身高，体重）–&gt;15k。</p>
<p>有时这些输入的属性值并不能直接被我们的学习模型所用，需要进行相应的处理，对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；对于离散值的属性，可作下面的处理：</p>
<ul>
<li><p>若属性值之间存在“序关系”，则可以将其转化为连续值，例如：身高属性分为“高”“中等”“矮”，可转化为数值：{1， 0.5， 0}。</p>
</li>
<li><p>若属性值之间不存在“序关系”，则通常将其转化为向量的形式，例如：性别属性分为“男”“女”，可转化为二维向量：{（1，0），（0，1）}。</p>
</li>
</ul>
<p>（1）当输入属性只有一个的时候，就是最简单的情形，也就是我们高中时最熟悉的“最小二乘法”（Euclidean distance），首先计算出每个样本预测值与真实值之间的误差并求和，通过最小化均方误差MSE，使用求偏导等于零的方法计算出拟合直线y=wx+b的两个参数w和b，计算过程如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0ccec4.png" alt="2.png"></p>
<p>（2）当输入属性有多个的时候，例如对于一个样本有d个属性{（x1,x2…xd）,y}，则y=wx+b需要写成：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72567b8bcd.png" alt="0.png"></p>
<p>通常对于多元问题，常常使用矩阵的形式来表示数据。在本问题中，将具有m个样本的数据集表示成矩阵X，将系数w与b合并成一个列向量，这样每个样本的预测值以及所有样本的均方误差最小化就可以写成下面的形式：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0ad8f7.png" alt="3.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0af652.png" alt="4.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b090543.png" alt="5.png"></p>
<p><img src="/blog_picture/line04.jpg" alt="avatar"></p>
<p><img src="/blog_picture/line05.jpg" alt="avatar"></p>
<p><img src="/blog_picture/line06.jpg" alt="avatar"></p>
<p><img src="/blog_picture/line07.jpg" alt="avatar"></p>
<h3 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h3><p>同样地，我们使用最小二乘法对w和b进行估计，令均方误差的求导等于0，需要注意的是，当一个矩阵的行列式不等于0时，我们才可能对其求逆，因此对于下式，我们需要考虑矩阵（X的转置*X）的行列式是否为0，若不为0，则可以求出其解，若为0，则需要使用其它的方法进行计算，书中提到了引入正则化，此处不进行深入。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0cde33.png" alt="6.png"></p>
<p>然而现实任务中当特征数量大于样本数时，XTX不满秩，此时θ有多个解；而且当数据量大时，求矩阵的逆非常耗时；对于不可逆矩阵（特征之间不相互独立），这种正规方程方法是不能用的。所以，还可以采用梯度下降法，利用迭代的方式求解θ。</p>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>梯度下降法是按下面的流程进行的：<br>1）首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。<br>2）改变θ的值，使得θ按梯度下降的方向进行减少。</p>
<p><img src="/blog_picture/line01.jpg" alt="avatar"></p>
<p>对于只有两维属性的样本，J(θ)即J(θ0,θ1)的等高线图</p>
<p><img src="/blog_picture/line02.jpg" alt="avatar"></p>
<p><img src="/blog_picture/line03.jpg" alt="avatar"></p>
<p>迭代更新的方式有多种</p>
<ul>
<li>批量梯度下降（batch gradient descent），也就是是梯度下降法最原始的形式，对全部的训练数据求得误差后再对θ<br>进行更新，优点是每步都趋向全局最优解；缺点是对于大量数据，由于每步要计算整体数据，训练过程慢；</li>
<li>随机梯度下降（stochastic gradient descent），每一步随机选择一个样本对θ<br>进行更新，优点是训练速度快；缺点是每次的前进方向不好确定，容易陷入局部最优；</li>
<li>微型批量梯度下降（mini-batch gradient descent），每步选择一小批数据进行批量梯度下降更新θ<br>，属于批量梯度下降和随机梯度下降的一种折中，非常适合并行处理。</li>
</ul>
<p>另一方面，有时像上面这种原始的线性回归可能并不能满足需求，例如：y值并不是线性变化，而是在指数尺度上变化。这时我们可以采用线性模型来逼近y的衍生物，例如lny，这时衍生的线性模型如下所示，实际上就是相当于将指数曲线投影在一条直线上，如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b103cbf.png" alt="7.png"></p>
<p>更一般地，考虑所有y的衍生物的情形，就得到了“广义的线性模型”（generalized linear model），其中，g（*）称为联系函数（link function）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0a2841.png" alt="8.png"></p>
<h3 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a>对数几率回归</h3><p>回归就是通过输入的属性值得到一个预测值，利用上述广义线性模型的特征，是否可以通过一个联系函数，将预测值转化为离散值从而进行分类呢？线性几率回归正是研究这样的问题。对数几率引入了一个对数几率函数（logistic function）,将预测值投影到0-1之间，从而将线性回归问题转化为二分类问题。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0c7748.png" alt="9.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0a655d.png" alt="10.png"></p>
<p>若将y看做样本为正例的概率，（1-y）看做样本为反例的概率，则上式实际上使用线性回归模型的预测结果器逼近真实标记的对数几率。因此这个模型称为“对数几率回归”（logistic regression），也有一些书籍称之为“逻辑回归”。下面使用最大似然估计的方法来计算出w和b两个参数的取值，下面只列出求解的思路，不列出具体的计算过程。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b824f0c.png" alt="11.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b817961.png" alt="12.png"></p>
<h3 id="线性判别分析"><a href="#线性判别分析" class="headerlink" title="线性判别分析"></a>线性判别分析</h3><p>线性判别分析（Linear Discriminant Analysis，简称LDA）,其基本思想是：将训练样本投影到一条直线上，使得同类的样例尽可能近，不同类的样例尽可能远。如图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b863ebb.png" alt="13.png"><img src="https://i.loli.net/2018/10/17/5bc723b85bfa9.png" alt="14.png"></p>
<p>想让同类样本点的投影点尽可能接近，不同类样本点投影之间尽可能远，即：让各类的协方差之和尽可能小，不用类之间中心的距离尽可能大。基于这样的考虑，LDA定义了两个散度矩阵。</p>
<ul>
<li>类内散度矩阵（within-class scatter matrix）</li>
</ul>
<p><img src="https://i.loli.net/2018/10/17/5bc723b8156e1.png" alt="15.png"></p>
<ul>
<li>类间散度矩阵(between-class scaltter matrix)</li>
</ul>
<p><img src="https://i.loli.net/2018/10/17/5bc723b7e9db3.png" alt="16.png"></p>
<p>因此得到了LDA的最大化目标：“广义瑞利商”（generalized Rayleigh quotient）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b7e8a61.png" alt="17.png"></p>
<p>从而分类问题转化为最优化求解w的问题，当求解出w后，对新的样本进行分类时，只需将该样本点投影到这条直线上，根据与各个类别的中心值进行比较，从而判定出新样本与哪个类别距离最近。求解w的方法如下所示，使用的方法为λ乘子。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b83d5e0.png" alt="18.png"></p>
<p>若将w看做一个投影矩阵，类似PCA的思想，则LDA可将样本投影到N-1维空间（N为类簇数），投影的过程使用了类别信息（标记信息），因此LDA也常被视为一种经典的监督降维技术。    </p>
<h3 id="回归与欠-过拟合"><a href="#回归与欠-过拟合" class="headerlink" title="回归与欠/过拟合"></a>回归与欠/过拟合</h3><p><img src="/blog_picture/line08.jpg" alt="avatar">    </p>
<h3 id="线性回归与正则化"><a href="#线性回归与正则化" class="headerlink" title="线性回归与正则化"></a>线性回归与正则化</h3><p><img src="/blog_picture/line09.jpg" alt="avatar">     </p>
<h3 id="多分类学习"><a href="#多分类学习" class="headerlink" title="多分类学习"></a>多分类学习</h3><p>现实中我们经常遇到不只两个类别的分类问题，即多分类问题，在这种情形下，我们常常运用“拆分”的策略，通过多个二分类学习器来解决多分类问题，即将多分类问题拆解为多个二分类问题，训练出多个二分类学习器，最后将多个分类结果进行集成得出结论。最为经典的拆分策略有三种：“一对一”（OvO）、“一对其余”（OvR）和“多对多”（MvM），核心思想与示意图如下所示。</p>
<ul>
<li><p>OvO：给定数据集D，假定其中有N个真实类别，将这N个类别进行两两配对（一个正类/一个反类），从而产生N（N-1）/2个二分类学习器，在测试阶段，将新样本放入所有的二分类学习器中测试，得出N（N-1）个结果，最终通过投票产生最终的分类结果。</p>
</li>
<li><p>OvM：给定数据集D，假定其中有N个真实类别，每次取出一个类作为正类，剩余的所有类别作为一个新的反类，从而产生N个二分类学习器，在测试阶段，得出N个结果，若仅有一个学习器预测为正类，则对应的类标作为最终分类结果。</p>
</li>
<li><p>MvM：给定数据集D，假定其中有N个真实类别，每次取若干个类作为正类，若干个类作为反类（通过ECOC码给出，编码），若进行了M次划分，则生成了M个二分类学习器，在测试阶段（解码），得出M个结果组成一个新的码，最终通过计算海明/欧式距离选择距离最小的类别作为最终分类结果。</p>
</li>
</ul>
<p><img src="https://i.loli.net/2018/10/17/5bc723b862bfb.png" alt="19.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b8300d5.png" alt="20.png"></p>
<h3 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h3><p>类别不平衡（class-imbanlance）就是指分类问题中不同类别的训练样本相差悬殊的情况，例如正例有900个，而反例只有100个，这个时候我们就需要进行相应的处理来平衡这个问题。常见的做法有三种：</p>
<ol>
<li>在训练样本较多的类别中进行“欠采样”（undersampling）,比如从正例中采出100个，常见的算法有：EasyEnsemble。</li>
<li>在训练样本较少的类别中进行“过采样”（oversampling）,例如通过对反例中的数据进行插值，来产生额外的反例，常见的算法有SMOTE。</li>
<li>直接基于原数据集进行学习，对预测值进行“再缩放”处理。其中再缩放也是代价敏感学习的基础。<img src="https://i.loli.net/2018/10/17/5bc726fe87ae2.png" alt="21.png"></li>
</ol>
<h3 id="LR应用经验"><a href="#LR应用经验" class="headerlink" title="LR应用经验"></a>LR应用经验</h3><p>LR实现简单高效易解释，计算速度快，易并行，在大规模数据情况下非常适用，更适合于应对数值型和标称型数据，主要适合解决线性可分的问题，但容易欠拟合，大多数情况下需要手动进行特征工程，构建组合特征，分类精度不高。</p>
<p>LR直接对分类可能性进行建模，无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题<br>LR能以概率的形式输出，而非知识0，1判定，对许多利用概率辅助决策的任务很有用<br>对率函数任意阶可导，具有很好的数学性质，许多现有的数值优化算法都可以用来求最优解，训练速度快<br>适用情景：LR是很多分类算法的基础组件，它的好处是输出值自然地落在0到1之间，并且有概率意义。因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。虽然效果一般，却胜在模型清晰，背后的概率学经得住推敲。它拟合出来的参数就代表了每一个特征(feature)对结果的影响。也是一个理解数据的好工具。</p>
<p>应用上： </p>
<ul>
<li>CTR预估，推荐系统的learning to rank，各种分类场景 </li>
<li>某搜索引擎厂的广告CTR预估基线版是LR </li>
<li>某电商搜索排序基线版是LR </li>
<li>某新闻app排序基线版是LR</li>
</ul>
<p>大规模工业实时数据，需要可解释性的金融数据，需要快速部署低耗时数据<br>LR就是简单，可解释，速度快，消耗资源少，分布式性能好</p>
<p>ADMM-LR:用ADMM求解LogisticRegression的优化方法称作ADMM_LR。ADMM算法是一种求解约束问题的最优化方法，它适用广泛。相比于SGD，ADMM在精度要求不高的情况下，在少数迭代轮数时就达到一个合理的精度，但是收敛到很精确的解则需要很多次迭代。</p>
<p><img src="/blog_picture/line10.jpg" alt="avatar">   </p>
<p><img src="/blog_picture/line11.jpg" alt="avatar">   </p>
<p><img src="/blog_picture/line12.jpg" alt="avatar">   </p>
<h2 id="LR多分类推广-Softmax回归"><a href="#LR多分类推广-Softmax回归" class="headerlink" title="LR多分类推广 - Softmax回归"></a>LR多分类推广 - Softmax回归</h2><p>LR是一个传统的二分类模型，它也可以用于多分类任务，其基本思想是：将多分类任务拆分成若干个二分类任务，然后对每个二分类任务训练一个模型，最后将多个模型的结果进行集成以获得最终的分类结果。一般来说，可以采取的拆分策略有：</p>
<h3 id="one-vs-one策略"><a href="#one-vs-one策略" class="headerlink" title="one vs one策略"></a>one vs one策略</h3><p>　　假设我们有N个类别，该策略基本思想就是不同类别两两之间训练一个分类器，这时我们一共会训练出<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171104175530607-1392543504.png" alt="img">种不同的分类器。在预测时，我们将样本提交给所有的分类器，一共会获得N(N-1)个结果，最终结果通过<strong>投票</strong>产生。</p>
<h3 id="one-vs-all策略"><a href="#one-vs-all策略" class="headerlink" title="one vs all策略"></a>one vs all策略</h3><p>　　该策略基本思想就是将第i种类型的所有样本作为正例，将剩下的所有样本作为负例，进行训练得到一个分类器。这样我们就一共可以得到N个分类器。在预测时，我们将样本提交给所有的分类器，一共会获得N个结果，我们<strong>选择其中概率值最大</strong>的那个作为最终分类结果。 <img src="https://images2017.cnblogs.com/blog/1251096/201710/1251096-20171021171313943-1199609768.png" alt="img"></p>
<h2 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h2><p>　　softmax是LR在多分类的推广。与LR一样，同属于广义线性模型。什么是Softmax函数？假设我们有一个数组A，<img src="https://images2017.cnblogs.com/blog/1251096/201710/1251096-20171021164616881-992414484.png" alt="img">表示的是数组A中的第i个元素，那么这个元素的Softmax值就是</p>
<p>　　　　　　　　　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201710/1251096-20171021165228865-866731732.png" alt="img"></p>
<p>也就是说，是该元素的指数，与所有元素指数和的比值。那么 softmax回归模型的假设函数又是怎么样的呢？</p>
<p>　　　　　　　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105100956795-1587348606.png" alt="img"></p>
<p>由上式很明显可以得出，假设函数的分母其实就是对概率分布进行了归一化，使得所有类别的概率之和为1；也可以看出LR其实就是K=2时的Softmax。在参数获得上，我们可以采用one vs all策略获得K个不同的训练数据集进行训练，进而针对每一类别都会得到一组参数向量<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102153701-629755133.png" alt="img">。当测试样本特征向量<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102416560-1451219507.png" alt="img">输入时，我们先用假设函数针对每一个类别<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102622045-1005416234.png" alt="img">估算出概率值<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102706623-369597312.png" alt="img">。因此我们的假设函数将要输出一个K维的向量（向量元素和为1）来表示K个类别的估计概率，我们选择其中得分最大的类别作为该输入的预测类别。Softmax看起来和one vs all 的LR很像，它们最大的不同在与Softmax得到的K个类别的得分和为1，而one vs all的LR并不是。</p>
<h3 id="softmax的代价函数"><a href="#softmax的代价函数" class="headerlink" title="softmax的代价函数"></a>softmax的代价函数</h3><p>　　类似于LR，其似然函数我们采用对数似然，故：</p>
<p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105113747779-692061991.png" alt="img"></p>
<p>加入<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171104162333248-539020480.png" alt="img">正则项的损失函数为：</p>
<p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105114132232-517057992.png" alt="img"></p>
<p>此处的<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105110553560-1026190635.png" alt="img">为符号函数。对于其参数的求解过程，我们依然采用梯度下降法。</p>
<h3 id="softmax的梯度的求解"><a href="#softmax的梯度的求解" class="headerlink" title="softmax的梯度的求解"></a>softmax的梯度的求解</h3><p>　　正则化项的求导很简单，就等于<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105120226607-495282914.png" alt="img">，下面我们主要讨论没有加正则项的损失函数的梯度求解，即</p>
<p>　　　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105113747779-692061991.png" alt="img"></p>
<p>的导数（梯度）。为了使得求解过程看起来简便、易于理解，我们仅仅只对于一个样本（x,y）情况（SGD）进行讨论，</p>
<p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105161912482-1607069737.png" alt="img"></p>
<p>此时，我们令</p>
<p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105162625888-1575402902.png" alt="img"></p>
<p>可以得到</p>
<p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105163457810-492161690.png" alt="img"></p>
<p>故：</p>
<p><img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105170233232-810575386.png" alt="img"></p>
<p>所以，正则化之后的损失函数的梯度为</p>
<p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105171748341-1281292385.png" alt="img"></p>
<p>然后通过梯度下降法最小化 <img src="http://ufldl.stanford.edu/wiki/images/math/c/e/0/ce027336c1cb3c0cd461406c81369ebf.png" alt="\textstyle J(\theta)">，我们就能实现一个可用的 softmax 回归模型了。</p>
<h3 id="多分类LR与Softmax回归"><a href="#多分类LR与Softmax回归" class="headerlink" title="多分类LR与Softmax回归"></a>多分类LR与Softmax回归</h3><p>　　有了多分类的处理方法，那么我们什么时候该用多分类LR？什么时候要用softmax呢？</p>
<p>总的来说，若待分类的<strong>类别互斥</strong>，我们就使用Softmax方法；若待分类的<strong>类别有相交</strong>，我们则要选用多分类LR，然后投票表决。</p>
<h2 id="Softmax分类器"><a href="#Softmax分类器" class="headerlink" title="Softmax分类器"></a>Softmax分类器</h2><p>SVM是最常用的两个分类器之一，而另一个就是<strong>Softmax分类器，</strong>它的损失函数与SVM的损失函数不同。对于学习过二元逻辑回归分类器的读者来说，Softmax分类器就可以理解为逻辑回归分类器面对多个分类的一般化归纳。SVM将输出<img src="https://www.zhihu.com/equation?tex=f%28x_i%2CW%29" alt="[公式]">作为每个分类的评分（因为无定标，所以难以直接解释）。与SVM不同，Softmax的输出（归一化的分类概率）更加直观，并且从概率上可以解释，这一点后文会讨论。在Softmax分类器中，函数映射<img src="https://www.zhihu.com/equation?tex=f%28x_i%3BW%29%3DWx_i" alt="[公式]">保持不变，但将这些评分值视为每个分类的未归一化的对数概率，并且将<em>折叶损失（hinge loss）</em>替换为<strong>交叉熵损失</strong>（<strong>cross-entropy loss）</strong>。公式如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+Li%3D-log%28%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%29" alt="[公式]"> 或等价的 <img src="https://www.zhihu.com/equation?tex=L_i%3D-f_%7By_i%7D%2Blog%28%5Csum_je%5E%7Bf_j%7D%29" alt="[公式]"></p>
<p>在上式中，使用<img src="https://www.zhihu.com/equation?tex=f_j" alt="[公式]">来表示分类评分向量<img src="https://www.zhihu.com/equation?tex=f" alt="[公式]">中的第j个元素。和之前一样，整个数据集的损失值是数据集中所有样本数据的损失值<img src="https://www.zhihu.com/equation?tex=L_i" alt="[公式]">的均值与正则化损失<img src="https://www.zhihu.com/equation?tex=R%28W%29" alt="[公式]">之和。其中函数<img src="https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D" alt="[公式]">被称作<strong>softmax 函数</strong>：其输入值是一个向量，向量中元素为任意实数的评分值（<img src="https://www.zhihu.com/equation?tex=z" alt="[公式]">中的），函数对其进行压缩，输出一个向量，其中每个元素值在0到1之间，且所有元素之和为1。所以，包含softmax函数的完整交叉熵损失看起唬人，实际上还是比较容易理解的。</p>
<p><strong>信息理论视角</strong>：在“真实”分布<img src="https://www.zhihu.com/equation?tex=p" alt="[公式]">和估计分布<img src="https://www.zhihu.com/equation?tex=q" alt="[公式]">之间的<em>交叉熵</em>定义如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29" alt="[公式]"></p>
<p><strong>*译者注</strong>：Kullback-Leibler差异（Kullback-Leibler Divergence）也叫做相对熵（Relative Entropy），它衡量的是相同事件空间里的两个概率分布的差异情况。*</p>
<p><strong>概率论解释</strong>：先看下面的公式：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28y_i%7Cx_i%2CW%29%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D" alt="[公式]"></p>
<p><strong>实操事项：数值稳定。</strong>编程实现softmax函数计算的时候，中间项<img src="https://www.zhihu.com/equation?tex=e%5E%7Bf_%7By_i%7D%7D" alt="[公式]">和<img src="https://www.zhihu.com/equation?tex=%5Csum_j+e%5E%7Bf_j%7D" alt="[公式]">因为存在指数函数，所以数值可能非常大。除以大数值可能导致数值计算的不稳定，所以学会使用归一化技巧非常重要。如果在分式的分子和分母都乘以一个常数<img src="https://www.zhihu.com/equation?tex=C" alt="[公式]">，并把它变换到求和之中，就能得到一个从数学上等价的公式：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7BCe%5E%7Bf_%7By_i%7D%7D%7D%7BC%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%2BlogC%7D%7D%7B%5Csum_je%5E%7Bf_j%2BlogC%7D%7D" alt="[公式]"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">f = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>]) <span class="comment"># 例子中有3个分类，每个评分的数值都很大</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 不妙：数值问题，可能导致数值爆炸</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 那么将f中的值平移到最大值为0：</span></span><br><span class="line">f -= np.max(f) <span class="comment"># f becomes [-666, -333, 0]</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 现在OK了，将给出正确结果</span></span><br></pre></td></tr></table></figure>

<p><strong>让人迷惑的命名规则</strong>：精确地说，SVM分类器使用的是<em>折叶损失（hinge loss）</em>，有时候又被称为<em>最大边界损失（max-margin loss）</em>。Softmax分类器使用的是<em>交叉熵损失（corss-entropy loss）</em>。Softmax分类器的命名是从<em>softmax函数</em>那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。注意从技术上说“softmax损失（softmax loss）”是没有意义的，因为softmax只是一个压缩数值的函数。但是在这个说法常常被用来做简称。</p>
<h2 id="SVM和Softmax的比较"><a href="#SVM和Softmax的比较" class="headerlink" title="SVM和Softmax的比较"></a>SVM和Softmax的比较</h2><p>下图有助于区分这 Softmax和SVM这两种分类器：</p>
<p>————————————————————————————————————————</p>
<p><img src="https://pic1.zhimg.com/80/a90ce9e0ff533f3efee4747305382064_hd.png" alt="img"></p>
<p>针对一个数据点，SVM和Softmax分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量<strong>f</strong>（本节中是通过矩阵乘来实现）。不同之处在于对<strong>f</strong>中分值的解释：SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别2）的分值比其他分类的分值高出至少一个边界值。Softmax分类器将这些数值看做是每个分类没有归一化的<strong>对数概率</strong>，鼓励正确分类的归一化的对数概率变高，其余的变低。SVM的最终的损失值是1.58，Softmax的最终的损失值是0.452，但要注意这两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。</p>
<p>————————————————————————————————————————</p>
<p><strong>Softmax分类器为每个分类提供了“可能性”</strong>：SVM的计算是无标定的，而且难以针对所有分类的评分值给出直观解释。Softmax分类器则不同，它允许我们计算出对于所有分类标签的可能性。举个例子，针对给出的图像，SVM分类器可能给你的是一个[12.5, 0.6, -23.0]对应分类“猫”，“狗”，“船”。而softmax分类器可以计算出这三个标签的”可能性“是[0.9, 0.09, 0.01]，这就让你能看出对于不同分类准确性的把握。为什么我们要在”可能性“上面打引号呢？这是因为可能性分布的集中或离散程度是由正则化参数λ直接决定的，λ是你能直接控制的一个输入参数。举个例子，假设3个分类的原始分数是[1, -2, 0]，那么softmax函数就会计算：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5B1%2C-2%2C0%5D%5Cto%5Be%5E1%2Ce%5E%7B-2%7D%2Ce%5E0%5D%3D%5B2.71%2C0.14%2C1%5D%5Cto%5B0.7%2C0.04%2C0.26%5D" alt="[公式]"></p>
<p>现在，如果正则化参数λ更大，那么权重W就会被惩罚的更多，然后他的权重数值就会更小。这样算出来的分数也会更小，假设小了一半吧[0.5, -1, 0]，那么softmax函数的计算就是：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5B0.5%2C-1%2C0%5D%5Cto%5Be%5E%7B0.5%7D%2Ce%5E%7B-1%7D%2Ce%5E0%5D%3D%5B1.65%2C0.73%2C1%5D%5Cto%5B0.55%2C0.12%2C0.33%5D" alt="[公式]"></p>
<p>现在看起来，概率的分布就更加分散了。还有，随着正则化参数λ不断增强，权重数值会越来越小，最后输出的概率会接近于均匀分布。这就是说，softmax分类器算出来的概率最好是看成一种对于分类正确性的自信。和SVM一样，数字间相互比较得出的大小顺序是可以解释的，但其绝对值则难以直观解释<strong>。</strong></p>
<p><strong>在实际使用中，SVM和Softmax经常是相似的</strong>：通常说来，两种分类器的表现差别很小，不同的人对于哪个分类器更好有不同的看法。相对于Softmax分类器，SVM更加“局部目标化（local objective）”，这既可以看做是一个特性，也可以看做是一个劣势。考虑一个评分是[10, -2, 3]的数据，其中第一个分类是正确的。那么一个SVM（<img src="https://www.zhihu.com/equation?tex=%5CDelta+%3D1" alt="[公式]">）会看到正确分类相较于不正确分类，已经得到了比边界值还要高的分数，它就会认为损失值是0。SVM对于数字个体的细节是不关心的：如果分数是[10, -100, -100]或者[10, 9, 9]，对于SVM来说没设么不同，只要满足超过边界值等于1，那么损失值就等于0。</p>
<p>对于softmax分类器，情况则不同。对于[10, 9, 9]来说，计算出的损失值就远远高于[10, -100, -100]的。换句话来说，softmax分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。但是，SVM只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。这可以被看做是SVM的一种特性。举例说来，一个汽车的分类器应该把他的大量精力放在如何分辨小轿车和大卡车上，而不应该纠结于如何与青蛙进行区分，因为区分青蛙得到的评分已经足够低了。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LR/">LR</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/softmax/">softmax</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</div>
      <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2019-2020 MingmingYe
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>

    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>