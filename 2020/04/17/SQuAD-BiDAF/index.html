<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no">
<meta name="author" content="MingmingYe">



<meta name="description" content="代码是在githubBiDAF-pytorch上下载的，我把代码弄成了下面jupyter notebook格式，代码是在kaggle GPU跑的， 数据集如果不能下载的可以到我的网盘下载，包括数据集和训练好的模型，比较大：百度网盘下载地址 整个代码跑下来，训练集可以跑通，测试集当时跑的时候kaggle内存不够了，报错了，有兴趣可以试下最终的效果。 In [ ]: 12  In [ ]: 12345">
<meta name="keywords" content="SQuAD-BiDAF">
<meta property="og:type" content="article">
<meta property="og:title" content="SQuAD-BiDAF">
<meta property="og:url" content="http://mmyblog.cn/2020/04/17/SQuAD-BiDAF/index.html">
<meta property="og:site_name" content="Stay hungry, Stay foolish.">
<meta property="og:description" content="代码是在githubBiDAF-pytorch上下载的，我把代码弄成了下面jupyter notebook格式，代码是在kaggle GPU跑的， 数据集如果不能下载的可以到我的网盘下载，包括数据集和训练好的模型，比较大：百度网盘下载地址 整个代码跑下来，训练集可以跑通，测试集当时跑的时候kaggle内存不够了，报错了，有兴趣可以试下最终的效果。 In [ ]: 12  In [ ]: 12345">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://s1.ax1x.com/2020/06/09/t4C2yd.jpg">
<meta property="og:updated_time" content="2020-06-09T01:31:28.801Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SQuAD-BiDAF">
<meta name="twitter:description" content="代码是在githubBiDAF-pytorch上下载的，我把代码弄成了下面jupyter notebook格式，代码是在kaggle GPU跑的， 数据集如果不能下载的可以到我的网盘下载，包括数据集和训练好的模型，比较大：百度网盘下载地址 整个代码跑下来，训练集可以跑通，测试集当时跑的时候kaggle内存不够了，报错了，有兴趣可以试下最终的效果。 In [ ]: 12  In [ ]: 12345">
<meta name="twitter:image" content="https://s1.ax1x.com/2020/06/09/t4C2yd.jpg">

<link rel="apple-touch-icon" href="/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="Stay hungry, Stay foolish." type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">



<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>SQuAD-BiDAF | Stay hungry, Stay foolish.</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: true
    }
</script>


    <script>
        yiliaConfig.jquery_ui = [true, "//cdn.bootcss.com/jqueryui/1.10.4/jquery-ui.min.js", "//cdn.bootcss.com/jqueryui/1.10.4/css/jquery-ui.min.css"];
    </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






</head></html>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/deep.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">MingmingYe</a></h1>
        </hgroup>

        
        <p class="header-subtitle">当你的才华撑不起你的野心时，只有静下心来好好学习！纵使命运注定是个打酱油的，也要打一瓶与别人不一样的酱油！</p>
        

        
            <form id="search-form">
            <input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false" />
            <i class="fa fa-times" onclick="resetSearch()"></i>
            </form>
            <div id="local-search-result"></div>
            <p class='no-result'>No results found <i class='fa fa-spinner fa-pulse'></i></p>
        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:878759487@qq.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AUC/">AUC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention/">Attention</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/">BERT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BLUE/">BLUE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Beam-search/">Beam search</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CRF/">CRF</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ConvNet/">ConvNet</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ELMo/">ELMo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GPT/">GPT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GRU/">GRU</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gradient-Clipping/">Gradient Clipping</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LR/">LR</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LSTM/">LSTM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linear/">Linear</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Parsing/">Parsing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PyTorch/">PyTorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/QA/">QA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN-LSTM/">RNN/LSTM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ROC/">ROC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recursive-Neural-Networks/">Recursive Neural Networks</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQuAD-BiDAF/">SQuAD-BiDAF</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Seq2Seq/">Seq2Seq</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TorchText/">TorchText</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/">Transformer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer-XL/">Transformer-XL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/XLNet/">XLNet</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/boosting/">boosting</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cbow/">cbow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hierarchical-softmax/">hierarchical softmax</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/inference/">inference</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jiaba/">jiaba</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jieba/">jieba</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mumpy/">mumpy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/negative-sampling/">negative sampling</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pandas/">pandas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pyspark/">pyspark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/seaborn/">seaborn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/skip-gram/">skip-gram</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/softmax/">softmax</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/word-embedding/">word-embedding</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/wxBot/">wxBot</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/中文分词/">中文分词</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/优化方法/">优化方法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/决策树/">决策树</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/图神经网络/">图神经网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/微积分/">微积分</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/朴素贝叶斯/">朴素贝叶斯</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/概率/">概率</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/模型调优/">模型调优</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/正则表达式/">正则表达式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习速查表/">深度学习速查表</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/特征工程/">特征工程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/线性代数/">线性代数</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/统计/">统计</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/聊天机器人/">聊天机器人</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/聚类/">聚类</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/语言模型/">语言模型</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/贝叶斯分类器/">贝叶斯分类器</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/降维/">降维</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/集成学习/">集成学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/面试/">面试</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://mmyblog.cn/">mmy</a>
                    
                    </div>
                </section>
                

                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">MingmingYe</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/deep.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">MingmingYe</a></h1>
            </hgroup>
            
            <p class="header-subtitle">当你的才华撑不起你的野心时，只有静下心来好好学习！纵使命运注定是个打酱油的，也要打一瓶与别人不一样的酱油！</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:878759487@qq.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap"><article id="post-SQuAD-BiDAF" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/04/17/SQuAD-BiDAF/" class="article-date">
      <time datetime="2020-04-16T23:32:02.000Z" itemprop="datePublished">2020-04-17</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      SQuAD-BiDAF
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SQuAD-BiDAF/">SQuAD-BiDAF</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>代码是在github<a href="https://github.com/galsang/BiDAF-pytorch" target="_blank" rel="noopener">BiDAF-pytorch</a>上下载的，我把代码弄成了下面jupyter notebook格式，代码是在kaggle GPU跑的，</p>
<p>数据集如果不能下载的可以到我的网盘下载，包括数据集和训练好的模型，比较大：<a href="https://pan.baidu.com/s/1XCEUG6E2biCdqFyaxIGtBw" target="_blank" rel="noopener">百度网盘下载地址</a></p>
<p>整个代码跑下来，训练集可以跑通，测试集当时跑的时候kaggle内存不够了，报错了，有兴趣可以试下最终的效果。</p>
<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This Python 3 environment comes with many helpful analytics libraries installed</span></span><br><span class="line"><span class="comment"># It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python</span></span><br><span class="line"><span class="comment"># For example, here's several helpful packages to load in </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># linear algebra</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># data processing, CSV file I/O (e.g. pd.read_csv)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Input data files are available in the "../input/" directory.</span></span><br><span class="line"><span class="comment"># For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">print(os.listdir(<span class="string">"../input"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Any results you write to the current directory are saved as output.</span></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!cp -r /kaggle/input/bidaf-pytorch-master/BiDAF-pytorch-master /kaggle/working</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.chdir(&quot;BiDAF-pytorch-master&quot;)</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.chdir(&quot;..&quot;)</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!ls</span><br><span class="line">!pwd</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> copy, json, os</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> gmtime, strftime</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> GloVe</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br></pre></td></tr></table></figure>

<h1 id="一、定义初始变量参数"><a href="#一、定义初始变量参数" class="headerlink" title="一、定义初始变量参数"></a>一、定义初始变量参数</h1><p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有关argparse的官方文档操作请查看：https://docs.python.org/3/library/argparse.html#module-argparse，</span></span><br><span class="line"><span class="comment"># 下面的参数代码注释，我也不是特别懂，仅供参考</span></span><br><span class="line"><span class="comment"># 关于parser.add_argument(）的详解请查看：https://blog.csdn.net/u013177568/article/details/62432761/</span></span><br><span class="line"><span class="comment"># 对于下面函数add_argument()第一个是选项是必须写的参数，该参数接受选项参数或者是位置参数（一串文件名）</span></span><br><span class="line"><span class="comment"># 第二个是default默认值，如果第一个选项参数没有单独指定，那选项参数的值就是默认值</span></span><br><span class="line"><span class="comment"># 第三个是参数数据类型，代表你的选项参数必须是是int还是float字符型数据。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_args</span><span class="params">()</span>:</span></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">'--char-dim'</span>, default=<span class="number">8</span>, type=int)</span><br><span class="line">    <span class="comment"># char-dim 默认值是8</span></span><br><span class="line">    parser.add_argument(<span class="string">'--char-channel-width'</span>, default=<span class="number">5</span>, type=int)</span><br><span class="line">    <span class="comment"># char-channel-width 默认值是5 以下类似</span></span><br><span class="line">    parser.add_argument(<span class="string">'--char-channel-size'</span>, default=<span class="number">100</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--context-threshold'</span>, default=<span class="number">400</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--dev-batch-size'</span>, default=<span class="number">100</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--dev-file'</span>, default=<span class="string">'dev-v1.1.json'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--dropout'</span>, default=<span class="number">0.2</span>, type=float)</span><br><span class="line">    parser.add_argument(<span class="string">'--epoch'</span>, default=<span class="number">12</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--exp-decay-rate'</span>, default=<span class="number">0.999</span>, type=float)</span><br><span class="line">    parser.add_argument(<span class="string">'--gpu'</span>, default=<span class="number">0</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--hidden-size'</span>, default=<span class="number">100</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--learning-rate'</span>, default=<span class="number">0.5</span>, type=float)</span><br><span class="line">    parser.add_argument(<span class="string">'--print-freq'</span>, default=<span class="number">250</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--train-batch-size'</span>, default=<span class="number">60</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--train-file'</span>, default=<span class="string">'train-v1.1.json'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--word-dim'</span>, default=<span class="number">100</span>, type=int)</span><br><span class="line">    args = parser.parse_args(args=[]) </span><br><span class="line"><span class="comment"># .parse_args()是将之前所有add_argument定义的参数在括号里进行赋值，没有赋值(args=[])，就返回参数各自default的默认值。</span></span><br><span class="line"><span class="comment"># 返回值args相当于是个参数命名空间的集合，可以调用上面第一项选项参数的名字，就可以得到default值了。</span></span><br><span class="line"><span class="comment"># 比如调用上面参数方式：args.char_dim,args.char_channel_width....默认情况下，中划线会转换为下划线.</span></span><br><span class="line">    <span class="keyword">return</span> args</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args = parse_args()</span><br></pre></td></tr></table></figure>

<h1 id="二、SQuAD问答数据预处理"><a href="#二、SQuAD问答数据预处理" class="headerlink" title="二、SQuAD问答数据预处理"></a>二、SQuAD问答数据预处理</h1><h2 id="1、查看数据集结构"><a href="#1、查看数据集结构" class="headerlink" title="1、查看数据集结构"></a>1、查看数据集结构</h2><p>SQuAD问答数据介绍：<a href="https://rajpurkar.github.io/SQuAD-explorer/" target="_blank" rel="noopener">https://rajpurkar.github.io/SQuAD-explorer/</a> 这个数据集有两个文件，验证集和测试集：train-v1.1.json，dev-v1.1.json</p>
<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'data/squad/dev-v1.1.json'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                line = f.readline() <span class="comment"># 该方法每次读出一行内容</span></span><br><span class="line">                <span class="keyword">if</span> line:</span><br><span class="line">                    print(<span class="string">"type(line)"</span>,type(line)) <span class="comment"># 直接打印就是字符串格式</span></span><br><span class="line">                    r = json.loads(line)</span><br><span class="line">                    print(<span class="string">"type(r)"</span>,type(r)) <span class="comment"># 使用json.loads将字符串转化为字典</span></span><br><span class="line">                    print(r)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            f.close()</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据架构如下</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"data"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"title"</span>: <span class="string">"Super_Bowl_50"</span>, <span class="comment"># 第一个主题</span></span><br><span class="line">            <span class="string">"paragraphs"</span>: [</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">"context"</span>: <span class="string">" numerals 50......."</span>, <span class="comment"># 每个主题会有很多context短文,这里只列出一个</span></span><br><span class="line">                    <span class="string">"qas"</span>: [  <span class="comment"># 这个列表里放问题和答案的位置，每篇context会有很有很多answer和question，这里只列出一个</span></span><br><span class="line">                        &#123;</span><br><span class="line">                            <span class="string">"answers"</span>: [  <span class="comment"># 一个问题会有三个答案，三个答案都是对的，只是在context不同或相同位置</span></span><br><span class="line">                                &#123;         <span class="comment"># 下面三个答案都在相同的位置</span></span><br><span class="line">                                    <span class="string">"answer_start"</span>: <span class="number">177</span>,  <span class="comment"># 答案在文中的起始位置是第177的字符。</span></span><br><span class="line">                                    <span class="string">"text"</span>: <span class="string">"Denver Broncos"</span></span><br><span class="line">                                &#125;,</span><br><span class="line">                                &#123;</span><br><span class="line">                                    <span class="string">"answer_start"</span>: <span class="number">177</span>,</span><br><span class="line">                                    <span class="string">"text"</span>: <span class="string">"Denver Broncos"</span></span><br><span class="line">                                &#125;,</span><br><span class="line">                                &#123;</span><br><span class="line">                                    <span class="string">"answer_start"</span>: <span class="number">177</span>,</span><br><span class="line">                                    <span class="string">"text"</span>: <span class="string">"Denver Broncos"</span></span><br><span class="line">                                &#125;</span><br><span class="line">                            ],</span><br><span class="line">                            <span class="string">"question"</span>: <span class="string">"Which NFL team represented the AFC at Super Bowl 50?"</span>,</span><br><span class="line">                            <span class="string">"id"</span>: <span class="string">"56be4db0acb8001400a502ec"</span></span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                    ]</span><br><span class="line">                &#125;</span><br><span class="line">                </span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"title"</span>: <span class="string">"Warsaw"</span>, <span class="comment"># 第二个主题</span></span><br><span class="line">            <span class="string">"paragraphs"</span>:   </span><br><span class="line">        &#125;,</span><br><span class="line">        </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"title"</span>: <span class="string">"Normans"</span>, <span class="comment"># 第三个主题</span></span><br><span class="line">            <span class="string">"paragraphs"</span>: </span><br><span class="line">        &#125;,</span><br><span class="line">        </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"title"</span>: <span class="string">"Nikola_Tesla"</span>, <span class="comment"># 第四个主题</span></span><br><span class="line">            <span class="string">"paragraphs"</span>: </span><br><span class="line">        &#125;,</span><br><span class="line">        ........... <span class="comment"># 还有很多</span></span><br><span class="line">        </span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"version"</span>: <span class="string">"1.1"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="2、定义分词方法"><a href="#2、定义分词方法" class="headerlink" title="2、定义分词方法"></a>2、定义分词方法</h2><p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_tokenize</span><span class="params">(tokens)</span>:</span></span><br><span class="line">    tokens = [token.replace(<span class="string">"''"</span>, <span class="string">'"'</span>).replace(<span class="string">"``"</span>, <span class="string">'"'</span>) <span class="keyword">for</span> token <span class="keyword">in</span> nltk.word_tokenize(tokens)]</span><br><span class="line">    <span class="comment"># nltk.word_tokenize(tokens)分词，replace规范化引号，方便后面处理</span></span><br><span class="line">    <span class="keyword">return</span> tokens</span><br></pre></td></tr></table></figure>

<h2 id="3、清洗数据，并生成数据迭代器"><a href="#3、清洗数据，并生成数据迭代器" class="headerlink" title="3、清洗数据，并生成数据迭代器"></a>3、清洗数据，并生成数据迭代器</h2><p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SQuAD</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 以下定好中间输出缓存文件的路径</span></span><br><span class="line">        path = <span class="string">'data/squad'</span> </span><br><span class="line">        dataset_path = path + <span class="string">'/torch_text/'</span> </span><br><span class="line">        train_examples_path = dataset_path + <span class="string">'train_examples.pt'</span></span><br><span class="line">        dev_examples_path = dataset_path + <span class="string">'dev_examples.pt'</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">"preprocessing data files..."</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.train_file&#125;</span>l'</span>):</span><br><span class="line">            <span class="comment"># 字符串前以f开头表示在字符串内支持大括号内的 python 表达式</span></span><br><span class="line">            <span class="comment"># args.train_file = 'train-v1.1.json'</span></span><br><span class="line">            print(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.train_file&#125;</span>'</span>)</span><br><span class="line">            self.preprocess_file(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.train_file&#125;</span>'</span>)  </span><br><span class="line">            <span class="comment"># preprocess_file下面函数有定义，完成文件的预处理</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.dev_file&#125;</span>l'</span>):</span><br><span class="line">            <span class="comment"># args.dev_file = 'dev-v1.1.json'</span></span><br><span class="line">            self.preprocess_file(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.dev_file&#125;</span>'</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 下面是用torchtext处理数据的步骤看不懂了，有知道的可以交流下   </span></span><br><span class="line">        self.RAW = data.RawField()<span class="comment"># 这个是完全空白的field，意味着不经过任何处理</span></span><br><span class="line">        <span class="comment"># explicit declaration for torchtext compatibility</span></span><br><span class="line">        self.RAW.is_target = <span class="literal">False</span></span><br><span class="line">        self.CHAR_NESTING = data.Field(batch_first=<span class="literal">True</span>, tokenize=list, lower=<span class="literal">True</span>)</span><br><span class="line">        self.CHAR = data.NestedField(self.CHAR_NESTING, tokenize=word_tokenize)</span><br><span class="line">        self.WORD = data.Field(batch_first=<span class="literal">True</span>, tokenize=word_tokenize, lower=<span class="literal">True</span>, include_lengths=<span class="literal">True</span>)</span><br><span class="line">        self.LABEL = data.Field(sequential=<span class="literal">False</span>, unk_token=<span class="literal">None</span>, use_vocab=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        dict_fields = &#123;<span class="string">'id'</span>: (<span class="string">'id'</span>, self.RAW),</span><br><span class="line">                       <span class="string">'s_idx'</span>: (<span class="string">'s_idx'</span>, self.LABEL),</span><br><span class="line">                       <span class="string">'e_idx'</span>: (<span class="string">'e_idx'</span>, self.LABEL),</span><br><span class="line">                       <span class="string">'context'</span>: [(<span class="string">'c_word'</span>, self.WORD), (<span class="string">'c_char'</span>, self.CHAR)],</span><br><span class="line">                       <span class="string">'question'</span>: [(<span class="string">'q_word'</span>, self.WORD), (<span class="string">'q_char'</span>, self.CHAR)]&#125;</span><br><span class="line"></span><br><span class="line">        list_fields = [(<span class="string">'id'</span>, self.RAW), (<span class="string">'s_idx'</span>, self.LABEL), (<span class="string">'e_idx'</span>, self.LABEL),</span><br><span class="line">                       (<span class="string">'c_word'</span>, self.WORD), (<span class="string">'c_char'</span>, self.CHAR),</span><br><span class="line">                       (<span class="string">'q_word'</span>, self.WORD), (<span class="string">'q_char'</span>, self.CHAR)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(dataset_path):</span><br><span class="line">            print(<span class="string">"loading splits..."</span>)</span><br><span class="line">            train_examples = torch.load(train_examples_path)</span><br><span class="line">            dev_examples = torch.load(dev_examples_path)</span><br><span class="line"></span><br><span class="line">            self.train = data.Dataset(examples=train_examples, fields=list_fields)</span><br><span class="line">            self.dev = data.Dataset(examples=dev_examples, fields=list_fields)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"building splits..."</span>)</span><br><span class="line">             <span class="comment"># 划分训练集和验证集</span></span><br><span class="line">            self.train, self.dev = data.TabularDataset.splits(</span><br><span class="line">                path=path,</span><br><span class="line">                train=<span class="string">f'<span class="subst">&#123;args.train_file&#125;</span>l'</span>,</span><br><span class="line">                validation=<span class="string">f'<span class="subst">&#123;args.dev_file&#125;</span>l'</span>,</span><br><span class="line">                format=<span class="string">'json'</span>,</span><br><span class="line">                fields=dict_fields)</span><br><span class="line"></span><br><span class="line">            os.makedirs(dataset_path)</span><br><span class="line">            torch.save(self.train.examples, train_examples_path)</span><br><span class="line">            torch.save(self.dev.examples, dev_examples_path)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#cut too long context in the training set for efficiency.</span></span><br><span class="line">        <span class="keyword">if</span> args.context_threshold &gt; <span class="number">0</span>:</span><br><span class="line">            self.train.examples = [e <span class="keyword">for</span> e <span class="keyword">in</span> self.train.examples <span class="keyword">if</span> len(e.c_word) &lt;= args.context_threshold]</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"building vocab..."</span>)</span><br><span class="line">        self.CHAR.build_vocab(self.train, self.dev) <span class="comment"># 字符向量没有设置vector</span></span><br><span class="line">        self.WORD.build_vocab(self.train, self.dev, vectors=GloVe(name=<span class="string">'6B'</span>, dim=args.word_dim))</span><br><span class="line">        <span class="comment"># 加载Glove向量，args.word_dim = 100</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">"building iterators..."</span>)</span><br><span class="line">        device = torch.device(<span class="string">f"cuda:<span class="subst">&#123;args.gpu&#125;</span>"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">        <span class="comment"># 生成迭代器</span></span><br><span class="line">        self.train_iter, self.dev_iter = \</span><br><span class="line">            data.BucketIterator.splits((self.train, self.dev),</span><br><span class="line">                                       batch_sizes=[args.train_batch_size, args.dev_batch_size],</span><br><span class="line">                                       device=device,</span><br><span class="line">                                       sort_key=<span class="keyword">lambda</span> x: len(x.c_word))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preprocess_file</span><span class="params">(self,path)</span>:</span></span><br><span class="line">        dump = []</span><br><span class="line">        abnormals = [<span class="string">' '</span>, <span class="string">'\n'</span>, <span class="string">'\u3000'</span>, <span class="string">'\u202f'</span>, <span class="string">'\u2009'</span>]</span><br><span class="line">        <span class="comment"># 空白无效字符列表</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> open(path, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            data = json.load(f) <span class="comment"># 直接文件句柄转化为字典</span></span><br><span class="line">            data = data[<span class="string">'data'</span>] <span class="comment"># 返回值data是个列表，字典是列表的元素</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> article <span class="keyword">in</span> data:</span><br><span class="line">                <span class="comment"># 每个article是一个字典，一个字典包含一个title的信息</span></span><br><span class="line">                <span class="keyword">for</span> paragraph <span class="keyword">in</span> article[<span class="string">'paragraphs'</span>]:</span><br><span class="line">                    <span class="comment"># 每个paragraph是一个字典，一个字典里有一个context和qas的信息，qas是问题和答案。</span></span><br><span class="line">                    context = paragraph[<span class="string">'context'</span>]</span><br><span class="line">                    <span class="comment"># context的内容，是字符串，如：" numerals 50............."</span></span><br><span class="line">                    tokens = word_tokenize(context) <span class="comment"># 对context进行分词</span></span><br><span class="line">                    <span class="keyword">for</span> qa <span class="keyword">in</span> paragraph[<span class="string">'qas'</span>]:</span><br><span class="line">                        <span class="comment"># 每个qa是一个字典，一个字典包含一对answers和question的信息</span></span><br><span class="line">                        id = qa[<span class="string">'id'</span>]</span><br><span class="line">                        <span class="comment"># 取出这对answers和question的id信息，如："56be4db0acb8001400a502ec"</span></span><br><span class="line">                        question = qa[<span class="string">'question'</span>]</span><br><span class="line">                        <span class="comment"># 取出question，如："Which NFL team represented the AFC at Super Bowl 50?"</span></span><br><span class="line">                        <span class="keyword">for</span> ans <span class="keyword">in</span> qa[<span class="string">'answers'</span>]:</span><br><span class="line">                            <span class="comment"># ans为每个答案，共有三个标准答案，可以相同，可以不同，统一为3个。</span></span><br><span class="line">                            answer = ans[<span class="string">'text'</span>]</span><br><span class="line">                            <span class="comment"># 问题的每个回答，如："Denver Broncos"</span></span><br><span class="line">                            s_idx = ans[<span class="string">'answer_start'</span>]</span><br><span class="line">                            <span class="comment"># 每个回答的start位置，数值代表context中第几个字符，如：177</span></span><br><span class="line">                            e_idx = s_idx + len(answer)</span><br><span class="line">                            <span class="comment"># 每个回答的end位置</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                            <span class="comment"># 下面重新更新字符的起始位置，使用字符计算位置改为使用单词计算位置</span></span><br><span class="line">                            <span class="comment"># 请看下面单元格的示例输出有助理解。</span></span><br><span class="line">                            l = <span class="number">0</span></span><br><span class="line">                            s_found = <span class="literal">False</span></span><br><span class="line">                            <span class="keyword">for</span> i, t <span class="keyword">in</span> enumerate(tokens):</span><br><span class="line">                                <span class="comment"># 循环t次，t为分词后的单词数量</span></span><br><span class="line">                                <span class="keyword">while</span> l &lt; len(context):</span><br><span class="line">                                    <span class="keyword">if</span> context[l] <span class="keyword">in</span> abnormals:</span><br><span class="line">                                        <span class="comment"># context中有空白无效字符，就计数</span></span><br><span class="line">                                        l += <span class="number">1</span></span><br><span class="line">                                    <span class="keyword">else</span>:    <span class="comment"># 一碰到不是空白字符的就break</span></span><br><span class="line">                                        <span class="keyword">break</span></span><br><span class="line">                                <span class="comment"># exceptional cases</span></span><br><span class="line">                                <span class="keyword">if</span> t[<span class="number">0</span>] == <span class="string">'"'</span> <span class="keyword">and</span> context[l:l + <span class="number">2</span>] == <span class="string">'\'\''</span>:</span><br><span class="line">                                    <span class="comment"># 专门计算context=''an 这种长度，这个长度为4</span></span><br><span class="line">                                    t = <span class="string">'\'\''</span> + t[<span class="number">1</span>:] </span><br><span class="line">                                <span class="keyword">elif</span> t == <span class="string">'"'</span> <span class="keyword">and</span> context[l:l + <span class="number">2</span>] == <span class="string">'\'\''</span>:</span><br><span class="line">                                    <span class="comment"># 专门计算context='' 这种长度</span></span><br><span class="line">                                    <span class="comment"># 上面t[0] == '"'表达式包含了这种，所以我认为这个表达式没用上</span></span><br><span class="line">                                    t = <span class="string">'\'\''</span></span><br><span class="line"></span><br><span class="line">                                l += len(t)</span><br><span class="line">                                <span class="keyword">if</span> l &gt; s_idx <span class="keyword">and</span> s_found == <span class="literal">False</span>:</span><br><span class="line">                                    <span class="comment"># 只要计数超过起始位置值，这个单词就是start的单词</span></span><br><span class="line">                                    s_idx = i</span><br><span class="line">                                    s_found = <span class="literal">True</span></span><br><span class="line">                                <span class="keyword">if</span> l &gt;= e_idx:</span><br><span class="line">                                    <span class="comment"># 这里不出错的话，等于e_idx就是end的单词</span></span><br><span class="line">                                    e_idx = i</span><br><span class="line">                                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">                            <span class="comment"># 这里把三个answer分开，每个answer都放进字典中,并作为一个样本</span></span><br><span class="line">                            dump.append(dict([(<span class="string">'id'</span>, id),</span><br><span class="line">                                              (<span class="string">'context'</span>, context),</span><br><span class="line">                                              (<span class="string">'question'</span>, question),</span><br><span class="line">                                              (<span class="string">'answer'</span>, answer),</span><br><span class="line">                                              (<span class="string">'s_idx'</span>, s_idx),</span><br><span class="line">                                              (<span class="string">'e_idx'</span>, e_idx)]))</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">f'<span class="subst">&#123;path&#125;</span>l'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> dump:</span><br><span class="line">                <span class="comment"># line为字典，一个样本存储</span></span><br><span class="line">                json.dump(line, f)</span><br><span class="line">                <span class="comment">#dump：将dict类型转换为json字符串格式，写入到文件</span></span><br><span class="line">                print(<span class="string">''</span>, file=f) <span class="comment"># 这里print的作用就是换行用的。</span></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = SQuAD(args)</span><br></pre></td></tr></table></figure>

<h3 id="上面不太明白的举例子"><a href="#上面不太明白的举例子" class="headerlink" title="上面不太明白的举例子"></a>上面不太明白的举例子</h3><p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 举例子</span></span><br><span class="line">a = <span class="string">" \u2009\n\u3000Super Bowl 50 was ''an'' American football     \u3000game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50."</span></span><br><span class="line">tokens = word_tokenize(a)</span><br><span class="line">print(nltk.word_tokenize(a)) <span class="comment"># 所有的“\u2009”，“\n”，“\u3000”等空白字符都去掉了</span></span><br><span class="line">print(<span class="string">"----"</span>*<span class="number">20</span>)</span><br><span class="line">print(tokens)</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># </span></span><br><span class="line">print(a[<span class="number">0</span>]) <span class="comment"># 空白字符打印不出来</span></span><br><span class="line">print(a[<span class="number">1</span>]) <span class="comment"># 空白字符打印不出来</span></span><br><span class="line">print(a[<span class="number">2</span>]) <span class="comment"># 空白字符打印不出来</span></span><br><span class="line">print(a[<span class="number">3</span>]) <span class="comment"># 空白字符打印不出来</span></span><br><span class="line">print(a[<span class="number">4</span>]) </span><br><span class="line">print(a[<span class="number">5</span>])</span><br><span class="line">a[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面特别注意</span></span><br><span class="line">print(tokens[<span class="number">4</span>][<span class="number">0</span>]== <span class="string">'"'</span>) <span class="comment"># 虽然切分后看起来是"''"，但实际上是'"'</span></span><br><span class="line">print(tokens[<span class="number">4</span>][<span class="number">0</span>]== <span class="string">"''"</span>) </span><br><span class="line">print(tokens[<span class="number">5</span>]== <span class="string">'"'</span>)</span><br><span class="line">print(len(<span class="string">'"'</span>)) <span class="comment"># 这种长度为1</span></span><br><span class="line">print(len(<span class="string">"''"</span>)) <span class="comment"># 这种长度为2</span></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看输出理解</span></span><br><span class="line">s_idx = <span class="number">177</span></span><br><span class="line">e_idx = s_idx + len(<span class="string">"Denver Broncos"</span>)</span><br><span class="line">l=<span class="number">0</span></span><br><span class="line">context = <span class="string">" \u2009\n\u3000Super Bowl 50 was ''an'' American football     \u3000game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50."</span></span><br><span class="line">tokens = word_tokenize(context)</span><br><span class="line">abnormals = [<span class="string">' '</span>, <span class="string">'\n'</span>, <span class="string">'\u3000'</span>, <span class="string">'\u202f'</span>, <span class="string">'\u2009'</span>]</span><br><span class="line">s_found = <span class="literal">False</span></span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> enumerate(tokens):</span><br><span class="line">    print(<span class="string">"t="</span>,t)</span><br><span class="line">    <span class="keyword">while</span> l &lt; len(context):</span><br><span class="line">        <span class="keyword">if</span> context[l] <span class="keyword">in</span> abnormals:</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    print(<span class="string">"l"</span>,l)</span><br><span class="line">    <span class="comment"># exceptional cases</span></span><br><span class="line">    <span class="keyword">if</span> t[<span class="number">0</span>] == <span class="string">'"'</span> <span class="keyword">and</span> context[l:l + <span class="number">2</span>] == <span class="string">'\'\''</span>:</span><br><span class="line">        print(<span class="string">"1111111111111111111"</span>)</span><br><span class="line">        print(t)</span><br><span class="line">        print(t[<span class="number">1</span>:])</span><br><span class="line">        t = <span class="string">'\'\''</span> + t[<span class="number">1</span>:]</span><br><span class="line">        print(t)</span><br><span class="line">    <span class="keyword">elif</span> t == <span class="string">'"'</span> <span class="keyword">and</span> context[l:l + <span class="number">2</span>] == <span class="string">'\'\''</span>:</span><br><span class="line">        <span class="comment"># 看输出结果，这个表达式没有用到</span></span><br><span class="line">        print(<span class="string">"22222222222222222222"</span>)</span><br><span class="line">        print(t)</span><br><span class="line">        t = <span class="string">'\'\''</span></span><br><span class="line">    print(<span class="string">"len(t)"</span>,len(t))</span><br><span class="line">    l += len(t)</span><br><span class="line">    print(<span class="string">"l"</span>,l)</span><br><span class="line">    <span class="keyword">if</span> l &gt; s_idx <span class="keyword">and</span> s_found == <span class="literal">False</span>:</span><br><span class="line">        s_idx = i</span><br><span class="line">        print(<span class="string">"s_idx"</span>,s_idx)</span><br><span class="line">        s_found = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> l &gt;= e_idx:</span><br><span class="line">        e_idx = i</span><br><span class="line">        print(<span class="string">"e_idx"</span>,e_idx)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch = next(iter(data.train_iter)) <span class="comment">#一个batch的信息</span></span><br><span class="line">print(batch)</span><br><span class="line"><span class="comment"># 训练集的batch_sizes=60</span></span><br><span class="line"><span class="comment"># batch.c_word = 60x293，293是60个样本中最长样本token的单词数</span></span><br><span class="line"><span class="comment"># batch.c_char = 60x293x25，25是某个单词字符的最大的数量</span></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(batch.q_word)</span><br><span class="line">print(batch.q_char[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面为args新增参数，并赋值</span></span><br><span class="line"><span class="comment"># hasattr() getattr() setattr() 函数使用方法详解https://www.cnblogs.com/cenyu/p/5713686.html</span></span><br><span class="line">setattr(args, <span class="string">'char_vocab_size'</span>, len(data.CHAR.vocab)) <span class="comment"># 设置属性args.char_vocab_size的值 = len(data.CHAR.vocab)</span></span><br><span class="line">setattr(args, <span class="string">'word_vocab_size'</span>, len(data.WORD.vocab))</span><br><span class="line">setattr(args, <span class="string">'dataset_file'</span>, <span class="string">f'data/squad/<span class="subst">&#123;args.dev_file&#125;</span>'</span>)</span><br><span class="line">setattr(args, <span class="string">'prediction_file'</span>, <span class="string">f'prediction<span class="subst">&#123;args.gpu&#125;</span>.out'</span>)</span><br><span class="line">setattr(args, <span class="string">'model_time'</span>, strftime(<span class="string">'%H:%M:%S'</span>, gmtime())) <span class="comment"># 时间</span></span><br><span class="line">print(<span class="string">'data loading complete!'</span>)</span><br></pre></td></tr></table></figure>

<h2 id="BIDAF"><a href="#BIDAF" class="headerlink" title="BIDAF"></a>BIDAF</h2><p><img src="https://s1.ax1x.com/2020/06/09/t4C2yd.jpg" alt="avatar"></p>
<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTM</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, batch_first=False, num_layers=<span class="number">1</span>, bidirectional=False, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        <span class="comment"># input_size=args.hidden_size * 2 = 200,</span></span><br><span class="line">        <span class="comment"># hidden_size=args.hidden_size = 100,</span></span><br><span class="line">        <span class="comment"># bidirectional=True,</span></span><br><span class="line">        <span class="comment"># batch_first=True, </span></span><br><span class="line">        <span class="comment"># dropout=args.dropout = 0.2</span></span><br><span class="line">        super(LSTM, self).__init__()</span><br><span class="line">        self.rnn = nn.LSTM(input_size=input_size,</span><br><span class="line">                           hidden_size=hidden_size,</span><br><span class="line">                           num_layers=num_layers,</span><br><span class="line">                           bidirectional=bidirectional,</span><br><span class="line">                           batch_first=batch_first)</span><br><span class="line">        self.reset_params() <span class="comment"># 重置参数</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_params</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.rnn.num_layers):</span><br><span class="line">            nn.init.orthogonal_(getattr(self.rnn, <span class="string">f'weight_hh_l<span class="subst">&#123;i&#125;</span>'</span>)) <span class="comment"># hidden-hidden weights</span></span><br><span class="line">            <span class="comment"># weight_hh_l&#123;i&#125;、weight_ih_l&#123;i&#125;、bias_hh_l&#123;i&#125;、bias_ih_l&#123;i&#125; 都是nn.LSTM源码里的参数</span></span><br><span class="line">            <span class="comment"># getattr取出源码里参数的值，用nn.init.orthogonal_正交进行重新初始化</span></span><br><span class="line">            <span class="comment"># nn.init初始化方法看这个链接：https://www.aiuai.cn/aifarm613.html</span></span><br><span class="line">            nn.init.kaiming_normal_(getattr(self.rnn, <span class="string">f'weight_ih_l<span class="subst">&#123;i&#125;</span>'</span>)) <span class="comment"># input-hidden weights</span></span><br><span class="line">            nn.init.constant_(getattr(self.rnn, <span class="string">f'bias_hh_l<span class="subst">&#123;i&#125;</span>'</span>), val=<span class="number">0</span>) <span class="comment"># hidden-hidden bias</span></span><br><span class="line">            nn.init.constant_(getattr(self.rnn, <span class="string">f'bias_ih_l<span class="subst">&#123;i&#125;</span>'</span>), val=<span class="number">0</span>) <span class="comment"># input-hidden bias</span></span><br><span class="line">            getattr(self.rnn, <span class="string">f'bias_hh_l<span class="subst">&#123;i&#125;</span>'</span>).chunk(<span class="number">4</span>)[<span class="number">1</span>].fill_(<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># .chunk看下这个链接：https://blog.csdn.net/XuM222222/article/details/92380538</span></span><br><span class="line">            <span class="comment"># .fill_(1),下划线代表直接替换，看链接：https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.fill.html</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.rnn.bidirectional: <span class="comment"># 双向，需要初始化反向的参数</span></span><br><span class="line">                nn.init.orthogonal_(getattr(self.rnn, <span class="string">f'weight_hh_l<span class="subst">&#123;i&#125;</span>_reverse'</span>))</span><br><span class="line">                nn.init.kaiming_normal_(getattr(self.rnn, <span class="string">f'weight_ih_l<span class="subst">&#123;i&#125;</span>_reverse'</span>))</span><br><span class="line">                nn.init.constant_(getattr(self.rnn, <span class="string">f'bias_hh_l<span class="subst">&#123;i&#125;</span>_reverse'</span>), val=<span class="number">0</span>)</span><br><span class="line">                nn.init.constant_(getattr(self.rnn, <span class="string">f'bias_ih_l<span class="subst">&#123;i&#125;</span>_reverse'</span>), val=<span class="number">0</span>)</span><br><span class="line">                getattr(self.rnn, <span class="string">f'bias_hh_l<span class="subst">&#123;i&#125;</span>_reverse'</span>).chunk(<span class="number">4</span>)[<span class="number">1</span>].fill_(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># x是一个元组(c, c_lens)</span></span><br><span class="line">        x, x_len = x</span><br><span class="line">        <span class="comment"># x = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        <span class="comment"># x_len = (batch) 一个batch中所有context或question的样本长度</span></span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 下面一顿操作和第七课机器翻译的一样，</span></span><br><span class="line">        <span class="comment"># 看下这篇博客理解：https://www.cnblogs.com/sbj123456789/p/9834018.html</span></span><br><span class="line">        x_len_sorted, x_idx = torch.sort(x_len, descending=<span class="literal">True</span>)</span><br><span class="line">        x_sorted = x.index_select(dim=<span class="number">0</span>, index=x_idx)</span><br><span class="line">        _, x_ori_idx = torch.sort(x_idx)</span><br><span class="line"></span><br><span class="line">        x_packed = nn.utils.rnn.pack_padded_sequence(x_sorted, x_len_sorted, batch_first=<span class="literal">True</span>)</span><br><span class="line">        x_packed, (h, c) = self.rnn(x_packed)</span><br><span class="line"></span><br><span class="line">        x = nn.utils.rnn.pad_packed_sequence(x_packed, batch_first=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">        x = x.index_select(dim=<span class="number">0</span>, index=x_ori_idx)</span><br><span class="line">        h = h.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>).contiguous().view(<span class="number">-1</span>, h.size(<span class="number">0</span>) * h.size(<span class="number">2</span>)).squeeze()</span><br><span class="line">        h = h.index_select(dim=<span class="number">0</span>, index=x_ori_idx)</span><br><span class="line">        <span class="comment"># x = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        <span class="comment"># h = (1, batch, hidden_size * 2) 这个维度不用管</span></span><br><span class="line">        <span class="keyword">return</span> x, h</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features, dropout=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(Linear, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.linear = nn.Linear(in_features=in_features, out_features=out_features)</span><br><span class="line">        <span class="comment"># in_features = hidden_size * 2</span></span><br><span class="line">        <span class="comment"># out_features = hidden_size * 2</span></span><br><span class="line">        <span class="keyword">if</span> dropout &gt; <span class="number">0</span>:</span><br><span class="line">            self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        self.reset_params()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_params</span><span class="params">(self)</span>:</span></span><br><span class="line">        nn.init.kaiming_normal_(self.linear.weight)</span><br><span class="line">        nn.init.constant_(self.linear.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> hasattr(self, <span class="string">'dropout'</span>): <span class="comment"># 判断self有没有'dropout'这个参数，返回bool值</span></span><br><span class="line">            x = self.dropout(x)</span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args.char_dim</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 看英文论文或这篇博客理解模型：https://blog.csdn.net/u014665013/article/details/79793395</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiDAF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args, pretrained)</span>:</span></span><br><span class="line">        <span class="comment"># pretrained = data.WORD.vocab.vectors = (108777, 100)</span></span><br><span class="line">        super(BiDAF, self).__init__()</span><br><span class="line">        self.args = args</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. Character Embedding Layer 是模型示意图左边的层的名字，从下往上</span></span><br><span class="line">        <span class="comment"># 字符编码层</span></span><br><span class="line">        self.char_emb = nn.Embedding(args.char_vocab_size, args.char_dim, padding_idx=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># args.char_vocab_size = 1307，args.char_dim = 8</span></span><br><span class="line">        nn.init.uniform_(self.char_emb.weight, <span class="number">-0.001</span>, <span class="number">0.001</span>)</span><br><span class="line">        <span class="comment"># 初始化权重</span></span><br><span class="line"></span><br><span class="line">        self.char_conv = nn.Conv2d(<span class="number">1</span>, args.char_channel_size, (args.char_dim, args.char_channel_width))</span><br><span class="line">        <span class="comment"># args.char_channel_size = 100 卷积核数量 </span></span><br><span class="line">        <span class="comment"># (args.char_dim, args.char_channel_width) = (8,5) 过滤器大小</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. Word Embedding Layer</span></span><br><span class="line">        <span class="comment"># 单词编码层</span></span><br><span class="line">        <span class="comment"># initialize word embedding with GloVe</span></span><br><span class="line">        self.word_emb = nn.Embedding.from_pretrained(pretrained, freeze=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 初始化词向量权重，用的Glove向量</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># highway network</span></span><br><span class="line">        <span class="keyword">assert</span> self.args.hidden_size * <span class="number">2</span> == (self.args.char_channel_size + self.args.word_dim)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            setattr(self, <span class="string">f'highway_linear<span class="subst">&#123;i&#125;</span>'</span>,</span><br><span class="line">                    nn.Sequential(Linear(args.hidden_size * <span class="number">2</span>, args.hidden_size * <span class="number">2</span>),</span><br><span class="line">                                  nn.ReLU()))</span><br><span class="line">            <span class="comment"># 设置highway_linear0 = nn.Sequential(Linear(args.hidden_size * 2, args.hidden_size * 2)</span></span><br><span class="line">            <span class="comment"># 设置highway_linear1 = nn.Sequential(Linear(args.hidden_size * 2, args.hidden_size * 2)</span></span><br><span class="line">            <span class="comment"># args.hidden_size = 100</span></span><br><span class="line">                                </span><br><span class="line">            setattr(self, <span class="string">f'highway_gate<span class="subst">&#123;i&#125;</span>'</span>,</span><br><span class="line">                    nn.Sequential(Linear(args.hidden_size * <span class="number">2</span>, args.hidden_size * <span class="number">2</span>),</span><br><span class="line">                                  nn.Sigmoid()))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. Contextual Embedding Layer</span></span><br><span class="line">        <span class="comment"># 上下文，和答案嵌入层，用的LSTM</span></span><br><span class="line">        <span class="comment"># 下面LSTM定位到了自定义的class LSTM(nn.Module)。</span></span><br><span class="line">        self.context_LSTM = LSTM(input_size=args.hidden_size * <span class="number">2</span>,</span><br><span class="line">                                 hidden_size=args.hidden_size,</span><br><span class="line">                                 bidirectional=<span class="literal">True</span>,</span><br><span class="line">                                 batch_first=<span class="literal">True</span>,</span><br><span class="line">                                 dropout=args.dropout) </span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. Attention Flow Layer</span></span><br><span class="line">        <span class="comment"># 注意力层</span></span><br><span class="line">        self.att_weight_c = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.att_weight_q = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.att_weight_cq = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5. Modeling Layer</span></span><br><span class="line">        self.modeling_LSTM1 = LSTM(input_size=args.hidden_size * <span class="number">8</span>,</span><br><span class="line">                                   hidden_size=args.hidden_size,</span><br><span class="line">                                   bidirectional=<span class="literal">True</span>,</span><br><span class="line">                                   batch_first=<span class="literal">True</span>,</span><br><span class="line">                                   dropout=args.dropout)</span><br><span class="line"></span><br><span class="line">        self.modeling_LSTM2 = LSTM(input_size=args.hidden_size * <span class="number">2</span>,</span><br><span class="line">                                   hidden_size=args.hidden_size,</span><br><span class="line">                                   bidirectional=<span class="literal">True</span>,</span><br><span class="line">                                   batch_first=<span class="literal">True</span>,</span><br><span class="line">                                   dropout=args.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 6. Output Layer</span></span><br><span class="line">        self.p1_weight_g = Linear(args.hidden_size * <span class="number">8</span>, <span class="number">1</span>, dropout=args.dropout)</span><br><span class="line">        self.p1_weight_m = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>, dropout=args.dropout)</span><br><span class="line">        self.p2_weight_g = Linear(args.hidden_size * <span class="number">8</span>, <span class="number">1</span>, dropout=args.dropout)</span><br><span class="line">        self.p2_weight_m = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>, dropout=args.dropout)</span><br><span class="line"></span><br><span class="line">        self.output_LSTM = LSTM(input_size=args.hidden_size * <span class="number">2</span>,</span><br><span class="line">                                hidden_size=args.hidden_size,</span><br><span class="line">                                bidirectional=<span class="literal">True</span>,</span><br><span class="line">                                batch_first=<span class="literal">True</span>,</span><br><span class="line">                                dropout=args.dropout)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(p=args.dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, batch)</span>:</span></span><br><span class="line">        <span class="comment"># batch里面有'id','s_idx','e_idx', 'c_word','c_char','q_word', 'q_char'数据</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> More memory-efficient architecture</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">char_emb_layer</span><span class="params">(x)</span>:</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            :param x: (batch, seq_len, word_len)</span></span><br><span class="line"><span class="string">            :return: (batch, seq_len, char_channel_size)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            <span class="comment"># x = (batch_sizes,seq_len,word_len)</span></span><br><span class="line">            batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">            x = self.dropout(self.char_emb(x))</span><br><span class="line">            <span class="comment"># (batch, seq_len, word_len, char_dim)</span></span><br><span class="line">            x = x.view(<span class="number">-1</span>, self.args.char_dim, x.size(<span class="number">2</span>)).unsqueeze(<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># (batch * seq_len, 1, char_dim, word_len) 1是输入的channel的维度</span></span><br><span class="line">            x = self.char_conv(x).squeeze()</span><br><span class="line">            <span class="comment"># (batch * seq_len, char_channel_size, 1, conv_len) -&gt; </span></span><br><span class="line">            <span class="comment"># (batch * seq_len, char_channel_size, conv_len) conv_len不用管，下一步都会pool掉</span></span><br><span class="line">            x = F.max_pool1d(x, x.size(<span class="number">2</span>)).squeeze()</span><br><span class="line">            <span class="comment"># (batch * seq_len, char_channel_size, 1) -&gt; (batch * seq_len, char_channel_size)</span></span><br><span class="line">            x = x.view(batch_size, <span class="number">-1</span>, self.args.char_channel_size)</span><br><span class="line">            <span class="comment"># (batch, seq_len, char_channel_size)</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">highway_network</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            :param x1: (batch, seq_len, char_channel_size)</span></span><br><span class="line"><span class="string">            :param x2: (batch, seq_len, word_dim)</span></span><br><span class="line"><span class="string">            :return: (batch, seq_len, hidden_size * 2)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            </span><br><span class="line">            x = torch.cat([x1, x2], dim=<span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># x = (batch, seq_len, char_channel_size + word_dim)</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">                h = getattr(self, <span class="string">f'highway_linear<span class="subst">&#123;i&#125;</span>'</span>)(x) <span class="comment"># 调用Linear的forward方法</span></span><br><span class="line">                <span class="comment"># h = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">                g = getattr(self, <span class="string">f'highway_gate<span class="subst">&#123;i&#125;</span>'</span>)(x)</span><br><span class="line">                <span class="comment"># g = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">                x = g * h + (<span class="number">1</span> - g) * x</span><br><span class="line">            <span class="comment"># (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">att_flow_layer</span><span class="params">(c, q)</span>:</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            :param c: (batch, c_len, hidden_size * 2)</span></span><br><span class="line"><span class="string">            :param q: (batch, q_len, hidden_size * 2)</span></span><br><span class="line"><span class="string">            :return: (batch, c_len, q_len)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            c_len = c.size(<span class="number">1</span>)</span><br><span class="line">            q_len = q.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (batch, c_len, q_len, hidden_size * 2)</span></span><br><span class="line">            <span class="comment">#c_tiled = c.unsqueeze(2).expand(-1, -1, q_len, -1)</span></span><br><span class="line">            <span class="comment"># (batch, c_len, q_len, hidden_size * 2)</span></span><br><span class="line">            <span class="comment">#q_tiled = q.unsqueeze(1).expand(-1, c_len, -1, -1)</span></span><br><span class="line">            <span class="comment"># (batch, c_len, q_len, hidden_size * 2)</span></span><br><span class="line">            <span class="comment">#cq_tiled = c_tiled * q_tiled</span></span><br><span class="line">            <span class="comment">#cq_tiled = c.unsqueeze(2).expand(-1, -1, q_len, -1) * q.unsqueeze(1).expand(-1, c_len, -1, -1)</span></span><br><span class="line"><span class="comment">#        # 4. Attention Flow Layer</span></span><br><span class="line"><span class="comment">#         # 注意力层</span></span><br><span class="line"><span class="comment">#         self.att_weight_c = Linear(args.hidden_size * 2, 1)</span></span><br><span class="line"><span class="comment">#         self.att_weight_q = Linear(args.hidden_size * 2, 1)</span></span><br><span class="line"><span class="comment">#         self.att_weight_cq = Linear(args.hidden_size * 2, 1)</span></span><br><span class="line">            cq = []</span><br><span class="line">            <span class="comment"># 1、相似度计算方式，看下这篇博客理解：https://blog.csdn.net/u014665013/article/details/79793395</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(q_len):</span><br><span class="line">                qi = q.select(<span class="number">1</span>, i).unsqueeze(<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># (batch, 1, hidden_size * 2)</span></span><br><span class="line">                <span class="comment"># .select看这个：https://blog.csdn.net/hungryof/article/details/51802829</span></span><br><span class="line">                ci = self.att_weight_cq(c * qi).squeeze()</span><br><span class="line">                <span class="comment"># (batch, c_len, 1)</span></span><br><span class="line">                cq.append(ci)</span><br><span class="line">            cq = torch.stack(cq, dim=<span class="number">-1</span>) </span><br><span class="line">            <span class="comment"># (batch, c_len, q_len) cp是共享相似度矩阵</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 2、计算对每一个 context word 而言哪些 query words 和它最相关。</span></span><br><span class="line">            <span class="comment"># context-to-query attention(C2Q):</span></span><br><span class="line">            s = self.att_weight_c(c).expand(<span class="number">-1</span>, <span class="number">-1</span>, q_len) + \</span><br><span class="line">                self.att_weight_q(q).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).expand(<span class="number">-1</span>, c_len, <span class="number">-1</span>) + cq</span><br><span class="line">            <span class="comment"># (batch, c_len, q_len) </span></span><br><span class="line">            a = F.softmax(s, dim=<span class="number">2</span>) </span><br><span class="line">            <span class="comment"># (batch, c_len, q_len)</span></span><br><span class="line">            c2q_att = torch.bmm(a, q) </span><br><span class="line">            <span class="comment"># (batch, c_len, q_len) * (batch, q_len, hidden_size * 2) -&gt; (batch, c_len, hidden_size * 2)</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 3、计算对每一个 query word 而言哪些 context words 和它最相关</span></span><br><span class="line">            <span class="comment"># query-to-context attention(Q2C):</span></span><br><span class="line">            b = F.softmax(torch.max(s, dim=<span class="number">2</span>)[<span class="number">0</span>], dim=<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># (batch, 1, c_len)</span></span><br><span class="line">            q2c_att = torch.bmm(b, c).squeeze()</span><br><span class="line">            <span class="comment"># (batch, 1, c_len) * (batch, c_len, hidden_size * 2) -&gt; (batch, hidden_size * 2)</span></span><br><span class="line">            q2c_att = q2c_att.unsqueeze(<span class="number">1</span>).expand(<span class="number">-1</span>, c_len, <span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># (batch, c_len, hidden_size * 2) (tiled)</span></span><br><span class="line">            <span class="comment"># q2c_att = torch.stack([q2c_att] * c_len, dim=1)</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 4、最后将context embedding和C2Q、Q2C的结果（三个矩阵）拼接起来</span></span><br><span class="line">            x = torch.cat([c, c2q_att, c * c2q_att, c * q2c_att], dim=<span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># (batch, c_len, hidden_size * 8)</span></span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">output_layer</span><span class="params">(g, m, l)</span>:</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            :param g: (batch, c_len, hidden_size * 8)</span></span><br><span class="line"><span class="string">            :param m: (batch, c_len ,hidden_size * 2)</span></span><br><span class="line"><span class="string">             #  l = c_lens</span></span><br><span class="line"><span class="string">            :return: p1: (batch, c_len), p2: (batch, c_len)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            p1 = (self.p1_weight_g(g) + self.p1_weight_m(m)).squeeze()</span><br><span class="line">            <span class="comment"># (batch, c_len)</span></span><br><span class="line">            m2 = self.output_LSTM((m, l))[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># (batch, c_len, hidden_size * 2)</span></span><br><span class="line">            p2 = (self.p2_weight_g(g) + self.p2_weight_m(m2)).squeeze()</span><br><span class="line">            <span class="comment"># (batch, c_len)</span></span><br><span class="line">            <span class="keyword">return</span> p1, p2</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. Character Embedding Layer</span></span><br><span class="line">        <span class="comment"># 令:一个batch中单词数量最多的样本长度为seq_len</span></span><br><span class="line">        <span class="comment"># 令:一个batch中某个单词长度最长的单词长度为word_len</span></span><br><span class="line">        </span><br><span class="line">        c_char = char_emb_layer(batch.c_char) </span><br><span class="line">        <span class="comment"># batch.c_char = (batch,seq_len,word_len) 后两个维度对应context</span></span><br><span class="line">        <span class="comment"># c_char = (batch, seq_len, char_channel_size)</span></span><br><span class="line"></span><br><span class="line">        q_char = char_emb_layer(batch.q_char)</span><br><span class="line">        <span class="comment"># batch.c_char = (batch,seq_len,word_len) 后两个维度对应question</span></span><br><span class="line">        <span class="comment"># c_char = (batch, seq_len, char_channel_size)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. Word Embedding Layer</span></span><br><span class="line">        c_word = self.word_emb(batch.c_word[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># batch.c_word[0] = (batch,seq_len) 后一个维度对应context</span></span><br><span class="line">        <span class="comment"># c_word = (batch, seq_len, word_dim) word_dim是Glove词向量维度</span></span><br><span class="line">        q_word = self.word_emb(batch.q_word[<span class="number">0</span>]) </span><br><span class="line">        <span class="comment"># batch.q_word[0] = (batch,seq_len) 后一个维度对应question</span></span><br><span class="line">        <span class="comment"># q_word = (batch, seq_len, word_dim)</span></span><br><span class="line">        c_lens = batch.c_word[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># c_lens：一个batch中所有context的样本长度</span></span><br><span class="line">        q_lens = batch.q_word[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># q_lens：一个batch中所有question的样本长度</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Highway network</span></span><br><span class="line">        c = highway_network(c_char, c_word)</span><br><span class="line">        <span class="comment"># c = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        q = highway_network(q_char, q_word)</span><br><span class="line">        <span class="comment"># q = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. Contextual Embedding Layer</span></span><br><span class="line">        c = self.context_LSTM((c, c_lens))[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># c = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        q = self.context_LSTM((q, q_lens))[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># q = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 4. Attention Flow Layer</span></span><br><span class="line">        g = att_flow_layer(c, q)</span><br><span class="line">        <span class="comment"># (batch, c_len, hidden_size * 8)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 5. Modeling Layer</span></span><br><span class="line">        m = self.modeling_LSTM2((self.modeling_LSTM1((g, c_lens))[<span class="number">0</span>], c_lens))[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># self.modeling_LSTM1((g, c_lens))[0] = (batch, c_len, hidden_size * 2) # 2因为是双向</span></span><br><span class="line">        <span class="comment"># m = (batch, c_len, hidden_size * 2) 2因为是双向</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 6. Output Layer</span></span><br><span class="line">        p1, p2 = output_layer(g, m, c_lens) <span class="comment"># 预测开始位置和结束位置</span></span><br><span class="line">        <span class="comment"># (batch, c_len), (batch, c_len)</span></span><br><span class="line">        <span class="keyword">return</span> p1, p2</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand((<span class="number">2</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line">print(x)</span><br><span class="line">y = x.select(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(len(data.WORD.vocab)) <span class="comment"># 108777个单词</span></span><br><span class="line">print(data.WORD.vocab.vectors.shape) <span class="comment"># 词向量维度</span></span><br><span class="line"></span><br><span class="line">print(data.WORD.vocab.itos[:<span class="number">50</span>]) <span class="comment"># 前50个词频最高的单词</span></span><br><span class="line">print(<span class="string">"------"</span>*<span class="number">10</span>)</span><br><span class="line">print(list(data.WORD.vocab.stoi.items())[<span class="number">0</span>:<span class="number">50</span>]) <span class="comment"># 对应的索引</span></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(len(data.CHAR.vocab)) <span class="comment"># 1307个单词</span></span><br><span class="line">print(data.CHAR.vocab.itos[:<span class="number">50</span>]) <span class="comment"># 108777个单词</span></span><br><span class="line">print(<span class="string">"------"</span>*<span class="number">10</span>)</span><br><span class="line">print(list(data.CHAR.vocab.stoi.items())[<span class="number">0</span>:<span class="number">50</span>]) <span class="comment"># 对应的索引</span></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">f"cuda:<span class="subst">&#123;args.gpu&#125;</span>"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">model = BiDAF(args, data.WORD.vocab.vectors).to(device)</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EMA</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mu)</span>:</span></span><br><span class="line">        <span class="comment"># mu = args.exp_decay_rate = 0.999</span></span><br><span class="line">        self.mu = mu</span><br><span class="line">        self.shadow = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">register</span><span class="params">(self, name, val)</span>:</span></span><br><span class="line">        <span class="comment"># name:各个参数层的名字, param.data；参数层的数据</span></span><br><span class="line">        self.shadow[name] = val.clone() <span class="comment"># 建立字典</span></span><br><span class="line">        <span class="comment"># clone()得到的Tensor不仅拷贝了原始的value，而且会计算梯度传播信息，copy_()只拷贝数值</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.shadow[name]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, name, x)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> name <span class="keyword">in</span> self.shadow</span><br><span class="line">        new_average = (<span class="number">1.0</span> - self.mu) * x + self.mu * self.shadow[name]</span><br><span class="line">        self.shadow[name] = new_average.clone()</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(model, ema, args, data)</span>:</span></span><br><span class="line">    device = torch.device(<span class="string">f"cuda:<span class="subst">&#123;args.gpu&#125;</span>"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    answers = dict()</span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    backup_params = EMA(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">            backup_params.register(name, param.data) <span class="comment"># 重新建立字典</span></span><br><span class="line">            param.data.copy_(ema.get(name))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.set_grad_enabled(<span class="literal">False</span>):</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> iter(data.dev_iter):</span><br><span class="line">            p1, p2 = model(batch)</span><br><span class="line">            print(p1.shape,p2.shape)</span><br><span class="line">            print(batch.s_idx,batch.e_idx)</span><br><span class="line">            batch_loss = criterion(p1, batch.s_idx<span class="number">-1</span>) + criterion(p2, batch.e_idx<span class="number">-1</span>)</span><br><span class="line">            print(<span class="string">"batch_loss"</span>,batch_loss)</span><br><span class="line">            print(<span class="string">"----"</span>*<span class="number">40</span>)</span><br><span class="line">            loss += batch_loss.item()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (batch, c_len, c_len)</span></span><br><span class="line">            batch_size, c_len = p1.size()</span><br><span class="line">            ls = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">            mask = (torch.ones(c_len, c_len) * float(<span class="string">'-inf'</span>)).to(device).tril(<span class="number">-1</span>).unsqueeze(<span class="number">0</span>).expand(batch_size, <span class="number">-1</span>, <span class="number">-1</span>)</span><br><span class="line">            score = (ls(p1).unsqueeze(<span class="number">2</span>) + ls(p2).unsqueeze(<span class="number">1</span>)) + mask</span><br><span class="line">            score, s_idx = score.max(dim=<span class="number">1</span>)</span><br><span class="line">            score, e_idx = score.max(dim=<span class="number">1</span>)</span><br><span class="line">            s_idx = torch.gather(s_idx, <span class="number">1</span>, e_idx.view(<span class="number">-1</span>, <span class="number">1</span>)).squeeze()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">                id = batch.id[i]</span><br><span class="line">                answer = batch.c_word[<span class="number">0</span>][i][s_idx[i]:e_idx[i]+<span class="number">1</span>]</span><br><span class="line">                answer = <span class="string">' '</span>.join([data.WORD.vocab.itos[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> answer])</span><br><span class="line">                answers[id] = answer</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                param.data.copy_(backup_params.get(name))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(args.prediction_file, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        print(json.dumps(answers), file=f)</span><br><span class="line"></span><br><span class="line">    results = evaluate.main(args)</span><br><span class="line">    <span class="keyword">return</span> loss, results[<span class="string">'exact_match'</span>], results[<span class="string">'f1'</span>]</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    print(name)</span><br><span class="line">    print(param.requires_grad)</span><br><span class="line">    print(param.data.shape)</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">strftime(&apos;%H:%M:%S&apos;, gmtime())</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">iterator = data.train_iter</span><br><span class="line">n= <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    print(<span class="string">"j="</span>,j)</span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(iterator):</span><br><span class="line">        print(<span class="string">"当前epoch"</span>,int(iterator.epoch))</span><br><span class="line">        print(<span class="string">"-----"</span>*<span class="number">10</span>)</span><br><span class="line">        print(i)</span><br><span class="line">        print(batch)</span><br><span class="line">        n+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> n&gt;<span class="number">3</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(args, data)</span>:</span></span><br><span class="line">    device = torch.device(<span class="string">f"cuda:<span class="subst">&#123;args.gpu&#125;</span>"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">    model = BiDAF(args, data.WORD.vocab.vectors).to(device) <span class="comment"># 定义主模型类实例</span></span><br><span class="line"></span><br><span class="line">    ema = EMA(args.exp_decay_rate) <span class="comment"># args.exp_decay_rate = 0.999</span></span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters(): </span><br><span class="line">        <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">            ema.register(name, param.data) <span class="comment"># 参数名字和对应的参数数据形成字典</span></span><br><span class="line">    parameters = filter(<span class="keyword">lambda</span> p: p.requires_grad, model.parameters())</span><br><span class="line">    <span class="comment"># p.requires_grad = True or False 保留有梯度的参数</span></span><br><span class="line">    optimizer = optim.Adadelta(parameters, lr=args.learning_rate)</span><br><span class="line">    <span class="comment"># args.learning_rate = 0.5,优化器选用Adadelta</span></span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    <span class="comment"># 交叉熵损失</span></span><br><span class="line"></span><br><span class="line">    writer = SummaryWriter(log_dir=<span class="string">'runs/'</span> + args.model_time)</span><br><span class="line">    <span class="comment"># args.model_time = strftime('%H:%M:%S', gmtime()) 文件夹命名为写入文件的当地时间</span></span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line">    loss, last_epoch = <span class="number">0</span>, <span class="number">-1</span></span><br><span class="line">    max_dev_exact, max_dev_f1 = <span class="number">-1</span>, <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    iterator = data.train_iter</span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(iterator):</span><br><span class="line">        present_epoch = int(iterator.epoch) </span><br><span class="line">        <span class="comment">#print("当前epoch",present_epoch)# 这个我打印了下，一直是0，觉得有问题</span></span><br><span class="line">        <span class="keyword">if</span> present_epoch == args.epoch:</span><br><span class="line">            <span class="comment"># args.epoch=12</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> present_epoch &gt; last_epoch:</span><br><span class="line">            print(<span class="string">'epoch:'</span>, present_epoch + <span class="number">1</span>)</span><br><span class="line">        last_epoch = present_epoch</span><br><span class="line"></span><br><span class="line">        p1, p2 = model(batch)</span><br><span class="line">        <span class="comment"># (batch, c_len), (batch, c_len)</span></span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        batch_loss = criterion(p1, batch.s_idx) + criterion(p2, batch.e_idx)</span><br><span class="line">        <span class="comment"># 最后的目标函数：batch.s_idx是答案开始的位置，batch.e_idx是答案结束的位置</span></span><br><span class="line">        loss += batch_loss.item()</span><br><span class="line">        batch_loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                ema.update(name, param.data) <span class="comment"># 更新训练完后的的参数数据</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % args.print_freq == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"i"</span>,i)</span><br><span class="line">            dev_loss, dev_exact, dev_f1 = test(model, ema, args, data)</span><br><span class="line">            c = (i + <span class="number">1</span>) // args.print_freq</span><br><span class="line"></span><br><span class="line">            writer.add_scalar(<span class="string">'loss/train'</span>, loss, c)</span><br><span class="line">            writer.add_scalar(<span class="string">'loss/dev'</span>, dev_loss, c)</span><br><span class="line">            writer.add_scalar(<span class="string">'exact_match/dev'</span>, dev_exact, c)</span><br><span class="line">            writer.add_scalar(<span class="string">'f1/dev'</span>, dev_f1, c)</span><br><span class="line">            print(<span class="string">f'train loss: <span class="subst">&#123;loss:<span class="number">.3</span>f&#125;</span> / dev loss: <span class="subst">&#123;dev_loss:<span class="number">.3</span>f&#125;</span>'</span></span><br><span class="line">                  <span class="string">f' / dev EM: <span class="subst">&#123;dev_exact:<span class="number">.3</span>f&#125;</span> / dev F1: <span class="subst">&#123;dev_f1:<span class="number">.3</span>f&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> dev_f1 &gt; max_dev_f1:</span><br><span class="line">                max_dev_f1 = dev_f1</span><br><span class="line">                max_dev_exact = dev_exact</span><br><span class="line">                best_model = copy.deepcopy(model)</span><br><span class="line"></span><br><span class="line">            loss = <span class="number">0</span></span><br><span class="line">            model.train()</span><br><span class="line"></span><br><span class="line">    writer.close()</span><br><span class="line"><span class="comment">#     print(f'max dev EM: &#123;max_dev_exact:.3f&#125; / max dev F1: &#123;max_dev_f1:.3f&#125;')</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> best_model</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(&apos;training start!&apos;)</span><br><span class="line">best_model = train(args, data)</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/pytorch/pytorch/issues/4144</span><br></pre></td></tr></table></figure>
      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>本文标题:</span><a href="/2020/04/17/SQuAD-BiDAF/">SQuAD-BiDAF</a></p>
        <p><span>文章作者:</span><a href="/" title="回到主页">MingmingYe</a></p>
        <p><span>发布时间:</span>2020-04-17, 07:32:02</p>
        <p><span>最后更新:</span>2020-06-09, 09:31:28</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/2020/04/17/SQuAD-BiDAF/" title="SQuAD-BiDAF">http://mmyblog.cn/2020/04/17/SQuAD-BiDAF/</a>
            <span class="copy-path" data-clipboard-text="原文: http://mmyblog.cn/2020/04/17/SQuAD-BiDAF/　　作者: MingmingYe" title="点击复制文章链接"><i class="fa fa-clipboard"></i></span>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
        </p>
    </div>



    <nav id="article-nav">
        
            <div id="article-nav-newer" class="article-nav-title">
                <a href="/2020/04/18/语言模型/">
                    语言模型
                </a>
            </div>
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/2020/04/15/NLP中的ConvNet/">
                    NLP中的ConvNet
                </a>
            </div>
        
    </nav>

  
  
    <! -- 添加捐赠图标 -->
<div class ="post-donate">
    <div id="donate_board" class="donate_bar center">
        <a id="btn_donate" class="btn_donate" href="javascript:;" title="打赏"></a>
        <span class="donate_txt">
           ↑<br>
           欣赏此文？求鼓励，求支持！
        </span>
        <br>
      </div>  
    <div id="donate_guide" class="donate_bar center hidden" >
        
            <!-- <img src="/img/Alipay.jpg" alt="支付宝打赏">
            <img src="/img/WeChatpay.jpg" alt="微信打赏"> -->
       
        <!-- 方式二；
            step1：在_config.yml中添加配置
                Alipay: /img/Alipay.jpg
                WeChatpay: /img/WeChatpay.jpg
            step2：此处两张图片的路径分别设置为如下
                <img src=""
                <img src=""
        -->
        <!-- 支付宝打赏图案 -->
        <img src="/img/Alipay.jpg" alt="支付宝打赏">
        <!-- 微信打赏图案 -->
        <img src="/img//WeChatpay.jpg" alt="微信打赏">
    </div>
    <script type="text/javascript">
        document.getElementById('btn_donate').onclick = function(){
            $('#donate_board').addClass('hidden');
            $('#donate_guide').removeClass('hidden');
        }
    </script>
</div>
<! -- 添加捐赠图标 -->
  
</article>

    <div id="toc" class="toc-article">
        <strong class="toc-title">文章目录</strong>
        
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#一、定义初始变量参数"><span class="toc-number">1.</span> <span class="toc-text">一、定义初始变量参数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#二、SQuAD问答数据预处理"><span class="toc-number">2.</span> <span class="toc-text">二、SQuAD问答数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、查看数据集结构"><span class="toc-number">2.1.</span> <span class="toc-text">1、查看数据集结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、定义分词方法"><span class="toc-number">2.2.</span> <span class="toc-text">2、定义分词方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、清洗数据，并生成数据迭代器"><span class="toc-number">2.3.</span> <span class="toc-text">3、清洗数据，并生成数据迭代器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#上面不太明白的举例子"><span class="toc-number">2.3.1.</span> <span class="toc-text">上面不太明白的举例子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BIDAF"><span class="toc-number">2.4.</span> <span class="toc-text">BIDAF</span></a></li></ol></li></ol>
        
    </div>
    <style>
        .left-col .switch-btn,
        .left-col .switch-area {
            display: none;
        }
        .toc-level-5 i,
        .toc-level-5 ol {
            display: none !important;
        }
    </style>

    <input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">

    <script>
        yiliaConfig.toc = ["隐藏目录", "显示目录", !!"false"];
    </script>



    
<div class="share">
    
        <div class="bdsharebuttonbox">
            <a href="#" class="fa fa-twitter bds_twi" data-cmd="twi" title="分享到推特"></a>
            <a href="#" class="fa fa-weibo bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
            <a href="#" class="fa fa-qq bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a>
            <a href="#" class="fa fa-files-o bds_copy" data-cmd="copy" title="复制网址"></a>
            <a href="#" class="fa fa fa-envelope-o bds_mail" data-cmd="mail" title="通过邮件分享"></a>
            <a href="#" class="fa fa-weixin bds_weixin" data-cmd="weixin" title="生成文章二维码"></a>
            <a href="#" class="fa fa-share-alt bds_more" data-cmd="more"></i></a>
        </div>
        <script>
            window._bd_share_config={
                "common":{"bdSnsKey":{},"bdText":"SQuAD-BiDAF　| Stay hungry, Stay foolish.　","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
        </script>
    

    
</div>







    




    <div class="scroll" id="post-nav-button">
        
            <a href="/2020/04/18/语言模型/" title="上一篇: 语言模型">
                <i class="fa fa-angle-left"></i>
            </a>
        

        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/2020/04/15/NLP中的ConvNet/" title="下一篇: NLP中的ConvNet">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/06/09/扩展内容/">扩展内容</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/09/XLNet/">XLNet</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/08/PyTorch/">PyTorch</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/08/朴素贝叶斯/">朴素贝叶斯</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/30/GPT模型/">GPT模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/30/BERT系列预训练模型/">BERT系列预训练模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/19/阅读理解/">阅读理解</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/18/Transformer模型解读/">Transformer模型解读</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/16/Transformer-XL/">Transformer-XL</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/12/英文书籍word级别的文本生成代码注释/">英文书籍word级别的文本生成代码注释</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/10/文本生成任务/">文本生成任务</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/09/常见预训练模型/">BERT&ELMo&co</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/01/大规模无监督预训练语言模型与应用上/">大规模无监督预训练语言模型与应用上</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/24/word2vec/">word2vec</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/20/特征工程与模型调优/">特征工程与模型调优</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/18/语言模型/">语言模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/17/SQuAD-BiDAF/">SQuAD-BiDAF</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/15/NLP中的ConvNet/">NLP中的ConvNet</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/13/seq2seq/">seq2seq</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/09/机器翻译与文本摘要/">机器翻译与文本摘要</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/28/sentiment情感分析代码注释/">sentiment情感分析代码注释</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/11/聊天机器人二/">聊天机器人二</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/10/聊天机器人一/">聊天机器人一</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/09/结构化预测/">结构化预测</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/20/SVM/">SVM</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/11/word-embedding/">word-embedding</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/08/酒店评价情感分类与CNN模型/">酒店评价情感分类与CNN模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/28/NLP技术基础整理/">NLP技术基础整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/18/CNN-Image-Classification/">CNN-Image-Classification</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/08/丘吉尔的人物传记char级别的文本生成代码注释/">丘吉尔的人物传记char级别的文本生成代码注释</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/08/用朴素贝叶斯完成语种检测/">用朴素贝叶斯完成语种检测</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/09/01/深度学习速查表/">深度学习速查表</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/09/01/模型调优/">模型调优</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/09/01/集成学习与boosting模型/">集成学习与boosting模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/09/01/聚类与降维/">聚类与降维</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/09/01/贝叶斯分类器/">贝叶斯分类器</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/09/01/决策树与随机森林/">决策树与随机森林</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/29/机器学习逻辑回归与softmax/">机器学习逻辑回归与softmax</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/24/文本分类问题/">文本分类问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/23/机器学习基本概念/">机器学习基本概念</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/23/简洁版机器学习速查表/">简洁版机器学习速查表</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/23/CS229版机器学习速查表/">CS229版机器学习速查表</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/葫芦书学习笔记/">葫芦书学习笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/06/数学基础知识整理/">数学基础知识整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/24/大数据基础/">大数据基础</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/22/数据分析常用工具总结/">数据分析常用工具总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/20/python基础知识整理/">python基础知识整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/08/python正则表达式/">python正则表达式</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/08/jieba中文处理/">jieba中文处理</a></li></ul>




    <script>
        
    </script>

</div>
      <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2019-2020 MingmingYe
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>

    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>