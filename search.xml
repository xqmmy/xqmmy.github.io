<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>特征工程与模型调优</title>
      <link href="/2020/04/20/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B8%8E%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98/"/>
      <url>/2020/04/20/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B8%8E%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<h2 id="机器学习特征工程"><a href="#机器学习特征工程" class="headerlink" title="机器学习特征工程"></a>机器学习特征工程</h2><h3 id="机器学习流程与概念"><a href="#机器学习流程与概念" class="headerlink" title="机器学习流程与概念"></a>机器学习流程与概念</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBncGV.jpg" alt></p><h3 id="机器学习建模流程"><a href="#机器学习建模流程" class="headerlink" title="机器学习建模流程"></a>机器学习建模流程</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBn2xU.png" alt></p><h3 id="机器学习特征工程一览"><a href="#机器学习特征工程一览" class="headerlink" title="机器学习特征工程一览"></a>机器学习特征工程一览</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBnOMD.jpg" alt></p><h3 id="机器学习特征工程介绍"><a href="#机器学习特征工程介绍" class="headerlink" title="机器学习特征工程介绍"></a>机器学习特征工程介绍</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBnjqH.jpg" alt></p><h3 id="特征清洗"><a href="#特征清洗" class="headerlink" title="特征清洗"></a>特征清洗</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBumon.jpg" alt><br><img src="https://s1.ax1x.com/2020/04/24/JBuKJ0.jpg" alt><br><img src="https://s1.ax1x.com/2020/04/24/JBu3yF.jpg" alt></p><h3 id="数值型数据上的特征工程"><a href="#数值型数据上的特征工程" class="headerlink" title="数值型数据上的特征工程"></a>数值型数据上的特征工程</h3><p>数值型数据通常以标量的形式表示数据，描述观测值、记录或者测量值。本文的数值型数据是指连续型数据而不是离散型数据，表示不同类目的数据就是后者。数值型数据也可以用向量来表示，向量的每个值或分量代表一个特征。整数和浮点数是连续型数值数据中最常见也是最常使用的数值型数据类型。即使数值型数据可以直接输入到机器学习模型中，你仍需要在建模前设计与场景、问题和领域相关的特征。因此仍需要特征工程。让我们利用 python 来看看在数值型数据上做特征工程的一些策略。我们首先加载下面一些必要的依赖（通常在 <a href="http://jupyter.org/" target="_blank" rel="noopener"><strong>Jupyter</strong> </a> botebook 上）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">&gt;</span><br><span class="line">&gt; <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">&gt;</span><br><span class="line">&gt; <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">&gt;</span><br><span class="line">&gt; <span class="keyword">import</span> scipy.stats <span class="keyword">as</span> spstats</span><br><span class="line">&gt;</span><br><span class="line">&gt; %matplotlib inline</span><br></pre></td></tr></table></figure><p>原始度量</p><p>正如我们先前提到的，根据上下文和数据的格式，原始数值型数据通常可直接输入到机器学习模型中。原始的度量方法通常用数值型变量来直接表示为特征，而不需要任何形式的变换或特征工程。通常这些特征可以表示一些值或总数。让我们加载四个数据集之一的 <a href="https://www.kaggle.com/abcsds/pokemon/data" target="_blank" rel="noopener">Pokemon </a>数据集，该数据集也在 <a href="https://www.kaggle.com/abcsds/pokemon/data" target="_blank" rel="noopener">Kaggle </a>上公布了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">poke_df = pd.read_csv(<span class="string">'datasets/Pokemon.csv'</span>, encoding=<span class="string">'utf-8'</span>) </span><br><span class="line"></span><br><span class="line">poke_df.head()</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f55514768e.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="我们的Pokemon数据集截图"><a href="#我们的Pokemon数据集截图" class="headerlink" title="我们的Pokemon数据集截图"></a>我们的Pokemon数据集截图</h5><p>Pokemon 是一个大型多媒体游戏，包含了各种口袋妖怪（Pokemon）角色。简而言之，你可以认为他们是带有超能力的动物！这些数据集由这些口袋妖怪角色构成，每个角色带有各种统计信息。</p><h4 id="数值"><a href="#数值" class="headerlink" title="数值"></a>数值</h4><p>如果你仔细地观察上图中这些数据，你会看到几个代表数值型原始值的属性，它可以被直接使用。下面的这行代码挑出了其中一些重点特征。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poke_df[[&apos;HP&apos;, &apos;Attack&apos;, &apos;Defense&apos;]].head()</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f557552811.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="带（连续型）数值数据的特征"><a href="#带（连续型）数值数据的特征" class="headerlink" title="带（连续型）数值数据的特征"></a>带（连续型）数值数据的特征</h5><p>这样，你可以直接将这些属性作为特征，如上图所示。这些特征包括 Pokemon 的 HP（血量），Attack（攻击）和 Defense（防御）状态。事实上，我们也可以基于这些字段计算出一些基本的统计量。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poke_df[[&apos;HP&apos;, &apos;Attack&apos;, &apos;Defense&apos;]].describe()</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f559f61c14.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p><strong>数值特征形式的基本描述性统计量</strong></p><p>这样你就对特征中的统计量如总数、平均值、标准差和四分位数有了一个很好的印象。</p><h4 id="记数"><a href="#记数" class="headerlink" title="记数"></a>记数</h4><p>原始度量的另一种形式包括代表频率、总数或特征属性发生次数的特征。让我们看看 <a href="https://labrosa.ee.columbia.edu/millionsong/" target="_blank" rel="noopener"><strong>millionsong</strong></a> <strong>数据集</strong>中的一个例子，其描述了某一歌曲被各种用户收听的总数或频数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">popsong_df = pd.read_csv(&apos;datasets/song_views.csv&apos;,encoding=&apos;utf-8&apos;)</span><br><span class="line"></span><br><span class="line">popsong_df.head(10)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f55bf6176f.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="数值特征形式的歌曲收听总数"><a href="#数值特征形式的歌曲收听总数" class="headerlink" title="数值特征形式的歌曲收听总数"></a>数值特征形式的歌曲收听总数</h5><p>根据这张截图，显而易见 listen_count 字段可以直接作为基于数值型特征的频数或总数。</p><h4 id="二值化"><a href="#二值化" class="headerlink" title="二值化"></a>二值化</h4><p>基于要解决的问题构建模型时，通常原始频数或总数可能与此不相关。比如如果我要建立一个推荐系统用来推荐歌曲，我只希望知道一个人是否感兴趣或是否听过某歌曲。我不需要知道一首歌被听过的次数，因为我更关心的是一个人所听过的各种各样的歌曲。在这个例子中，二值化的特征比基于计数的特征更合适。我们二值化 listen_count 字段如下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; watched = np.array(popsong_df[&apos;listen_count&apos;])</span><br><span class="line">&gt;</span><br><span class="line">&gt; watched[watched &gt;= 1] = 1</span><br><span class="line">&gt;</span><br><span class="line">&gt; popsong_df[&apos;watched&apos;] = watched</span><br></pre></td></tr></table></figure><p>你也可以使用 scikit-learn 中 preprocessing 模块的 Binarizer 类来执行同样的任务，而不一定使用 numpy 数组。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import Binarizer</span><br><span class="line"></span><br><span class="line">bn = Binarizer(threshold=0.9)</span><br><span class="line"></span><br><span class="line">pd_watched =bn.transform([popsong_df[&apos;listen_count&apos;]])[0]</span><br><span class="line"></span><br><span class="line">popsong_df[&apos;pd_watched&apos;] = pd_watched</span><br><span class="line"></span><br><span class="line">popsong_df.head(11)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56505e8ff.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="歌曲收听总数的二值化结构"><a href="#歌曲收听总数的二值化结构" class="headerlink" title="歌曲收听总数的二值化结构"></a>歌曲收听总数的二值化结构</h5><p>你可以从上面的截图中清楚地看到，两个方法得到了相同的结果。因此我们得到了一个二值化的特征来表示一首歌是否被每个用户听过，并且可以在相关的模型中使用它。</p><h4 id="数据舍入"><a href="#数据舍入" class="headerlink" title="数据舍入"></a>数据舍入</h4><p>处理连续型数值属性如比例或百分比时，我们通常不需要高精度的原始数值。因此通常有必要将这些高精度的百分比舍入为整数型数值。这些整数可以直接作为原始数值甚至分类型特征（基于离散类的）使用。让我们试着将这个观念应用到一个虚拟数据集上，该数据集描述了库存项和他们的流行度百分比。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">items_popularity =pd.read_csv(<span class="string">'datasets/item_popularity.csv'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">items_popularity[<span class="string">'popularity_scale_10'</span>] = np.array(np.round((items_popularity[<span class="string">'pop_percent'</span>] * <span class="number">10</span>)),dtype=<span class="string">'int'</span>)</span><br><span class="line"></span><br><span class="line">items_popularity[<span class="string">'popularity_scale_100'</span>] = np.array(np.round((items_popularity[<span class="string">'pop_percent'</span>] * <span class="number">100</span>)),dtype=<span class="string">'int'</span>)</span><br><span class="line"></span><br><span class="line">items_popularity</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f566e30ad2.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="不同尺度下流行度舍入结果"><a href="#不同尺度下流行度舍入结果" class="headerlink" title="不同尺度下流行度舍入结果"></a>不同尺度下流行度舍入结果</h5><p>基于上面的输出，你可能猜到我们试了两种不同的舍入方式。这些特征表明项目流行度的特征现在既有 1-10 的尺度也有 1-100 的尺度。基于这个场景或问题你可以使用这些值同时作为数值型或分类型特征。</p><h4 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h4><p>高级机器学习模型通常会对作为输入特征变量函数的输出响应建模（离散类别或连续数值）。例如，一个简单的线性回归方程可以表示为</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56ab22fb7.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>其中输入特征用变量表示为</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56c69ac66.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>权重或系数可以分别表示为</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56de74ee7.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>目标是预测响应 <strong>*y*</strong>.</p><p>在这个例子中，仅仅根据单个的、分离的输入特征，这个简单的线性模型描述了输出与输入之间的关系。</p><p>然而，在一些真实场景中，有必要试着捕获这些输入特征集一部分的特征变量之间的相关性。上述带有相关特征的线性回归方程的展开式可以简单表示为</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f5701419ee.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>此处特征可表示为</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f57162d4f7.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>表示了相关特征。现在让我们试着在 Pokemon 数据集上设计一些相关特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">atk_def = poke_df[[<span class="string">'Attack'</span>, <span class="string">'Defense'</span>]]</span><br><span class="line"></span><br><span class="line">atk_def.head()</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f572bad2cc.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>从输出数据框中，我们可以看到我们有两个数值型（连续的）特征，Attack 和 Defence。现在我们可以利用 scikit-learn 建立二度特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">pf = PolynomialFeatures(degree=<span class="number">2</span>,</span><br><span class="line"></span><br><span class="line">interaction_only=<span class="literal">False</span>,include_bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">res = pf.fit_transform(atk_def)</span><br><span class="line"></span><br><span class="line">res</span><br><span class="line"></span><br><span class="line">**Output**</span><br><span class="line"></span><br><span class="line">**------**</span><br><span class="line"></span><br><span class="line">array([[ <span class="number">49.</span>, <span class="number">49.</span>, <span class="number">2401.</span>, <span class="number">2401.</span>, <span class="number">2401.</span>],</span><br><span class="line"></span><br><span class="line">  [ <span class="number">62.</span>, <span class="number">63.</span>, <span class="number">3844.</span>, <span class="number">3906.</span>, <span class="number">3969.</span>],</span><br><span class="line"></span><br><span class="line">  [ <span class="number">82.</span>, <span class="number">83.</span>, <span class="number">6724.</span>, <span class="number">6806.</span>, <span class="number">6889.</span>],</span><br><span class="line"></span><br><span class="line">  ...,</span><br><span class="line"></span><br><span class="line">  [ <span class="number">110.</span>, <span class="number">60.</span>, <span class="number">12100.</span>, <span class="number">6600.</span>, <span class="number">3600.</span>],</span><br><span class="line"></span><br><span class="line">  [ <span class="number">160.</span>, <span class="number">60.</span>, <span class="number">25600.</span>, <span class="number">9600.</span>, <span class="number">3600.</span>],</span><br><span class="line"></span><br><span class="line">[ <span class="number">110.</span>, <span class="number">120.</span>, <span class="number">12100.</span>, <span class="number">13200.</span>, <span class="number">14400.</span>]])</span><br></pre></td></tr></table></figure><p>上面的特征矩阵一共描述了 5 个特征，其中包括新的相关特征。我们可以看到上述矩阵中每个特征的度，如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.DataFrame(pf.powers_, columns=[<span class="string">'Attack_degree'</span>,<span class="string">'Defense_degree'</span>])</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f575a65683.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>基于这个输出，现在我们可以通过每个特征的度知道它实际上代表什么。在此基础上，现在我们可以对每个特征进行命名如下。这仅仅是为了便于理解，你可以给这些特征取更好的、容易使用和简单的名字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">intr_features = pd.DataFrame(res, columns=[<span class="string">'Attack'</span>,<span class="string">'Defense'</span>,<span class="string">'Attack^2'</span>,<span class="string">'Attack x Defense'</span>,<span class="string">'Defense^2'</span>])</span><br><span class="line"></span><br><span class="line">intr_features.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f576e91376.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="数值型特征及其相关特征"><a href="#数值型特征及其相关特征" class="headerlink" title="数值型特征及其相关特征"></a>数值型特征及其相关特征</h5><p>因此上述数据代表了我们原始的特征以及它们的相关特征。</p><h4 id="分区间处理数据"><a href="#分区间处理数据" class="headerlink" title="分区间处理数据"></a>分区间处理数据</h4><p>处理原始、连续的数值型特征问题通常会导致这些特征值的分布被破坏。这表明有些值经常出现而另一些值出现非常少。除此之外，另一个问题是这些特征的值的变化范围。比如某个音乐视频的观看总数会非常大（<a href="https://www.youtube.com/watch?v=kJQP7kiw5Fk" target="_blank" rel="noopener">Despacito</a>，说你呢）而一些值会非常小。直接使用这些特征会产生很多问题，反而会影响模型表现。因此出现了处理这些问题的技巧，包括分区间法和变换。</p><p>分区间（Bining），也叫做量化，用于将连续型数值特征转换为离散型特征（类别）。可以认为这些离散值或数字是类别或原始的连续型数值被分区间或分组之后的数目。每个不同的区间大小代表某种密度，因此一个特定范围的连续型数值会落在里面。对数据做分区间的具体技巧包括等宽分区间以及自适应分区间。我们使用从 <a href="https://github.com/freeCodeCamp/2016-new-coder-survey" target="_blank" rel="noopener">2016 年 FreeCodeCamp 开发者和编码员调查报告</a>中抽取出来的一个子集中的数据，来讨论各种针对编码员和软件开发者的属性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df =pd.read_csv(<span class="string">'datasets/fcc_2016_coder_survey_subset.csv'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'EmploymentField'</span>, <span class="string">'Age'</span>,<span class="string">'Income'</span>]].head()</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f578e01139.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="来自FCC编码员调查数据集的样本属性"><a href="#来自FCC编码员调查数据集的样本属性" class="headerlink" title="来自FCC编码员调查数据集的样本属性"></a>来自FCC编码员调查数据集的样本属性</h5><p>对于每个参加调查的编码员或开发者，ID.x 变量基本上是一个唯一的标识符而其他字段是可自我解释的。</p><h4 id="等宽分区间"><a href="#等宽分区间" class="headerlink" title="等宽分区间"></a>等宽分区间</h4><p>就像名字表明的那样，在等宽分区间方法中，每个区间都是固定宽度的，通常可以预先分析数据进行定义。基于一些领域知识、规则或约束，每个区间有个预先固定的值的范围，只有处于范围内的数值才被分配到该区间。基于数据舍入操作的分区间是一种方式，你可以使用数据舍入操作来对原始值进行分区间，我们前面已经讲过。</p><p>现在我们分析编码员调查报告数据集的 Age 特征并看看它的分布。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Age'</span>].hist(color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Age Histogram'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Age'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f57b05846b.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="描述开发者年龄分布的直方图"><a href="#描述开发者年龄分布的直方图" class="headerlink" title="描述开发者年龄分布的直方图"></a>描述开发者年龄分布的直方图</h5><p>上面的直方图表明，如预期那样，开发者年龄分布仿佛往左侧倾斜（上年纪的开发者偏少）。现在我们根据下面的模式，将这些原始年龄值分配到特定的区间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Age Range: Bin</span><br><span class="line"></span><br><span class="line">\---------------</span><br><span class="line"></span><br><span class="line"><span class="number">0</span> - <span class="number">9</span> : <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="number">10</span> - <span class="number">19</span> : <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="number">20</span> - <span class="number">29</span> : <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="number">30</span> - <span class="number">39</span> : <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="number">40</span> - <span class="number">49</span> : <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="number">50</span> - <span class="number">59</span> : <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="number">60</span> - <span class="number">69</span> : <span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="meta">... </span><span class="keyword">and</span> so on</span><br></pre></td></tr></table></figure><p>我们可以简单地使用我们先前学习到的数据舍入部分知识，先将这些原始年龄值除以 10，然后通过 floor 函数对原始年龄数值进行截断。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df[<span class="string">'Age_bin_round'</span>] = np.array(np.floor(np.array(fcc_survey_df[<span class="string">'Age'</span>]) / <span class="number">10.</span>))</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>,<span class="string">'Age_bin_round'</span>]].iloc[<span class="number">1071</span>:<span class="number">1076</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f57d916a6f.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="通过舍入法分区间"><a href="#通过舍入法分区间" class="headerlink" title="通过舍入法分区间"></a>通过舍入法分区间</h5><p>你可以看到基于数据舍入操作的每个年龄对应的区间。但是如果我们需要更灵活的操作怎么办？如果我们想基于我们的规则或逻辑，确定或修改区间的宽度怎么办？基于常用范围的分区间方法将帮助我们完成这个。让我们来定义一些通用年龄段位，使用下面的方式来对开发者年龄分区间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Age Range : Bin</span><br><span class="line"></span><br><span class="line">\---------------</span><br><span class="line"></span><br><span class="line"><span class="number">0</span> - <span class="number">15</span> : <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="number">16</span> - <span class="number">30</span> : <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="number">31</span> - <span class="number">45</span> : <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="number">46</span> - <span class="number">60</span> : <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="number">61</span> - <span class="number">75</span> : <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="number">75</span> - <span class="number">100</span> : <span class="number">6</span></span><br></pre></td></tr></table></figure><p>基于这些常用的分区间方式，我们现在可以对每个开发者年龄值的区间打标签，我们将存储区间的范围和相应的标签。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">bin_ranges = [<span class="number">0</span>, <span class="number">15</span>, <span class="number">30</span>, <span class="number">45</span>, <span class="number">60</span>, <span class="number">75</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">bin_names = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Age_bin_custom_range'</span>] = pd.cut(np.array(fcc_survey_df[<span class="string">'Age'</span>]),bins=bin_ranges)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Age_bin_custom_label'</span>] = pd.cut(np.array(fcc_survey_df[<span class="string">'Age'</span>]),bins=bin_ranges, labels=bin_names)</span><br><span class="line"></span><br><span class="line">\<span class="comment"># view the binned features</span></span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Age_bin_round'</span>,<span class="string">'Age_bin_custom_range'</span>,<span class="string">'Age_bin_custom_label'</span>]].iloc[<span class="number">10</span>a71:<span class="number">1076</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58143c35f.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="开发者年龄的常用分区间方式"><a href="#开发者年龄的常用分区间方式" class="headerlink" title="开发者年龄的常用分区间方式"></a>开发者年龄的常用分区间方式</h5><h4 id="自适应分区间"><a href="#自适应分区间" class="headerlink" title="自适应分区间"></a>自适应分区间</h4><p>使用等宽分区间的不足之处在于，我们手动决定了区间的值范围，而由于落在某个区间中的数据点或值的数目是不均匀的，因此可能会得到不规则的区间。一些区间中的数据可能会非常的密集，一些区间会非常稀疏甚至是空的！自适应分区间方法是一个更安全的策略，在这些场景中，我们让数据自己说话！这样，我们使用数据分布来决定区间的范围。</p><p>基于分位数的分区间方法是自适应分箱方法中一个很好的技巧。量化对于特定值或切点有助于将特定数值域的连续值分布划分为离散的互相挨着的区间。因此 q 分位数有助于将数值属性划分为 q 个相等的部分。关于量化比较流行的例子包括 2 分位数，也叫中值，将数据分布划分为2个相等的区间；4 分位数，也简称分位数，它将数据划分为 4 个相等的区间；以及 10 分位数，也叫十分位数，创建 10 个相等宽度的区间，现在让我们看看开发者数据集的 Income 字段的数据分布。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>].hist(bins=<span class="number">30</span>, color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Income Histogram'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Developer Income'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f583631eff.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p><strong>描述开发者收入分布的直方图</strong></p><p>上述的分布描述了一个在收入上右歪斜的分布，少数人赚更多的钱，多数人赚更少的钱。让我们基于自适应分箱方式做一个 4-分位数或分位数。我们可以很容易地得到如下的分位数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">quantile_list = [<span class="number">0</span>, <span class="number">.25</span>, <span class="number">.5</span>, <span class="number">.75</span>, <span class="number">1.</span>]</span><br><span class="line"></span><br><span class="line">quantiles =</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>].quantile(quantile_list)</span><br><span class="line"></span><br><span class="line">quantiles</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**Output**</span><br><span class="line"></span><br><span class="line">**------**</span><br><span class="line"></span><br><span class="line"><span class="number">0.00</span> <span class="number">6000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">0.25</span> <span class="number">20000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">0.50</span> <span class="number">37000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">0.75</span> <span class="number">60000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">1.00</span> <span class="number">200000.0</span></span><br><span class="line"></span><br><span class="line">Name: Income, dtype: float64</span><br></pre></td></tr></table></figure><p>现在让我们在原始的分布直方图中可视化下这些分位数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>].hist(bins=<span class="number">30</span>, color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> quantile <span class="keyword">in</span> quantiles:</span><br><span class="line"></span><br><span class="line">qvl = plt.axvline(quantile, color=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">ax.legend([qvl], [<span class="string">'Quantiles'</span>], fontsize=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Income Histogram with Quantiles'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Developer Income'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f5853f1a2c.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="带分位数形式描述开发者收入分布的直方图"><a href="#带分位数形式描述开发者收入分布的直方图" class="headerlink" title="带分位数形式描述开发者收入分布的直方图"></a>带分位数形式描述开发者收入分布的直方图</h5><p>上面描述的分布中红色线代表了分位数值和我们潜在的区间。让我们利用这些知识来构建我们基于分区间策略的分位数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">quantile_labels = [<span class="string">'0-25Q'</span>, <span class="string">'25-50Q'</span>, <span class="string">'50-75Q'</span>, <span class="string">'75-100Q'</span>]</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_quantile_range'</span>] = pd.qcut(</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>],q=quantile_list)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_quantile_label'</span>] = pd.qcut(</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>],q=quantile_list,labels=quantile_labels)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Income'</span>,<span class="string">'Income_quantile_range'</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">'Income_quantile_label'</span>]].iloc[<span class="number">4</span>:<span class="number">9</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f586dbd8f4.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="基于分位数的开发者收入的区间范围和标签"><a href="#基于分位数的开发者收入的区间范围和标签" class="headerlink" title="基于分位数的开发者收入的区间范围和标签"></a>基于分位数的开发者收入的区间范围和标签</h5><p>通过这个例子，你应该对如何做基于分位数的自适应分区间法有了一个很好的认识。一个需要重点记住的是，分区间的结果是离散值类型的分类特征，当你在模型中使用分类数据之前，可能需要额外的特征工程相关步骤。我们将在接下来的部分简要地讲述分类数据的特征工程技巧。</p><h4 id="统计变换"><a href="#统计变换" class="headerlink" title="统计变换"></a>统计变换</h4><p>我们讨论下先前简单提到过的数据分布倾斜的负面影响。现在我们可以考虑另一个特征工程技巧，即利用统计或数学变换。我们试试看 Log 变换和 Box-Cox 变换。这两种变换函数都属于幂变换函数簇，通常用来创建单调的数据变换。它们的主要作用在于它能帮助稳定方差，始终保持分布接近于正态分布并使得数据与分布的平均值无关。</p><h4 id="Log变换"><a href="#Log变换" class="headerlink" title="Log变换"></a>Log变换</h4><p>log 变换属于幂变换函数簇。该函数用数学表达式表示为</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f588a0f6a5.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>读为以 b 为底 x 的对数等于 y。这可以变换为</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f589e77242.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>表示以b为底指数必须达到多少才等于x。自然对数使用 b=e，e=2.71828，通常叫作欧拉常数。你可以使用通常在十进制系统中使用的 b=10 作为底数。</p><p><strong>当应用于倾斜分布时 Log 变换是很有用的，因为他们倾向于拉伸那些落在较低的幅度范围内自变量值的范围，倾向于压缩或减少更高幅度范围内的自变量值的范围</strong>。从而使得倾斜分布尽可能的接近正态分布。让我们对先前使用的开发者数据集的 Income 特征上使用log变换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df[<span class="string">'Income_log'</span>] = np.log((<span class="number">1</span>+fcc_survey_df[<span class="string">'Income'</span>]))</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Income'</span>,<span class="string">'Income_log'</span>]].iloc[<span class="number">4</span>:<span class="number">9</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58b3ed249.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="开发者收入log变换后结构"><a href="#开发者收入log变换后结构" class="headerlink" title="开发者收入log变换后结构"></a>开发者收入log变换后结构</h5><p>Income_log 字段描述了经过 log 变换后的特征。现在让我们来看看字段变换后数据的分布。</p><p>基于上面的图，我们可以清楚地看到与先前倾斜分布相比，该分布更加像正态分布或高斯分布。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">income_log_mean =np.round(np.mean(fcc_survey_df[<span class="string">'Income_log'</span>]), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_log'</span>].hist(bins=<span class="number">30</span>,color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">plt.axvline(income_log_mean, color=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Income Histogram after Log Transform'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Developer Income (log scale)'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.text(<span class="number">11.5</span>, <span class="number">450</span>, <span class="string">r'$\mu$='</span>+str(income_log_mean),fontsize=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58cdaf02a.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p><strong>经过log变换后描述开发者收入分布的直方图</strong></p><h4 id="Box-Cox变换"><a href="#Box-Cox变换" class="headerlink" title="Box-Cox变换"></a>Box-Cox变换</h4><p>Box-Cox 变换是另一个流行的幂变换函数簇中的一个函数。该函数有一个前提条件，即数值型值必须先变换为正数（与 log 变换所要求的一样）。万一出现数值是负的，使用一个常数对数值进行偏移是有帮助的。数学上，Box-Cox 变换函数可以表示如下。</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58e556c08.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>生成的变换后的输出y是输入 x 和变换参数的函数；当 λ=0 时，该变换就是自然对数 log 变换，前面我们已经提到过了。λ 的最佳取值通常由最大似然或最大对数似然确定。现在让我们在开发者数据集的收入特征上应用 Box-Cox 变换。首先我们从数据分布中移除非零值得到最佳的值，结果如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">income = np.array(fcc_survey_df[<span class="string">'Income'</span>])</span><br><span class="line"></span><br><span class="line">income_clean = income[~np.isnan(income)]</span><br><span class="line"></span><br><span class="line">l, opt_lambda = spstats.boxcox(income_clean)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Optimal lambda value:'</span>, opt_lambda)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**Output**</span><br><span class="line"></span><br><span class="line">**------**</span><br><span class="line"></span><br><span class="line">Optimal <span class="keyword">lambda</span> value: <span class="number">0.117991239456</span></span><br></pre></td></tr></table></figure><p>现在我们得到了最佳的值，让我们在取值为 0 和 λ（最佳取值 λ ）时使用 Box-Cox 变换对开发者收入特征进行变换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df[<span class="string">'Income_boxcox_lambda_0'</span>] = spstats.boxcox((<span class="number">1</span>+fcc_survey_df[<span class="string">'Income'</span>]),lmbda=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_boxcox_lambda_opt'</span>] = spstats.boxcox(fcc_survey_df[<span class="string">'Income'</span>],lmbda=opt_lambda)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Income'</span>, <span class="string">'Income_log'</span>,<span class="string">'Income_boxcox_lambda_0'</span>,<span class="string">'Income_boxcox_lambda_opt'</span>]].iloc[<span class="number">4</span>:<span class="number">9</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58fd7fd5e.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="经过-Box-Cox-变换后开发者的收入分布"><a href="#经过-Box-Cox-变换后开发者的收入分布" class="headerlink" title="经过 Box-Cox 变换后开发者的收入分布"></a>经过 Box-Cox 变换后开发者的收入分布</h5><p>变换后的特征在上述数据框中描述了。就像我们期望的那样，Income_log 和 Income_boxcox_lamba_0具有相同的取值。让我们看看经过最佳λ变换后 Income 特征的分布。</p><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;income_boxcox_mean = np.round(np.mean(fcc_survey_df[<span class="string">'Income_boxcox_lambda_opt'</span>]),<span class="number">2</span>)</span><br><span class="line">&gt; </span><br><span class="line">&gt;fig, ax = plt.subplots()</span><br><span class="line">&gt; </span><br><span class="line">&gt;fcc_survey_df[<span class="string">'Income_boxcox_lambda_opt'</span>].hist(bins=<span class="number">30</span>,  color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>, grid=<span class="literal">False</span>)</span><br><span class="line">&gt;    plt.axvline(income_boxcox_mean, color=<span class="string">'r'</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.set_title(<span class="string">'Developer Income Histogram after Box–Cox Transform'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.set_xlabel(<span class="string">'Developer Income (Box–Cox transform)'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.text(<span class="number">24</span>, <span class="number">450</span>, <span class="string">r'$\mu$='</span>+str(income_boxcox_mean),fontsize=<span class="number">10</span>)       </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f591679bfb.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p><strong>经过Box-Cox变换后描述开发者收入分布的直方图</strong></p><p> 分布看起来更像是正态分布，与我们经过 log 变换后的分布相似。</p><h3 id="类别型数据上的特征工程"><a href="#类别型数据上的特征工程" class="headerlink" title="类别型数据上的特征工程"></a>类别型数据上的特征工程</h3><p>在深入研究特征工程之前，让我们先了解一下分类数据。通常，在<strong>自然界中可分类的任意数据属性都是离散值，这意味着它们属于某一特定的有限类别</strong>。在模型预测的属性或者变量（通常被称为<strong>响应变量 response variables</strong>）中，这些也经常被称为类别或者标签。这些离散值在自然界中可以是文本或者数字（甚至是诸如图像这样的非结构化数据）。分类数据有两大类——<strong>定类（Nominal）和定序（Ordinal）</strong>。</p><p>在任意定类分类数据属性中，这些属性值之间<strong>没有顺序的概念</strong>。如下图所示，举个简单的例子，天气分类。我们可以看到，在这个特定的场景中，主要有六个大类，而这些类之间没有任何顺序上的关系（刮风天并不总是发生在晴天之前，并且也不能说比晴天来的更小或者更大）</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af6bc87b4e.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>将天气作为分类属性</p><p>与天气相类似的属性还有很多，比如电影、音乐、电子游戏、国家、食物和美食类型等等，这些都属于定类分类属性。</p><p>定序分类的属性值则存在着一定的顺序意义或概念。例如，下图中的字母标识了衬衫的大小。显而易见的是，当我们考虑衬衫的时候，它的“大小”属性是很重要的（S 码比 M 码来的小，而 M 码又小于 L 码等等）。</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af6d3b83ac.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>衬衫大小作为定序分类属性</p><p>鞋号、受教育水平和公司职位则是定序分类属性的一些其它例子。既然已经对分类数据有了一个大致的理解之后，接下来我们来看看一些特征工程的策略。</p><p>在接受像文本标签这样复杂的分类数据类型问题上，各种机器学习框架均已取得了许多的进步。通常，特征工程中的任意标准工作流都涉及将这些分类值转换为数值标签的某种形式，然后对这些值应用一些<strong>编码方案</strong>。我们将在开始之前导入必要的工具包。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h4 id="定类属性转换-LabelEncoding"><a href="#定类属性转换-LabelEncoding" class="headerlink" title="定类属性转换(LabelEncoding)"></a>定类属性转换(LabelEncoding)</h4><p><strong>定类属性由离散的分类值组成，它们没有先后顺序概念</strong>。这里的思想是将这些属性转换成更具代表性的数值格式，这样可以很容易被下游的代码和流水线所理解。我们来看一个关于视频游戏销售的新数据集。这个数据集也可以在 <a href="https://www.kaggle.com/gregorut/videogamesales" target="_blank" rel="noopener">Kaggle</a> 和我的 <a href="https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/notebooks/Ch04_Feature_Engineering_and_Selection" target="_blank" rel="noopener">GitHub</a> 仓库中找到。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vg_df = pd.read_csv(<span class="string">'datasets/vgsales.csv'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">vg_df[[<span class="string">'Name'</span>, <span class="string">'Platform'</span>, <span class="string">'Year'</span>, <span class="string">'Genre'</span>, <span class="string">'Publisher'</span>]].iloc[<span class="number">1</span>:<span class="number">7</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af756b687d.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>游戏销售数据</p><p>让我们首先专注于上面数据框中“视频游戏风格（Genre）”属性。显而易见的是，这是一个类似于“发行商（Publisher）”和“平台（Platform）”属性一样的定类分类属性。我们可以很容易得到一个独特的视频游戏风格列表，如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">genres = np.unique(vg_df[<span class="string">'Genre'</span>])</span><br><span class="line"></span><br><span class="line">genres</span><br><span class="line"></span><br><span class="line">Output</span><br><span class="line"></span><br><span class="line">\------</span><br><span class="line"></span><br><span class="line">array([<span class="string">'Action'</span>, <span class="string">'Adventure'</span>, <span class="string">'Fighting'</span>, <span class="string">'Misc'</span>, <span class="string">'Platform'</span>, <span class="string">'Puzzle'</span>, <span class="string">'Racing'</span>, <span class="string">'Role-Playing'</span>, <span class="string">'Shooter'</span>, <span class="string">'Simulation'</span>, <span class="string">'Sports'</span>, <span class="string">'Strategy'</span>], dtype=object)</span><br></pre></td></tr></table></figure><p>输出结果表明，我们有 12 种不同的视频游戏风格。我们现在可以生成一个标签编码方法，即利用 scikit-learn 将每个类别映射到一个数值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line">gle = LabelEncoder()</span><br><span class="line"></span><br><span class="line">genre_labels = gle.fit_transform(vg_df[<span class="string">'Genre'</span>])</span><br><span class="line"></span><br><span class="line">genre_mappings = &#123;index: label <span class="keyword">for</span> index, label <span class="keyword">in</span> enumerate(gle.classes_)&#125;</span><br><span class="line"></span><br><span class="line">genre_mappings</span><br><span class="line"></span><br><span class="line">Output</span><br><span class="line"></span><br><span class="line">\------</span><br><span class="line"></span><br><span class="line">&#123;<span class="number">0</span>: <span class="string">'Action'</span>, <span class="number">1</span>: <span class="string">'Adventure'</span>, <span class="number">2</span>: <span class="string">'Fighting'</span>, <span class="number">3</span>: <span class="string">'Misc'</span>, <span class="number">4</span>: <span class="string">'Platform'</span>, <span class="number">5</span>: <span class="string">'Puzzle'</span>, <span class="number">6</span>: <span class="string">'Racing'</span>, <span class="number">7</span>: <span class="string">'Role-Playing'</span>, <span class="number">8</span>: <span class="string">'Shooter'</span>, <span class="number">9</span>: <span class="string">'Simulation'</span>, <span class="number">10</span>: <span class="string">'Sports'</span>, <span class="number">11</span>: <span class="string">'Strategy'</span>&#125;</span><br></pre></td></tr></table></figure><p>因此，在 <em>LabelEncoder</em> 类的实例对象 <em>gle</em> 的帮助下生成了一个映射方案，成功地将每个风格属性映射到一个数值。转换后的标签存储在 <em>genre_labels</em> 中，该变量允许我们将其写回数据表中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vg_df[<span class="string">'GenreLabel'</span>] = genre_labels</span><br><span class="line"></span><br><span class="line">vg_df[[<span class="string">'Name'</span>, <span class="string">'Platform'</span>, <span class="string">'Year'</span>, <span class="string">'Genre'</span>, <span class="string">'GenreLabel'</span>]].iloc[<span class="number">1</span>:<span class="number">7</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af8164e6db.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>视频游戏风格及其编码标签</p><p>如果你打算将它们用作预测的响应变量，那么这些标签通常可以直接用于诸如 sikit-learn 这样的框架。但是如前所述，我们还需要额外的编码步骤才能将它们用作特征。</p><h4 id="定序属性编码"><a href="#定序属性编码" class="headerlink" title="定序属性编码"></a>定序属性编码</h4><p><strong>定序属性是一种带有先后顺序概念的分类属性</strong>。这里我将以本系列文章第一部分所使用的<a href="https://www.kaggle.com/abcsds/pokemon/data" target="_blank" rel="noopener">神奇宝贝数据集</a>进行说明。让我们先专注于 「世代（Generation）」 属性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; poke_df = pd.read_csv(<span class="string">'datasets/Pokemon.csv'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; poke_df = poke_df.sample(random_state=<span class="number">1</span>, frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; np.unique(poke_df[<span class="string">'Generation'</span>])</span><br><span class="line">&gt;</span><br><span class="line">&gt; Output</span><br><span class="line">&gt;</span><br><span class="line">&gt; \------</span><br><span class="line">&gt;</span><br><span class="line">&gt; array([<span class="string">'Gen 1'</span>, <span class="string">'Gen 2'</span>, <span class="string">'Gen 3'</span>, <span class="string">'Gen 4'</span>, <span class="string">'Gen 5'</span>, <span class="string">'Gen 6'</span>], dtype=object)</span><br></pre></td></tr></table></figure><p>根据上面的输出，我们可以看到一共有 6 代，并且每个神奇宝贝通常属于视频游戏的特定世代（依据发布顺序），而且电视系列也遵循了相似的时间线。这个属性通常是定序的（需要相关的领域知识才能理解），因为属于第一代的大多数神奇宝贝在第二代的视频游戏或者电视节目中也会被更早地引入。神奇宝贝的粉丝们可以看下下图，然后记住每一代中一些比较受欢迎的神奇宝贝（不同的粉丝可能有不同的看法）。</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af8f58f535.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>基于不同类型和世代选出的一些受欢迎的神奇宝贝</p><p>因此，它们之间存在着先后顺序。一般来说，没有通用的模块或者函数可以根据这些顺序自动将这些特征转换和映射到数值表示。因此，我们可以使用自定义的编码\映射方案。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">gen_ord_map = &#123;<span class="string">'Gen 1'</span>: <span class="number">1</span>, <span class="string">'Gen 2'</span>: <span class="number">2</span>, <span class="string">'Gen 3'</span>: <span class="number">3</span>, <span class="string">'Gen 4'</span>: <span class="number">4</span>, <span class="string">'Gen 5'</span>: <span class="number">5</span>, <span class="string">'Gen 6'</span>: <span class="number">6</span>&#125; </span><br><span class="line"></span><br><span class="line">poke_df[<span class="string">'GenerationLabel'</span>] = poke_df[<span class="string">'Generation'</span>].map(gen_ord_map)</span><br><span class="line"></span><br><span class="line">poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'GenerationLabel'</span>]].iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af95f94dc8.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>神奇宝贝世代编码</p><p>从上面的代码中可以看出，来自 <em>pandas</em> 库的 <em>map(…)</em> 函数在转换这种定序特征的时候非常有用。</p><h4 id="编码分类属性–独热编码方案（One-hot-Encoding-Scheme）"><a href="#编码分类属性–独热编码方案（One-hot-Encoding-Scheme）" class="headerlink" title="编码分类属性–独热编码方案（One-hot Encoding Scheme）"></a>编码分类属性–独热编码方案（One-hot Encoding Scheme）</h4><p>如果你还记得我们之前提到过的内容，通常对分类数据进行特征工程就涉及到一个转换过程，我们在前一部分描述了一个转换过程，还有一个强制编码过程，我们应用特定的编码方案为特定的每个类别创建虚拟变量或特征分类属性。</p><p>你可能想知道，我们刚刚在上一节说到将类别转换为数字标签，为什么现在我们又需要这个？原因很简单。考虑到视频游戏风格，如果我们直接将 <em>GenereLabel</em> 作为属性特征提供给机器学习模型，则模型会认为它是一个连续的数值特征，从而认为值 10 （体育）要大于值 6 （赛车），然而事实上这种信息是毫无意义的，因为<em>体育类型</em>显然并不大于或者小于<em>赛车类型</em>，这些不同值或者类别无法直接进行比较。因此我们需要另一套编码方案层，它要能为每个属性的所有不同类别中的每个唯一值或类别创建虚拟特征。</p><p>考虑到任意具有 m 个标签的分类属性（变换之后）的数字表示，独热编码方案将该属性编码或变换成 m 个二进制特征向量（向量中的每一维的值只能为 0 或 1）。那么在这个分类特征中每个属性值都被转换成一个 m 维的向量，其中只有某一维的值为 1。让我们来看看神奇宝贝数据集的一个子集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Legendary'</span>]].iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af9b37bf97.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>神奇宝贝数据集子集</p><p>这里关注的属性是神奇宝贝的「世代（Generation）」和「传奇（Legendary）」状态。第一步是根据之前学到的将这些属性转换为数值表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder, LabelEncoder</span><br><span class="line"></span><br><span class="line">\<span class="comment"># transform and map pokemon generations</span></span><br><span class="line"></span><br><span class="line">gen_le = LabelEncoder()</span><br><span class="line"></span><br><span class="line">gen_labels = gen_le.fit_transform(poke_df[<span class="string">'Generation'</span>])</span><br><span class="line"></span><br><span class="line">poke_df[<span class="string">'Gen_Label'</span>] = gen_labels</span><br><span class="line"></span><br><span class="line">\<span class="comment"># transform and map pokemon legendary status</span></span><br><span class="line"></span><br><span class="line">leg_le = LabelEncoder()</span><br><span class="line"></span><br><span class="line">leg_labels = leg_le.fit_transform(poke_df[<span class="string">'Legendary'</span>])</span><br><span class="line"></span><br><span class="line">poke_df[<span class="string">'Lgnd_Label'</span>] = leg_labels</span><br><span class="line"></span><br><span class="line">poke_df_sub = poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>, <span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>]]</span><br><span class="line"></span><br><span class="line">poke_df_sub.iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afa18d27fc.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>转换后的标签属性</p><p><em>Gen_Label</em> 和 <em>Lgnd_Label</em> 特征描述了我们分类特征的数值表示。现在让我们在这些特征上应用独热编码方案。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encode generation labels using one-hot encoding scheme</span></span><br><span class="line"></span><br><span class="line">gen_ohe = OneHotEncoder()</span><br><span class="line"></span><br><span class="line">gen_feature_arr = gen_ohe.fit_transform(poke_df[[<span class="string">'Gen_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">gen_feature_labels = list(gen_le.classes_)</span><br><span class="line"></span><br><span class="line">gen_features = pd.DataFrame(gen_feature_arr, columns=gen_feature_labels)</span><br><span class="line"></span><br><span class="line">\<span class="comment"># encode legendary status labels using one-hot encoding scheme</span></span><br><span class="line"></span><br><span class="line">leg_ohe = OneHotEncoder()</span><br><span class="line"></span><br><span class="line">leg_feature_arr = leg_ohe.fit_transform(poke_df[[<span class="string">'Lgnd_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">leg_feature_labels = [<span class="string">'Legendary_'</span>+str(cls_label) <span class="keyword">for</span> cls_label <span class="keyword">in</span> leg_le.classes_]</span><br><span class="line"></span><br><span class="line">leg_features = pd.DataFrame(leg_feature_arr, columns=leg_feature_labels)</span><br></pre></td></tr></table></figure><p>通常来说，你可以使用 <em>fit_transform</em> 函数将两个特征一起编码（通过将两个特征的二维数组一起传递给函数，详情<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" target="_blank" rel="noopener">查看文档</a>）。但是我们分开编码每个特征，这样可以更易于理解。除此之外，我们还可以创建单独的数据表并相应地标记它们。现在让我们链接这些特征表（Feature frames）然后看看最终的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">poke_df_ohe = pd.concat([poke_df_sub, gen_features, leg_features], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">columns = sum([[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>], gen_feature_labels, [<span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>], leg_feature_labels], [])</span><br><span class="line"></span><br><span class="line">poke_df_ohe[columns].iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afab9940ae.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>神奇宝贝世代和传奇状态的独热编码特征</p><p>此时可以看到已经为「世代（Generation）」生成 6 个虚拟变量或者二进制特征，并为「传奇（Legendary）」生成了 2 个特征。这些特征数量是这些属性中不同类别的总数。<strong>某一类别的激活状态通过将对应的虚拟变量置 1 来表示</strong>，这从上面的数据表中可以非常明显地体现出来。</p><p>考虑你在训练数据上建立了这个编码方案，并建立了一些模型，现在你有了一些新的数据，这些数据必须在预测之前进行如下设计。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_poke_df = pd.DataFrame([[<span class="string">'PikaZoom'</span>, <span class="string">'Gen 3'</span>, <span class="literal">True</span>], [<span class="string">'CharMyToast'</span>, <span class="string">'Gen 4'</span>, <span class="literal">False</span>]], columns=[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Legendary'</span>])</span><br><span class="line"></span><br><span class="line">new_poke_df</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afaf0cb42e.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>新数据</p><p>你可以通过调用之前构建的 <em>LabelEncoder</em> 和 <em>OneHotEncoder</em> 对象的 <em>transform()</em> 方法来处理新数据。请记得我们的工作流程，首先我们要做转换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">new_gen_labels = gen_le.transform(new_poke_df[<span class="string">'Generation'</span>])</span><br><span class="line"></span><br><span class="line">new_poke_df[<span class="string">'Gen_Label'</span>] = new_gen_labels</span><br><span class="line"></span><br><span class="line">new_leg_labels = leg_le.transform(new_poke_df[<span class="string">'Legendary'</span>])</span><br><span class="line"></span><br><span class="line">new_poke_df[<span class="string">'Lgnd_Label'</span>] = new_leg_labels</span><br><span class="line"></span><br><span class="line">new_poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>, <span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>]]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afb428f0dc.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>转换之后的分类属性</p><p>在得到了数值标签之后，接下来让我们应用编码方案吧！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">new_gen_feature_arr = gen_ohe.transform(new_poke_df[[<span class="string">'Gen_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">new_gen_features = pd.DataFrame(new_gen_feature_arr, columns=gen_feature_labels)</span><br><span class="line"></span><br><span class="line">new_leg_feature_arr = leg_ohe.transform(new_poke_df[[<span class="string">'Lgnd_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">new_leg_features = pd.DataFrame(new_leg_feature_arr, columns=leg_feature_labels)</span><br><span class="line"></span><br><span class="line">new_poke_ohe = pd.concat([new_poke_df, new_gen_features, new_leg_features], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">columns = sum([[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>], gen_feature_labels, [<span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>], leg_feature_labels], [])</span><br><span class="line"></span><br><span class="line">new_poke_ohe[columns]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afb91bb3be.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>独热编码之后的分类属性</p><p>因此，通过利用 scikit-learn 强大的 API，我们可以很容易将编码方案应用于新数据。</p><p>你也可以通过利用来自 pandas 的 <em>to_dummies()</em> 函数轻松应用独热编码方案。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gen_onehot_features = pd.get_dummies(poke_df[<span class="string">'Generation'</span>])</span><br><span class="line"></span><br><span class="line">pd.concat([poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>]], gen_onehot_features], axis=<span class="number">1</span>).iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afbcc0ff2b.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>使用 pandas 实现的独热编码特征</p><p>上面的数据表描述了应用在「世代（Generation）」属性上的独热编码方案，结果与之前的一致。</p><h4 id="区间计数方案（Bin-counting-Scheme）"><a href="#区间计数方案（Bin-counting-Scheme）" class="headerlink" title="区间计数方案（Bin-counting Scheme）"></a>区间计数方案（Bin-counting Scheme）</h4><p>到目前为止，我们所讨论的编码方案在分类数据方面效果还不错，但是当任意特征的不同类别数量变得很大的时候，问题开始出现。对于具有 m 个不同标签的任意分类特征这点非常重要，你将得到 m 个独立的特征。这会很容易地增加特征集的大小，从而导致在时间、空间和内存方面出现存储问题或者模型训练问题。除此之外，我们还必须处理“<a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" target="_blank" rel="noopener">维度诅咒</a>”问题，通常指的是拥有大量的特征，却缺乏足够的代表性样本，然后模型的性能开始受到影响并导致过拟合。</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afd0459749.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>因此，我们需要针对那些可能具有非常多种类别的特征（如 IP 地址），研究其它分类数据特征工程方案。区间计数方案是处理具有多个类别的分类变量的有效方案。在这个方案中，我们使用<strong>基于概率的统计信息和在建模过程中所要预测的实际目标或者响应值</strong>，而不是使用实际的标签值进行编码。一个简单的例子是，基于过去的 IP 地址历史数据和 DDOS 攻击中所使用的历史数据，我们可以为任一 IP 地址会被 DDOS 攻击的可能性建立概率模型。使用这些信息，我们可以对输入特征进行编码，该输入特征描述了如果将来出现相同的 IP 地址，则引起 DDOS 攻击的概率值是多少。<strong>这个方案需要历史数据作为先决条件，并且要求数据非常详尽。</strong></p><h4 id="特征哈希方案"><a href="#特征哈希方案" class="headerlink" title="特征哈希方案"></a>特征哈希方案</h4><p>特征哈希方案（Feature Hashing Scheme）是处理大规模分类特征的另一个有用的特征工程方案。在该方案中，哈希函数通常与预设的编码特征的数量（作为预定义长度向量）一起使用，使得特征的哈希值被用作这个预定义向量中的索引，并且值也要做相应的更新。由于哈希函数将大量的值映射到一个小的有限集合中，因此<strong>多个不同值可能会创建相同的哈希</strong>，这一现象称为<strong>冲突</strong>。典型地，使用带符号的哈希函数，使得从哈希获得的值的符号被用作那些在适当的索引处存储在最终特征向量中的值的符号。这样能够确保实现较少的冲突和由于冲突导致的误差累积。</p><p>哈希方案适用于字符串、数字和其它结构（如向量）。你可以将哈希输出看作一个有限的 <em>b bins</em> 集合，以便于当将哈希函数应用于相同的值\类别时，哈希函数能根据哈希值将其分配到 <em>b bins</em> 中的同一个 bin（或者 bins 的子集）。我们可以预先定义 <em>b</em> 的值，它成为我们使用特征哈希方案编码的每个分类属性的编码特征向量的最终尺寸。</p><p>因此，即使我们有一个特征拥有超过 <strong>1000</strong> 个不同的类别，我们设置 <strong>b = 10</strong> 作为最终的特征向量长度，那么最终输出的特征将只有 10 个特征。而采用独热编码方案则有 1000 个二进制特征。我们来考虑下视频游戏数据集中的「风格（Genre）」属性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">unique_genres = np.unique(vg_df[[<span class="string">'Genre'</span>]])</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Total game genres:"</span>, len(unique_genres))</span><br><span class="line"></span><br><span class="line">print(unique_genres)</span><br><span class="line"></span><br><span class="line">Output</span><br><span class="line"></span><br><span class="line">\------</span><br><span class="line"></span><br><span class="line">Total game genres: <span class="number">12</span></span><br><span class="line"></span><br><span class="line">[<span class="string">'Action'</span> <span class="string">'Adventure'</span> <span class="string">'Fighting'</span> <span class="string">'Misc'</span> <span class="string">'Platform'</span> <span class="string">'Puzzle'</span> <span class="string">'Racing'</span> <span class="string">'Role-Playing'</span> <span class="string">'Shooter'</span> <span class="string">'Simulation'</span> <span class="string">'Sports'</span> <span class="string">'Strategy'</span>]</span><br></pre></td></tr></table></figure><p>我们可以看到，总共有 12 中风格的游戏。如果我们在“风格”特征中采用独热编码方案，则将得到 12 个二进制特征。而这次，我们将通过 scikit-learn 的 <em>FeatureHasher</em> 类来使用特征哈希方案，该类使用了一个有符号的 32 位版本的 <em>Murmurhash3</em> 哈希函数。在这种情况下，我们将预先定义最终的特征向量大小为 6。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> FeatureHasher</span><br><span class="line"></span><br><span class="line">fh = FeatureHasher(n_features=<span class="number">6</span>, input_type=<span class="string">'string'</span>)</span><br><span class="line"></span><br><span class="line">hashed_features = fh.fit_transform(vg_df[<span class="string">'Genre'</span>])</span><br><span class="line"></span><br><span class="line">hashed_features = hashed_features.toarray()pd.concat([vg_df[[<span class="string">'Name'</span>, <span class="string">'Genre'</span>]], pd.DataFrame(hashed_features)], axis=<span class="number">1</span>).iloc[<span class="number">1</span>:<span class="number">7</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afd62f2a51.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>风格属性的特征哈希</p><p>基于上述输出，「风格（Genre）」属性已经使用哈希方案编码成 6 个特征而不是 12 个。我们还可以看到，第 1 行和第 6 行表示相同风格的游戏「平台（Platform）」，而它们也被正确编码成了相同的特征向量。</p><h3 id="时间型"><a href="#时间型" class="headerlink" title="时间型"></a>时间型</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBQ8OS.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQrOU.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQcTJ.jpg" alt="avatar"></p><h3 id="文本型"><a href="#文本型" class="headerlink" title="文本型"></a>文本型</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBQhSx.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQIOO.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQzX8.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBlC7Q.jpg" alt="avatar"></p>]]></content>
      
      
      <categories>
          
          <category> 特征工程 </category>
          
          <category> 模型调优 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模型调优 </tag>
            
            <tag> python </tag>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习速查表</title>
      <link href="/2019/09/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%80%9F%E6%9F%A5%E8%A1%A8/"/>
      <url>/2019/09/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%80%9F%E6%9F%A5%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p>请参考</p><ul><li><a href="https://github.com/fengdu78/deeplearning_ai_books" target="_blank" rel="noopener">吴恩达老师的深度学习课程笔记及资源</a></li><li><a href="https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning" target="_blank" rel="noopener">CS229深度学习速查表</a></li><li><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks" target="_blank" rel="noopener">CS230深度学习速查表</a></li></ul><h2 id="深度学习各种模型与知识速查表"><a href="#深度学习各种模型与知识速查表" class="headerlink" title="深度学习各种模型与知识速查表"></a>深度学习各种模型与知识速查表</h2><p><img src="/blog_picture/dl-0001.jpg" alt="avatar"><br><img src="/blog_picture/dl-0002.jpg" alt="avatar"><br><img src="/blog_picture/dl-0003.jpg" alt="avatar"><br><img src="/blog_picture/dl-0004.jpg" alt="avatar"><br><img src="/blog_picture/dl-0005.jpg" alt="avatar"><br><img src="/blog_picture/dl-0006.jpg" alt="avatar"><br><img src="/blog_picture/dl-0007.jpg" alt="avatar"><br><img src="/blog_picture/dl-0008.jpg" alt="avatar"><br><img src="/blog_picture/dl-0009.jpg" alt="avatar"><br><img src="/blog_picture/dl-0010.jpg" alt="avatar"><br><img src="/blog_picture/dl-0011.jpg" alt="avatar"><br><img src="/blog_picture/dl-0012.jpg" alt="avatar"><br><img src="/blog_picture/dl-0013.jpg" alt="avatar"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习速查表 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模型调优</title>
      <link href="/2019/09/01/%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98/"/>
      <url>/2019/09/01/%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<p>参考阅读材料：</p><ul><li><a href="https://www.zhihu.com/question/34470160" target="_blank" rel="noopener">机器学习各种算法怎么调参?</a></li><li><a href="https://blog.csdn.net/mozhizun/article/details/60966354" target="_blank" rel="noopener">机器学习模型应用以及模型优化的一些思路</a></li><li><a href="https://blog.csdn.net/mozhizun/article/details/71438821" target="_blank" rel="noopener">机器学习模型优化中常见问题和解决思路</a></li><li><a href="https://blog.csdn.net/bitcarmanlee/article/details/71753056" target="_blank" rel="noopener">机器学习中模型优化不得不思考的几个问题</a></li><li><a href="https://www.jianshu.com/p/9fe84a7a5ba8" target="_blank" rel="noopener">机器学习中模型优化的两个问题</a></li></ul><h2 id="模型选择与调优"><a href="#模型选择与调优" class="headerlink" title="模型选择与调优"></a>模型选择与调优</h2><h3 id="不同模型的选择"><a href="#不同模型的选择" class="headerlink" title="不同模型的选择"></a>不同模型的选择</h3><p><img src="/blog_picture/model_tuning1.png" alt="avatar"></p><h3 id="模型超参数的选择"><a href="#模型超参数的选择" class="headerlink" title="模型超参数的选择"></a>模型超参数的选择</h3><p><img src="/blog_picture/model_tuning2.png" alt="avatar"></p><h3 id="评估方法-超参数产出方法"><a href="#评估方法-超参数产出方法" class="headerlink" title="评估方法+超参数产出方法"></a>评估方法+超参数产出方法</h3><p><img src="/blog_picture/model_tuning4.png" alt="avatar"><br><img src="/blog_picture/model_tuning5.png" alt="avatar"><br><img src="/blog_picture/model_tuning6.png" alt="avatar"></p><h3 id="模型状态"><a href="#模型状态" class="headerlink" title="模型状态"></a>模型状态</h3><p><img src="/blog_picture/model_tuning3.png" alt="avatar"><br><img src="/blog_picture/model_tuning7.png" alt="avatar"><br><img src="/blog_picture/model_tuning8.png" alt="avatar"><br><img src="/blog_picture/model_tuning9.png" alt="avatar"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模型调优 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>集成学习与boosting模型</title>
      <link href="/2019/09/01/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B8%8Eboosting%E6%A8%A1%E5%9E%8B/"/>
      <url>/2019/09/01/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B8%8Eboosting%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="机器学习中的集成学习"><a href="#机器学习中的集成学习" class="headerlink" title="机器学习中的集成学习"></a>机器学习中的集成学习</h2><p>顾名思义，集成学习（ensemble learning）指的是将多个学习器进行有效地结合，组建一个“学习器委员会”，其中每个学习器担任委员会成员并行使投票表决权，使得委员会最后的决定更能够四方造福普度众生<del>…</del>，即其泛化性能要能优于其中任何一个学习器。</p><h3 id="个体与集成"><a href="#个体与集成" class="headerlink" title="个体与集成"></a>个体与集成</h3><p>集成学习的基本结构为：先产生一组个体学习器，再使用某种策略将它们结合在一起。集成模型如下图所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0c15683.png" alt="1.png"></p><p>在上图的集成模型中，若个体学习器都属于同一类别，例如都是决策树或都是神经网络，则称该集成为同质的（homogeneous）;若个体学习器包含多种类型的学习算法，例如既有决策树又有神经网络，则称该集成为异质的（heterogenous）。</p><blockquote><p><strong>同质集成</strong>：个体学习器称为“基学习器”（base learner），对应的学习算法为“基学习算法”（base learning algorithm）。</p></blockquote><blockquote><p><strong>异质集成</strong>：个体学习器称为“组件学习器”（component learner）或直称为“个体学习器”。</p></blockquote><p>上面我们已经提到要让集成起来的泛化性能比单个学习器都要好，虽说团结力量大但也有木桶短板理论调皮捣蛋，那如何做到呢？这就引出了集成学习的两个重要概念：<strong>准确性</strong>和<strong>多样性</strong>（diversity）。准确性指的是个体学习器不能太差，要有一定的准确度；多样性则是个体学习器之间的输出要具有差异性。通过下面的这三个例子可以很容易看出这一点，准确度较高，差异度也较高，可以较好地提升集成性能。</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0d23e13.png" alt="2.png"></p><p>现在考虑二分类的简单情形，假设基分类器之间相互独立（能提供较高的差异度），且错误率相等为 ε，则可以将集成器的预测看做一个伯努利实验，易知当所有基分类器中不足一半预测正确的情况下，集成器预测错误，所以集成器的错误率可以计算为：</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0cce0bb.png" alt="3.png"></p><p>此时，集成器错误率随着基分类器的个数的增加呈指数下降，但前提是基分类器之间相互独立，在实际情形中显然是不可能的，假设训练有A和B两个分类器，对于某个测试样本，显然满足：P（A=1 | B=1）&gt; P（A=1），因为A和B为了解决相同的问题而训练，因此在预测新样本时存在着很大的联系。因此，<strong>个体学习器的“准确性”和“差异性”本身就是一对矛盾的变量</strong>，准确性高意味着牺牲多样性，所以产生“<strong>好而不同</strong>”的个体学习器正是集成学习研究的核心。现阶段有三种主流的集成学习方法：Boosting、Bagging以及随机森林（Random Forest），接下来将进行逐一介绍。</p><h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>Boosting是一种串行的工作机制，即个体学习器的训练存在依赖关系，必须一步一步序列化进行。其基本思想是：增加前一个基学习器在训练训练过程中预测错误样本的权重，使得后续基学习器更加关注这些打标错误的训练样本，尽可能纠正这些错误，一直向下串行直至产生需要的T个基学习器，Boosting最终对这T个学习器进行加权结合，产生学习器委员会。</p><p>Boosting族算法最著名、使用最为广泛的就是AdaBoost，因此下面主要是对AdaBoost算法进行介绍。AdaBoost使用的是<strong>指数损失函数</strong>，因此AdaBoost的权值与样本分布的更新都是围绕着最小化指数损失函数进行的。看到这里回想一下之前的机器学习算法，<strong>不难发现机器学习的大部分带参模型只是改变了最优化目标中的损失函数</strong>：如果是Square loss，那就是最小二乘了；如果是Hinge Loss，那就是著名的SVM了；如果是log-Loss，那就是Logistic Regression了。</p><p>定义基学习器的集成为加权结合，则有：</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0ca2ca5.png" alt="4.png"></p><p>AdaBoost算法的指数损失函数定义为：</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0d10461.png" alt="5.png"></p><p>具体说来，整个Adaboost 迭代算法分为3步：</p><ul><li>初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。</li><li>训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。</li><li>将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。</li></ul><p>整个AdaBoost的算法流程如下所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0d7c057.png" alt="6.png"></p><p>可以看出：<strong>AdaBoost的核心步骤就是计算基学习器权重和样本权重分布</strong>，那为何是上述的计算公式呢？这就涉及到了我们之前为什么说大部分带参机器学习算法只是改变了损失函数，就是因为<strong>大部分模型的参数都是通过最优化损失函数（可能还加个规则项）而计算（梯度下降，坐标下降等）得到</strong>，这里正是通过最优化指数损失函数从而得到这两个参数的计算公式，具体的推导过程此处不进行展开。</p><p>Boosting算法要求基学习器能对特定分布的数据进行学习，即每次都更新样本分布权重，这里书上提到了两种方法：“重赋权法”（re-weighting）和“重采样法”（re-sampling），书上的解释有些晦涩，这里进行展开一下：</p><blockquote><p><strong>重赋权法</strong> : 对每个样本附加一个权重，这时涉及到样本属性与标签的计算，都需要乘上一个权值。<br><strong>重采样法</strong> : 对于一些无法接受带权样本的及学习算法，适合用“重采样法”进行处理。方法大致过程是，根据各个样本的权重，对训练数据进行重采样，初始时样本权重一样，每个样本被采样到的概率一致，每次从N个原始的训练样本中按照权重有放回采样N个样本作为训练集，然后计算训练集错误率，然后调整权重，重复采样，集成多个基学习器。</p></blockquote><p>从偏差-方差分解来看：Boosting算法主要关注于降低偏差，每轮的迭代都关注于训练过程中预测错误的样本，将弱学习提升为强学习器。从AdaBoost的算法流程来看，标准的AdaBoost只适用于二分类问题。在此，当选为数据挖掘十大算法之一的AdaBoost介绍到这里，能够当选正是说明这个算法十分婀娜多姿，背后的数学证明和推导充分证明了这一点，限于篇幅不再继续展开。</p><h3 id="Bagging与Random-Forest"><a href="#Bagging与Random-Forest" class="headerlink" title="Bagging与Random Forest"></a>Bagging与Random Forest</h3><p>相比之下，Bagging与随机森林算法就简洁了许多，上面已经提到产生“好而不同”的个体学习器是集成学习研究的核心，即在保证基学习器准确性的同时增加基学习器之间的多样性。而这两种算法的基本思（tao）想（lu）都是通过“自助采样”的方法来增加多样性。</p><h4 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h4><p>Bagging是一种并行式的集成学习方法，即基学习器的训练之间没有前后顺序可以同时进行，Bagging使用“有放回”采样的方式选取训练集，对于包含m个样本的训练集，进行m次有放回的随机采样操作，从而得到m个样本的采样集，这样训练集中有接近36.8%的样本没有被采到。按照相同的方式重复进行，我们就可以采集到T个包含m个样本的数据集，从而训练出T个基学习器，最终对这T个基学习器的输出进行结合。</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0ce62fc.png" alt="7.png"></p><p>Bagging算法的流程如下所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0d0e761.png" alt="8.png"></p><p>可以看出Bagging主要通过<strong>样本的扰动</strong>来增加基学习器之间的多样性，因此Bagging的基学习器应为那些对训练集十分敏感的不稳定学习算法，例如：神经网络与决策树等。从偏差-方差分解来看，Bagging算法主要关注于降低方差，即通过多次重复训练提高稳定性。不同于AdaBoost的是，Bagging可以十分简单地移植到多分类、回归等问题。总的说起来则是：<strong>AdaBoost关注于降低偏差，而Bagging关注于降低方差。</strong></p><h4 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h4><p>随机森林（Random Forest）是Bagging的一个拓展体，它的基学习器固定为决策树，多棵树也就组成了森林，而“随机”则在于选择划分属性的随机，随机森林在训练基学习器时，也采用有放回采样的方式添加样本扰动，同时它还引入了一种<strong>属性扰动</strong>，即在基决策树的训练过程中，在选择划分属性时，RF先从候选属性集中随机挑选出一个包含K个属性的子集，再从这个子集中选择最优划分属性，一般推荐K=log2（d）。</p><p>这样随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，从而进一步提升了基学习器之间的差异度。相比决策树的Bagging集成，随机森林的起始性能较差（由于属性扰动，基决策树的准确度有所下降），但随着基学习器数目的增多，随机森林往往会收敛到更低的泛化误差。同时不同于Bagging中决策树从所有属性集中选择最优划分属性，随机森林只在属性集的一个子集中选择划分属性，因此训练效率更高。<br><img src="../img/random-forest.png" alt><br><img src="https://i.loli.net/2018/10/18/5bc84d0d7a4fd.png" alt="9.png"></p><h3 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h3><p>结合策略指的是在训练好基学习器后，如何将这些基学习器的输出结合起来产生集成模型的最终输出，下面将介绍一些常用的结合策略：</p><h4 id="平均法（回归问题）"><a href="#平均法（回归问题）" class="headerlink" title="平均法（回归问题）"></a>平均法（回归问题）</h4><p><img src="https://i.loli.net/2018/10/18/5bc84d0d07983.png" alt="10.png"></p><p><img src="https://i.loli.net/2018/10/18/5bc84de1b74ff.png" alt="11.png"></p><p>易知简单平均法是加权平均法的一种特例，加权平均法可以认为是集成学习研究的基本出发点。由于各个基学习器的权值在训练中得出，<strong>一般而言，在个体学习器性能相差较大时宜使用加权平均法，在个体学习器性能相差较小时宜使用简单平均法</strong>。</p><h4 id="投票法（分类问题）"><a href="#投票法（分类问题）" class="headerlink" title="投票法（分类问题）"></a>投票法（分类问题）</h4><p><img src="https://i.loli.net/2018/10/18/5bc84de2629c4.png" alt="12.png"></p><p><img src="https://i.loli.net/2018/10/18/5bc84de25a74b.png" alt="13.png"></p><p><img src="https://i.loli.net/2018/10/18/5bc84de1bacc4.png" alt="14.png"></p><p>绝对多数投票法（majority voting）提供了拒绝选项，这在可靠性要求很高的学习任务中是一个很好的机制。同时，对于分类任务，各个基学习器的输出值有两种类型，分别为类标记和类概率。</p><p><img src="https://i.loli.net/2018/10/18/5bc84de2768c1.png" alt="15.png"></p><p>一些在产生类别标记的同时也生成置信度的学习器，置信度可转化为类概率使用，<strong>一般基于类概率进行结合往往比基于类标记进行结合的效果更好</strong>，需要注意的是对于异质集成，其类概率不能直接进行比较，此时需要将类概率转化为类标记输出，然后再投票。</p><h4 id="学习法"><a href="#学习法" class="headerlink" title="学习法"></a>学习法</h4><p>学习法是一种更高级的结合策略，即学习出一种“投票”的学习器，Stacking是学习法的典型代表。Stacking的基本思想是：首先训练出T个基学习器，对于一个样本它们会产生T个输出，将这T个基学习器的输出与该样本的真实标记作为新的样本，m个样本就会产生一个m<em>T的样本集，来训练一个新的“投票”学习器。投票学习器的输入属性与学习算法对Stacking集成的泛化性能有很大的影响，书中已经提到：*</em>投票学习器采用类概率作为输入属性，选用多响应线性回归（MLR）一般会产生较好的效果**。</p><p><img src="https://i.loli.net/2018/10/18/5bc84de25cbaf.png" alt="16.png"></p><h4 id="多样性（diversity）"><a href="#多样性（diversity）" class="headerlink" title="多样性（diversity）"></a>多样性（diversity）</h4><p>在集成学习中，基学习器之间的多样性是影响集成器泛化性能的重要因素。因此增加多样性对于集成学习研究十分重要，一般的思路是在学习过程中引入随机性，常见的做法主要是对数据样本、输入属性、输出表示、算法参数进行扰动。</p><blockquote><p><strong>数据样本扰动</strong>，即利用具有差异的数据集来训练不同的基学习器。例如：有放回自助采样法，但此类做法只对那些不稳定学习算法十分有效，例如：决策树和神经网络等，训练集的稍微改变能导致学习器的显著变动。<br><strong>输入属性扰动</strong>，即随机选取原空间的一个子空间来训练基学习器。例如：随机森林，从初始属性集中抽取子集，再基于每个子集来训练基学习器。但若训练集只包含少量属性，则不宜使用属性扰动。<br><strong>输出表示扰动</strong>，此类做法可对训练样本的类标稍作变动，或对基学习器的输出进行转化。<br><strong>算法参数扰动</strong>，通过随机设置不同的参数，例如：神经网络中，随机初始化权重与随机设置隐含层节点数。</p></blockquote><h2 id="机器学习中的Boosting模型：GBDT-vs-Xgboost-vs-LightGBM"><a href="#机器学习中的Boosting模型：GBDT-vs-Xgboost-vs-LightGBM" class="headerlink" title="机器学习中的Boosting模型：GBDT vs Xgboost vs LightGBM"></a>机器学习中的Boosting模型：GBDT vs Xgboost vs LightGBM</h2><p>见资料<strong>GBDT_wepon.pdf</strong></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 集成学习 </tag>
            
            <tag> boosting </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>聚类与降维</title>
      <link href="/2019/09/01/%E8%81%9A%E7%B1%BB%E4%B8%8E%E9%99%8D%E7%BB%B4/"/>
      <url>/2019/09/01/%E8%81%9A%E7%B1%BB%E4%B8%8E%E9%99%8D%E7%BB%B4/</url>
      
        <content type="html"><![CDATA[<h2 id="机器学习中的聚类算法"><a href="#机器学习中的聚类算法" class="headerlink" title="机器学习中的聚类算法"></a>机器学习中的聚类算法</h2><p>聚类是一种经典的<strong>无监督学习</strong>方法，<strong>无监督学习的目标是通过对无标记训练样本的学习，发掘和揭示数据集本身潜在的结构与规律</strong>，即不依赖于训练数据集的类标记信息。聚类则是试图将数据集的样本划分为若干个互不相交的类簇，从而每个簇对应一个潜在的类别。</p><p>聚类直观上来说是将相似的样本聚在一起，从而形成一个<strong>类簇（cluster）</strong>。那首先的问题是如何来<strong>度量相似性</strong>（similarity measure）呢？这便是<strong>距离度量</strong>，在生活中我们说差别小则相似，对应到多维样本，每个样本可以对应于高维空间中的一个数据点，若它们的距离相近，我们便可以称它们相似。那接着如何来评价聚类结果的好坏呢？这便是<strong>性能度量</strong>，性能度量为评价聚类结果的好坏提供了一系列有效性指标。</p><h3 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h3><p>谈及距离度量，最熟悉的莫过于欧式距离了，从年头一直用到年尾的距离计算公式：即对应属性之间相减的平方和再开根号。度量距离还有其它的很多经典方法，通常它们需要满足一些基本性质：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed4c0390.png" alt="1.png"></p><p>最常用的距离度量方法是<strong>“闵可夫斯基距离”（Minkowski distance)</strong>：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed49e31f.png" alt="2.png"></p><p>当p=1时，闵可夫斯基距离即<strong>曼哈顿距离（Manhattan distance）</strong>：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed49c31f.png" alt="3.png"></p><p>当p=2时，闵可夫斯基距离即<strong>欧氏距离（Euclidean distance）</strong>：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed497613.png" alt="4.png"></p><p>我们知道属性分为两种：<strong>连续属性</strong>和<strong>离散属性</strong>（有限个取值）。对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；而对于离散值的属性，需要作下面进一步的处理：</p><blockquote><p>若属性值之间<strong>存在序关系</strong>，则可以将其转化为连续值，例如：身高属性“高”“中等”“矮”，可转化为{1, 0.5, 0}。<br>若属性值之间<strong>不存在序关系</strong>，则通常将其转化为向量的形式，例如：性别属性“男”“女”，可转化为{（1,0），（0,1）}。</p></blockquote><p>在进行距离度量时，易知<strong>连续属性和存在序关系的离散属性都可以直接参与计算</strong>，因为它们都可以反映一种程度，我们称其为“<strong>有序属性</strong>”；而对于不存在序关系的离散属性，我们称其为：“<strong>无序属性</strong>”，显然无序属性再使用闵可夫斯基距离就行不通了。</p><p><strong>对于无序属性，我们一般采用VDM进行距离的计算</strong>，例如：对于离散属性的两个取值a和b，定义：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed4e9560.png" alt="5.png"></p><p>于是，在计算两个样本之间的距离时，我们可以将闵可夫斯基距离和VDM混合在一起进行计算：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed507bc7.png" alt="6.png"></p><p>若我们定义的距离计算方法是用来度量相似性，例如下面将要讨论的聚类问题，即距离越小，相似性越大，反之距离越大，相似性越小。这时距离的度量方法并不一定需要满足前面所说的四个基本性质，这样的方法称为：<strong>非度量距离（non-metric distance）</strong>。</p><h3 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h3><p>由于聚类算法不依赖于样本的真实类标，就不能像监督学习的分类那般，通过计算分对分错（即精确度或错误率）来评价学习器的好坏或作为学习过程中的优化目标。一般聚类有两类性能度量指标：<strong>外部指标</strong>和<strong>内部指标</strong>。</p><h4 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h4><p>即将聚类结果与某个参考模型的结果进行比较，<strong>以参考模型的输出作为标准，来评价聚类好坏</strong>。假设聚类给出的结果为λ，参考模型给出的结果是λ*，则我们将样本进行两两配对，定义：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed59160e.png" alt="7.png"></p><p>显然a和b代表着聚类结果好坏的正能量，b和c则表示参考结果和聚类结果相矛盾，基于这四个值可以导出以下常用的外部评价指标：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed587438.png" alt="8.png"></p><h4 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h4><p>内部指标即不依赖任何外部模型，直接对聚类的结果进行评估，聚类的目的是想将那些相似的样本尽可能聚在一起，不相似的样本尽可能分开，直观来说：<strong>簇内高内聚紧紧抱团，簇间低耦合老死不相往来</strong>。定义：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed581852.png" alt="9.png"></p><p>基于上面的四个距离，可以导出下面这些常用的内部评价指标：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed582854.png" alt="10.png"></p><h3 id="原型聚类"><a href="#原型聚类" class="headerlink" title="原型聚类"></a>原型聚类</h3><p>原型聚类即“<strong>基于原型的聚类</strong>”（prototype-based clustering），原型表示模板的意思，就是通过参考一个模板向量或模板分布的方式来完成聚类的过程，常见的K-Means便是基于簇中心来实现聚类，混合高斯聚类则是基于簇分布来实现聚类。</p><h4 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h4><p>K-Means的思想十分简单，<strong>首先随机指定类中心，根据样本与类中心的远近划分类簇，接着重新计算类中心，迭代直至收敛</strong>。但是其中迭代的过程并不是主观地想象得出，事实上，若将样本的类别看做为“隐变量”（latent variable），类中心看作样本的分布参数，这一过程正是通过<strong>EM算法</strong>的两步走策略而计算出，其根本的目的是为了最小化平方误差函数E：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb82b5d3.png" alt="11.png"></p><p>K-Means的算法流程如下所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb9c0817.png" alt="12.png"></p><p><img src="../img/K-Means.png" alt></p><h4 id="高斯混合聚类"><a href="#高斯混合聚类" class="headerlink" title="高斯混合聚类"></a>高斯混合聚类</h4><p>现在可以看出K-Means与LVQ都试图以类中心作为原型指导聚类，高斯混合聚类则采用高斯分布来描述原型。现假设<strong>每个类簇中的样本都服从一个多维高斯分布，那么空间中的样本可以看作由k个多维高斯分布混合而成</strong>。</p><p>对于多维高斯分布，其概率密度函数如下所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb870d98.png" alt="14.png"></p><p>其中u表示均值向量，∑表示协方差矩阵，可以看出一个多维高斯分布完全由这两个参数所确定。接着定义高斯混合分布为：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb876794.png" alt="15.png"></p><p>α称为混合系数，这样空间中样本的采集过程则可以抽象为：<strong>（1）先选择一个类簇（高斯分布），（2）再根据对应高斯分布的密度函数进行采样</strong>，这时候贝叶斯公式又能大展身手了：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb9191d9.png" alt="16.png"></p><p>此时只需要选择PM最大时的类簇并将该样本划分到其中，看到这里很容易发现：这和那个传说中的贝叶斯分类不是神似吗，都是通过贝叶斯公式展开，然后计算类先验概率和类条件概率。但遗憾的是：<strong>这里没有真实类标信息，对于类条件概率，并不能像贝叶斯分类那样通过最大似然法美好地计算出来</strong>，因为这里的样本可能属于所有的类簇，这里的似然函数变为：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb871d4a.png" alt="17.png"></p><p>可以看出：简单的最大似然法根本无法求出所有的参数，这样PM也就没法计算。<strong>这里就要召唤出之前的EM大法，首先对高斯分布的参数及混合系数进行随机初始化，计算出各个PM（即γji，第i个样本属于j类），再最大化似然函数（即LL（D）分别对α、u和∑求偏导 ），对参数进行迭代更新</strong>。</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb8a6f32.png" alt="18.png"></p><p>高斯混合聚类的算法流程如下图所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb9c4fa4.png" alt="19.png"></p><h4 id="密度聚类"><a href="#密度聚类" class="headerlink" title="密度聚类"></a>密度聚类</h4><p>密度聚类则是基于密度的聚类，它从样本分布的角度来考察样本之间的可连接性，并基于可连接性（密度可达）不断拓展疆域（类簇）。其中最著名的便是<strong>DBSCAN</strong>算法，首先定义以下概念：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb9bd69c.png" alt="20.png"></p><p><img src="https://i.loli.net/2018/10/18/5bc8509f8d619.png" alt="21.png"></p><p>简单来理解DBSCAN便是：<strong>找出一个核心对象所有密度可达的样本集合形成簇</strong>。首先从数据集中任选一个核心对象A，找出所有A密度可达的样本集合，将这些样本形成一个密度相连的类簇，直到所有的核心对象都遍历完。DBSCAN算法的流程如下图所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc8509feb587.png" alt="22.png"></p><h4 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h4><p>层次聚类是一种基于树形结构的聚类方法，常用的是<strong>自底向上</strong>的结合策略（<strong>AGNES算法</strong>）。假设有N个待聚类的样本，其基本步骤是：</p><ul><li>1.初始化–&gt;把每个样本归为一类，计算每两个类之间的距离，也就是样本与样本之间的相似度；</li><li>2.寻找各个类之间最近的两个类，把他们归为一类（这样类的总数就少了一个）；</li><li>3.重新计算新生成的这个<strong>类与各个旧类之间的相似度</strong>；</li><li>4.重复2和3直到所有样本点都归为一类，结束。</li></ul><p>可以看出其中最关键的一步就是<strong>计算两个类簇的相似度</strong>，这里有多种度量方法：</p><pre><code>* 单链接（single-linkage）:取类间最小距离。</code></pre><p><img src="https://i.loli.net/2018/10/18/5bc8509ebb022.png" alt="23.png"></p><pre><code>* 全链接（complete-linkage）:取类间最大距离</code></pre><p><img src="https://i.loli.net/2018/10/18/5bc8509eb2b30.png" alt="24.png"></p><pre><code>* 均链接（average-linkage）:取类间两两的平均距离</code></pre><p><img src="https://i.loli.net/2018/10/18/5bc8509f089a7.png" alt="25.png"></p><p>很容易看出：<strong>单链接的包容性极强，稍微有点暧昧就当做是自己人了，全链接则是坚持到底，只要存在缺点就坚决不合并，均连接则是从全局出发顾全大局</strong>。层次聚类法的算法流程如下所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc8509f9d4a0.png" alt="26.png"><br><img src="/blog_picture/clustering.png" alt="avatar"></p><h2 id="机器学习中的PCA降维"><a href="#机器学习中的PCA降维" class="headerlink" title="机器学习中的PCA降维"></a>机器学习中的PCA降维</h2><p>资料from<a href="http://blog.codinglabs.org/articles/pca-tutorial.html" target="_blank" rel="noopener">PCA的数学原理</a></p><p><img src="/blog_picture/PCA_.jpg" alt="avatar"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 聚类 </tag>
            
            <tag> 降维 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>贝叶斯分类器</title>
      <link href="/2019/09/01/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/"/>
      <url>/2019/09/01/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<p>参考阅读材料：</p><ul><li><a href="https://blog.csdn.net/han_xiaoyang/article/details/50616559" target="_blank" rel="noopener">NLP系列(2)_用朴素贝叶斯进行文本分类(上)</a></li><li><a href="https://blog.csdn.net/han_xiaoyang/article/details/50629587" target="_blank" rel="noopener">NLP系列(3)_用朴素贝叶斯进行文本分类(下)</a></li></ul><p>朴素贝叶斯</p><ul><li>贝叶斯公式 + 条件独立假设</li><li>平滑算法</li></ul><h2 id="机器学习中的贝叶斯分类器"><a href="#机器学习中的贝叶斯分类器" class="headerlink" title="机器学习中的贝叶斯分类器"></a>机器学习中的贝叶斯分类器</h2><p>贝叶斯分类器是一种概率框架下的统计学习分类器，对分类任务而言，假设在相关概率都已知的情况下，贝叶斯分类器考虑如何基于这些概率为样本判定最优的类标。在开始介绍贝叶斯决策论之前，我们首先来回顾下概率论委员会常委–贝叶斯公式。</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd7a2575.png" alt="1.png"></p><h3 id="贝叶斯决策论"><a href="#贝叶斯决策论" class="headerlink" title="贝叶斯决策论"></a>贝叶斯决策论</h3><p>若将上述定义中样本空间的划分Bi看做为类标，A看做为一个新的样本，则很容易将条件概率理解为样本A是类别Bi的概率。在机器学习训练模型的过程中，往往我们都试图去优化一个风险函数，因此在概率框架下我们也可以为贝叶斯定义“<strong>条件风险</strong>”（conditional risk）。</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd15db94.png" alt="2.png"></p><p>我们的任务就是寻找一个判定准则最小化所有样本的条件风险总和，因此就有了<strong>贝叶斯判定准则</strong>（Bayes decision rule）:为最小化总体风险，只需在每个样本上选择那个使得条件风险最小的类标。</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd308600.png" alt="3.png"></p><p>若损失函数λ取0-1损失，则有：</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd37c502.png" alt="4.png"></p><p>即对于每个样本x，选择其后验概率P（c | x）最大所对应的类标，能使得总体风险函数最小，从而将原问题转化为估计后验概率P（c | x）。一般这里有两种策略来对后验概率进行估计：</p><pre><code>* 判别式模型：直接对 P（c | x）进行建模求解。例我们前面所介绍的决策树、神经网络、SVM都是属于判别式模型。* 生成式模型：通过先对联合分布P（x,c）建模，从而进一步求解 P（c | x）。</code></pre><p>贝叶斯分类器就属于生成式模型，基于贝叶斯公式对后验概率P（c | x） 进行一项神奇的变换，巴拉拉能量…. P（c | x）变身：</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd501ad3.png" alt="5.png"></p><p>对于给定的样本x，P（x）与类标无关，P（c）称为类先验概率，p（x | c ）称为类条件概率。这时估计后验概率P（c | x）就变成为估计类先验概率和类条件概率的问题。对于先验概率和后验概率，在看这章之前也是模糊了我好久，这里普及一下它们的基本概念。</p><pre><code>* 先验概率： 根据以往经验和分析得到的概率。* 后验概率：后验概率是基于新的信息，修正原来的先验概率后所获得的更接近实际情况的概率估计。</code></pre><p>实际上先验概率就是在没有任何结果出来的情况下估计的概率，而后验概率则是在有一定依据后的重新估计，直观意义上后验概率就是条件概率。下面直接上Wiki上的一个例子，简单粗暴快速完事…</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd799610.png" alt="6.png"></p><p>回归正题，对于类先验概率P（c），p（c）就是样本空间中各类样本所占的比例，根据大数定理（当样本足够多时，频率趋于稳定等于其概率），这样当训练样本充足时，p(c)可以使用各类出现的频率来代替。因此只剩下类条件概率p（x | c ），它表达的意思是在类别c中出现x的概率，它涉及到属性的联合概率问题，若只有一个离散属性还好，当属性多时采用频率估计起来就十分困难，因此这里一般采用极大似然法进行估计。</p><h3 id="极大似然法"><a href="#极大似然法" class="headerlink" title="极大似然法"></a>极大似然法</h3><p>极大似然估计（Maximum Likelihood Estimation，简称MLE），是一种根据数据采样来估计概率分布的经典方法。常用的策略是先假定总体具有某种确定的概率分布，再基于训练样本对概率分布的参数进行估计。运用到类条件概率p（x | c ）中，假设p（x | c ）服从一个参数为θ的分布，问题就变为根据已知的训练样本来估计θ。极大似然法的核心思想就是：估计出的参数使得已知样本出现的概率最大，即使得训练数据的似然最大。</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd70fb73.png" alt="7.png"></p><p>所以，贝叶斯分类器的训练过程就是参数估计。总结最大似然法估计参数的过程，一般分为以下四个步骤：</p><pre><code>* 1.写出似然函数；* 2.对似然函数取对数，并整理；* 3.求导数，令偏导数为0，得到似然方程组；* 4.解似然方程组，得到所有参数即为所求。</code></pre><p>例如：假设样本属性都是连续值，p（x | c ）服从一个多维高斯分布，则通过MLE计算出的参数刚好分别为：</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd705729.png" alt="8.png"></p><p>上述结果看起来十分合乎实际，但是采用最大似然法估计参数的效果很大程度上依赖于作出的假设是否合理，是否符合潜在的真实数据分布。这就需要大量的经验知识，搞统计越来越值钱也是这个道理，大牛们掐指一算比我们搬砖几天更有效果。</p><h3 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h3><p>不难看出：原始的贝叶斯分类器最大的问题在于联合概率密度函数的估计，首先需要根据经验来假设联合概率分布，其次当属性很多时，训练样本往往覆盖不够，参数的估计会出现很大的偏差。为了避免这个问题，朴素贝叶斯分类器（naive Bayes classifier）采用了“属性条件独立性假设”，即样本数据的所有属性之间相互独立。这样类条件概率p（x | c ）可以改写为：</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd55e102.png" alt="9.png"></p><p>这样，为每个样本估计类条件概率变成为每个样本的每个属性估计类条件概率。</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd6678cd.png" alt="10.png"></p><p>相比原始贝叶斯分类器，朴素贝叶斯分类器基于单个的属性计算类条件概率更加容易操作，需要注意的是：若某个属性值在训练集中和某个类别没有一起出现过，这样会抹掉其它的属性信息，因为该样本的类条件概率被计算为0。因此在估计概率值时，常常用进行平滑（smoothing）处理，拉普拉斯修正（Laplacian correction）就是其中的一种经典方法，具体计算方法如下：</p><p><img src="https://i.loli.net/2018/10/18/5bc83fe54aaed.png" alt="11.png"></p><p>当训练集越大时，拉普拉斯修正引入的影响越来越小。对于贝叶斯分类器，模型的训练就是参数估计，因此可以事先将所有的概率储存好，当有新样本需要判定时，直接查表计算即可。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 贝叶斯分类器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/01/SVM/"/>
      <url>/2019/09/01/SVM/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="https://zhuanlan.zhihu.com/p/36332083" target="_blank" rel="noopener">攀登传统机器学习的珠峰-SVM (上)</a></li><li><a href="https://zhuanlan.zhihu.com/p/36379394" target="_blank" rel="noopener">攀登传统机器学习的珠峰-SVM (中)</a></li><li><a href="https://zhuanlan.zhihu.com/p/36535299" target="_blank" rel="noopener">攀登传统机器学习的珠峰-SVM (下)</a></li></ul><h2 id="机器学习中的SVM"><a href="#机器学习中的SVM" class="headerlink" title="机器学习中的SVM"></a>机器学习中的SVM</h2><p>支持向量机是一种经典的二分类模型，基本模型定义为特征空间中最大间隔的线性分类器，其学习的优化目标便是间隔最大化，因此支持向量机本身可以转化为一个凸二次规划求解的问题。</p><h3 id="函数间隔与几何间隔"><a href="#函数间隔与几何间隔" class="headerlink" title="函数间隔与几何间隔"></a>函数间隔与几何间隔</h3><p>对于二分类学习，假设现在的数据是线性可分的，这时分类学习最基本的想法就是找到一个合适的超平面，该超平面能够将不同类别的样本分开，类似二维平面使用ax+by+c=0来表示，超平面实际上表示的就是高维的平面，如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f6a2ec8a.png" alt="1.png"></p><p>对数据点进行划分时，易知：当超平面距离与它最近的数据点的间隔越大，分类的鲁棒性越好，即当新的数据点加入时，超平面对这些点的适应性最强，出错的可能性最小。因此需要让所选择的超平面能够最大化这个间隔Gap（如下图所示）， 常用的间隔定义有两种，一种称之为函数间隔，一种为几何间隔，下面将分别介绍这两种间隔，并对SVM为什么会选用几何间隔做了一些阐述。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f6a06d5a.png" alt="2.png"></p><h4 id="函数间隔"><a href="#函数间隔" class="headerlink" title="函数间隔"></a>函数间隔</h4><p>在超平面w’x+b=0确定的情况下，|w’x<em>+b|能够代表点x*距离超平面的远近，易知：当w’x</em>+b&gt;0时，表示x<em>在超平面的一侧（正类，类标为1），而当w’x</em>+b&lt;0时，则表示x<em>在超平面的另外一侧（负类，类别为-1），因此（w’x</em>+b）y* 的正负性恰能表示数据点x<em>是否被分类正确。于是便引出了*</em>函数间隔**的定义（functional margin）:</p><p><img src="https://i.loli.net/2018/10/17/5bc72f690a14b.png" alt="3.png"></p><p>而超平面（w,b）关于所有样本点（Xi，Yi）的函数间隔最小值则为超平面在训练数据集T上的函数间隔：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f690ac26.png" alt="4.png"></p><p>可以看出：这样定义的函数间隔在处理SVM上会有问题，当超平面的两个参数w和b同比例改变时，函数间隔也会跟着改变，但是实际上超平面还是原来的超平面，并没有变化。例如：w1x1+w2x2+w3x3+b=0其实等价于2w1x1+2w2x2+2w3x3+2b=0，但计算的函数间隔却翻了一倍。从而引出了能真正度量点到超平面距离的概念–几何间隔（geometrical margin）。</p><h4 id="几何间隔"><a href="#几何间隔" class="headerlink" title="几何间隔"></a>几何间隔</h4><p><strong>几何间隔</strong>代表的则是数据点到超平面的真实距离，对于超平面w’x+b=0，w代表的是该超平面的法向量，设x<em>为超平面外一点x在法向量w方向上的投影点，x与超平面的距离为r，则有x</em>=x-r(w/||w||)，又x<em>在超平面上，即w’x</em>+b=0，代入即可得：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f697d499.png" alt="5.png"></p><p>为了得到r的绝对值，令r呈上其对应的类别y，即可得到几何间隔的定义：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f696fd10.png" alt="6.png"></p><p>从上述函数间隔与几何间隔的定义可以看出：实质上函数间隔就是|w’x+b|，而几何间隔就是点到超平面的距离。</p><h3 id="最大间隔与支持向量"><a href="#最大间隔与支持向量" class="headerlink" title="最大间隔与支持向量"></a>最大间隔与支持向量</h3><p>通过前面的分析可知：函数间隔不适合用来最大化间隔，因此这里我们要找的最大间隔指的是几何间隔，于是最大间隔分类器的目标函数定义为：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f69af163.png" alt="7.png"></p><p>一般地，我们令r^为1（这样做的目的是为了方便推导和目标函数的优化），从而上述目标函数转化为：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f697bb1d.png" alt="8.png"></p><p>对于y(w’x+b)=1的数据点，即下图中位于w’x+b=1或w’x+b=-1上的数据点，我们称之为<strong>支持向量</strong>（support vector），易知：对于所有的支持向量，它们恰好满足y<em>(w’x</em>+b)=1，而所有不是支持向量的点，有y<em>(w’x</em>+b)&gt;1。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f6a838c4.png" alt="9.png"></p><h3 id="从原始优化问题到对偶问题"><a href="#从原始优化问题到对偶问题" class="headerlink" title="从原始优化问题到对偶问题"></a>从原始优化问题到对偶问题</h3><p>对于上述得到的目标函数，求1/||w||的最大值相当于求||w||^2的最小值，因此很容易将原来的目标函数转化为：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f6978cbb.png" alt="10.png"></p><p>即变为了一个带约束的凸二次规划问题，按书上所说可以使用现成的优化计算包（QP优化包）求解，但由于SVM的特殊性，一般我们将原问题变换为它的<strong>对偶问题</strong>，接着再对其对偶问题进行求解。为什么通过对偶问题进行求解，有下面两个原因：</p><pre><code>* 一是因为使用对偶问题更容易求解；* 二是因为通过对偶问题求解出现了向量内积的形式，从而能更加自然地引出核函数。</code></pre><p>对偶问题，顾名思义，可以理解成优化等价的问题，更一般地，是将一个原始目标函数的最小化转化为它的对偶函数最大化的问题。对于当前的优化问题，首先我们写出它的朗格朗日函数：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9332be7.png" alt="11.png"></p><p>上式很容易验证：当其中有一个约束条件不满足时，L的最大值为 ∞（只需令其对应的α为 ∞即可）；当所有约束条件都满足时，L的最大值为1/2||w||^2（此时令所有的α为0），因此实际上原问题等价于：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f93321c5.png" alt="12.png"></p><p>由于这个的求解问题不好做，因此一般我们将最小和最大的位置交换一下（需满足KKT条件） ，变成原问题的对偶问题：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9330967.png" alt="13.png"></p><p>这样就将原问题的求最小变成了对偶问题求最大（用对偶这个词还是很形象），接下来便可以先求L对w和b的极小，再求L对α的极大。</p><p>（1）首先求L对w和b的极小，分别求L关于w和b的偏导，可以得出：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9333e66.png" alt="14.png"></p><p>将上述结果代入L得到：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f935ae21.png" alt="15.png"></p><p>（2）接着L关于α极大求解α（通过SMO算法求解，此处不做深入）。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9338a9d.png" alt="16.png"></p><p>（3）最后便可以根据求解出的α，计算出w和b，从而得到分类超平面函数。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f93419ca.png" alt="17.png"></p><p>在对新的点进行预测时，实际上就是将数据点x*代入分类函数f(x)=w’x+b中，若f(x)&gt;0，则为正类，f(x)&lt;0，则为负类，根据前面推导得出的w与b，分类函数如下所示，此时便出现了上面所提到的内积形式。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9353166.png" alt="18.png"></p><p>这里实际上只需计算新样本与支持向量的内积，因为对于非支持向量的数据点，其对应的拉格朗日乘子一定为0，根据最优化理论（K-T条件），对于不等式约束y(w’x+b)-1≥0，满足：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f933c947.png" alt="19.png">        </p><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>由于上述的超平面只能解决线性可分的问题，对于线性不可分的问题，例如：异或问题，我们需要使用核函数将其进行推广。一般地，解决线性不可分问题时，常常采用<strong>映射</strong>的方式，将低维原始空间映射到高维特征空间，使得数据集在高维空间中变得线性可分，从而再使用线性学习器分类。如果原始空间为有限维，即属性数有限，那么总是存在一个高维特征空间使得样本线性可分。若∅代表一个映射，则在特征空间中的划分函数变为：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f934303e.png" alt="20.png"></p><p>按照同样的方法，先写出新目标函数的拉格朗日函数，接着写出其对偶问题，求L关于w和b的极大，最后运用SOM求解α。可以得出：</p><p>（1）原对偶问题变为：</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc68b3b.png" alt="21.png"></p><p>（2）原分类函数变为：<br><img src="https://i.loli.net/2018/10/17/5bc730cc1b673.png" alt="22.png"></p><p>求解的过程中，只涉及到了高维特征空间中的内积运算，由于特征空间的维数可能会非常大，例如：若原始空间为二维，映射后的特征空间为5维，若原始空间为三维，映射后的特征空间将是19维，之后甚至可能出现无穷维，根本无法进行内积运算了，此时便引出了<strong>核函数</strong>（Kernel）的概念。</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc49adc.png" alt="23.png"></p><p>因此，核函数可以直接计算隐式映射到高维特征空间后的向量内积，而不需要显式地写出映射后的结果，它虽然完成了将特征从低维到高维的转换，但最终却是在低维空间中完成向量内积计算，与高维特征空间中的计算等效<strong>（低维计算，高维表现）</strong>，从而避免了直接在高维空间无法计算的问题。引入核函数后，原来的对偶问题与分类函数则变为：</p><p>（1）对偶问题：</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc173b2.png" alt="24.png"></p><p>（2）分类函数：</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc05959.png" alt="25.png"></p><p>因此，在线性不可分问题中，核函数的选择成了支持向量机的最大变数，若选择了不合适的核函数，则意味着将样本映射到了一个不合适的特征空间，则极可能导致性能不佳。同时，核函数需要满足以下这个必要条件：</p><p><img src="https://i.loli.net/2018/10/17/5bc730ccc468c.png" alt="26.png"></p><p>由于核函数的构造十分困难，通常我们都是从一些常用的核函数中选择，下面列出了几种常用的核函数：</p><p><img src="https://i.loli.net/2018/10/17/5bc730ccc541a.png" alt="27.png"></p><h3 id="软间隔支持向量机"><a href="#软间隔支持向量机" class="headerlink" title="软间隔支持向量机"></a>软间隔支持向量机</h3><p>前面的讨论中，我们主要解决了两个问题：当数据线性可分时，直接使用最大间隔的超平面划分；当数据线性不可分时，则通过核函数将数据映射到高维特征空间，使之线性可分。然而在现实问题中，对于某些情形还是很难处理，例如数据中有<strong>噪声</strong>的情形，噪声数据（<strong>outlier</strong>）本身就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束，如果不要这些噪声数据还好，当加入这些outlier后导致划分超平面被挤歪了，如下图所示，对支持向量机的泛化性能造成很大的影响。</p><p><img src="https://i.loli.net/2018/10/17/5bc730ccce68e.png" alt="28.png"></p><p>为了解决这一问题，我们需要允许某一些数据点不满足约束，即可以在一定程度上偏移超平面，同时使得不满足约束的数据点尽可能少，这便引出了<strong>“软间隔”支持向量机</strong>的概念</p><pre><code>* 允许某些数据点不满足约束y(w&apos;x+b)≥1；* 同时又使得不满足约束的样本尽可能少。</code></pre><p>这样优化目标变为：</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc6c9fe.png" alt="29.png"></p><p>如同阶跃函数，0/1损失函数虽然表示效果最好，但是数学性质不佳。因此常用其它函数作为“替代损失函数”。</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc5e5a9.png" alt="30.png"></p><p>支持向量机中的损失函数为<strong>hinge损失</strong>，引入<strong>“松弛变量”</strong>，目标函数与约束条件可以写为：</p><p><img src="https://i.loli.net/2018/10/17/5bc7317aa3411.png" alt="31.png"></p><p>其中C为一个参数，控制着目标函数与新引入正则项之间的权重，这样显然每个样本数据都有一个对应的松弛变量，用以表示该样本不满足约束的程度，将新的目标函数转化为拉格朗日函数得到：</p><p><img src="https://i.loli.net/2018/10/17/5bc7317a4c96e.png" alt="32.png"></p><p>按照与之前相同的方法，先让L求关于w，b以及松弛变量的极小，再使用SMO求出α，有：</p><p><img src="https://i.loli.net/2018/10/17/5bc7317a6dff2.png" alt="33.png"></p><p>将w代入L化简，便得到其对偶问题：</p><p><img src="https://i.loli.net/2018/10/17/5bc7317ab6646.png" alt="34.png"></p><p>将“软间隔”下产生的对偶问题与原对偶问题对比可以发现：新的对偶问题只是约束条件中的α多出了一个上限C，其它的完全相同，因此在引入核函数处理线性不可分问题时，便能使用与“硬间隔”支持向量机完全相同的方法。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>决策树与随机森林</title>
      <link href="/2019/09/01/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
      <url>/2019/09/01/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/</url>
      
        <content type="html"><![CDATA[<h2 id="机器学习中的决策树模型"><a href="#机器学习中的决策树模型" class="headerlink" title="机器学习中的决策树模型"></a>机器学习中的决策树模型</h2><ul><li>① 树模型不用做scaling</li><li>② 树模型不太需要做离散化</li><li>③ 用Xgboost等工具库，是不需要做缺失值填充</li><li>④ 树模型是非线性模型，有非线性的表达能力</li></ul><h3 id="决策树基本概念"><a href="#决策树基本概念" class="headerlink" title="决策树基本概念"></a>决策树基本概念</h3><ul><li>决策时是一种树形结构，其中每个内部节点表示在一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别</li><li>决策树学习是以实例为基础的归纳学习</li><li>决策树学习采用的是自顶向下的归纳方法，其基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子节点处的熵值为0，此时每个叶子节点中的实例都属于同一类。</li></ul><p>顾名思义，决策树是基于树结构来进行决策的，，在网上看到一个例子十分有趣，放在这里正好合适。现想象一位捉急的母亲想要给自己的女娃介绍一个男朋友，于是有了下面的对话：</p><hr><pre><code>女儿：多大年纪了？母亲：26。女儿：长的帅不帅？母亲：挺帅的。女儿：收入高不？母亲：不算很高，中等情况。女儿：是公务员不？母亲：是，在税务局上班呢。女儿：那好，我去见见。</code></pre><hr><p>这个女孩的挑剔过程就是一个典型的决策树，即相当于通过年龄、长相、收入和是否公务员将男童鞋分为两个类别：见和不见。假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么使用下图就能很好地表示女孩的决策逻辑（即一颗决策树）。</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec84a77.png" alt="1.png"></p><p>在上图的决策树中，决策过程的每一次判定都是对某一属性的“测试”，决策最终结论则对应最终的判定结果。一般一颗决策树包含：一个根节点、若干个内部节点和若干个叶子节点，易知：</p><pre><code>* 每个非叶节点表示一个特征属性测试。* 每个分支代表这个特征属性在某个值域上的输出。* 每个叶子节点存放一个类别。* 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。</code></pre><h3 id="决策树的构造"><a href="#决策树的构造" class="headerlink" title="决策树的构造"></a>决策树的构造</h3><p>决策树的构造是一个递归的过程，有三种情形会导致递归返回：(1) 当前结点包含的样本全属于同一类别，这时直接将该节点标记为叶节点，并设为相应的类别；(2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别；(3) 当前结点包含的样本集合为空，不能划分，这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。算法的基本流程如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc728ecc27fe.png" alt="2.png"></p><p>可以看出：决策树学习的关键在于如何选择划分属性，不同的划分属性得出不同的分支结构，从而影响整颗决策树的性能。属性划分的目标是让各个划分出来的子节点尽可能地“纯”，即属于同一类别。因此下面便是介绍量化纯度的具体方法，决策树最常用的算法有三种：ID3，C4.5和CART。##</p><h4 id="决策树学习算法的特点"><a href="#决策树学习算法的特点" class="headerlink" title="决策树学习算法的特点"></a>决策树学习算法的特点</h4><p>决策树学习算法最大的优点是，它可以自学习。在学习的过程中，不需要使用者了解过多背景知识，只需要对训练实例进行较好的标注，就能进行学习。显然，属于有监督学习。从一类无序、无规则的事物中推理出决策树表示分类的规则。</p><h4 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h4><p>ID3算法使用信息增益为准则来选择划分属性，“信息熵”(information entropy)是度量样本结合纯度的常用指标，假定当前样本集合D中第k类样本所占比例为pk，则样本集合D的信息熵定义为：</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec515a5.png" alt="3.png"></p><p>假定通过属性划分样本集D，产生了V个分支节点，v表示其中第v个分支节点，易知：分支节点包含的样本数越多，表示该分支节点的影响力越大。故可以计算出划分后相比原始数据集D获得的“信息增益”（information gain）。</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec3e067.png" alt="4.png"></p><p>Ent(D)划分前的信息增益 -划分后的信息增益</p><p>DV/D表示第V个分支的权重，样本越多越重要</p><p>信息增益越大，表示使用该属性划分样本集D的效果越好，因此ID3算法在递归过程中，每次选择最大信息增益的属性作为当前的划分属性。</p><h4 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h4><p>ID3算法存在一个问题，就是偏向于取值数目较多的属性，例如：如果存在一个唯一标识，这样样本集D将会被划分为|D|个分支，每个分支只有一个样本，这样划分后的信息熵为零，十分纯净，但是对分类毫无用处。因此C4.5算法使用了“增益率”（gain ratio）来选择划分属性，来避免这个问题带来的困扰。首先使用ID3算法计算出信息增益高于平均水平的候选属性，接着C4.5计算这些候选属性的增益率，增益率定义为：</p><p>启发式：先从候选划分属性中找出信息增益高于平均水平的，再从中选取增益率最高的</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec69647.png" alt="5.png"></p><h4 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h4><p>CART决策树使用“基尼指数”（Gini index）来选择划分属性，基尼指数反映的是从样本集D中随机抽取两个样本，其类别标记不一致的概率，因此Gini(D)越小越好，数据集D的纯度越高。基尼指数定义如下：</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec5a2ff.png" alt="6.png"></p><p>进而，使用属性α划分后的基尼指数为：</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec62eaf.png" alt="7.png"></p><p>二分类视角看CART</p><ul><li>每一个产生分支的过程是一个二分类过程</li><li>这个过程叫作“决策树桩”</li><li>一棵CART是由许多决策树桩拼接起来的</li><li>决策树桩是只有一层的决策树</li></ul><h4 id="三种不同的决策树"><a href="#三种不同的决策树" class="headerlink" title="三种不同的决策树"></a>三种不同的决策树</h4><ul><li>ID3:取值多的属性，更容易使数据更纯，其信息增益更大；训练得到的是一棵庞大且深度浅的树：不合理</li><li>C4.5:采用信息增益率替代信息增益</li><li>CART：以基尼系数替代熵；最小化不纯度，而不是最大化信息增益</li></ul><h4 id="剪枝处理"><a href="#剪枝处理" class="headerlink" title="剪枝处理"></a>剪枝处理</h4><p>从决策树的构造流程中我们可以直观地看出：不管怎么样的训练集，决策树总是能很好地将各个类别分离开来，这时就会遇到之前提到过的问题：过拟合（overfitting），即太依赖于训练样本。剪枝（pruning）则是决策树算法对付过拟合的主要手段，剪枝的策略有两种如下：</p><pre><code>* 预剪枝（prepruning）：在构造的过程中先评估，再考虑是否分支。* 后剪枝（post-pruning）：在构造好一颗完整的决策树后，自底向上，评估分支的必要性。</code></pre><p>评估指的是性能度量，即决策树的泛化性能。之前提到：可以使用测试集作为学习器泛化性能的近似，因此可以将数据集划分为训练集和测试集。预剪枝表示在构造数的过程中，对一个节点考虑是否分支时，首先计算决策树不分支时在测试集上的性能，再计算分支之后的性能，若分支对性能没有提升，则选择不分支（即剪枝）。后剪枝则表示在构造好一颗完整的决策树后，从最下面的节点开始，考虑该节点分支对模型的性能是否有提升，若无则剪枝，即将该节点标记为叶子节点，类别标记为其包含样本最多的类别。</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec80d34.png" alt="8.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc728ec9e330.png" alt="9.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc728ec9d497.png" alt="10.png"></p><p>上图分别表示不剪枝处理的决策树、预剪枝决策树和后剪枝决策树。预剪枝处理使得决策树的很多分支被剪掉，因此大大降低了训练时间开销，同时降低了过拟合的风险，但另一方面由于剪枝同时剪掉了当前节点后续子节点的分支，因此预剪枝“贪心”的本质阻止了分支的展开，在一定程度上带来了欠拟合的风险。而后剪枝则通常保留了更多的分支，因此采用后剪枝策略的决策树性能往往优于预剪枝，但其自底向上遍历了所有节点，并计算性能，训练时间开销相比预剪枝大大提升。</p><h4 id="连续值与缺失值处理"><a href="#连续值与缺失值处理" class="headerlink" title="连续值与缺失值处理"></a>连续值与缺失值处理</h4><p>对于连续值的属性，若每个取值作为一个分支则显得不可行，因此需要进行离散化处理，常用的方法为二分法，基本思想为：给定样本集D与连续属性α，二分法试图找到一个划分点t将样本集D在属性α上分为≤t与＞t。</p><pre><code>* 首先将α的所有取值按升序排列，所有相邻属性的均值作为候选划分点（n-1个，n为α所有的取值数目）。* 计算每一个划分点划分集合D（即划分为两个分支）后的信息增益。* 选择最大信息增益的划分点作为最优划分点。</code></pre><p><img src="https://i.loli.net/2018/10/17/5bc72a0968fad.png" alt="11.png"></p><p>现实中常会遇到不完整的样本，即某些属性值缺失。有时若简单采取剔除，则会造成大量的信息浪费，因此在属性值缺失的情况下需要解决两个问题：（1）如何选择划分属性。（2）给定划分属性，若某样本在该属性上缺失值，如何划分到具体的分支上。假定为样本集中的每一个样本都赋予一个权重，根节点中的权重初始化为1，则定义：</p><p><img src="https://i.loli.net/2018/10/17/5bc72a098f3be.png" alt="12.png"></p><p>对于（1）：通过在样本集D中选取在属性α上没有缺失值的样本子集，计算在该样本子集上的信息增益，最终的信息增益等于该样本子集划分后信息增益乘以样本子集占样本集的比重。即：</p><p><img src="https://i.loli.net/2018/10/17/5bc72a096ccc3.png" alt="13.png"></p><p>对于（2）：若该样本子集在属性α上的值缺失，则将该样本以不同的权重（即每个分支所含样本比例）划入到所有分支节点中。该样本在分支节点中的权重变为：</p><p><img src="https://i.loli.net/2018/10/17/5bc72a093ed3c.png" alt="14.png"></p><h4 id="Bootstraping"><a href="#Bootstraping" class="headerlink" title="Bootstraping"></a>Bootstraping</h4><p>称为自助法，它是一种有放回的抽样方法</p><p>####Bagging的策略</p><ul><li>bootstrap aggregation</li><li>从样本集中重采样(有重复的)选出n个样本</li><li>在所有属性上，对这n个样本建立分类器(ID3、C4.5、CART、SVM、Logistic回归等)</li><li>重复以上两步m次，即获得了m个分类器</li><li>将数据放在这m个分类器上，最后根据这m个分类器的投票结果，决定数据属于哪一类</li></ul><h4 id="OOB数据"><a href="#OOB数据" class="headerlink" title="OOB数据"></a>OOB数据</h4><p>可以发现，Bootstrap每次约有36.79%的样本不会出现在Bootstrap所采集的样本集合中，将未参与模型训练的数据称为袋外数据(out of bag)。它可以用于取代测试集用于误差估计。得到的模型参数是无偏估计。</p><h4 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h4><p>随机森林在bagging基础上做了修改</p><ul><li>从样本集中用bootstrap采样选出n个样本</li><li>从所有属性中随机选择k个属性，选择最佳分割属性作为节点建立CART决策树</li><li>重复以上两步m次，即建立m课CART决策树</li><li>这m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类</li></ul><h4 id="随机森林-bagging和决策树的关系"><a href="#随机森林-bagging和决策树的关系" class="headerlink" title="随机森林/bagging和决策树的关系"></a>随机森林/bagging和决策树的关系</h4><ul><li>当然可以使用决策树作为基本分类器</li><li>但也可以使用SVM、Logistics回归等其他分类，习惯上，这些分类器组成的“总分类器”，仍然叫做随机森林</li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 决策树 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习逻辑回归与softmax</title>
      <link href="/2019/08/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8Esoftmax/"/>
      <url>/2019/08/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8Esoftmax/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习逻辑回归与softmax"><a href="#机器学习逻辑回归与softmax" class="headerlink" title="机器学习逻辑回归与softmax"></a>机器学习逻辑回归与softmax</h1><h2 id="机器学习中的线性模型"><a href="#机器学习中的线性模型" class="headerlink" title="机器学习中的线性模型"></a>机器学习中的线性模型</h2><p>谈及线性模型，其实我们很早就已经与它打过交道，还记得高中数学必修3课本中那个顽皮的“最小二乘法”吗？这就是线性模型的经典算法之一：根据给定的（x，y）点对，求出一条与这些点拟合效果最好的直线y=ax+b，之前我们利用下面的公式便可以计算出拟合直线的系数a,b（3.1中给出了具体的计算过程），从而对于一个新的x，可以预测它所对应的y值。前面我们提到：在机器学习的术语中，当预测值为连续值时，称为“回归问题”，离散值时为“分类问题”。本篇先从线性回归任务开始，接着讨论分类和多分类问题。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b068e48.png" alt="1.png"></p><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>线性回归问题就是试图学到一个线性模型尽可能准确地预测新样本的输出值，例如：通过历年的人口数据预测2017年人口数量。在这类问题中，往往我们会先得到一系列的有标记数据，例如：2000–&gt;13亿…2016–&gt;15亿，这时输入的属性只有一个，即年份；也有输入多属性的情形，假设我们预测一个人的收入，这时输入的属性值就不止一个了，例如：（学历，年龄，性别，颜值，身高，体重）–&gt;15k。</p><p>有时这些输入的属性值并不能直接被我们的学习模型所用，需要进行相应的处理，对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；对于离散值的属性，可作下面的处理：</p><ul><li><p>若属性值之间存在“序关系”，则可以将其转化为连续值，例如：身高属性分为“高”“中等”“矮”，可转化为数值：{1， 0.5， 0}。</p></li><li><p>若属性值之间不存在“序关系”，则通常将其转化为向量的形式，例如：性别属性分为“男”“女”，可转化为二维向量：{（1，0），（0，1）}。</p></li></ul><p>（1）当输入属性只有一个的时候，就是最简单的情形，也就是我们高中时最熟悉的“最小二乘法”（Euclidean distance），首先计算出每个样本预测值与真实值之间的误差并求和，通过最小化均方误差MSE，使用求偏导等于零的方法计算出拟合直线y=wx+b的两个参数w和b，计算过程如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0ccec4.png" alt="2.png"></p><p>（2）当输入属性有多个的时候，例如对于一个样本有d个属性{（x1,x2…xd）,y}，则y=wx+b需要写成：</p><p><img src="https://i.loli.net/2018/10/17/5bc72567b8bcd.png" alt="0.png"></p><p>通常对于多元问题，常常使用矩阵的形式来表示数据。在本问题中，将具有m个样本的数据集表示成矩阵X，将系数w与b合并成一个列向量，这样每个样本的预测值以及所有样本的均方误差最小化就可以写成下面的形式：</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0ad8f7.png" alt="3.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc722b0af652.png" alt="4.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc722b090543.png" alt="5.png"></p><p><img src="/blog_picture/line04.jpg" alt="avatar"></p><p><img src="/blog_picture/line05.jpg" alt="avatar"></p><p><img src="/blog_picture/line06.jpg" alt="avatar"></p><p><img src="/blog_picture/line07.jpg" alt="avatar"></p><h3 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h3><p>同样地，我们使用最小二乘法对w和b进行估计，令均方误差的求导等于0，需要注意的是，当一个矩阵的行列式不等于0时，我们才可能对其求逆，因此对于下式，我们需要考虑矩阵（X的转置*X）的行列式是否为0，若不为0，则可以求出其解，若为0，则需要使用其它的方法进行计算，书中提到了引入正则化，此处不进行深入。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0cde33.png" alt="6.png"></p><p>然而现实任务中当特征数量大于样本数时，XTX不满秩，此时θ有多个解；而且当数据量大时，求矩阵的逆非常耗时；对于不可逆矩阵（特征之间不相互独立），这种正规方程方法是不能用的。所以，还可以采用梯度下降法，利用迭代的方式求解θ。</p><h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>梯度下降法是按下面的流程进行的：<br>1）首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。<br>2）改变θ的值，使得θ按梯度下降的方向进行减少。</p><p><img src="/blog_picture/line01.jpg" alt="avatar"></p><p>对于只有两维属性的样本，J(θ)即J(θ0,θ1)的等高线图</p><p><img src="/blog_picture/line02.jpg" alt="avatar"></p><p><img src="/blog_picture/line03.jpg" alt="avatar"></p><p>迭代更新的方式有多种</p><ul><li>批量梯度下降（batch gradient descent），也就是是梯度下降法最原始的形式，对全部的训练数据求得误差后再对θ<br>进行更新，优点是每步都趋向全局最优解；缺点是对于大量数据，由于每步要计算整体数据，训练过程慢；</li><li>随机梯度下降（stochastic gradient descent），每一步随机选择一个样本对θ<br>进行更新，优点是训练速度快；缺点是每次的前进方向不好确定，容易陷入局部最优；</li><li>微型批量梯度下降（mini-batch gradient descent），每步选择一小批数据进行批量梯度下降更新θ<br>，属于批量梯度下降和随机梯度下降的一种折中，非常适合并行处理。</li></ul><p>另一方面，有时像上面这种原始的线性回归可能并不能满足需求，例如：y值并不是线性变化，而是在指数尺度上变化。这时我们可以采用线性模型来逼近y的衍生物，例如lny，这时衍生的线性模型如下所示，实际上就是相当于将指数曲线投影在一条直线上，如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc722b103cbf.png" alt="7.png"></p><p>更一般地，考虑所有y的衍生物的情形，就得到了“广义的线性模型”（generalized linear model），其中，g（*）称为联系函数（link function）。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0a2841.png" alt="8.png"></p><h3 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a>对数几率回归</h3><p>回归就是通过输入的属性值得到一个预测值，利用上述广义线性模型的特征，是否可以通过一个联系函数，将预测值转化为离散值从而进行分类呢？线性几率回归正是研究这样的问题。对数几率引入了一个对数几率函数（logistic function）,将预测值投影到0-1之间，从而将线性回归问题转化为二分类问题。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0c7748.png" alt="9.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc722b0a655d.png" alt="10.png"></p><p>若将y看做样本为正例的概率，（1-y）看做样本为反例的概率，则上式实际上使用线性回归模型的预测结果器逼近真实标记的对数几率。因此这个模型称为“对数几率回归”（logistic regression），也有一些书籍称之为“逻辑回归”。下面使用最大似然估计的方法来计算出w和b两个参数的取值，下面只列出求解的思路，不列出具体的计算过程。</p><p><img src="https://i.loli.net/2018/10/17/5bc723b824f0c.png" alt="11.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc723b817961.png" alt="12.png"></p><h3 id="线性判别分析"><a href="#线性判别分析" class="headerlink" title="线性判别分析"></a>线性判别分析</h3><p>线性判别分析（Linear Discriminant Analysis，简称LDA）,其基本思想是：将训练样本投影到一条直线上，使得同类的样例尽可能近，不同类的样例尽可能远。如图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc723b863ebb.png" alt="13.png"><img src="https://i.loli.net/2018/10/17/5bc723b85bfa9.png" alt="14.png"></p><p>想让同类样本点的投影点尽可能接近，不同类样本点投影之间尽可能远，即：让各类的协方差之和尽可能小，不用类之间中心的距离尽可能大。基于这样的考虑，LDA定义了两个散度矩阵。</p><ul><li>类内散度矩阵（within-class scatter matrix）</li></ul><p><img src="https://i.loli.net/2018/10/17/5bc723b8156e1.png" alt="15.png"></p><ul><li>类间散度矩阵(between-class scaltter matrix)</li></ul><p><img src="https://i.loli.net/2018/10/17/5bc723b7e9db3.png" alt="16.png"></p><p>因此得到了LDA的最大化目标：“广义瑞利商”（generalized Rayleigh quotient）。</p><p><img src="https://i.loli.net/2018/10/17/5bc723b7e8a61.png" alt="17.png"></p><p>从而分类问题转化为最优化求解w的问题，当求解出w后，对新的样本进行分类时，只需将该样本点投影到这条直线上，根据与各个类别的中心值进行比较，从而判定出新样本与哪个类别距离最近。求解w的方法如下所示，使用的方法为λ乘子。</p><p><img src="https://i.loli.net/2018/10/17/5bc723b83d5e0.png" alt="18.png"></p><p>若将w看做一个投影矩阵，类似PCA的思想，则LDA可将样本投影到N-1维空间（N为类簇数），投影的过程使用了类别信息（标记信息），因此LDA也常被视为一种经典的监督降维技术。    </p><h3 id="回归与欠-过拟合"><a href="#回归与欠-过拟合" class="headerlink" title="回归与欠/过拟合"></a>回归与欠/过拟合</h3><p><img src="/blog_picture/line08.jpg" alt="avatar">    </p><h3 id="线性回归与正则化"><a href="#线性回归与正则化" class="headerlink" title="线性回归与正则化"></a>线性回归与正则化</h3><p><img src="/blog_picture/line09.jpg" alt="avatar">     </p><h3 id="多分类学习"><a href="#多分类学习" class="headerlink" title="多分类学习"></a>多分类学习</h3><p>现实中我们经常遇到不只两个类别的分类问题，即多分类问题，在这种情形下，我们常常运用“拆分”的策略，通过多个二分类学习器来解决多分类问题，即将多分类问题拆解为多个二分类问题，训练出多个二分类学习器，最后将多个分类结果进行集成得出结论。最为经典的拆分策略有三种：“一对一”（OvO）、“一对其余”（OvR）和“多对多”（MvM），核心思想与示意图如下所示。</p><ul><li><p>OvO：给定数据集D，假定其中有N个真实类别，将这N个类别进行两两配对（一个正类/一个反类），从而产生N（N-1）/2个二分类学习器，在测试阶段，将新样本放入所有的二分类学习器中测试，得出N（N-1）个结果，最终通过投票产生最终的分类结果。</p></li><li><p>OvM：给定数据集D，假定其中有N个真实类别，每次取出一个类作为正类，剩余的所有类别作为一个新的反类，从而产生N个二分类学习器，在测试阶段，得出N个结果，若仅有一个学习器预测为正类，则对应的类标作为最终分类结果。</p></li><li><p>MvM：给定数据集D，假定其中有N个真实类别，每次取若干个类作为正类，若干个类作为反类（通过ECOC码给出，编码），若进行了M次划分，则生成了M个二分类学习器，在测试阶段（解码），得出M个结果组成一个新的码，最终通过计算海明/欧式距离选择距离最小的类别作为最终分类结果。</p></li></ul><p><img src="https://i.loli.net/2018/10/17/5bc723b862bfb.png" alt="19.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc723b8300d5.png" alt="20.png"></p><h3 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h3><p>类别不平衡（class-imbanlance）就是指分类问题中不同类别的训练样本相差悬殊的情况，例如正例有900个，而反例只有100个，这个时候我们就需要进行相应的处理来平衡这个问题。常见的做法有三种：</p><ol><li>在训练样本较多的类别中进行“欠采样”（undersampling）,比如从正例中采出100个，常见的算法有：EasyEnsemble。</li><li>在训练样本较少的类别中进行“过采样”（oversampling）,例如通过对反例中的数据进行插值，来产生额外的反例，常见的算法有SMOTE。</li><li>直接基于原数据集进行学习，对预测值进行“再缩放”处理。其中再缩放也是代价敏感学习的基础。<img src="https://i.loli.net/2018/10/17/5bc726fe87ae2.png" alt="21.png"></li></ol><h3 id="LR应用经验"><a href="#LR应用经验" class="headerlink" title="LR应用经验"></a>LR应用经验</h3><p>LR实现简单高效易解释，计算速度快，易并行，在大规模数据情况下非常适用，更适合于应对数值型和标称型数据，主要适合解决线性可分的问题，但容易欠拟合，大多数情况下需要手动进行特征工程，构建组合特征，分类精度不高。</p><p>LR直接对分类可能性进行建模，无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题<br>LR能以概率的形式输出，而非知识0，1判定，对许多利用概率辅助决策的任务很有用<br>对率函数任意阶可导，具有很好的数学性质，许多现有的数值优化算法都可以用来求最优解，训练速度快<br>适用情景：LR是很多分类算法的基础组件，它的好处是输出值自然地落在0到1之间，并且有概率意义。因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。虽然效果一般，却胜在模型清晰，背后的概率学经得住推敲。它拟合出来的参数就代表了每一个特征(feature)对结果的影响。也是一个理解数据的好工具。</p><p>应用上： </p><ul><li>CTR预估，推荐系统的learning to rank，各种分类场景 </li><li>某搜索引擎厂的广告CTR预估基线版是LR </li><li>某电商搜索排序基线版是LR </li><li>某新闻app排序基线版是LR</li></ul><p>大规模工业实时数据，需要可解释性的金融数据，需要快速部署低耗时数据<br>LR就是简单，可解释，速度快，消耗资源少，分布式性能好</p><p>ADMM-LR:用ADMM求解LogisticRegression的优化方法称作ADMM_LR。ADMM算法是一种求解约束问题的最优化方法，它适用广泛。相比于SGD，ADMM在精度要求不高的情况下，在少数迭代轮数时就达到一个合理的精度，但是收敛到很精确的解则需要很多次迭代。</p><p><img src="/blog_picture/line10.jpg" alt="avatar">   </p><p><img src="/blog_picture/line11.jpg" alt="avatar">   </p><p><img src="/blog_picture/line12.jpg" alt="avatar">   </p><h2 id="LR多分类推广-Softmax回归"><a href="#LR多分类推广-Softmax回归" class="headerlink" title="LR多分类推广 - Softmax回归"></a>LR多分类推广 - Softmax回归</h2><p>LR是一个传统的二分类模型，它也可以用于多分类任务，其基本思想是：将多分类任务拆分成若干个二分类任务，然后对每个二分类任务训练一个模型，最后将多个模型的结果进行集成以获得最终的分类结果。一般来说，可以采取的拆分策略有：</p><h3 id="one-vs-one策略"><a href="#one-vs-one策略" class="headerlink" title="one vs one策略"></a>one vs one策略</h3><p>　　假设我们有N个类别，该策略基本思想就是不同类别两两之间训练一个分类器，这时我们一共会训练出<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171104175530607-1392543504.png" alt="img">种不同的分类器。在预测时，我们将样本提交给所有的分类器，一共会获得N(N-1)个结果，最终结果通过<strong>投票</strong>产生。</p><h3 id="one-vs-all策略"><a href="#one-vs-all策略" class="headerlink" title="one vs all策略"></a>one vs all策略</h3><p>　　该策略基本思想就是将第i种类型的所有样本作为正例，将剩下的所有样本作为负例，进行训练得到一个分类器。这样我们就一共可以得到N个分类器。在预测时，我们将样本提交给所有的分类器，一共会获得N个结果，我们<strong>选择其中概率值最大</strong>的那个作为最终分类结果。 <img src="https://images2017.cnblogs.com/blog/1251096/201710/1251096-20171021171313943-1199609768.png" alt="img"></p><h2 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h2><p>　　softmax是LR在多分类的推广。与LR一样，同属于广义线性模型。什么是Softmax函数？假设我们有一个数组A，<img src="https://images2017.cnblogs.com/blog/1251096/201710/1251096-20171021164616881-992414484.png" alt="img">表示的是数组A中的第i个元素，那么这个元素的Softmax值就是</p><p>　　　　　　　　　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201710/1251096-20171021165228865-866731732.png" alt="img"></p><p>也就是说，是该元素的指数，与所有元素指数和的比值。那么 softmax回归模型的假设函数又是怎么样的呢？</p><p>　　　　　　　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105100956795-1587348606.png" alt="img"></p><p>由上式很明显可以得出，假设函数的分母其实就是对概率分布进行了归一化，使得所有类别的概率之和为1；也可以看出LR其实就是K=2时的Softmax。在参数获得上，我们可以采用one vs all策略获得K个不同的训练数据集进行训练，进而针对每一类别都会得到一组参数向量<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102153701-629755133.png" alt="img">。当测试样本特征向量<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102416560-1451219507.png" alt="img">输入时，我们先用假设函数针对每一个类别<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102622045-1005416234.png" alt="img">估算出概率值<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102706623-369597312.png" alt="img">。因此我们的假设函数将要输出一个K维的向量（向量元素和为1）来表示K个类别的估计概率，我们选择其中得分最大的类别作为该输入的预测类别。Softmax看起来和one vs all 的LR很像，它们最大的不同在与Softmax得到的K个类别的得分和为1，而one vs all的LR并不是。</p><h3 id="softmax的代价函数"><a href="#softmax的代价函数" class="headerlink" title="softmax的代价函数"></a>softmax的代价函数</h3><p>　　类似于LR，其似然函数我们采用对数似然，故：</p><p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105113747779-692061991.png" alt="img"></p><p>加入<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171104162333248-539020480.png" alt="img">正则项的损失函数为：</p><p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105114132232-517057992.png" alt="img"></p><p>此处的<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105110553560-1026190635.png" alt="img">为符号函数。对于其参数的求解过程，我们依然采用梯度下降法。</p><h3 id="softmax的梯度的求解"><a href="#softmax的梯度的求解" class="headerlink" title="softmax的梯度的求解"></a>softmax的梯度的求解</h3><p>　　正则化项的求导很简单，就等于<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105120226607-495282914.png" alt="img">，下面我们主要讨论没有加正则项的损失函数的梯度求解，即</p><p>　　　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105113747779-692061991.png" alt="img"></p><p>的导数（梯度）。为了使得求解过程看起来简便、易于理解，我们仅仅只对于一个样本（x,y）情况（SGD）进行讨论，</p><p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105161912482-1607069737.png" alt="img"></p><p>此时，我们令</p><p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105162625888-1575402902.png" alt="img"></p><p>可以得到</p><p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105163457810-492161690.png" alt="img"></p><p>故：</p><p><img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105170233232-810575386.png" alt="img"></p><p>所以，正则化之后的损失函数的梯度为</p><p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105171748341-1281292385.png" alt="img"></p><p>然后通过梯度下降法最小化 <img src="http://ufldl.stanford.edu/wiki/images/math/c/e/0/ce027336c1cb3c0cd461406c81369ebf.png" alt="\textstyle J(\theta)">，我们就能实现一个可用的 softmax 回归模型了。</p><h3 id="多分类LR与Softmax回归"><a href="#多分类LR与Softmax回归" class="headerlink" title="多分类LR与Softmax回归"></a>多分类LR与Softmax回归</h3><p>　　有了多分类的处理方法，那么我们什么时候该用多分类LR？什么时候要用softmax呢？</p><p>总的来说，若待分类的<strong>类别互斥</strong>，我们就使用Softmax方法；若待分类的<strong>类别有相交</strong>，我们则要选用多分类LR，然后投票表决。</p><h2 id="Softmax分类器"><a href="#Softmax分类器" class="headerlink" title="Softmax分类器"></a>Softmax分类器</h2><p>SVM是最常用的两个分类器之一，而另一个就是<strong>Softmax分类器，</strong>它的损失函数与SVM的损失函数不同。对于学习过二元逻辑回归分类器的读者来说，Softmax分类器就可以理解为逻辑回归分类器面对多个分类的一般化归纳。SVM将输出<img src="https://www.zhihu.com/equation?tex=f%28x_i%2CW%29" alt="[公式]">作为每个分类的评分（因为无定标，所以难以直接解释）。与SVM不同，Softmax的输出（归一化的分类概率）更加直观，并且从概率上可以解释，这一点后文会讨论。在Softmax分类器中，函数映射<img src="https://www.zhihu.com/equation?tex=f%28x_i%3BW%29%3DWx_i" alt="[公式]">保持不变，但将这些评分值视为每个分类的未归一化的对数概率，并且将<em>折叶损失（hinge loss）</em>替换为<strong>交叉熵损失</strong>（<strong>cross-entropy loss）</strong>。公式如下：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+Li%3D-log%28%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%29" alt="[公式]"> 或等价的 <img src="https://www.zhihu.com/equation?tex=L_i%3D-f_%7By_i%7D%2Blog%28%5Csum_je%5E%7Bf_j%7D%29" alt="[公式]"></p><p>在上式中，使用<img src="https://www.zhihu.com/equation?tex=f_j" alt="[公式]">来表示分类评分向量<img src="https://www.zhihu.com/equation?tex=f" alt="[公式]">中的第j个元素。和之前一样，整个数据集的损失值是数据集中所有样本数据的损失值<img src="https://www.zhihu.com/equation?tex=L_i" alt="[公式]">的均值与正则化损失<img src="https://www.zhihu.com/equation?tex=R%28W%29" alt="[公式]">之和。其中函数<img src="https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D" alt="[公式]">被称作<strong>softmax 函数</strong>：其输入值是一个向量，向量中元素为任意实数的评分值（<img src="https://www.zhihu.com/equation?tex=z" alt="[公式]">中的），函数对其进行压缩，输出一个向量，其中每个元素值在0到1之间，且所有元素之和为1。所以，包含softmax函数的完整交叉熵损失看起唬人，实际上还是比较容易理解的。</p><p><strong>信息理论视角</strong>：在“真实”分布<img src="https://www.zhihu.com/equation?tex=p" alt="[公式]">和估计分布<img src="https://www.zhihu.com/equation?tex=q" alt="[公式]">之间的<em>交叉熵</em>定义如下：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29" alt="[公式]"></p><p><strong>*译者注</strong>：Kullback-Leibler差异（Kullback-Leibler Divergence）也叫做相对熵（Relative Entropy），它衡量的是相同事件空间里的两个概率分布的差异情况。*</p><p><strong>概率论解释</strong>：先看下面的公式：</p><p><img src="https://www.zhihu.com/equation?tex=P%28y_i%7Cx_i%2CW%29%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D" alt="[公式]"></p><p><strong>实操事项：数值稳定。</strong>编程实现softmax函数计算的时候，中间项<img src="https://www.zhihu.com/equation?tex=e%5E%7Bf_%7By_i%7D%7D" alt="[公式]">和<img src="https://www.zhihu.com/equation?tex=%5Csum_j+e%5E%7Bf_j%7D" alt="[公式]">因为存在指数函数，所以数值可能非常大。除以大数值可能导致数值计算的不稳定，所以学会使用归一化技巧非常重要。如果在分式的分子和分母都乘以一个常数<img src="https://www.zhihu.com/equation?tex=C" alt="[公式]">，并把它变换到求和之中，就能得到一个从数学上等价的公式：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7BCe%5E%7Bf_%7By_i%7D%7D%7D%7BC%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%2BlogC%7D%7D%7B%5Csum_je%5E%7Bf_j%2BlogC%7D%7D" alt="[公式]"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">f = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>]) <span class="comment"># 例子中有3个分类，每个评分的数值都很大</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 不妙：数值问题，可能导致数值爆炸</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 那么将f中的值平移到最大值为0：</span></span><br><span class="line">f -= np.max(f) <span class="comment"># f becomes [-666, -333, 0]</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 现在OK了，将给出正确结果</span></span><br></pre></td></tr></table></figure><p><strong>让人迷惑的命名规则</strong>：精确地说，SVM分类器使用的是<em>折叶损失（hinge loss）</em>，有时候又被称为<em>最大边界损失（max-margin loss）</em>。Softmax分类器使用的是<em>交叉熵损失（corss-entropy loss）</em>。Softmax分类器的命名是从<em>softmax函数</em>那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。注意从技术上说“softmax损失（softmax loss）”是没有意义的，因为softmax只是一个压缩数值的函数。但是在这个说法常常被用来做简称。</p><h2 id="SVM和Softmax的比较"><a href="#SVM和Softmax的比较" class="headerlink" title="SVM和Softmax的比较"></a>SVM和Softmax的比较</h2><p>下图有助于区分这 Softmax和SVM这两种分类器：</p><p>————————————————————————————————————————</p><p><img src="https://pic1.zhimg.com/80/a90ce9e0ff533f3efee4747305382064_hd.png" alt="img"></p><p>针对一个数据点，SVM和Softmax分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量<strong>f</strong>（本节中是通过矩阵乘来实现）。不同之处在于对<strong>f</strong>中分值的解释：SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别2）的分值比其他分类的分值高出至少一个边界值。Softmax分类器将这些数值看做是每个分类没有归一化的<strong>对数概率</strong>，鼓励正确分类的归一化的对数概率变高，其余的变低。SVM的最终的损失值是1.58，Softmax的最终的损失值是0.452，但要注意这两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。</p><p>————————————————————————————————————————</p><p><strong>Softmax分类器为每个分类提供了“可能性”</strong>：SVM的计算是无标定的，而且难以针对所有分类的评分值给出直观解释。Softmax分类器则不同，它允许我们计算出对于所有分类标签的可能性。举个例子，针对给出的图像，SVM分类器可能给你的是一个[12.5, 0.6, -23.0]对应分类“猫”，“狗”，“船”。而softmax分类器可以计算出这三个标签的”可能性“是[0.9, 0.09, 0.01]，这就让你能看出对于不同分类准确性的把握。为什么我们要在”可能性“上面打引号呢？这是因为可能性分布的集中或离散程度是由正则化参数λ直接决定的，λ是你能直接控制的一个输入参数。举个例子，假设3个分类的原始分数是[1, -2, 0]，那么softmax函数就会计算：</p><p><img src="https://www.zhihu.com/equation?tex=%5B1%2C-2%2C0%5D%5Cto%5Be%5E1%2Ce%5E%7B-2%7D%2Ce%5E0%5D%3D%5B2.71%2C0.14%2C1%5D%5Cto%5B0.7%2C0.04%2C0.26%5D" alt="[公式]"></p><p>现在，如果正则化参数λ更大，那么权重W就会被惩罚的更多，然后他的权重数值就会更小。这样算出来的分数也会更小，假设小了一半吧[0.5, -1, 0]，那么softmax函数的计算就是：</p><p><img src="https://www.zhihu.com/equation?tex=%5B0.5%2C-1%2C0%5D%5Cto%5Be%5E%7B0.5%7D%2Ce%5E%7B-1%7D%2Ce%5E0%5D%3D%5B1.65%2C0.73%2C1%5D%5Cto%5B0.55%2C0.12%2C0.33%5D" alt="[公式]"></p><p>现在看起来，概率的分布就更加分散了。还有，随着正则化参数λ不断增强，权重数值会越来越小，最后输出的概率会接近于均匀分布。这就是说，softmax分类器算出来的概率最好是看成一种对于分类正确性的自信。和SVM一样，数字间相互比较得出的大小顺序是可以解释的，但其绝对值则难以直观解释<strong>。</strong></p><p><strong>在实际使用中，SVM和Softmax经常是相似的</strong>：通常说来，两种分类器的表现差别很小，不同的人对于哪个分类器更好有不同的看法。相对于Softmax分类器，SVM更加“局部目标化（local objective）”，这既可以看做是一个特性，也可以看做是一个劣势。考虑一个评分是[10, -2, 3]的数据，其中第一个分类是正确的。那么一个SVM（<img src="https://www.zhihu.com/equation?tex=%5CDelta+%3D1" alt="[公式]">）会看到正确分类相较于不正确分类，已经得到了比边界值还要高的分数，它就会认为损失值是0。SVM对于数字个体的细节是不关心的：如果分数是[10, -100, -100]或者[10, 9, 9]，对于SVM来说没设么不同，只要满足超过边界值等于1，那么损失值就等于0。</p><p>对于softmax分类器，情况则不同。对于[10, 9, 9]来说，计算出的损失值就远远高于[10, -100, -100]的。换句话来说，softmax分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。但是，SVM只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。这可以被看做是SVM的一种特性。举例说来，一个汽车的分类器应该把他的大量精力放在如何分辨小轿车和大卡车上，而不应该纠结于如何与青蛙进行区分，因为区分青蛙得到的评分已经足够低了。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> LR </tag>
            
            <tag> softmax </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习基本概念</title>
      <link href="/2019/08/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
      <url>/2019/08/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<h2 id="机器学习基本概念"><a href="#机器学习基本概念" class="headerlink" title="机器学习基本概念"></a>机器学习基本概念</h2><p><strong>1  机器学习</strong></p><p><strong>1.1 机器学习的定义</strong></p><p>正如我们根据过去的经验来判断明天的天气，吃货们希望从购买经验中挑选一个好瓜，那能不能让计算机帮助人类来实现这个呢？机器学习正是这样的一门学科，人的“经验”对应计算机中的“数据”，让计算机来学习这些经验数据，生成一个算法模型，在面对新的情况中，计算机便能作出有效的判断，这便是机器学习。</p><p>另一本经典教材的作者Mitchell给出了一个形式化的定义，假设：</p><ul><li>P：计算机程序在某任务类T上的性能。</li><li>T：计算机程序希望实现的任务类。</li><li>E：表示经验，即历史的数据集。</li></ul><p>若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习。</p><p><strong>1.2 机器学习的一些基本术语</strong><br><img src="../img/ml_concepts.png" alt><br>假设我们收集了一批西瓜的数据，例如：（色泽=青绿;根蒂=蜷缩;敲声=浊响)， (色泽=乌黑;根蒂=稍蜷;敲声=沉闷)， (色泽=浅自;根蒂=硬挺;敲声=清脆)……每对括号内是一个西瓜的记录，定义：     </p><ul><li><p>所有记录的集合为：数据集。</p></li><li><p>每一条记录为：一个实例（instance）或样本（sample）。</p></li><li><p>例如：色泽或敲声，单个的特点为特征（feature）或属性（attribute）。</p></li><li><p>对于一条记录，如果在坐标轴上表示，每个西瓜都可以用坐标轴中的一个点表示，一个点也是一个向量，例如（青绿，蜷缩，浊响），即每个西瓜为：一个特征向量（feature vector）。</p></li><li><p>一个样本的特征数为：维数（dimensionality），该西瓜的例子维数为3，当维数非常大时，也就是现在说的“维数灾难”。</p><p> 计算机程序学习经验数据生成算法模型的过程中，每一条记录称为一个“训练样本”，同时在训练好模型后，我们希望使用新的样本来测试模型的效果，则每一个新的样本称为一个“测试样本”。定义：    </p></li><li><p>所有训练样本的集合为：训练集（trainning set），[特殊]。</p></li><li><p>所有测试样本的集合为：测试集（test set），[一般]。  </p></li><li><p>机器学习出来的模型适用于新样本的能力为：泛化能力（generalization），即从特殊到一般。</p><p> 西瓜的例子中，我们是想计算机通过学习西瓜的特征数据，训练出一个决策模型，来判断一个新的西瓜是否是好瓜。可以得知我们预测的是：西瓜是好是坏，即好瓜与差瓜两种，是离散值。同样地，也有通过历年的人口数据，来预测未来的人口数量，人口数量则是连续值。定义：    </p></li><li><p>预测值为离散值的问题为：分类（classification）。</p></li><li><p>预测值为连续值的问题为：回归（regression）。</p><p> 我们预测西瓜是否是好瓜的过程中，很明显对于训练集中的西瓜，我们事先已经知道了该瓜是否是好瓜，学习器通过学习这些好瓜或差瓜的特征，从而总结出规律，即训练集中的西瓜我们都做了标记，称为标记信息。但也有没有标记信息的情形，例如：我们想将一堆西瓜根据特征分成两个小堆，使得某一堆的西瓜尽可能相似，即都是好瓜或差瓜，对于这种问题，我们事先并不知道西瓜的好坏，样本没有标记信息。定义：    </p></li><li><p>训练数据有标记信息的学习任务为：监督学习（supervised learning），容易知道上面所描述的分类和回归都是监督学习的范畴。</p></li><li><p>训练数据没有标记信息的学习任务为：无监督学习（unsupervised learning），常见的有聚类和关联规则。</p></li></ul><p><strong>2  模型的评估与选择</strong></p><p><strong>2.1 误差与过拟合</strong></p><p>我们将学习器对样本的实际预测结果与样本的真实值之间的差异成为：误差（error）。定义：    </p><ul><li>在训练集上的误差称为训练误差（training error）或经验误差（empirical error）。</li><li>在测试集上的误差称为测试误差（test error）。</li><li>学习器在所有新样本上的误差称为泛化误差（generalization error）。</li></ul><p>显然，我们希望得到的是在新样本上表现得很好的学习器，即泛化误差小的学习器。因此，我们应该让学习器尽可能地从训练集中学出普适性的“一般特征”，这样在遇到新样本时才能做出正确的判别。然而，当学习器把训练集学得“太好”的时候，即把一些训练样本的自身特点当做了普遍特征；同时也有学习能力不足的情况，即训练集的基本特征都没有学习出来。我们定义：</p><ul><li>学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了，称为：过拟合（overfitting）。</li><li>学习能太差，训练样本的一般性质尚未学好，称为：欠拟合（underfitting）。</li></ul><p>可以得知：在过拟合问题中，训练误差十分小，但测试误差教大；在欠拟合问题中，训练误差和测试误差都比较大。目前，欠拟合问题比较容易克服，例如增加迭代次数等，但过拟合问题还没有十分好的解决方案，过拟合是机器学习面临的关键障碍。</p><p><img src="https://i.loli.net/2018/10/17/5bc7181172996.png" alt></p><p><strong>2.2 评估方法</strong></p><p>在现实任务中，我们往往有多种算法可供选择，那么我们应该选择哪一个算法才是最适合的呢？如上所述，我们希望得到的是泛化误差小的学习器，理想的解决方案是对模型的泛化误差进行评估，然后选择泛化误差最小的那个学习器。但是，泛化误差指的是模型在所有新样本上的适用能力，我们无法直接获得泛化误差。</p><p>因此，通常我们采用一个“测试集”来测试学习器对新样本的判别能力，然后以“测试集”上的“测试误差”作为“泛化误差”的近似。显然：我们选取的测试集应尽可能与训练集互斥，下面用一个小故事来解释why：</p><p>假设老师出了10 道习题供同学们练习，考试时老师又用同样的这10道题作为试题，可能有的童鞋只会做这10 道题却能得高分，很明显：这个考试成绩并不能有效地反映出真实水平。回到我们的问题上来，我们希望得到泛化性能好的模型，好比希望同学们课程学得好并获得了对所学知识”举一反三”的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试。显然，若测试样本被用作训练了，则得到的将是过于”乐观”的估计结果。</p><p><strong>2.3 训练集与测试集的划分方法</strong></p><p>如上所述：我们希望用一个“测试集”的“测试误差”来作为“泛化误差”的近似，因此我们需要对初始数据集进行有效划分，划分出互斥的“训练集”和“测试集”。下面介绍几种常用的划分方法：</p><p><strong>2.3.1 留出法</strong></p><p>将数据集D划分为两个互斥的集合，一个作为训练集S，一个作为测试集T，满足D=S∪T且S∩T=∅，常见的划分为：大约2/3-4/5的样本用作训练，剩下的用作测试。需要注意的是：训练/测试集的划分要尽可能保持数据分布的一致性，以避免由于分布的差异引入额外的偏差，常见的做法是采取分层抽样。同时，由于划分的随机性，单次的留出法结果往往不够稳定，一般要采用若干次随机划分，重复实验取平均值的做法。</p><p><strong>2.3.2 交叉验证法</strong></p><p>将数据集D划分为k个大小相同的互斥子集，满足D=D1∪D2∪…∪Dk，Di∩Dj=∅（i≠j），同样地尽可能保持数据分布的一致性，即采用分层抽样的方法获得这些子集。交叉验证法的思想是：每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就有K种训练集/测试集划分的情况，从而可进行k次训练和测试，最终返回k次测试结果的均值。交叉验证法也称“k折交叉验证”，k最常用的取值是10，下图给出了10折交叉验证的示意图。</p><p><img src="https://i.loli.net/2018/10/17/5bc718115d224.png" alt></p><p>与留出法类似，将数据集D划分为K个子集的过程具有随机性，因此K折交叉验证通常也要重复p次，称为p次k折交叉验证，常见的是10次10折交叉验证，即进行了100次训练/测试。特殊地当划分的k个子集的每个子集中只有一个样本时，称为“留一法”，显然，留一法的评估结果比较准确，但对计算机的消耗也是巨大的。</p><p><strong>2.3.3 自助法</strong></p><p>我们希望评估的是用整个D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。“自助法”正是解决了这样的问题。</p><p>自助法的基本思想是：给定包含m个样本的数据集D，每次随机从D 中挑选一个样本，将其拷贝放入D’，然后再将该样本放回初始数据集D 中，使得该样本在下次采样时仍有可能被采到。重复执行m 次，就可以得到了包含m个样本的数据集D’。可以得知在m次采样中，样本始终不被采到的概率取极限为：</p><p><img src="https://i.loli.net/2018/10/17/5bc71811246dd.png" alt></p><p>这样，通过自助采样，初始样本集D中大约有36.8%的样本没有出现在D’中，于是可以将D’作为训练集，D-D’作为测试集。自助法在数据集较小，难以有效划分训练集/测试集时很有用，但由于自助法产生的数据集（随机抽样）改变了初始数据集的分布，因此引入了估计偏差。在初始数据集足够时，留出法和交叉验证法更加常用。</p><p><strong>2.4 调参</strong></p><p>大多数学习算法都有些参数(parameter) 需要设定，参数配置不同，学得模型的性能往往有显著差别，这就是通常所说的”参数调节”或简称”调参” (parameter tuning)。</p><p>学习算法的很多参数是在实数范围内取值，因此，对每种参数取值都训练出模型来是不可行的。常用的做法是：对每个参数选定一个范围和步长λ，这样使得学习的过程变得可行。例如：假定算法有3 个参数，每个参数仅考虑5 个候选值，这样对每一组训练/测试集就有5<em>5</em>5= 125 个模型需考察，由此可见：拿下一个参数（即经验值）对于算法人员来说是有多么的happy。</p><p>最后需要注意的是：当选定好模型和调参完成后，我们需要使用初始的数据集D重新训练模型，即让最初划分出来用于评估的测试集也被模型学习，增强模型的学习效果。用上面考试的例子来比喻：就像高中时大家每次考试完，要将考卷的题目消化掉（大多数题目都还是之前没有见过的吧？），这样即使考差了也能开心的玩耍了~。</p><p><strong>3  机器学习评估与度量指标</strong></p><p>这里的内容主要包括：性能度量、比较检验和偏差与方差。在上一个notebook中，我们解决了评估学习器泛化性能的方法，即用测试集的“测试误差”作为“泛化误差”的近似，当我们划分好训练/测试集后，那如何计算“测试误差”呢？这就是性能度量，例如：均方差，错误率等，即“测试误差”的一个评价标准。有了评估方法和性能度量，就可以计算出学习器的“测试误差”，但由于“测试误差”受到很多因素的影响，例如：算法随机性或测试集本身的选择，那如何对两个或多个学习器的性能度量结果做比较呢？这就是比较检验。最后偏差与方差是解释学习器泛化性能的一种重要工具。</p><p><strong>3.1 性能度量</strong></p><p>性能度量（performance measure）是衡量模型泛化能力的评价标准，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。本节除2.5.1外，其它主要介绍分类模型的性能度量。</p><p><strong>3.1.1 最常见的性能度量</strong></p><p>在回归任务中，即预测连续值的问题，最常用的性能度量是“均方误差”（mean squared error）,很多的经典算法都是采用了MSE作为评价函数，想必大家都十分熟悉。</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf76276.png" alt="1.png"></p><p>在分类任务中，即预测离散值的问题，最常用的是错误率和精度，错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例，易知：错误率+精度=1。</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf4c704.png" alt="2.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71daf6fb84.png" alt="3.png"></p><p><strong>3.1.2 查准率/查全率/F1</strong></p><p>错误率和精度虽然常用，但不能满足所有的需求，例如：在推荐系统中，我们只关心推送给用户的内容用户是否感兴趣（即查准率），或者说所有用户感兴趣的内容我们推送出来了多少（即查全率）。因此，使用查准/查全率更适合描述这类问题。对于二分类问题，分类结果混淆矩阵与查准/查全率定义如下：</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf885a4.png" alt="4.png"></p><p>初次接触时，FN与FP很难正确的理解，按照惯性思维容易把FN理解成：False-&gt;Negtive，即将错的预测为错的，这样FN和TN就反了，后来找到一张图，描述得很详细，为方便理解，把这张图也贴在了下边：</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf871a6.png" alt="5.png"></p><p>正如天下没有免费的午餐，查准率和查全率是一对矛盾的度量。例如我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容，这样就漏掉了一些用户感兴趣的内容，查全率就低了；如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上，宁可错杀一千，不可放过一个，这样查准率就很低了。</p><p>“P-R曲线”正是描述查准/查全率变化的曲线，P-R曲线定义如下：根据学习器的预测结果（一般为一个实值或概率）对测试样本进行排序，将最可能是“正例”的样本排在前面，最不可能是“正例”的排在后面，按此顺序逐个把样本作为“正例”进行预测，每次计算出当前的P值和R值，如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc71dafc4411.png" alt="6.png"></p><p>P-R曲线如何评估呢？若一个学习器A的P-R曲线被另一个学习器B的P-R曲线完全包住，则称：B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的，所以衍生出了“平衡点”（Break-Event Point，简称BEP），即当P=R时的取值，平衡点的取值越高，性能更优。</p><p>P和R指标有时会出现矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的加权调和平均，即：</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf40ff6.png" alt="7.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71daf75407.png" alt="8.png"></p><p>特别地，当β=1时，也就是常见的F1度量，是P和R的调和平均，当F1较高时，模型的性能越好。</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf20885.png" alt="9.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71daf4b90a.png" alt="10.png"></p><p>有时候我们会有多个二分类混淆矩阵，例如：多次训练或者在多个数据集上训练，那么估算全局性能的方法有两种，分为宏观和微观。简单理解，宏观就是先算出每个混淆矩阵的P值和R值，然后取得平均P值macro-P和平均R值macro-R，在算出Fβ或F1，而微观则是计算出混淆矩阵的平均TP、FP、TN、FN，接着进行计算P、R，进而求出Fβ或F1。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed70230e.png" alt="11.png"></p><p><strong>3.1.3 ROC与AUC</strong></p><p>如上所述：学习器对测试样本的评估结果一般为一个实值或概率，设定一个阈值，大于阈值为正例，小于阈值为负例，因此这个实值的好坏直接决定了学习器的泛化性能，若将这些实值排序，则排序的好坏决定了学习器的性能高低。ROC曲线正是从这个角度出发来研究学习器的泛化性能，ROC曲线与P-R曲线十分类似，都是按照排序的顺序逐一按照正例预测，不同的是ROC曲线以“真正例率”（True Positive Rate，简称TPR）为横轴，纵轴为“假正例率”（False Positive Rate，简称FPR），ROC偏重研究基于测试样本评估值的排序好坏。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6bee91.png" alt="12.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71ed75cefe.png" alt="13.png"></p><p>简单分析图像，可以得知：当FN=0时，TN也必须0，反之也成立，我们可以画一个队列，试着使用不同的截断点（即阈值）去分割队列，来分析曲线的形状，（0,0）表示将所有的样本预测为负例，（1,1）则表示将所有的样本预测为正例，（0,1）表示正例全部出现在负例之前的理想情况，（1,0）则表示负例全部出现在正例之前的最差情况。限于篇幅，这里不再论述。</p><p>现实中的任务通常都是有限个测试样本，因此只能绘制出近似ROC曲线。绘制方法：首先根据测试样本的评估值对测试样本排序，接着按照以下规则进行绘制。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed740a24.png" alt="14.png"></p><p>同样地，进行模型的性能比较时，若一个学习器A的ROC曲线被另一个学习器B的ROC曲线完全包住，则称B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。ROC曲线下的面积定义为AUC（Area Uder ROC Curve），不同于P-R的是，这里的AUC是可估算的，即AOC曲线下每一个小矩形的面积之和。易知：AUC越大，证明排序的质量越好，AUC为1时，证明所有正例排在了负例的前面，AUC为0时，所有的负例排在了正例的前面。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6e2c57.png" alt="15.png"></p><p><strong>3.1.4 代价敏感错误率与代价曲线</strong></p><p>上面的方法中，将学习器的犯错同等对待，但在现实生活中，将正例预测成假例与将假例预测成正例的代价常常是不一样的，例如：将无疾病–&gt;有疾病只是增多了检查，但有疾病–&gt;无疾病却是增加了生命危险。以二分类为例，由此引入了“代价矩阵”（cost matrix）。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6ed582.png" alt="16.png"></p><p>在非均等错误代价下，我们希望的是最小化“总体代价”，这样“代价敏感”的错误率（2.5.1节介绍）为：</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed70bebe.png" alt="17.png"></p><p>同样对于ROC曲线，在非均等错误代价下，演变成了“代价曲线”，代价曲线横轴是取值在[0,1]之间的正例概率代价，式中p表示正例的概率，纵轴是取值为[0,1]的归一化代价。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6e952e.png" alt="18.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6eee7b.png" alt="19.png"></p><p>代价曲线的绘制很简单：设ROC曲线上一点的坐标为(TPR，FPR) ，则可相应计算出FNR，然后在代价平面上绘制一条从(0，FPR) 到(1，FNR) 的线段，线段下的面积即表示了该条件下的期望总体代价；如此将ROC 曲线土的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价，如图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed716e0d.png" alt="20.png"></p><p><strong>4  机器学习指标ROC与AUC</strong></p><p>AUC是一种模型分类指标，且仅仅是二分类模型的评价指标。AUC是Area Under Curve的简称，那么Curve就是ROC（Receiver Operating Characteristic），翻译为”接受者操作特性曲线”。</p><h3 id="ROC"><a href="#ROC" class="headerlink" title="ROC"></a>ROC</h3><p>曲线由两个变量TPR和FPR组成，这个组合以FPR对TPR，即是以代价(costs)对收益(benefits)。</p><ul><li><p>x轴为假阳性率（FPR）：在所有的负样本中，分类器预测错误的比例</p><p>$$FPR = \frac {FP}{FP+TN}$$</p></li><li><p>y轴为真阳性率（TPR）：在所有的正样本中，分类器预测正确的比例（等于Recall）</p><p>$$TPR = \frac {TP}{TP+FN}$$</p></li></ul><p>为了更好地理解ROC曲线，我们使用具体的实例来说明：</p><p>如在医学诊断中,判断有病的样本。那么尽量把有病的揪出来是主要任务，也就是第一个指标TPR，要越高越好。而把没病的样本误诊为有病的，也就是第二个指标FPR，要越低越好。</p><p>不难发现,这两个指标之间是相互制约的。如果某个医生对于有病的症状比较敏感，稍微的小症状都判断为有病,那么他的第一个指标应该会很高，但是第二个指标也就相应地变高。最极端的情况下,他把所有的样本都看做有病,那么第一个指标达到1,第二个指标也为1。</p><p>我们以FPR为横轴,TPR为纵轴,得到如下ROC空间。</p><img src="/blog_picture/1.7.png" width="60%"><p>我们可以看出，左上角的点(TPR=1，FPR=0)，为完美分类，也就是这个医生医术高明，诊断全对。点A(TPR&gt;FPR),医生A的判断大体是正确的。中线上的点B(TPR=FPR),也就是医生B全都是蒙的，蒙对一半，蒙错一半；下半平面的点C(TPR&lt;FPR)，这个医生说你有病，那么你很可能没有病，医生C的话我们要反着听，为真庸医。上图中一个阈值，得到一个点。现在我们需要一个独立于阈值的评价指标来衡量这个医生的医术如何，也就是遍历所有的阈值,得到ROC曲线。</p><p>假设如下就是某个医生的诊断统计图，直线代表阈值。通过改变不同的阈值$1.0 \rightarrow 0$，从而绘制出ROC曲线。下图为未得病人群（蓝色）和得病人群（红色）的模型输出概率分布图（横坐标表示模型输出概率，纵坐标表示概率对应的人群的数量）。阈值为1时，不管你什么症状，医生均未诊断出疾病（预测值都为N），此时FPR=TPR=0，位于左下。阈值为0时，不管你什么症状，医生都诊断结果都是得病（预测值都为P），此时FPR=TPR=1，位于右上。</p><img src="/blog_picture/1.8.png" width="50%"><p>曲线距离左上角越近,证明分类器效果越好。</p><img src="/blog_picture/1.9.png" width="60%"><p>如上，是三条ROC曲线，在0.23处取一条直线。那么，在同样的低FPR=0.23的情况下，红色分类器得到更高的PTR。也就表明，ROC越往左上，分类器效果越好。我们用一个标量值AUC来量化它。</p><h3 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h3><p><strong>AUC定义：</strong></p><p>AUC值为ROC曲线所覆盖的区域面积，显然，AUC越大，分类器分类效果越好。</p><p>AUC = 1，是完美分类器。绝大多数预测的场合，不存在完美分类器。</p><p>0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。</p><p>AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。</p><p>AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。</p><p>注：对于AUC小于0.5的模型，我们可以考虑取反（模型预测为positive，那我们就取negtive），这样就可以保证模型的性能不可能比随机猜测差。</p><p>以下为ROC曲线和AUC值的实例：</p><img src="/blog_picture/1.12.png" width="70%"><p><strong>AUC的物理意义</strong></p><p>AUC的物理意义正样本的预测结果大于负样本的预测结果的概率。所以AUC反应的是分类器对样本的排序能力。  </p><p>另外值得注意的是，AUC对样本类别是否均衡并不敏感，这也是不均衡样本通常用AUC评价分类器性能的一个原因。</p><p>下面从一个小例子解释AUC的含义：小明一家四口，小明5岁，姐姐10岁，爸爸35岁，妈妈33岁建立一个逻辑回归分类器，来预测小明家人为成年人概率，假设分类器已经对小明的家人做过预测，得到每个人为成人的概率。</p><ol><li>AUC更多的是关注对计算概率的排序，关注的是概率值的相对大小，与阈值和概率值的绝对大小没有关系</li></ol><p>例子中并不关注小明是不是成人，而关注的是，预测为成人的概率的排序。</p><p><strong>问题⑪：</strong>以下为三种模型的输出结果，求三种模型的AUC。</p><table><thead><tr><th></th><th>小明</th><th>姐姐</th><th>妈妈</th><th>爸爸</th></tr></thead><tbody><tr><td>a</td><td>0.12</td><td>0.35</td><td>0.76</td><td>0.85</td></tr><tr><td>b</td><td>0.12</td><td>0.35</td><td>0.44</td><td>0.49</td></tr><tr><td>c</td><td>0.52</td><td>0.65</td><td>0.76</td><td>0.85</td></tr></tbody></table><p>AUC只与概率的相对大小（概率排序）有关，和绝对大小没关系。由于三个模型概率排序的前两位都是未成年人（小明，姐姐），后两位都是成年人（妈妈，爸爸），因此三个模型的AUC都等于。</p><ol><li><p>AUC只关注正负样本之间的排序，并不关心正样本内部，或者负样本内部的排序。这也体现了AUC的本质：任意个正样本的概率都大于负样本的概率的能力  </p><p>例子中AUC只需要保证（小明和姐姐）（爸爸和妈妈），小明和姐姐在前2个排序，爸爸和妈妈在后2个排序，而不会考虑小明和姐姐谁在前，或者爸爸和妈妈谁在前。</p><p><strong>问题⑫：</strong>以下已经对分类器输出概率从小到大进行了排列，哪些情况的AUC等于1， 情况的AUC为0（其中背景色表示True value，红色表示成年人，蓝色表示未成年人）。</p><img src="/blog_picture/1.10.png" width="70%"><p>D 模型, E模型和F模型的AUC值为1，C模型的AUC值为0（爸妈为成年人的概率小于小明和姐姐，显然这个模型预测反了）。</p></li></ol><p><strong>AUC的计算：</strong></p><ul><li><p>法1：AUC为ROC曲线下的面积，那我们直接计算面积可得。面积为一个个小的梯形面积（曲线）之和。计算的精度与阈值的精度有关。</p></li><li><p>法2：根据AUC的物理意义，我们计算正样本预测结果大于负样本预测结果的概率。取n1*n0(n1为正样本数，n0为负样本数)个二元组，比较score（预测结果），最后得到AUC。时间复杂度为O(N*M)。</p></li><li><p>法3：我们首先把所有样本按照score排序，依次用rank表示他们，如最大score的样本，rank=n (n=n0+n1，其中n0为负样本个数，n1为正样本个数)，其次为n-1。那么对于正样本中rank最大的样本，rank_max，有n1-1个其他正样本比他score小,那么就有(rank_max-1)-(n1-1)个负样本比他score小。其次为(rank_second-1)-(n1-2)。最后我们得到正样本大于负样本的概率为</p><p><img src="/blog_picture/auc.jpg" alt="avatar"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> ROC </tag>
            
            <tag> AUC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>简洁版机器学习速查表</title>
      <link href="/2019/08/23/%E7%AE%80%E6%B4%81%E7%89%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%9F%E6%9F%A5%E8%A1%A8/"/>
      <url>/2019/08/23/%E7%AE%80%E6%B4%81%E7%89%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%9F%E6%9F%A5%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p><img src="/blog_picture/ml_1.jpg" alt="avatar"><br><img src="/blog_picture/ml_2.jpg" alt="avatar"><br><img src="/blog_picture/ml_3.jpg" alt="avatar"><br><img src="/blog_picture/ml_4.jpg" alt="avatar"><br><img src="/blog_picture/ml_5.jpg" alt="avatar"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS229版机器学习速查表</title>
      <link href="/2019/08/23/CS229%E7%89%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%9F%E6%9F%A5%E8%A1%A8/"/>
      <url>/2019/08/23/CS229%E7%89%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%9F%E6%9F%A5%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p><img src="https://s1.ax1x.com/2020/04/24/JBZ8U0.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBZg2D.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBZHG8.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBZzaq.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBePRU.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBeAsJ.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBmBh6.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBm2Bd.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBmhNt.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBmo38.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBm7jg.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBmv40.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBniDJ.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBnAER.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBneC6.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBnYPP.jpg" alt="avatar"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>葫芦书学习笔记</title>
      <link href="/2019/08/08/%E8%91%AB%E8%8A%A6%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2019/08/08/%E8%91%AB%E8%8A%A6%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="1-为什么需要对数值类型的特征做归一化处理？"><a href="#1-为什么需要对数值类型的特征做归一化处理？" class="headerlink" title="1.为什么需要对数值类型的特征做归一化处理？"></a>1.为什么需要对数值类型的特征做归一化处理？</h1><ul><li>为了方便后续进行梯度下降的时候加速收敛</li><li>归一化通常主要分为两种：min-max(线性函数归一化)，Z-Score(零均值归一化)</li><li>需要进行归一化的模型：线性回归，LR，SVM，神经网络等</li><li>决策树模型不适用归一化处理，因为决策树在进行节点分裂时主要依据数据集D关于特征x的信息增益比，而信息增益比和特征是否进行归一化无关，因为归一化并不改变样本在特征x上的信息增益。</li></ul><h1 id="2-􏳉􏴔􏳓在数据进行预处理时，应该怎样处理类别型特征？"><a href="#2-􏳉􏴔􏳓在数据进行预处理时，应该怎样处理类别型特征？" class="headerlink" title="2.􏳉􏴔􏳓在数据进行预处理时，应该怎样处理类别型特征？"></a>2.􏳉􏴔􏳓在数据进行预处理时，应该怎样处理类别型特征？</h1><ul><li>类别型特征原始输入形式通常是字符串形式、除了决策树等少量模型能直接处理字符串形式的输入，对于LR、SVM等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作</li><li>序号编码(ordinal)：类别间具有大小关系</li><li>独热编码(one-hot)：类别间没有大小关系，特征的每个值作为一列。维度过高可能导致维度灾难，产生过拟合问题。</li><li>二进制编码：先用序号编码给每个类别赋予一个类别ID，然后将ID转为二进制编码作为结果。</li></ul><h1 id="3-什么是组合特征？如何处理高维组合特征-解决高维组合特征维度过高的问题-？"><a href="#3-什么是组合特征？如何处理高维组合特征-解决高维组合特征维度过高的问题-？" class="headerlink" title="3.什么是组合特征？如何处理高维组合特征(解决高维组合特征维度过高的问题)？"></a>3.什么是组合特征？如何处理高维组合特征(解决高维组合特征维度过高的问题)？</h1><ul><li>可以将M * N矩阵分解为M * K和K * N两个矩阵相乘的形式，这样参数从M * N降到 K * (M+N)</li><li>为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。</li></ul><p>#4.怎样有效的找到组合特征？</p><ul><li>基于决策树的特征组合寻找。（原始输入构建决策树可以采用梯度提升决策树即每次都在之前构建的决策树的残差上构建下一课决策树）</li></ul><h1 id="5-有哪些文本模型？他们各有什么优缺点？"><a href="#5-有哪些文本模型？他们各有什么优缺点？" class="headerlink" title="5.有哪些文本模型？他们各有什么优缺点？"></a>5.有哪些文本模型？他们各有什么优缺点？</h1><ul><li>词袋模型(bag of words)：最基础的文本表示模型是词袋模型。顾名思义，就是将每篇文章看成一袋子词，并忽略每个词出现的顺序。将整段文本以词为单位切分开，然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对应得权重则则反映了这个词在原文章的重要程度。但是词袋忽略了由几个词组成一个意思这种情况（“如NBA吐槽大会”这种，分解成了NBA和吐槽大会，结果匹配了很多李诞这样和NBA完全不相关的物料）</li><li>N-gram模型：词袋模型的改进，N-gram将连续出现的N个词组成的词组也作为一维放到向量表示中去。但是N-gram不能识别两个不同的词有相同的主题</li><li>TF-TDF：TF-IDF(t,d) = TF(t,d)*IDF(t)其中，TF(t,d)为单词t在文档d中出现的频率，IDF(t) = log(文章总数/(包含单词t的文章总数+1)) ，IDF公式可理解为如果一个词出现的文章数越多那么说明它越是一个通用词，通用词对文档内容贡献度比较小</li><li>主题模型：主题模型用于从文本库发现有代表性的主题（得到每个主题上面词的分布特性），并且能够计算出每篇文章的主题分布。</li><li>词嵌入与深度学习模型：词嵌入是一类将词向量化的模型的统称，核心思想是将每个词都映射成低维空间（通常50-300维)上的一个稠密向量。K维空间中的每一维都可以看作是一个隐含的主题，只不过不像主题模型中的主题那么直观。由于词嵌入将每个词映射成一个K维的向量，如果一篇文章有N个词，就可以用一个N*K维的矩阵来表示这篇文档，但是这样表示过去底层。在实际应用中，如果仅仅把这个矩阵作为源文本的表示特征输入到机器学习模型中，通常很难得到满意的结果。因此，还需要在此基础上加工出更高层的特征。在传统的浅层机器学习模型中，一个好的特征工程往往可以带来算法效果的显著提示。深度学习模型正好为我们提供了一种自动 的进行特征工程的方式，模型中的每个隐层都可以认为对应着不同抽象层次的特征。卷积神经网络和循环神经网络的结构在文本表示中取得很好的效果，主要是由于他们能够更好的对文本进行建模，抽取出更高层的语义特征。。与全链接网络结构相比，卷积神经网络和RNN一方面很好的抓住了文本的特征，另一方面又减少了网络学习中待学习的参数，提高了训练速度，并且降低了过拟合的风险。</li></ul><h1 id="6-Word2Vec是如何工作的？它和LDA有什么区别与联系？"><a href="#6-Word2Vec是如何工作的？它和LDA有什么区别与联系？" class="headerlink" title="6.Word2Vec是如何工作的？它和LDA有什么区别与联系？"></a>6.Word2Vec是如何工作的？它和LDA有什么区别与联系？</h1><ul><li>word2vec实际上一种浅层的神经网络模型，它有两种网络结构，分别是CBOW(continues bag of words)和Skip-gram</li><li>CBOW的目标是根据上下文出现的词语来预测当前词的生成概率；skip-gram是根据当前词来预测上下文中各词的生成概率。</li><li>word2vec是google开发的一种词向量嵌入的模型，主要分为CBOW和skip-gram两种，最后得到得词向量是dense vector。</li><li>LDA是一种生成模型，最后可以得到文档与主题，主题与词之间的概率分布。</li></ul><h1 id="7-在图像分类任务中，训练数据不足会带来什么问题？如何缓解数据量不足带来的问题？"><a href="#7-在图像分类任务中，训练数据不足会带来什么问题？如何缓解数据量不足带来的问题？" class="headerlink" title="7.在图像分类任务中，训练数据不足会带来什么问题？如何缓解数据量不足带来的问题？"></a>7.在图像分类任务中，训练数据不足会带来什么问题？如何缓解数据量不足带来的问题？</h1><ul><li>训练数据不足主要表现在过拟合方面。</li><li>两类处理方法：一是基于模型的方法，主要是采用降低过拟合风险的措施包括简化模型(非线性简化为线性)、添加约束项以缩小假设空间、集成学习、Dropout超参数等。二是基于数据的的方法，主要是通过数据扩充</li></ul><h1 id="8-准确率的局限性"><a href="#8-准确率的局限性" class="headerlink" title="8.准确率的局限性"></a>8.准确率的局限性</h1><ul><li>不同类别的样本比例非常不均匀时，占比大的类别往往成为影响准确率的最主要因素</li></ul><h1 id="9-精确率与召回率的权衡"><a href="#9-精确率与召回率的权衡" class="headerlink" title="9.精确率与召回率的权衡"></a>9.精确率与召回率的权衡</h1><ul><li>只用某个点对应的精确率和召回率不能全面地衡量模型的性能，只有通过P-R曲线的整体表现，才能够对模型进行更为全面的评估</li></ul><h1 id="10-平方根误差的意外"><a href="#10-平方根误差的意外" class="headerlink" title="10.平方根误差的意外"></a>10.平方根误差的意外</h1><ul><li>一般情况下，RMSE能够很好的反映回归模型预测值与真实值的偏离程度。但在实际问题中，如果存在个别偏离程度非常大的离群点时，即使离群点数量非常少，也会让RMSE指标变得很差。</li><li>解决方法：一，在数据预处理时过滤这些噪声点。二，如果不认为这些离群点是噪声的话就要进一步提高模型的预测能力，将离群点产生的机制建模进去。三，找一个更合适的指标来评估该模型。</li></ul><h1 id="11-什么是ROC曲线"><a href="#11-什么是ROC曲线" class="headerlink" title="11.什么是ROC曲线"></a>11.什么是ROC曲线</h1><ul><li>ROC􏵻􏵲􏰋曲线是Receiver Operating Characteristic Curve􏰊􏵱􏲒􏰢􏱟的简称，中文名为“受试者工作特征曲线”。ROC曲线的横坐标为假阳性率FPR；纵坐标为真阳性率TPR。</li></ul><p>#12.如果绘制ROC􏵻􏵲􏰋曲线</p><ul><li>ROC􏵻􏵲􏰋曲线是通过不断移动分类器的“截断点”来生成曲线上一组关键点的。</li><li>首先根据样本标签统计出正负样本的数量，假设正样本数量为p，负样本数量为n；接下来，把横轴的刻度间隔设置为1/n,纵轴的刻度间隔设置为1/p；再根据模型输出的预测概率对样本进行排序依次遍历样本，同时从零点开始绘制ROC􏵻􏵲􏰋曲线，每遇到一个正样本就沿纵轴方向绘制一个刻度间隔的曲线，每遇到一个负样本就沿着横轴方向绘制一个刻度间隔的曲线，直到遍历完所有样本，曲线最终停在（1，1）这个点，整个ROC曲线绘制完成。</li></ul><h1 id="13-如何计算AUC"><a href="#13-如何计算AUC" class="headerlink" title="13.如何计算AUC"></a>13.如何计算AUC</h1><ul><li>沿着ROC横轴做积分。</li></ul><h1 id="14-ROC曲线相比P-R曲线有什么特点"><a href="#14-ROC曲线相比P-R曲线有什么特点" class="headerlink" title="14.ROC曲线相比P-R曲线有什么特点"></a>14.ROC曲线相比P-R曲线有什么特点</h1><ul><li>当正负样本的分布发生变化时，ROC曲线的形状能够基本保持不变，而P-R曲线的形状一般会发生较剧烈变化。</li><li>ROC曲线能够尽量降低不同测试集带来的干扰，更加客观地衡量模型本身的性能。ROC曲线的适用范围更广，适用于排序、推荐、广告。选择ROC曲线还是P-R曲线因实际问题而异，如果希望更多的看到模型在特定数据集上的表现，P-R曲线能够更直观地反映其性能。</li></ul><h1 id="15-结合你的学习和研究经历，探讨为什么在一些场景中要使用余弦相似度而不是欧氏距离"><a href="#15-结合你的学习和研究经历，探讨为什么在一些场景中要使用余弦相似度而不是欧氏距离" class="headerlink" title="15.结合你的学习和研究经历，探讨为什么在一些场景中要使用余弦相似度而不是欧氏距离"></a>15.结合你的学习和研究经历，探讨为什么在一些场景中要使用余弦相似度而不是欧氏距离</h1><ul><li>当一对文本相似度的长度差距很大、但内容相近时，如果采用词频或词向量作为特征，它们在特征空间中的欧式距离通常很大；而余弦相似度，它们之间的夹角可能很小，因而相似度更高。此外，在文本、图像、视频等领域，研究的对象的特征维度往往很高，余弦相似度在高维情况下依然保持“ 相同为1，正交是为0，相反时为-1”的性质，而欧式距离的数值则受维度的影响，范围不固定，并且含义也比较模糊。</li><li>在一些场景中，例如Word2Vec中，其向量的模长是经过归一化的，此时欧式距离与余弦距离有着单调的关系。此场景下余弦相似度和欧式距离的结果是相同的</li><li>欧式距离体现数值上的绝对差异，余弦距离体现方向上的相对差异。分析两个不同用户对于不同视频的偏好，更关注相对差异，显然应当用余弦距离。分析用户活跃度，以登陆次数和平均观看时长作为特征，余弦距离为认为(1,10)􏱤(10,100)两个用户距离很近；但显然这两个用户活跃度是有着极大的差异的，此时更关注数值绝对差异，应当使用欧式距离。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 面试 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 面试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数学基础知识整理</title>
      <link href="/2019/08/06/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/"/>
      <url>/2019/08/06/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>##线性代数</p><h4 id="线性相关与线性无关"><a href="#线性相关与线性无关" class="headerlink" title="线性相关与线性无关"></a>线性相关与线性无关</h4><ul><li><img src="/blog_picture/linear.jpg" alt="avatar"></li></ul><p>线性相关的判定：根据观察，利用定义即可判断。</p><p>线性相关的判断定理：</p><ul><li><p><img src="/blog_picture/2.jpg" alt="avatar"></p></li><li><p><img src="/blog_picture/3.jpg" alt="avatar"></p></li></ul><h4 id="矩阵的秩"><a href="#矩阵的秩" class="headerlink" title="矩阵的秩"></a>矩阵的秩</h4><p>一个向量组A的秩是A的线性无关的向量的个数</p><p>如果把一个向量组看成一个矩阵，则向量组的秩就是矩阵的秩</p><ul><li><p><img src="/blog_picture/4.jpg" alt="avatar"></p></li><li><p><img src="/blog_picture/5.jpg" alt="avatar"></p></li></ul><h4 id="向量的范数"><a href="#向量的范数" class="headerlink" title="向量的范数"></a>向量的范数</h4><ul><li><img src="/blog_picture/6.jpg" alt="avatar"></li></ul><p>####常用的向量范数</p><p>1-范数 $||x||<em>1=\sum</em>{i=1}^{n}|x|$</p><p>2-范数  $||x||<em>2=\sqrt{\sum</em>{i=1}^{n}{x_i}^2}$欧式范数</p><p>无穷范数   $||x||_n=max|x_i|$</p><h4 id="矩阵的范数"><a href="#矩阵的范数" class="headerlink" title="矩阵的范数"></a>矩阵的范数</h4><p><img src="/blog_picture/8.jpg" alt="avatar"></p><p>####常用的矩阵范数</p><p><img src="/blog_picture/9.jpg" alt="avatar"></p><p><img src="/blog_picture/10.jpg" alt="avatar"></p><p>范数的作用：机器学习的分类问题中，使用范数可以判断两个特征向量和矩阵的相似性</p><p>####矩阵的迹</p><p><img src="/blog_picture/11.jpg" alt="avatar"></p><h4 id="线性变换及其矩阵表示"><a href="#线性变换及其矩阵表示" class="headerlink" title="线性变换及其矩阵表示"></a>线性变换及其矩阵表示</h4><p><img src="/blog_picture/12.jpg" alt="avatar"></p><h4 id="特征值、特征向量"><a href="#特征值、特征向量" class="headerlink" title="特征值、特征向量"></a>特征值、特征向量</h4><p><img src="/blog_picture/13.jpg" alt="avatar"></p><p>####特征值的性质</p><p><img src="/blog_picture/14.jpg" alt="avatar"></p><p>####特征值和特征向量的求法</p><p><img src="/blog_picture/15.jpg" alt="avatar"></p><p><img src="/blog_picture/16.jpg" alt="avatar"></p><p>特征值和特征向量在机器学习中的应用：• 主成分分析• 流行学习• LDA</p><h4 id="正交投影"><a href="#正交投影" class="headerlink" title="正交投影"></a>正交投影</h4><p>在线性代数和泛函分析中，投影是从向量空间映射到自身的一种线性变换。具体来说，正交投影是指像空间U和零空间W相互正交子空间的投影</p><p>从解方程角度看，A x = b 可能无解，因为对任意 的 x , Ax 总是在A的列子空间里，若 向量 b 不在 列空间里，则方程无解。但是我们可以将 b 利用正 交投影矩阵投影到 A 的列子空间里得到正交投影 y， 然后求解A x = y，寻找一个最佳近似解 x。</p><p><img src="/blog_picture/17.jpg" alt="avatar"></p><h4 id="二次型"><a href="#二次型" class="headerlink" title="二次型"></a>二次型</h4><p><img src="/blog_picture/18.jpg" alt="avatar"></p><p><img src="/blog_picture/19.jpg" alt="avatar"></p><p>二次型补充知识点</p><p><img src="/blog_picture/20.jpg" alt="avatar"></p><h4 id="矩阵的QR分解"><a href="#矩阵的QR分解" class="headerlink" title="矩阵的QR分解"></a>矩阵的QR分解</h4><p><img src="/blog_picture/21.jpg" alt="avatar"></p><h4 id="SVD奇异值分解"><a href="#SVD奇异值分解" class="headerlink" title="SVD奇异值分解"></a>SVD奇异值分解</h4><p><img src="/blog_picture/22.jpg" alt="avatar"></p><p>##微积分</p><h4 id="集合的定义"><a href="#集合的定义" class="headerlink" title="集合的定义"></a>集合的定义</h4><p><img src="/blog_picture/23.jpg" alt="avatar"></p><p>####集合的表示方法</p><p><img src="/blog_picture/24.jpg" alt="avatar"></p><p>####集合的分类</p><p><img src="/blog_picture/25.jpg" alt="avatar"></p><h4 id="集合运算"><a href="#集合运算" class="headerlink" title="集合运算"></a>集合运算</h4><p><img src="/blog_picture/26.jpg" alt="avatar"></p><h4 id="Venn图"><a href="#Venn图" class="headerlink" title="Venn图"></a>Venn图</h4><p>表示集合的另一种形式</p><p><img src="/blog_picture/27.jpg" alt="avatar"></p><h4 id="函数定义"><a href="#函数定义" class="headerlink" title="函数定义"></a>函数定义</h4><p><img src="/blog_picture/28.jpg" alt="avatar"></p><h4 id="领域的定义"><a href="#领域的定义" class="headerlink" title="领域的定义"></a>领域的定义</h4><p><img src="/blog_picture/29.jpg" alt="avatar"></p><p>####函数的极限性质</p><h5 id="四则运算"><a href="#四则运算" class="headerlink" title="四则运算"></a>四则运算</h5><p><img src="/blog_picture/30.jpg" alt="avatar"></p><p>#####复合函数的极限</p><p><img src="/blog_picture/31.jpg" alt="avatar"></p><p>#####保号性</p><p><img src="/blog_picture/32.jpg" alt="avatar"></p><p>#####夹逼定理</p><p><img src="/blog_picture/33.jpg" alt="avatar"></p><p>#####洛必达法则</p><p><img src="/blog_picture/34.jpg" alt="avatar"></p><p>####函数的连续性</p><p><img src="/blog_picture/35.jpg" alt="avatar"></p><p>####间断的定义</p><p><img src="/blog_picture/36.jpg" alt="avatar"></p><p><img src="/blog_picture/37.jpg" alt="avatar"></p><h4 id="函数的导数"><a href="#函数的导数" class="headerlink" title="函数的导数"></a>函数的导数</h4><p><img src="/blog_picture/38.jpg" alt="avatar"></p><p>####导数的常用公式</p><p><img src="/blog_picture/39.jpg" alt="avatar"></p><h4 id="导数的性质"><a href="#导数的性质" class="headerlink" title="导数的性质"></a>导数的性质</h4><p>#####四则运算</p><p><img src="/blog_picture/40.jpg" alt="avatar"></p><p>#####复合函数求导</p><p><img src="/blog_picture/41.jpg" alt="avatar"></p><h5 id="导数作用"><a href="#导数作用" class="headerlink" title="导数作用"></a>导数作用</h5><p>链式求导法则:神经网络反向传播基础 </p><p>梯度下降法:最简单的优化方法</p><h4 id="函数的微分"><a href="#函数的微分" class="headerlink" title="函数的微分"></a>函数的微分</h4><p><img src="/blog_picture/42.jpg" alt="avatar"></p><h4 id="原函数"><a href="#原函数" class="headerlink" title="原函数"></a>原函数</h4><p><img src="/blog_picture/43.jpg" alt="avatar"></p><h4 id="不定积分"><a href="#不定积分" class="headerlink" title="不定积分"></a>不定积分</h4><p><img src="/blog_picture/44.jpg" alt="avatar"></p><p><img src="/blog_picture/45.jpg" alt="avatar"></p><p><img src="/blog_picture/46.jpg" alt="avatar"></p><p>####不定积分性质</p><p><img src="/blog_picture/47.jpg" alt="avatar"></p><p>####不定积分的基本公式</p><p><img src="/blog_picture/48.jpg" alt="avatar"></p><h4 id="定积分"><a href="#定积分" class="headerlink" title="定积分"></a>定积分</h4><p><img src="/blog_picture/49.jpg" alt="avatar"></p><p><img src="/blog_picture/50.jpg" alt="avatar"></p><p><img src="/blog_picture/51.jpg" alt="avatar"></p><p><img src="/blog_picture/52.jpg" alt="avatar"></p><p><img src="/blog_picture/53.jpg" alt="avatar"></p><p>####定积分的性质</p><p><img src="/blog_picture/54.jpg" alt="avatar"></p><p>牛顿-莱布尼兹公式</p><p><img src="/blog_picture/55.jpg" alt="avatar"></p><p><img src="/blog_picture/56.jpg" alt="avatar"></p><p>####二重积分</p><p><img src="/blog_picture/57.jpg" alt="avatar"></p><p><img src="/blog_picture/58.jpg" alt="avatar"></p><p><img src="/blog_picture/59.jpg" alt="avatar"></p><p><img src="/blog_picture/60.jpg" alt="avatar"></p><p>####导数</p><p><img src="/blog_picture/61.jpg" alt="avatar"></p><p>#####标量关于标量X的求导</p><p><img src="/blog_picture/62.jpg" alt="avatar"></p><p>#####向量关于标量X的求导</p><p><img src="/blog_picture/63.jpg" alt="avatar"></p><p>####矩阵关于标量X的求导</p><p><img src="/blog_picture/64.jpg" alt="avatar"></p><p>####标量关于向量x的导数</p><p><img src="/blog_picture/65.jpg" alt="avatar"></p><p>####向量关于向量x的导数</p><p><img src="/blog_picture/66.jpg" alt="avatar"></p><p>####矩阵关于向量 x 的导数</p><p><img src="/blog_picture/67.jpg" alt="avatar"></p><p>####标量关于矩阵的导数</p><p><img src="/blog_picture/68.jpg" alt="avatar"></p><p>####向量关于矩阵的导数</p><p><img src="/blog_picture/69.jpg" alt="avatar"></p><h4 id="矩阵关于矩阵的导数"><a href="#矩阵关于矩阵的导数" class="headerlink" title="矩阵关于矩阵的导数"></a>矩阵关于矩阵的导数</h4><p><img src="/blog_picture/70.jpg" alt="avatar"></p><p>####分子布局法与分母局部法区别</p><p><img src="/blog_picture/71.jpg" alt="avatar"></p><p>####Hessian矩阵</p><p><img src="/blog_picture/72.jpg" alt="avatar"></p><p><img src="/blog_picture/73.jpg" alt="avatar"></p><p>##概率论基础</p><p>####概率论基础</p><p>概率论与数理统计是研究什么的?</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">随机现象:不确定性与统计规律性 </span><br><span class="line">概率论:从数量上研究随机现象的统计规律性的科学</span><br><span class="line">数理统计:从应用角度研究处理随机性数据，建立有效的统计方法，进行统计推理</span><br></pre></td></tr></table></figure><p>随机试验</p><p>在概率论中，将具有下述三个特点的试验称为<strong>随机试验</strong>，简称试验。 随机试验常用E表示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.试验的可重复性 —— 在相同条件下可重复进行;</span><br><span class="line">2.一次试验结果的随机性 —— 一次试验的可能结果不止一个，且试验之前无法确定具体是哪种结果出现; </span><br><span class="line">3.全部试验结果的可知性 —— 所有可能的结果是预先可知的，且每次试验有且仅有一个结果出现。</span><br></pre></td></tr></table></figure><p>####样本空间与样本点</p><p><img src="/blog_picture/74.jpg" alt="avatar"></p><p>####随机事件</p><p><img src="/blog_picture/75.jpg" alt="avatar"></p><p>####事件的性质与运算</p><p>事件的本质是集合，集合的一切性质和运算都适用与事件</p><p>####频率与概率</p><p><img src="/blog_picture/76.jpg" alt="avatar"></p><h4 id="概率的性质"><a href="#概率的性质" class="headerlink" title="概率的性质"></a>概率的性质</h4><p><img src="/blog_picture/77.jpg" alt="avatar"></p><h4 id="古典概型"><a href="#古典概型" class="headerlink" title="古典概型"></a>古典概型</h4><p><img src="/blog_picture/78.jpg" alt="avatar"></p><p><img src="/blog_picture/79.jpg" alt="avatar"></p><h4 id="几何概型"><a href="#几何概型" class="headerlink" title="几何概型"></a>几何概型</h4><p><img src="/blog_picture/80.jpg" alt="avatar"></p><p><img src="/blog_picture/1565069627088.jpg" alt="avatar"></p><h4 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h4><p><img src="/blog_picture/1565069696978.jpg" alt="avatar"></p><p><img src="/blog_picture/1565069726956.jpg" alt="avatar"></p><p>####条件概率的几何意义</p><p><img src="/blog_picture/1565069854934.jpg" alt="avatar"></p><p>####加法公式</p><p><img src="/blog_picture/1565069969698.jpg" alt="avatar"></p><p>####乘法公式</p><p><img src="/blog_picture/1565070039368.jpg" alt="avatar"></p><p>####排列组合</p><p><img src="/blog_picture/1565070090722.jpg" alt="avatar"></p><p>####全概率公式</p><p><img src="/blog_picture/1565070149616.jpg" alt="avatar"></p><p>####离散分布 vs 连续分布</p><p><img src="/blog_picture/1565070338442.jpg" alt="avatar"></p><p>####伯努利分布</p><p><img src="/blog_picture/1565070378085.jpg" alt="avatar"></p><p>####二项分布</p><p><img src="/blog_picture/1565070413168.jpg" alt="avatar"></p><p>####期望</p><p><img src="/blog_picture/1565070571975.jpg" alt="avatar"></p><h5 id="期望的性质"><a href="#期望的性质" class="headerlink" title="期望的性质"></a>期望的性质</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">1、E (C ) = C</span><br><span class="line">2、E (aX ) = a E (X )</span><br><span class="line">3、E (X + Y ) = E (X ) + E (Y )</span><br><span class="line">4、当X ,Y 相互独立时，E (X Y ) = E (X )E (Y )</span><br></pre></td></tr></table></figure><h5 id="期望的数学含义"><a href="#期望的数学含义" class="headerlink" title="期望的数学含义"></a>期望的数学含义</h5><p>反应了数据的平均取值情况</p><p>####方差</p><p><img src="/blog_picture/1565070731625.jpg" alt="avatar"></p><p><img src="/blog_picture/1565070796641.jpg" alt="avatar"></p><p>####数据归一化</p><p><img src="/blog_picture/1565070856261.jpg" alt="avatar"></p><p>####高斯分布</p><p><img src="/blog_picture/1565070907312.jpg" alt="avatar"></p><p><img src="/blog_picture/1565070933026.jpg" alt="avatar"></p><p>####分布函数</p><p><img src="/blog_picture/1565071005463.jpg" alt="avatar"></p><p><img src="/blog_picture/1565071040441.jpg" alt="avatar"></p><p>####均匀分布</p><p><img src="/blog_picture/1565071087634.jpg" alt="avatar"></p><p><img src="/blog_picture/1565071126393.jpg" alt="avatar"></p><p>####指数分布</p><p><img src="/blog_picture/1565071160548.jpg" alt="avatar"></p><p><img src="/blog_picture/1565071258034.jpg" alt="avatar"></p><p>####二维随机变量</p><p><img src="/blog_picture/1565071336855.jpg" alt="avatar"></p><p>####联合分布函数</p><p><img src="/blog_picture/1565071371964.jpg" alt="avatar"></p><p><img src="/blog_picture/1565071397963.jpg" alt="avatar"></p><p>####联合分布列</p><p><img src="/blog_picture/1565071449242.jpg" alt="avatar"></p><p>####二维连续型随机变量及其密度函数</p><p><img src="/blog_picture/1565071492637.jpg" alt="avatar"></p><p>####联合密度性质</p><p><img src="/blog_picture/1565071532659.jpg" alt="avatar"></p><p><img src="/blog_picture/1565071741870.jpg" alt="avatar"></p><p><img src="/blog_picture/1565071745834.jpg" alt="avatar"></p><p><img src="/blog_picture/1565071749581.jpg" alt="avatar"></p><p>####边缘分布</p><p><img src="/blog_picture/1565071827339.jpg" alt="avatar"></p><p><img src="/blog_picture/1565071830825.jpg" alt="avatar"></p><p><img src="/blog_picture/1565072132127.jpg" alt="avatar"></p><p><img src="/blog_picture/1565072287072.jpg" alt="avatar"></p><p>####多维分布</p><p>在机器学习中，一个 样本有多个特征，研究多个特征的概率分布与统计情况</p><p>####二维随机变量</p><p><img src="/blog_picture/1565072474902.jpg" alt="avatar"></p><p>####为什么需要协方差?</p><p><img src="/blog_picture/1565072501064.jpg" alt="avatar"></p><p>####协方差</p><p><img src="/blog_picture/1565072546171.jpg" alt="avatar"></p><p>####协方差的性质</p><p><img src="/blog_picture/1565072627923.jpg" alt="avatar"></p><p>####协方差矩阵</p><p><img src="/blog_picture/1565072672838.jpg" alt="avatar"></p><h4 id="主成分分析法"><a href="#主成分分析法" class="headerlink" title="主成分分析法"></a>主成分分析法</h4><p>#####PCA的意义</p><p><img src="/blog_picture/1565072734381.jpg" alt="avatar"></p><p>#####PCA的数学模型</p><p><img src="/blog_picture/1565072773199.jpg" alt="avatar"></p><p>####PCA推导</p><p><img src="/blog_picture/1565072881626.jpg" alt="avatar"></p><p><img src="/blog_picture/1565072885469.jpg" alt="avatar"></p><p><img src="/blog_picture/1565072888797.jpg" alt="avatar"></p><p>####PCA实施</p><p><img src="/blog_picture/1565072955696.jpg" alt="avatar"></p><p><img src="/blog_picture/1565072959543.jpg" alt="avatar"></p><h2 id="概率论与信息论"><a href="#概率论与信息论" class="headerlink" title="概率论与信息论"></a>概率论与信息论</h2><p>####切比雪夫不等式</p><p><img src="/blog_picture/1565073174283.jpg" alt="avatar"></p><p>####中心极限定理</p><p><img src="/blog_picture/1565073177541.jpg" alt="avatar"></p><p><img src="/blog_picture/1565073180613.jpg" alt="avatar"></p><p>####关于正态分布计算的补充</p><p><img src="/blog_picture/1565073253138.jpg" alt="avatar"></p><p>####矩的概念</p><p><img src="/blog_picture/1565073324278.jpg" alt="avatar"></p><p><img src="/blog_picture/1565073327543.jpg" alt="avatar"></p><p>####矩估计</p><p><img src="/blog_picture/1565073407133.jpg" alt="avatar"></p><p><img src="/blog_picture/1565073410261.jpg" alt="avatar"></p><p>####极大似然估计的思想</p><p><img src="/blog_picture/1565073555795.jpg" alt="avatar"></p><p>####极大似然估计</p><p><img src="/blog_picture/1565073575910.jpg" alt="avatar"></p><p><img src="/blog_picture/1565073579745.jpg" alt="avatar"></p><p>####极大似然估计求法</p><p><img src="/blog_picture/1565073673751.jpg" alt="avatar"></p><p><img src="/blog_picture/1565073677603.jpg" alt="avatar"></p><p><img src="/blog_picture/1565073681686.jpg" alt="avatar"></p><h4 id="MLE在机器学习中的应用"><a href="#MLE在机器学习中的应用" class="headerlink" title="MLE在机器学习中的应用"></a>MLE在机器学习中的应用</h4><p>参数估计 逻辑回归的参数估计</p><h4 id="最大后验估MAP"><a href="#最大后验估MAP" class="headerlink" title="最大后验估MAP"></a>最大后验估MAP</h4><p>####先验信息</p><p><img src="/blog_picture/1565073949228.jpg" alt="avatar"></p><p>####先验分布</p><p><img src="/blog_picture/1565073979451.jpg" alt="avatar"></p><p>####如何利用先验信息?</p><p>在样本少的情况下，如何 加入先验信息? 后验概率</p><p>####后验概率</p><p><img src="/blog_picture/1565074058577.jpg" alt="avatar"></p><p>####最大后验估计</p><p><img src="/blog_picture/1565074072524.jpg" alt="avatar"></p><p>####贝叶斯法则</p><p><img src="/blog_picture/1565074197261.jpg" alt="avatar"></p><p><img src="/blog_picture/1565074201110.jpg" alt="avatar"></p><p><img src="/blog_picture/1565074204451.jpg" alt="avatar"></p><p>####贝叶斯意义</p><p><img src="/blog_picture/1565074264162.jpg" alt="avatar"></p><p>####贝叶斯公式的密度函数形式</p><p><img src="/blog_picture/1565074294912.jpg" alt="avatar"></p><p><img src="/blog_picture/1565074306035.jpg" alt="avatar"></p><p>####共轭分布</p><p><img src="/blog_picture/1565074406347.jpg" alt="avatar"></p><p><img src="/blog_picture/1565074410909.jpg" alt="avatar"></p><p><img src="/blog_picture/1565074415028.jpg" alt="avatar"></p><p>####如何度量信息的多少?</p><p><img src="/blog_picture/1565074557282.jpg" alt="avatar"></p><p>####自信息量</p><p><img src="/blog_picture/1565074568602.jpg" alt="avatar"></p><p>####信息熵</p><p><img src="/blog_picture/1565074580286.jpg" alt="avatar"></p><p><img src="/blog_picture/1565074642251.jpg" alt="avatar"></p><p>####交叉熵</p><p><img src="/blog_picture/1565074679965.jpg" alt="avatar"></p><p>####交叉熵在机器学习中的应用</p><p>交叉熵损失函数   衡量两个随机变量之间的相似度</p><p>####互信息</p><p><img src="/blog_picture/1565074818018.jpg" alt="avatar"></p><p>####KL散度</p><p><img src="/blog_picture/1565074821449.jpg" alt="avatar"></p><p>####KL散度的性质</p><p><img src="/blog_picture/1565074832615.jpg" alt="avatar"></p><p><img src="/blog_picture/1565074846596.jpg" alt="avatar"></p><p>##优化方法</p><h4 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h4><p>所谓最优化问题，指在某些约束条件下，决定某些可选择的变量应该取何值，使所选定的目标函数达到最优的问题。即运用最新科技手段和处理方法，使系统达到总体最优，从而为系统提出 设计、施工、管理、运行的最优方案。</p><p>为什么要用优化算法?</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">求导找函数的最小(大)值不行吗?</span><br><span class="line">考虑:1、多元函数</span><br><span class="line">2、局部最大最小值</span><br></pre></td></tr></table></figure><p>####线性规划</p><p><img src="/blog_picture/1565076518810.jpg" alt="avatar"></p><p><img src="/blog_picture/1565076541128.jpg" alt="avatar"></p><h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><p><img src="/blog_picture/1565076888078.jpg" alt="avatar"></p><h4 id="一维函数梯度"><a href="#一维函数梯度" class="headerlink" title="一维函数梯度"></a>一维函数梯度</h4><p><img src="/blog_picture/1565077155760.jpg" alt="avatar"></p><p>####梯度下降法</p><p><img src="/blog_picture/1565077206152.jpg" alt="avatar"></p><p><img src="/blog_picture/1565077232963.jpg" alt="avatar"></p><p><img src="/blog_picture/1565077254742.jpg" alt="avatar"></p><p>####梯度法的迭代过程</p><p><img src="/blog_picture/1565077296818.jpg" alt="avatar"></p><p>####批量梯度下降BGD</p><p><img src="/blog_picture/1565077637320.jpg" alt="avatar"></p><p><img src="/blog_picture/1565077674463.jpg" alt="avatar"></p><p>####随机梯度下降SGD</p><p><img src="/blog_picture/1565078179419.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078182517.jpg" alt="avatar"></p><p>####小批量梯度下降法MBGD</p><p><img src="/blog_picture/1565078253357.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078257221.jpg" alt="avatar"></p><h4 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h4><p>求解无约束极值问题得最古老算法之一，已发展成为一类算法:Newton型方法。<br>在局部，用一个二次函数近似代替目标函数 f(x)，然后用近似函数的极小 点作为f(x) 的近似极小点。</p><p><img src="/blog_picture/1565078413953.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078417533.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078421199.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078424695.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078428776.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078432769.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078436246.jpg" alt="avatar"></p><h4 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h4><p>拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用 正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。 拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化， 构造一个目标函数的模型使之足以产生超线性收敛性。 这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的 信息，所以有时比牛顿法更为有效。</p><p><strong>用不包含二阶导数的矩阵近似<em>Hesse*</em></strong>矩阵的*</p><p><img src="/blog_picture/1565078600068.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078603177.jpg" alt="avatar"></p><p>####常用的拟牛顿法</p><p><img src="/blog_picture/1565078654456.jpg" alt="avatar"></p><h4 id="共轭方向法"><a href="#共轭方向法" class="headerlink" title="共轭方向法"></a>共轭方向法</h4><p><strong>共轭方向法</strong>是介于最速下降法与牛顿法之间的一类方法。</p><p>它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了存储和 计算牛顿法所需要的二阶导数信息。</p><p><img src="/blog_picture/1565078732264.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078735596.jpg" alt="avatar"></p><p>####共轭方向法的几何意义</p><p><img src="/blog_picture/1565078861764.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078864870.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078867892.jpg" alt="avatar"></p><h4 id="共轭梯度法"><a href="#共轭梯度法" class="headerlink" title="共轭梯度法"></a>共轭梯度法</h4><p>⚫ <strong>共轭梯度法</strong>(conjugate gradient method, CG)是以共轭方向(conjugate direction)作为 搜索方向的一类算法。</p><p>⚫ CG法是由Hesteness和Stiefel于1952年为求解线性方程组而提出的。后来用于求解无约束最优 化问题，它是一种重要的数学优化方法。这种方法具有<strong>二次终止性</strong></p><p>CG的基本思想是把共轭性与最速下降法相结合，利用已知迭代点的梯度方向 构造一组共轭方向，并沿着此组方向进行搜索，求出目标函数的极小点。</p><p><strong>什么是二次终止性?</strong></p><p>如果某算法用于求解目标函数为二次函数的无约束问题时，只需要经过有限迭代就能 达到最优解，则该算法具有二次终止性。<br>共轭梯度法就有二次终止性</p><p><img src="/blog_picture/1565078980017.jpg" alt="avatar"></p><p><img src="/blog_picture/1565079002662.jpg" alt="avatar"></p><p><img src="/blog_picture/1565079045462.jpg" alt="avatar"></p><p><img src="/blog_picture/1565079049180.jpg" alt="avatar"></p><p>####动量梯度下降法法Momentum</p><p><img src="/blog_picture/1565079107750.jpg" alt="avatar"></p><p><img src="/blog_picture/1565079110804.jpg" alt="avatar"></p><p>####均方根优化法RMSp</p><p><img src="/blog_picture/1565079198345.jpg" alt="avatar"></p><p><img src="/blog_picture/1565079201526.jpg" alt="avatar"></p><p>####自适应矩估计法Adam</p><p><img src="/blog_picture/1565079255381.jpg" alt="avatar"></p><p><img src="/blog_picture/1565079258315.jpg" alt="avatar"></p><p>####学习率衰减</p><p><img src="/blog_picture/1565079349701.jpg" alt="avatar"></p><p><img src="/blog_picture/1565079353025.jpg" alt="avatar"></p><p>####早停</p><p><img src="/blog_picture/1565079408798.jpg" alt="avatar"></p><p><strong>核心思想:</strong></p><p>如果训练数轮后准确率(损失函数)没有上升(下降)，就停止训练</p><p><strong>应用场景:</strong></p><p>大批量数据，训练时间长</p><p>####局部最优值</p><p><img src="/blog_picture/1565079488243.jpg" alt="avatar"></p><p>####鞍点问题</p><p><img src="/blog_picture/1565079501278.jpg" alt="avatar"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性代数 </tag>
            
            <tag> 优化方法 </tag>
            
            <tag> 微积分 </tag>
            
            <tag> 概率 </tag>
            
            <tag> 统计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据基础</title>
      <link href="/2019/07/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80/"/>
      <url>/2019/07/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h1 id="pyspark-RDD基础"><a href="#pyspark-RDD基础" class="headerlink" title="pyspark-RDD基础"></a>pyspark-RDD基础</h1><h3 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyspark是spark的python API,允许python调用spark编程模型</span><br></pre></td></tr></table></figure><h3 id="初始化spark"><a href="#初始化spark" class="headerlink" title="初始化spark"></a>初始化spark</h3><h4 id="SparkContext"><a href="#SparkContext" class="headerlink" title="SparkContext"></a>SparkContext</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line">sc = SparkContext(master=<span class="string">'local[2]'</span>)</span><br></pre></td></tr></table></figure><h4 id="核查SparkContext"><a href="#核查SparkContext" class="headerlink" title="核查SparkContext"></a>核查SparkContext</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sc.version获取SparkContext的版本</span><br><span class="line">sc.pythonVer获取python版本</span><br><span class="line">sc.master要连接的Master URL</span><br><span class="line">str(sc.sparkHome)spark工作节点的安装路径</span><br><span class="line">str(sc.sparkUser())获取SparkContext的spark用户名</span><br><span class="line">sc.appName返回应用名称</span><br><span class="line">sc.applicationId返回应用程序ID</span><br><span class="line">sc.defaultParallelism返回默认并行级别</span><br><span class="line">sc.defaultMinPatitionsRDD默认最小分区数</span><br></pre></td></tr></table></figure><h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf,SparkContext</span><br><span class="line">conf = (SparkConf().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"my APP"</span>).set(<span class="string">"spark.executor.memory"</span>,<span class="string">"1g"</span>))</span><br><span class="line">sc = SparkContext(conf=conf)</span><br></pre></td></tr></table></figure><h4 id="使用shell"><a href="#使用shell" class="headerlink" title="使用shell"></a>使用shell</h4><p>pyspark shell已经为SparkContext创建了名为sc的变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-shell --master local[2]</span><br><span class="line">./bin/pyspark --master local[4] --py-files code.py</span><br></pre></td></tr></table></figure><p>用—master参数设定Context连接到哪个Master服务器，通过传递逗号分隔列表至—py-files添加Python.zip、egg或.py文件到Runtime路径</p><h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><h4 id="并行集合"><a href="#并行集合" class="headerlink" title="并行集合"></a>并行集合</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">'a'</span>,<span class="number">7</span>),(<span class="string">'a'</span>,<span class="number">2</span>),(<span class="string">'b'</span>,<span class="number">2</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">'a'</span>,<span class="number">2</span>),(<span class="string">'d'</span>,<span class="number">1</span>),(<span class="string">'b'</span>,<span class="number">1</span>)])</span><br><span class="line">rdd3 = sc.parallelize(range(<span class="number">100</span>))</span><br><span class="line">rdd4 = sc.parallelize([(<span class="string">"a"</span>,[<span class="string">"x"</span>,<span class="string">"y"</span>,<span class="string">"z"</span>]),(<span class="string">"b"</span>,[<span class="string">"p"</span>,<span class="string">"r"</span>])])</span><br></pre></td></tr></table></figure><h4 id="外部数据"><a href="#外部数据" class="headerlink" title="外部数据"></a>外部数据</h4><p>使用textFile()函数从HDFS、本地文件或其它支持hadoop的文件系统里读取文件，或使用wholeTextFiles()函数读取目录下所有文本文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">textFile = sc.textFile(<span class="string">'a.txt'</span>)</span><br><span class="line">textFile2 = sc.wholeTextFiles(/aa)</span><br></pre></td></tr></table></figure><h3 id="提取RDD信息"><a href="#提取RDD信息" class="headerlink" title="提取RDD信息"></a>提取RDD信息</h3><h4 id="基础信息"><a href="#基础信息" class="headerlink" title="基础信息"></a>基础信息</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">rdd.getNumPatitions()列出分区数</span><br><span class="line">rdd.count()计算RDD的实例数量</span><br><span class="line">rdd.countByKey()按键计算RDD实例数量</span><br><span class="line">defaultdict(&lt;type <span class="string">'int'</span>&gt;,(<span class="string">'a'</span>:<span class="number">2</span>,<span class="string">'b'</span>:<span class="number">1</span>))</span><br><span class="line">rdd.countByValue()按值计算RDD实例数量</span><br><span class="line">defaultdict(&lt;type <span class="string">'int'</span>&gt;,((<span class="string">'b'</span>,<span class="number">2</span>):<span class="number">1</span>,(<span class="string">'a'</span>,<span class="number">2</span>):<span class="number">1</span>,(<span class="string">'a'</span>,<span class="number">7</span>):<span class="number">1</span>))</span><br><span class="line">rdd.collectAsMap()以字典的形式返回键值</span><br><span class="line">(<span class="string">'a'</span>:<span class="number">2</span>,<span class="string">'b'</span>:<span class="number">2</span>)</span><br><span class="line">rdd.sum()汇总RDD元素</span><br><span class="line"><span class="number">4959</span></span><br><span class="line">sc.parallelize([]).isEmpty()检查RDD是否为空</span><br></pre></td></tr></table></figure><h4 id="汇总"><a href="#汇总" class="headerlink" title="汇总"></a>汇总</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rdd.max()RDD元素的最大值</span><br><span class="line">rdd.min()RDD元素的最小值</span><br><span class="line">rdd.mean()RDD元素的平均值</span><br><span class="line">rdd.stdev()RDD元素的标准差</span><br><span class="line">rdd.variance()RDD元素的方差</span><br><span class="line">rdd.histogram(<span class="number">3</span>)分箱（bin）生成直方图</span><br><span class="line">rdd.stats()综合统计包括：计数、平均值、标准差、最大值和最小值</span><br></pre></td></tr></table></figure><h4 id="应用函数"><a href="#应用函数" class="headerlink" title="应用函数"></a>应用函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd.map(<span class="keyword">lambda</span> x:x+(x[<span class="number">1</span>],x[<span class="number">0</span>])).collect()对每个RDD元素执行函数</span><br><span class="line">rdd.flatMap(<span class="keyword">lambda</span> x:x+(x[<span class="number">1</span>],x[<span class="number">0</span>]))对每个RDD元素执行函数，并拉平结果</span><br><span class="line">rdd.collect()</span><br><span class="line">rdd.flatMapValues(<span class="keyword">lambda</span> x:x).collect()不改变键，对rdd的每个键值对执行flatMap函数</span><br></pre></td></tr></table></figure><h4 id="选择数据"><a href="#选择数据" class="headerlink" title="选择数据"></a>选择数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">获取</span><br><span class="line">rdd.collect()返回包含所以RDD元素的列表</span><br><span class="line">rdd.take(<span class="number">4</span>)提取前<span class="number">4</span>个RDD元素</span><br><span class="line">rdd.first()提取第一个RDD元素</span><br><span class="line">rdd.top(<span class="number">2</span>)提取前两个RDD元素</span><br><span class="line">抽样</span><br><span class="line">rdd.sample(<span class="literal">False</span>,<span class="number">0.15</span>,<span class="number">81</span>)返回RDD的采样子集</span><br><span class="line">筛选</span><br><span class="line">rdd.filter(<span class="keyword">lambda</span> x:<span class="string">'a'</span> <span class="keyword">in</span> x)筛选RDD</span><br><span class="line">rdd.distinct()返回RDD里的唯一值</span><br><span class="line">rdd.keys()返回RDD键值对里的键</span><br></pre></td></tr></table></figure><h4 id="迭代"><a href="#迭代" class="headerlink" title="迭代"></a>迭代</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">(x)</span>:</span>print(x)     </span><br><span class="line">rdd.foreach(g)</span><br></pre></td></tr></table></figure><h4 id="改变数据形状"><a href="#改变数据形状" class="headerlink" title="改变数据形状"></a>改变数据形状</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">规约</span><br><span class="line">rdd.reduceByKey(<span class="keyword">lambda</span> x,y:x+y)合并每个键的值</span><br><span class="line">rdd.reduce(<span class="keyword">lambda</span> x,y:x+y)合并RDD的值</span><br><span class="line">分组</span><br><span class="line">rdd.groupBy(<span class="keyword">lambda</span> x:x%<span class="number">2</span>).mapValues(list)返回RDD的分组值</span><br><span class="line">rdd.groupByKey().mapValues(list)按键分组RDD</span><br><span class="line">集合</span><br><span class="line">seqOp = (<span class="keyword">lambda</span> x,y:(x[<span class="number">0</span>]+y,x[<span class="number">1</span>]+<span class="number">1</span>))</span><br><span class="line">combOP = (<span class="keyword">lambda</span> x,y:(x[<span class="number">0</span>]+y[<span class="number">0</span>],x[<span class="number">1</span>]+y[<span class="number">1</span>]))</span><br><span class="line">rdd.aggregate((<span class="number">0</span>,<span class="number">0</span>),seqOp,combOP) 汇总每个分区里的RDD元素，并输出结果</span><br><span class="line">rdd.aggregeteByKey((<span class="number">0</span>,<span class="number">0</span>),seqOp,combOP)汇总每个RDD的键的值</span><br><span class="line">rdd.fold(<span class="number">0</span>,add)汇总每个分区里的RDD元素，并输出结果</span><br><span class="line">rdd.foldByKey(<span class="number">0</span>,add)合并每个键的值</span><br><span class="line">rdd,keyBy(<span class="keyword">lambda</span> x:x+x)通过执行函数，创建RDD元素的元组</span><br></pre></td></tr></table></figure><h4 id="数学运算"><a href="#数学运算" class="headerlink" title="数学运算"></a>数学运算</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd.subtract(rdd2)返回RDD2里没有匹配键的rdd的兼职对</span><br><span class="line">rdd2.subtractByKey(rdd)返回rdd2里的每个（键、值）对，rdd中，没有匹配的键</span><br><span class="line">rdd.cartesian(rdd2)返回rdd和rdd2的笛卡尔积</span><br></pre></td></tr></table></figure><h4 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd.sortBy(lambda x:x[1])按给定函数排序RDD</span><br><span class="line">rdd.sortByKey()按键排序RDD的键值对</span><br></pre></td></tr></table></figure><h4 id="重分区"><a href="#重分区" class="headerlink" title="重分区"></a>重分区</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd.repartition(4)新建一个含4个分区的RDD</span><br><span class="line">rdd.coalesce(1)将RDD中的分区数缩减为1个</span><br></pre></td></tr></table></figure><h4 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd.saveAsTextFile(&quot;rdd.txt&quot;)</span><br><span class="line">rdd.saveAsHadoopFile(&quot;hdfs://namenodehost/parent/child&quot;,&apos;org.apache.hadoop.mapred.TextOutputFormat&apos;)</span><br></pre></td></tr></table></figure><h4 id="终止SparkContext"><a href="#终止SparkContext" class="headerlink" title="终止SparkContext"></a>终止SparkContext</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.stop()</span><br></pre></td></tr></table></figure><h4 id="执行程序"><a href="#执行程序" class="headerlink" title="执行程序"></a>执行程序</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit examples/src/main/python/pi.py</span><br></pre></td></tr></table></figure><h1 id="Pyspark-sql"><a href="#Pyspark-sql" class="headerlink" title="Pyspark_sql"></a>Pyspark_sql</h1><h4 id="Pyspark与Spark-SQL"><a href="#Pyspark与Spark-SQL" class="headerlink" title="Pyspark与Spark SQL"></a>Pyspark与Spark SQL</h4><p>Spark SQL是Apache Spark处理结构化数据的模块</p><h4 id="初始化SparkSession"><a href="#初始化SparkSession" class="headerlink" title="初始化SparkSession"></a>初始化SparkSession</h4><p>SparkSession用于创建数据框，将数据框注册为表，执行SQL查询，缓存表及读取Parquet文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">spark = SparkSession.builder.appName(&quot;my app&quot;).config(&quot;spark.some.config.option&quot;,&quot;some-value&quot;).getOrCreate()</span><br></pre></td></tr></table></figure><h4 id="创建数据框"><a href="#创建数据框" class="headerlink" title="创建数据框"></a>创建数据框</h4><h5 id="从RDD创建"><a href="#从RDD创建" class="headerlink" title="从RDD创建"></a>从RDD创建</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql.types import *</span><br><span class="line">推断Schema</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">lines = sc.textFile(&quot;people.txt&quot;)</span><br><span class="line">parts = lines.map(lambda l:l.split(&quot;,&quot;))</span><br><span class="line">people = parts.map(lambda p:Row(name=p[0],age=int(p[1])))</span><br><span class="line">peopledf = spark.createDataFrame(people)</span><br><span class="line">指定Schema</span><br><span class="line">people = parts.map(lambda p:Row(name=p[0],age=int(p[1].strip())))</span><br><span class="line">schemaString = &quot;name age&quot;</span><br><span class="line">fields = [StructField(field_name,StringType(),True) for field_name in schemaString.split()]</span><br><span class="line">schema = StructType(fields)</span><br><span class="line">spark.createDataFrame(people,schema).show()</span><br></pre></td></tr></table></figure><h5 id="从spark数据源创建"><a href="#从spark数据源创建" class="headerlink" title="从spark数据源创建"></a>从spark数据源创建</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">json</span><br><span class="line">df = spark.read.json(<span class="string">"customer.json"</span>)</span><br><span class="line">df.show()</span><br><span class="line">df2 = spark.read.load(<span class="string">"people.json"</span>,format = <span class="string">"json"</span>)</span><br><span class="line">Parquet文件</span><br><span class="line">df3 = spark.read.load(<span class="string">"users.parquet"</span>)</span><br><span class="line">文本文件</span><br><span class="line">df4 = spark.read.text(<span class="string">"people.txt"</span>)</span><br></pre></td></tr></table></figure><h5 id="查阅数据信息"><a href="#查阅数据信息" class="headerlink" title="查阅数据信息"></a>查阅数据信息</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">df.dtypes返回df的列名与数据类型</span><br><span class="line">df.show()显示df内容</span><br><span class="line">df.head()返回前n行数据</span><br><span class="line">df.first()返回第一行数据</span><br><span class="line">df.take(2)返回前两行数据</span><br><span class="line">df.schema返回df的schema</span><br><span class="line">df.describe().show()汇总统计数据</span><br><span class="line">df.columns返回df列名</span><br><span class="line">df.count()返回df的行数</span><br><span class="line">df.distinct().count()返回df中不重复的行数</span><br><span class="line">df.printSchema()返回df的Schema</span><br><span class="line">df.explain()返回逻辑与实体方案</span><br></pre></td></tr></table></figure><h5 id="重复值"><a href="#重复值" class="headerlink" title="重复值"></a>重复值</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = df.dropDuplicates()</span><br></pre></td></tr></table></figure><h5 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line">Select</span><br><span class="line">df.select(<span class="string">"firstName"</span>).show()显示firstName列的所有条目</span><br><span class="line">df.select(<span class="string">"firstName"</span>,<span class="string">"lastName"</span>.show())</span><br><span class="line">df.select(<span class="string">"firstName"</span>,<span class="string">"age"</span>,\</span><br><span class="line">          explode(<span class="string">"phoneNumber"</span>)\显示firstName、age的所有条目和类型</span><br><span class="line">          .alias(<span class="string">"contactInfo"</span>))\</span><br><span class="line">.select(<span class="string">"ContactInfo.type"</span>,<span class="string">"firstName"</span>,<span class="string">"age"</span>)</span><br><span class="line">df.select(df[<span class="string">"firstName"</span>],df[<span class="string">"age"</span>]+<span class="number">1</span>).show()显示firstName和age列的所有记录添加</span><br><span class="line">df.select(df[<span class="string">"age"</span>]&gt;<span class="number">24</span>).show()显示所有小于<span class="number">24</span>的记录</span><br><span class="line">When</span><br><span class="line">df.select(<span class="string">"firstName"</span>,F.when(df.age&gt;<span class="number">30</span>,<span class="number">1</span>))\显示firstName，且大于<span class="number">30</span>岁显示<span class="number">1</span>，小于<span class="number">30</span>显示<span class="number">0</span></span><br><span class="line">.otherwise(<span class="number">0</span>).show()</span><br><span class="line">df[df.firstName.isin(<span class="string">"Jane"</span>,<span class="string">"Boris"</span>)].collect()显示符合特定条件的firstName列的记录</span><br><span class="line">Like</span><br><span class="line">df.select(<span class="string">"firstName"</span>,df.lastName,\显示lastName列中包含Smith的firstName列的记录</span><br><span class="line">          like(<span class="string">"Smith"</span>)).show()</span><br><span class="line">Startswith-Endwith</span><br><span class="line">df.select(<span class="string">"firstName"</span>,df.lastName.\显示lastName列中以Sm开头的firstName列的记录</span><br><span class="line">          startswith(<span class="string">"Sm"</span>)).show()</span><br><span class="line">df.select(df.lastName.endswith(<span class="string">"th"</span>)).show()显示以th结尾的lastName</span><br><span class="line">Substring</span><br><span class="line">df.select(df.firstName.substr(<span class="number">1</span>,<span class="number">3</span>).alias(<span class="string">"name"</span>))返回firstName的子字符串</span><br><span class="line">Between</span><br><span class="line">df.select(df.age.between(<span class="number">22</span>,<span class="number">24</span>)).show()显示介于<span class="number">22</span>到<span class="number">24</span>直接的age列的所有记录</span><br></pre></td></tr></table></figure><h4 id="添加、修改、删除列"><a href="#添加、修改、删除列" class="headerlink" title="添加、修改、删除列"></a>添加、修改、删除列</h4><h5 id="添加列"><a href="#添加列" class="headerlink" title="添加列"></a>添加列</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df = df.withColumn(<span class="string">'city'</span>,df.address.city) \</span><br><span class="line">       .withColumn(<span class="string">'postalCode'</span>,df.address.postalCode) \</span><br><span class="line">    .withColumn(<span class="string">'state'</span>,df.address.state) \</span><br><span class="line">    .withColumn(<span class="string">'streetAddress'</span>,df.address.streetAddress) \</span><br><span class="line">    .withColumn(<span class="string">'telePhoneNumber'</span>,explode(df.phoneNumber.number)) \</span><br><span class="line">    .withColumn(<span class="string">'telePhoneType'</span>,explode(df.phoneNumber.type)) \</span><br></pre></td></tr></table></figure><h5 id="修改列"><a href="#修改列" class="headerlink" title="修改列"></a>修改列</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = df.withColumnRenamed(<span class="string">'telePhoneNumber'</span>,<span class="string">'phoneNumber'</span>)</span><br></pre></td></tr></table></figure><h5 id="删除列"><a href="#删除列" class="headerlink" title="删除列"></a>删除列</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = df.drop(&quot;address&quot;,&quot;phoneNumber&quot;)</span><br><span class="line">df = df.drop(df.address).drop(df.phoneNumber)</span><br></pre></td></tr></table></figure><h5 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.groupBy(<span class="string">"age"</span>).count().show()按age列分组，统计每组人数</span><br></pre></td></tr></table></figure><h5 id="筛选"><a href="#筛选" class="headerlink" title="筛选"></a>筛选</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.filter(df[<span class="string">"age"</span>]&gt;<span class="number">24</span>).show()按age列筛选，保留年龄大于<span class="number">24</span>岁的</span><br></pre></td></tr></table></figure><h5 id="排序-1"><a href="#排序-1" class="headerlink" title="排序"></a>排序</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">peopledf.sort(peopledf.age.desc()).collect()</span><br><span class="line">df.sort(&quot;age&quot;,ascending=False).collect()</span><br><span class="line">df.orderBy([&quot;age&quot;,&quot;city&quot;],ascending=[0,1]).collect()</span><br></pre></td></tr></table></figure><h5 id="替换缺失值"><a href="#替换缺失值" class="headerlink" title="替换缺失值"></a>替换缺失值</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.na.fill(<span class="number">50</span>).show()用一个值替换空值</span><br><span class="line">df.na.drop().show()去除df中为空值的行</span><br><span class="line">df.na.replace(<span class="number">10</span>,<span class="number">20</span>).show()用一个值去替换另一个值</span><br></pre></td></tr></table></figure><h5 id="重分区-1"><a href="#重分区-1" class="headerlink" title="重分区"></a>重分区</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.repartition(<span class="number">10</span>).rdd.getNumPartitions()将df拆分为<span class="number">10</span>个分区</span><br><span class="line">df.coalesce(<span class="number">1</span>).rdd.getNumPartitions()将df合并为<span class="number">1</span>个分区</span><br></pre></td></tr></table></figure><h4 id="运行SQL查询"><a href="#运行SQL查询" class="headerlink" title="运行SQL查询"></a>运行SQL查询</h4><h5 id="将数据框注册为视图"><a href="#将数据框注册为视图" class="headerlink" title="将数据框注册为视图"></a>将数据框注册为视图</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">peopledf.createGlobalTempView(<span class="string">"people"</span>)</span><br><span class="line">df.createTempView(<span class="string">"customer"</span>)</span><br><span class="line">df.createOrReplaceTempView(<span class="string">"customer"</span>)</span><br></pre></td></tr></table></figure><h5 id="查询视图"><a href="#查询视图" class="headerlink" title="查询视图"></a>查询视图</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = spark.sql(<span class="string">"select * from customer"</span>).show()</span><br><span class="line">peopledf = spark.sql(<span class="string">"select * from global_temp.people"</span>).show()</span><br></pre></td></tr></table></figure><h4 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h4><h5 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd1 = df.rdd将df转为rdd</span><br><span class="line">df.toJSON().first()将df转为rdd字符串</span><br><span class="line">df.toPandas()将df的内容转为Pandas的数据框</span><br></pre></td></tr></table></figure><h5 id="保存至文件"><a href="#保存至文件" class="headerlink" title="保存至文件"></a>保存至文件</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.select(<span class="string">"firstName"</span>,<span class="string">"city"</span>).write.save(<span class="string">"nameAndCity.parquet"</span>)</span><br><span class="line">df.select(<span class="string">"firstName"</span>,<span class="string">"age"</span>).write.save(<span class="string">"nameAndAges.json"</span>,format=<span class="string">"json"</span>)</span><br></pre></td></tr></table></figure><h5 id="终止SparkSession"><a href="#终止SparkSession" class="headerlink" title="终止SparkSession"></a>终止SparkSession</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pyspark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据分析常用工具总结</title>
      <link href="/2019/07/22/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E6%80%BB%E7%BB%93/"/>
      <url>/2019/07/22/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1. 优点:向量化数据操作比for循环,速度大大加强，numpy array比list好的地方在于切片</span><br><span class="line">2. array属性</span><br><span class="line">    np.random.random((2,2)) # 0-1随机数</span><br><span class="line">    np.random.randint(1,10,(3,3)) # 随机整数</span><br><span class="line">    array.shape, array.dtype # numpy两个属性</span><br><span class="line">    array.astype(np.float64) # 类型转换</span><br><span class="line">3. array切片操作</span><br><span class="line">    a[0,1] # 第一个维度为0,第二个维度1,第三个维度全选,类似于a[0,1,:]</span><br><span class="line">    a[a&gt;2] # boolean indexing, 利用broadcasting进行判断, 之后可以作为index进行数据的提取</span><br><span class="line">    a[a&gt;2]=0 # 也可以对满足条件的元素进行赋值</span><br><span class="line">4. array数学运算</span><br><span class="line">    broadcasting, 对不匹配的数据在高维上进行扩展,在取最小公倍数</span><br><span class="line">    np.sum(array) # 统计运算</span><br><span class="line">    np.dot # 矩阵乘法,点乘</span><br><span class="line">    np.multiply  # 逐个元素乘法,对应相乘</span><br></pre></td></tr></table></figure><p>#pandas</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">基于Numpy构建，利用它的高级数据结构和操作工具，可使数据分析工作变得更加便捷高效。</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br></pre></td></tr></table></figure><h2 id="基本数据结构"><a href="#基本数据结构" class="headerlink" title="基本数据结构"></a>基本数据结构</h2><h3 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">1. 基本概念</span><br><span class="line">    pd.__version__ # 查看版本</span><br><span class="line">    pd.Series # 可以使用不同类型,和list区别在于有index, 可以指定index</span><br><span class="line"></span><br><span class="line">2. Series构建</span><br><span class="line">    pd.Series([1,2,3], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;])</span><br><span class="line">    pd.Series(&#123;...&#125;, name=&quot;xxx&quot;) # 通过对dictionary进行构建pandas, 给Series赋予名字</span><br><span class="line"></span><br><span class="line">3. 切片</span><br><span class="line">    aseries[[1,4,3]]; aseries[1:]; aseries[:-1]  # 数字下标切片,即使index不是数字也ok</span><br><span class="line"></span><br><span class="line">4. 运算规则</span><br><span class="line">    series的相加是根据index对应相加的</span><br><span class="line"></span><br><span class="line">5. 取值</span><br><span class="line">    数学运算也是broadcasting方式</span><br><span class="line">    &apos;xxx&apos; in aseries # 判断xxx是否在aseries的index中</span><br><span class="line">    aseries.get(&apos;xxx&apos;, 0) # 类似于字典</span><br><span class="line">    aseries[aseries&lt;20] # boolean index也可以</span><br><span class="line">    aseries.median() # 除去缺失值之后进行统计运算</span><br><span class="line">    aseries[&apos;xxx&apos;] = 1000 # 对aseries[&apos;xxx&apos;]重新赋值</span><br><span class="line">    np.square(aseries) # 对每个运算进行计算平方</span><br></pre></td></tr></table></figure><h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">1. 基本概念</span><br><span class="line">    一组Series集合在一起</span><br><span class="line"></span><br><span class="line">2. DataFrame的构建</span><br><span class="line">    - pd.DataFrame(&#123;&apos;a&apos;:[1,2,3], &apos;b&apos;:[1,4,3]&#125;, columns = [&apos;b&apos;, &apos;a&apos;], index = [&apos;one&apos;, &apos;two&apos;, &apos;three&apos;]) # 构建DF, 指定列名以及index名</span><br><span class="line">    - pd.DataFrame([&#123;&apos;a&apos;:100,&apos;b&apos;:200&#125;, &#123;&apos;a&apos;:200, &apos;b&apos;:300&#125;], index=[&apos;one&apos;, &apos;two&apos;]) # 按照一行一行构建DF</span><br><span class="line">    - pd.DataFrame(&#123;&apos;a&apos;:seriesa, &apos;b&apos;:seriesb&#125; # 记住按照index对齐, 缺失值直接Nan填充</span><br><span class="line"></span><br><span class="line">3. 元素的提取以及增加及逻辑操作及转置</span><br><span class="line">    - aDF[&apos;xxx&apos;]/aDF.xxx # 取出来的是一个Series</span><br><span class="line">    - aDF[[&apos;xxx&apos;]] # 取出来的是一个DF</span><br><span class="line">    - aDF.loc([&apos;a&apos;,&apos;b&apos;],[&apos;c&apos;,&apos;d&apos;]) # 取对应的数据</span><br><span class="line">    - aDF.loc[:, &apos;newcol&apos;] = 2000 # 如果没有newcol那么就新加一列</span><br><span class="line">    - aDF.loc[(aDF[&apos;a&apos;]&gt;10) &amp; (aDF[&apos;b&apos;]&lt;100), :] # 也可以给条件进行筛选,&amp; | ~进行逻辑运算</span><br><span class="line">    - aDF.T # 进行转置</span><br><span class="line">4. 数据读入以及基本信息以及删除</span><br><span class="line">    - pd.read_csv(path, sep=&apos;\t&apos;, index_col=&apos;&apos;/int, usecols=[...], header=0, parse_dates=[0]/[&apos;Date&apos;]) # 读文件，第一列作为日期型，日期型处理参照: http://hshsh.me/post/2016-04-12-python-pandas-notes-01/</span><br><span class="line">    - aDF.to_csv(&apos;xxx.csv&apos;, sep=&apos;\t&apos;, index=True, header=True) # 写文件</span><br><span class="line">    - aDF.describe(include=[np.float64...]) / aDF.info() # 对数据进行统计，查看缺失值</span><br><span class="line">    - aDF.shape</span><br><span class="line">    - aDF.isnull() # 判断是是否为空</span><br><span class="line">    - aDF[aDF[&apos;xxx&apos;].isnull(), :] = 10 # 对空值赋值</span><br><span class="line">    - aDF.notnull() # 查看是否有值</span><br><span class="line">    - aDF.drop([&apos;one&apos;, &apos;two&apos;], axis=0) # 对index为one和two的两行进行删除, axis=1删除列</span><br><span class="line"></span><br><span class="line">5. 数据分组聚合</span><br><span class="line">    - aDF.groupby(&apos;name&apos;, sort=False).sum() # 对DF进行聚合操作,同时对相应聚合的列进行排序,然后计算其他值的和</span><br><span class="line">    - groupbyname=aDF.groupby(&apos;name&apos;); groupbyname.groups; len(groupbyname) # 得到对应的各个组别包含的index, 并且可以获取对应的group长度</span><br><span class="line">    - aDF.groupby(&apos;name&apos;).agg([np.sum, np.mean, np.std]) # 对不同类别的数据进行各类运算, 每个name对应三列分别是分组之后np.sum, np.mean, np.std计算</span><br><span class="line">    - aDF.groupby(&apos;name&apos;).agg([&apos;sum&apos;, &apos;median&apos;, &apos;mean&apos;]) # 和上面的作用相同</span><br><span class="line">    - aDF.groupby(&apos;name&apos;).agg([&apos;a&apos;:np.sum, &apos;b&apos;:median, &apos;c&apos;:np.mean]) # 对不同列进行不同操作</span><br><span class="line">    - aDF.groupby([&apos;name&apos;, &apos;year&apos;]).sum()/mean()/median()/describe() # 多组分类</span><br><span class="line">    - aDF.groupby([&apos;name&apos;, &apos;year&apos;]).size() # 多组分类, 每一组有多少个记录</span><br><span class="line">    - 提取group类别名称以及类别对应的数据行</span><br><span class="line">        for name,group in groupbyname:</span><br><span class="line">            print(name) # 类别名称</span><br><span class="line">            print(group) # 名称对应的数据行</span><br><span class="line">        groupbyname.get_group(&apos;jason&apos;) # 可以得到对应组别的数据行,DF格式</span><br><span class="line">6. transform/apply/filter 数据变换</span><br><span class="line">    transfrom可以对分组进行变换, apply对整个DF进行分类,filter对分组进行判断</span><br><span class="line">    - aDF[&apos;Date&apos;].dt.dayofweek # 可以得到对应的日期中的第几天</span><br><span class="line">    - aDF.groupby(aDF.index.year).mean() # 可以对相应的日期型的年进行分组聚合</span><br><span class="line">    - aDF.groupby(aDF.index.year).transform(lambda x: (x-x.mean())/x.std()) # 对每一年的数据求均值以及标准差,并对每个数据进行操作,之所以没以每年为单位进行展示主要是跟function有关,因为之前的是mean之类的</span><br><span class="line">    - aDF.groupby(aDF.index.year).apply(lambda x: (x-x.mean())/x.std()) # 可以起到相同的效果</span><br><span class="line">    - aDF.loc[:,&apos;new&apos;] = aDF[&apos;xxx&apos;].apply(afunc) # 可以对xxx这一列进行操作按照afunc进行操作,然后创建新的列</span><br><span class="line">    - aSer = pd.Series([1,1,2,2,2,3,3,4,5,5]); sSer.groupby(sSer).filter(lambda x:x.sum()&gt;4) # 对ser进行过滤,留下那些和大于4的类别</span><br><span class="line"></span><br><span class="line">7. 表格的拼接与合并(concat/append/merge/join)</span><br><span class="line">    - df1.append(df2, sort=False, ignore_index=True) # 追加在行上,同时忽略原先df1和df2的index,合并为新的index</span><br><span class="line">    - df1.append([df2, df3])  # 也可以追加两个DF, 参考: https://zhuanlan.zhihu.com/p/38184619</span><br><span class="line">    - pd.concat([df1.set_index(&apos;a&apos;), df2.set_index(&apos;a&apos;)], sort=False, axis=1, join=&apos;inner&apos;) # 和上述利用merge在a字段上进行内连接的效果类似,因为concat是基于index进行连接的,merge可以不基于index,指定字段</span><br><span class="line">    - pd.concat([df1, df2, df3], keys=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;], axis=0, join=&apos;outer&apos;, sort=False) #列对齐的方式对行进行拼接,缺少值则补充为None,可以对拼接的每个df进行key的命名,axis=1的时候行对齐列拼接; join指定连接方式,outer表示外连接,inner表示内连接,sort是否对合并的数据进行排序</span><br><span class="line">    - merge # 基于某个字段进行连接,之前的append和concat都是在行上或者列上进行连接的,merge类似于SQL里面的连接,可以指定某个字段或某几个字段,体现在on上,on接list就是多个字段为key</span><br><span class="line">    - pd.merge(df1, df4, on=&apos;city&apos;, how=&apos;outer&apos;/&apos;inner&apos;/&apos;left&apos;/&apos;right&apos;) # 基于两个表中的city字段进行表格的连接,把其他的列进行combine到一起,不指定on的话就会找字段相同的那个进行拼接,注意concat是基于index进行拼接的</span><br><span class="line">    - pd.merge(df1, df2, how=&apos;inner&apos;, left_index=True, right_on=&apos;id&apos;) # 对数据进行merge,左表以index作为连接关键字,右表用id作为关键字</span><br><span class="line">8. 链家Case study流程</span><br><span class="line">    - pd.to_datetime() # 日期类型转换</span><br><span class="line">    - df.drop(droplist, inplace=True, axis=1) # 删除一些列</span><br><span class="line">    - aDF.describe(include=&apos;all&apos;) # 字符串变量也会同时统计</span><br><span class="line">    - aDF.sort_values(by = &apos;xxx&apos;).tail() # 找出更新最晚的20套,但是有可能同一天超过20套</span><br><span class="line">    - 如果对数据进行处理发现转换未果可能是因为数据有缺失,做异常处理,缺失值作为Nan</span><br><span class="line">    - aDF.nsmallest(columns=&apos;age&apos;, n=20) # 取出年龄最小的20个数据</span><br><span class="line">    - groupby().agg() 之后一般会使用reset_index() 对数据进行归置然后再进行操作,ascending=False</span><br><span class="line">    - adf.value_counts(normalize=True) # 默认是按照value进行排序的</span><br><span class="line">    - aDF.apply(lambda x: &apos;xxx&apos; in x) # 筛选出xxx在某列的值中与否,返回Ture, False，正则表达式的字符串匹配</span><br><span class="line">    - 可以定义正则表达式对文本信息进行提取</span><br><span class="line">        def get_info(s, pattern, n):</span><br><span class="line">            result = re.search(pattern, s)</span><br><span class="line">            if result:</span><br><span class="line">                return result.group(n)</span><br><span class="line">            else:</span><br><span class="line">                return &apos;&apos;</span><br><span class="line">    - .astype(int) # 转换pd类型</span><br><span class="line">    - help(pd.Series.value_counts) # 打印帮助文档</span><br></pre></td></tr></table></figure><h1 id="python绘图"><a href="#python绘图" class="headerlink" title="python绘图"></a>python绘图</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">1. pandas 绘图</span><br><span class="line">    - pd.date_range(&apos;2018/12/28&apos;, periods=10) # 产生日期格式, 以2018/12/28为起始产生以天为单位的日期时间list</span><br><span class="line">    - pandas绘图需要把横坐标作为index,之后再画图</span><br><span class="line">    - 折线图绘制需要注意各列幅度，否则数值不明显</span><br><span class="line">    - df.plot.bar() # barplot, stacked=True, 堆叠</span><br><span class="line">    - df.plot.barh() # 绘制水平的barplot</span><br><span class="line">    - df.plot.hist(bins = 20) # 绘制直方图,单维度</span><br><span class="line">    - df.plot.box() # 对每列去看一些分布outlier</span><br><span class="line">    - df.plot.area # 堆叠区域图</span><br><span class="line">    - df.plot.scatter(x=&apos;a&apos;, y=&apos;b&apos;) # 散点图</span><br><span class="line">    - df.plot.pie(subplots=True) # 绘制带图例的饼图</span><br><span class="line">    </span><br><span class="line">2. matplotlib 绘图</span><br><span class="line">    - plt.rcParams[&apos;figure.figsize&apos;] = (12,8) / plt.figure(figsize=(12,8)) # 设置画布大小</span><br><span class="line">    - ax = plt.plot(x,y,color=&apos;green&apos;, linewidth=&apos;-&apos;, marker=&apos;./*/x&apos;, label=r&apos;$y=cos&#123;x&#125;$&apos;/r&apos;$y=sin&#123;x&#125;$&apos;/r&apos;$y=\sqrt&#123;x&#125;$&apos;) # 绘图</span><br><span class="line">    - ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 去掉右边的边框</span><br><span class="line">    - ax.xaxis.set_ticks_position(&apos;bottem&apos;) # ??????????????</span><br><span class="line">    - plt.xticks([2,4,6], [r&apos;a&apos;,r&apos;b&apos;,r&apos;c&apos;]) # 设置坐标轴刻度</span><br><span class="line">    - ax.spines[&apos;bottem&apos;].set_position(&apos;data&apos;, 0)  # 设置坐标轴从0开始</span><br><span class="line">    - plt.xlim(1,3) # 设置坐标位置</span><br><span class="line">    - plt.title() # 标题</span><br><span class="line">    - plt.xlabel(r&apos;xxx&apos;, fontsize=18, labelpad=12.5) # 绘制label, r值的是不转义的,$$值的是markdown格式</span><br><span class="line">    - plt.text(0.8, 0.9, r&apos;$$&apos;, color=&apos;k&apos;, fontsize=15) # 进行注解</span><br><span class="line">    - plt.scatter([8], [8], 50, color=&apos;m&apos;) # 在某个位置,点有多大,颜色是什么</span><br><span class="line">    - plt.annotate(r&apos;$xxx$&apos;, xy=(8,8), xytext=(8.2, 8.2), fontsize=16, color=&apos;m&apos;, arrowprops=dict(arrowstyle=&apos;-&gt;&apos;, connectionstyle=&apos;arc3, rad=0.1&apos;, color=&apos;m&apos;)) # 对某个点进行注解, 进行加箭头等等</span><br><span class="line">    - plt.grid(True) # 网格线 </span><br><span class="line">    - plt.plot(x, y) # xy应为np array,如果是pandas那么可以通过values进行取值转换</span><br><span class="line">3. matplotlib 绘图case</span><br><span class="line">    - 文件解压</span><br><span class="line">        x = zipfile.ZipFile(xxx, &apos;r&apos;) # 解压文件夹</span><br><span class="line">        x.extractall(&apos;xxxdir&apos;) # 解压到某个文件夹下</span><br><span class="line">        x.close() # 记得关闭</span><br><span class="line">    - matplotlib.rc(&apos;figure&apos;, figsize=(14,7)) # 设置一下图片尺寸</span><br><span class="line">    - matplotlib.rc(&apos;font&apos;, size=14) # 设置字体</span><br><span class="line">    - matplotlib.rc(&apos;axes.spines&apos;, top=False, right=False) # 设置边线</span><br><span class="line">    - matplotlib.rc(&apos;axes&apos;, grid=False) # 设置网格</span><br><span class="line">    - matplotlib.rc(&apos;axes&apos;, facecolor=&apos;white&apos;) # 设置颜色</span><br><span class="line">    - fig,ax含义</span><br><span class="line">        fig,ax = plt.subplots() # 创建绘图对象之后对ax进行操作，相当于先fig=plt.figure()再ax=fig.add_subplot(1,1,1)</span><br><span class="line">        https://blog.csdn.net/htuhxf/article/details/82986440</span><br><span class="line">    - ax.fill_between(x, low, upper, alpha=) # 对回归进行置信度绘制</span><br><span class="line">    - ax2 = ax1.twinx() # 共享同一个x轴</span><br><span class="line">    - ax2.spines[&apos;right&apos;].set_visible(True) # 对右侧坐标轴进行设置,得到相应的图</span><br><span class="line">    - 图的使用</span><br><span class="line">        关联分析:散点图,曲线图,置信区间曲线图,双坐标曲线图</span><br><span class="line">        分布分析:堆叠直方图, 密度图</span><br><span class="line">        组间分析:柱状图(带errorbar),boxplot,这个需要多看看,</span><br><span class="line">        </span><br><span class="line"> 4. seaborn 绘图</span><br><span class="line">    - 引入seaborn的同时也要引入matplotlib因为,是底层</span><br><span class="line">    - 颜色设置</span><br><span class="line">        sns.set(color_codes=True) # 一些集成的颜色</span><br><span class="line">        https://seaborn.pydata.org/tutorial/color_palettes.html</span><br><span class="line">    - sns.displot(x, kde=True, bins=20, rug=True, fit=stats.gamma) # histgram加密度线,样本分布情况, 拟合某些分布fit</span><br><span class="line">    - sns.kdeplot # 类似于上面的,kde是每个样本用正态分布画,如果样本多,高度就高,之后再做归一化</span><br><span class="line">    - sns.jointplot(x,y,data) # 绘制带有histgram以及散点图的图，两个变量</span><br><span class="line">    - sns.pairplot(df) # 直接绘制各个列之间的散点图以及对应的histgram，多个变量</span><br><span class="line">    - scatter plot的密度版</span><br><span class="line">        with sns.axes_style(&apos;ticks&apos;):</span><br><span class="line">            sns.jointplot(x,y,data, kind=&apos;hex&apos;/&apos;kde&apos;,color=&apos;m&apos;) #相当于对点很多的时候,六角箱图就能体现出点的多少,kde是等高线,密度联合分布</span><br><span class="line">    - 多图绘制1</span><br><span class="line">        g = sns.PairGrik(df) # 各个列混合,产出n*n个格子</span><br><span class="line">        g.map_diag(sns.kdeplot) # 对角线绘制</span><br><span class="line">        g.map_offdiag(sns.kdeplot, cmap=&apos;Blues_d&apos;, n_levels=20) # 绘制对角线是kde密度图其他为等高线的图</span><br><span class="line">    - 多图绘制2</span><br><span class="line">        g = FaceGrid(row=[..],aspect=1.5, data=)</span><br><span class="line">        g.map(sns.boxplot, x, y, hue, hue_order=[], ...)</span><br><span class="line">    - 多图绘制3</span><br><span class="line">        g = sns.PairGrid(data, x_vars=[], y_vars=[], aspect=0.5, size=3.5)</span><br><span class="line">        g.map(sns.violinplot, palette=&apos;bright&apos;) # x_vars数量*y_vars数量个子图，然后每个子图都绘制violinplot</span><br><span class="line">    - 关联分析 sns.lmplot</span><br><span class="line">        · sns.lmplot(x, y, data) # 散点图+线性回归,95%置信区间,适用于连续值</span><br><span class="line">        · sns.lmplot(x, y, data, x_jitter=0.08) # 左右抖动, 点如果离得近,会把点左右抖动开,适用于离散值</span><br><span class="line">        · sns.lmplot(x, y, data, x_estimator=np.mean, ci=95, scatter_kws=&#123;&apos;s&apos;:80&#125;, order=2, robust=True) # 对于离散值还可以这样操作,先求均值和95置信区间,之后再进行拟合, scatter_kws对点进行操作,order是说对数据点进行二次方的分布,而不是线性分布,robust打开的作用是踢除异常点,然后再进行绘制图</span><br><span class="line">        · sns.lmplot(x, y, data, x_estimator=np.mean, ci=95, scatter_kws=&#123;&apos;s&apos;:80&#125;, order=1, robust=True, logistic=True) # 相当于是说对二值化的数据进行logistic回归拟合,sigmoid拟合</span><br><span class="line">        · sns.lmplot(x, y, data, hue, col, row, col_wrap, aspect=0.5) # 散点图.线性回归,95%置信区间,适用于连续值,hue进行分组类似于pandas里面的groupby, hue变量一定是个离散变量, col也可以加一个变量,可以把图分成多列,row可以多行,如果row,col以及hue都指定,那么相当于在pandas里面groupby三个内容,col_wrap用于之指定每个col中的绘图数量</span><br><span class="line">    - sns.residplot() # 残差图</span><br><span class="line">    - sns.barplot(x,y,hue,ci=None)  # 是否打开置信区间</span><br><span class="line">    - sns.stripplot(x, y, data, jitter =True) # 基于x为离散数据的,类似于散点图的boxplot</span><br><span class="line">    - sns.swarmplot(x, y, data) #  蜂群图，类似于小提琴图的点版</span><br><span class="line">    - sns.boxplot()</span><br><span class="line">    - sns.violinplot(bw) # 属于kde以及boxplot的组合，既看了单变量分布，也看了各变量之间的差异</span><br><span class="line">    - sns.violinplot(split=True， hue， inner=&apos;stick&apos;) # split将hue为两个类型的进行拼接绘制小提琴图，stick，每个样本绘制竖线</span><br><span class="line">    - sns.countplot(x, data) # 绘制离散变量数量分布，类似于value_counts()，类似于barplot但是使用的统计量是数量</span><br><span class="line">    - sns.pointplot(x, y, hue) # 查看离散变量x以及hue在离散变量y上的差别，使用均值，画点</span><br><span class="line">    - sns.factorplot(x, y, hue, col, data, kind=&apos;swarm&apos;) # 是一种泛化的绘图函数</span><br><span class="line">    - a.savefig(&apos;xx&apos;) # 进行图片存储 plt函数</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mumpy </tag>
            
            <tag> pandas </tag>
            
            <tag> seaborn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python基础知识整理</title>
      <link href="/2019/07/20/python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/"/>
      <url>/2019/07/20/python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>#Canda环境安装以及包管理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">清华镜像下载https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/</span><br><span class="line">命令行启动jupyter notebook或点击快捷图标方式启动</span><br><span class="line">conda list <span class="comment"># 查看所在环境的安装的包</span></span><br><span class="line">conda upgrade --all <span class="comment"># 对包进行更新</span></span><br><span class="line">spyder <span class="comment"># 启动anaconda中的IDE</span></span><br><span class="line">conda install numpy pandas <span class="comment"># 在某个环境下能够安装某些Python包</span></span><br><span class="line">conda install numpy=<span class="number">1.10</span> <span class="comment"># 安装特定版本的包</span></span><br><span class="line">conda remove &lt; package_name &gt; <span class="comment"># 删除包</span></span><br><span class="line">conda env list <span class="comment"># 列出当前机器上创建的虚拟环境</span></span><br><span class="line">conda create -n env1 python=<span class="number">2.7</span> <span class="comment"># 创建一个名为env1的环境然后在其中安装python2.7</span></span><br><span class="line">conda create -n env1 numpy <span class="comment"># 创建一个名为env1的环境然后在其中安装numpy</span></span><br><span class="line">source activate env1 <span class="comment"># 进入env1虚拟环境，在window上不用加source</span></span><br><span class="line">source deactivate <span class="comment"># 离开环境</span></span><br><span class="line">conda install -n py27 ipykernel <span class="comment"># 在虚拟环境py27下安装ipykernel</span></span><br><span class="line">python -m ipykernel install --user --name py27 --display-name <span class="string">"python2"</span> <span class="comment"># 在py27环境内安装ipykernel并在菜单里命名为python2</span></span><br><span class="line">conda env remove -n py27 <span class="comment"># 移除py27的虚拟环境</span></span><br><span class="line">conda install jupyter notebook <span class="comment"># 在conda环境中安装jupyter notebook</span></span><br><span class="line">%matplotlib <span class="comment"># jupyter notebook中已交互式方式实现matplotlib的绘图</span></span><br><span class="line">%matplotlib inline <span class="comment"># 不跳出，直接内嵌在web中</span></span><br></pre></td></tr></table></figure><h1 id="jupyter-notebook常用配置"><a href="#jupyter-notebook常用配置" class="headerlink" title="jupyter notebook常用配置"></a>jupyter notebook常用配置</h1><h2 id="notebook中的magic开关"><a href="#notebook中的magic开关" class="headerlink" title="notebook中的magic开关"></a>notebook中的magic开关</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">为实现一些快捷操作，提升效率。notebook中提供magic开关，能极大的优化使用notebook的体验。</span><br><span class="line">magic开关分为两大类：%line magic &amp; %%cell magic</span><br></pre></td></tr></table></figure><p>​        </p><h2 id="magic开关总览"><a href="#magic开关总览" class="headerlink" title="magic开关总览"></a>magic开关总览</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">%quickref<span class="comment"># 所有magic命令 </span></span><br><span class="line">%lsmagic<span class="comment"># 打印所有magic命令</span></span><br><span class="line"></span><br><span class="line">%config ZMQInteractiveShell.ast_node_interactivity=<span class="string">'all'</span>/<span class="string">'last_expr'</span></span><br><span class="line">%pprint <span class="comment"># 打印所有结果,保证每次执行都输出,默认只输出最后一个内容</span></span><br><span class="line">%config ZMQInteractiveShell可以查看可选择的输出类型</span><br><span class="line">或者执行这个命令保证多输出</span><br><span class="line"><span class="keyword">from</span> IPython.core.interactiveshell <span class="keyword">import</span> InteractiveShell</span><br><span class="line">InteractiveShell.ast_node_interactivity = <span class="string">'all'</span>/<span class="string">'last_expr'</span></span><br><span class="line"></span><br><span class="line"> %%整个cell magic</span><br><span class="line"> %%writefile test.py <span class="comment"># 将cell中的命令写入文件test.py</span></span><br><span class="line"> %%timeit代码计时</span><br><span class="line"> %%bash <span class="comment"># 在cell内可以执行bash命令</span></span><br><span class="line"> %%writefile xx.py <span class="comment"># 把整个cell中的内容输入到xx.py中,如果新加内容可以%%writefile -a xx.py</span></span><br><span class="line"></span><br><span class="line"> %line magic命令</span><br><span class="line"> %matplotline inline <span class="comment"># 在jupyter内打印图片</span></span><br><span class="line"> %run utils.ipynb <span class="comment"># 执行本地的utils.ipynb文件,进行配置</span></span><br><span class="line"></span><br><span class="line"> line magic和cell magic区别就在于line magic只在一行有效,cell magic在多行都有效</span><br><span class="line"> 具体参考:https://gispark.readthedocs.io/zh_CN/latest/pystart/jupyter_magics.html</span><br></pre></td></tr></table></figure><h2 id="Jupyter-notebook扩展"><a href="#Jupyter-notebook扩展" class="headerlink" title="Jupyter notebook扩展"></a>Jupyter notebook扩展</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">jupyter_contrib_nbextensions</span><br><span class="line">直接安装官网conda命令安装即可</span><br><span class="line">conda install jupyter notebook</span><br><span class="line">conda install -c conda-forge jupyter_contrib_nbextensions </span><br><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple autopep8 </span><br><span class="line">pip安装加速镜像:https://www.cnblogs.com/microman/p/6107879.html</span><br><span class="line">jupyter 使用参考资料:https://zhuanlan.zhihu.com/p/33105153</span><br><span class="line">jupyter extension 参考资料:https://zhuanlan.zhihu.com/p/52890101</span><br></pre></td></tr></table></figure><h3 id="jupyter-使用linux命令"><a href="#jupyter-使用linux命令" class="headerlink" title="jupyter 使用linux命令"></a>jupyter 使用linux命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!head -n 5 xx.txt # 直接通过jupyter行使linux命令</span><br></pre></td></tr></table></figure><h1 id="python"><a href="#python" class="headerlink" title="python"></a>python</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">python语言是一种面向对象、动态数据类型的解释型语言</span><br><span class="line">1.运行方式</span><br><span class="line">    解释运行:直接py脚本运行</span><br><span class="line">    交互运行:jupyter输入一个输出一个</span><br><span class="line"></span><br><span class="line">2.命名规则:</span><br><span class="line">    常量大写，下划线隔开单词</span><br><span class="line">    类用驼峰命名</span><br><span class="line">     del xx 删除变量xx</span><br><span class="line"></span><br><span class="line">3.操作优先级：</span><br><span class="line">    函数调用，寻址，下标</span><br><span class="line">    幂运算</span><br><span class="line">    翻转运算符</span><br><span class="line">    正负号</span><br><span class="line">    * / %</span><br><span class="line">    - + </span><br><span class="line">4.赋值</span><br><span class="line">    多重赋值:a=b=10相当于a=10,b=10</span><br><span class="line">    多元赋值 a,b,c = 1,2,3</span><br><span class="line">    交换赋值 a,b = b,a # 指针</span><br><span class="line"></span><br><span class="line">5.解包(需要拓展 参考:https://zhuanlan.zhihu.com/p/33896402?utm_source=wechat_session&amp;utm_medium=social&amp;s_r=0)</span><br><span class="line">    l1 = [1,2,3,4,5,&apos;6&apos;]; a,b,*c,d = l1</span><br><span class="line">    l1=[1,2,3,4];b=&apos;sdaad&apos;;[*l1,*b]</span><br><span class="line">    b,=[[3,4,5]] # 逗号解包 </span><br><span class="line"></span><br><span class="line">6.python进制及基本类型</span><br><span class="line">    bin() 二进制</span><br><span class="line">    oct() 八进制</span><br><span class="line">    hex() 十六进制</span><br><span class="line"></span><br><span class="line">    float(&apos;inf&apos;) 正无穷</span><br></pre></td></tr></table></figure><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#行内注释</span><br><span class="line">&quot;&quot;&quot; &quot;&quot;&quot;多行注释</span><br><span class="line">？内省，显示对象的通用信息</span><br><span class="line">？？内省，显示出大部分函数的源代码</span><br><span class="line">help()显示一个对象的帮助文档</span><br><span class="line">%timeit 魔法命令，计算语句的平均执行时间</span><br><span class="line">type（x）查看变量x的数据类型</span><br><span class="line">in(x) 将变量x的数据类型转换为整型</span><br><span class="line">isinstance(x,float)检测变量x是否为浮点型，返回一个布尔型数值</span><br></pre></td></tr></table></figure><p>##字符串</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">s=u&quot; &quot;定义Unicode字符串</span><br><span class="line">s=r&quot; &quot;定义原始字符串，避免字符串中的字符串转义，在正则表达式中经常使用到</span><br><span class="line">len(s)返回s字符串的字数</span><br><span class="line">s.lower()字母全部转为小写</span><br><span class="line">s.upper()字母全部转为大写</span><br><span class="line">s.capitalize()将字符串s中的首个字符转换为大写，其余部分转换为小写</span><br><span class="line">s.replace(&apos;k&apos;,&apos;l&apos;)使用字符&quot;l&quot;替换掉s中所有的字符&quot;k&quot;,返回结果是cooldata</span><br><span class="line">s.strip()去掉s最前面和最后面的空格</span><br><span class="line">s.split(&quot;\t&quot;)使用制表符&quot;\t&quot;分割字符串</span><br><span class="line">&apos;%s is No.%d&apos;%(s,1)格式化</span><br><span class="line">&apos;&#123;&#125; is No.&#123;&#125;&apos;.format(s,1)格式化</span><br></pre></td></tr></table></figure><h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">list()空列表</span><br><span class="line">l[-1]返回列表的最后一个元素</span><br><span class="line">l[1:3]返回列表的第二个和第三个元素</span><br><span class="line">len(l)返回列表长度</span><br><span class="line">l[::-1]将列表进行逆序排列</span><br><span class="line">l.reverse()将列表进行逆序排列</span><br><span class="line">l.insert(1,&quot;b&quot;)在指定的索引位置插入&apos;b&apos;</span><br><span class="line">l.append()在列表末尾添加元素</span><br><span class="line">l.extend()等价于&quot;1+L&quot;，将列表L中的元素依次添加到1的末尾</span><br><span class="line">l.remove()删除列表中的某个元素</span><br><span class="line">l.pop()等价于del l[]，删除列表中对应索引位置的元素</span><br><span class="line">&quot; &quot;.join([&apos;&apos;c,&apos;o&apos;,&apos;o&apos;,&apos;k&apos;])将列表中的各个字符串元素用空格连接起来并转换为字符串，返回结果为cook</span><br></pre></td></tr></table></figure><h2 id="条件判断和循环语句"><a href="#条件判断和循环语句" class="headerlink" title="条件判断和循环语句"></a>条件判断和循环语句</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">if condition1:</span><br><span class="line">statement1</span><br><span class="line">elif condition2:</span><br><span class="line">statement2</span><br><span class="line">else:</span><br><span class="line">statement3</span><br><span class="line"></span><br><span class="line">for item in sequence:</span><br><span class="line">statement</span><br><span class="line"></span><br><span class="line">while condition:</span><br><span class="line">statement</span><br><span class="line"></span><br><span class="line">range(5)产生一个从0到5且间隔为1的整数列表[0，1，2，3，4]</span><br><span class="line">break从最内层for循环或while循环中跳出</span><br><span class="line">continue继续执行下一次循环</span><br><span class="line">pass占位符</span><br></pre></td></tr></table></figure><h2 id="enumerate（）and-zip（）"><a href="#enumerate（）and-zip（）" class="headerlink" title="enumerate（）and zip（）"></a>enumerate（）and zip（）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">for i,item in emumerate(l)在每一次循环时取出索引号和相应的值分别赋给和item</span><br><span class="line">s=&#123;item**2 for item in l&#125;集合推导式，对l中的每一个元素取平方得到的新集合</span><br><span class="line">D=&#123;key:value for key,value in zip(l,k)&#125;字典推导式，通过zip()函数将两个列表l和k中的元素组成键对并形成字典</span><br><span class="line">enumerate(list/set, start=0) # 遍历元素，start指定从哪个数字作为开始下标</span><br><span class="line">c = list(zip(a,b))</span><br><span class="line">c = set(zip(a,b))</span><br><span class="line">c = dict(zip(a,b))</span><br><span class="line">list(zip(*c)) # 解压</span><br></pre></td></tr></table></figure><h2 id="推导式"><a href="#推导式" class="headerlink" title="推导式"></a>推导式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">L=[item**2 for item in l]列表推导式，对l中每一个元素取平方得到新的列表</span><br><span class="line">S=&#123;item**2 for item in l&#125;集合推导式，对l中的每一个元素取平方得到新的集合</span><br><span class="line">D=&#123;key:value for key,value in zip(l,k)&#125;字典推导式，通过zip()函数将两个列表l和k中的元素组成键值对并形成字典</span><br><span class="line">[i for i in range(30) if 1%2==0] # 取0-29之间偶数</span><br><span class="line">[function(i) for i in range(30) if 1%2==0] # function可以自己定义</span><br><span class="line">[ x**2 if x%2 ==0 else x**3 for x in range(10)] # 两个条件</span><br></pre></td></tr></table></figure><h2 id="文件读写"><a href="#文件读写" class="headerlink" title="文件读写"></a>文件读写</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">读取文件</span><br><span class="line">f=open(filename,mode)返回一个文件对象f，读文件&quot;mode=r&quot;,写文件&quot;mode=w&quot;</span><br><span class="line">f.read(size)返回包含前size个字符的字符串</span><br><span class="line">f.readline()每次读取一行，返回该行字符串</span><br><span class="line">f.readlines()返回包含每个文件内容的列表，列表的元素为文件的每一行内容所构成的字符串</span><br><span class="line">f.close()关闭文件并释放它所占用的系统资源</span><br><span class="line">with open(&quot;aa.txt&quot;,&quot;r&quot;) as f:</span><br><span class="line">content = f.readlines()</span><br><span class="line">在with主体块语句执行完后，自动关闭文件并释放占用的系统资源</span><br><span class="line">import csv</span><br><span class="line">f=open(&quot;aa.csv&quot;,&quot;r&quot;)</span><br><span class="line">csvreader = csv.reader(f)</span><br><span class="line">content_list=list(csvreader)</span><br><span class="line">读取csv文件，并把数据存储为一个嵌套列表(列表的元素扔是一个对象)content_list</span><br><span class="line"></span><br><span class="line">写入文件</span><br><span class="line">f.write(s)</span><br><span class="line">print(s,file=f)</span><br><span class="line">两种等价的方式，将字符串s写入文件对象f中</span><br></pre></td></tr></table></figure><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def sum(a,b=1)</span><br><span class="line">return a+b</span><br><span class="line">def sum(*args,**kwargs)不定长参数，*args接收包含多个位置参数的元组，**kwargs接收包含多个关键字参数的字典。</span><br><span class="line">obj.methodname一个方法是一个&quot;属于&quot;对象并被命名为obj.methodname的函数</span><br></pre></td></tr></table></figure><h2 id="map-和lambda"><a href="#map-和lambda" class="headerlink" title="map()和lambda()"></a>map()和lambda()</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">map(func,sequence)将函数依次作用在序列的每个元素上，把结果作为一个新的序列返回。</span><br><span class="line">lambda a,b:a+b匿名函数，正常函数定义的语法糖</span><br><span class="line">lambda [参数列表]:表达式</span><br><span class="line">例如: sum = lambda x,y:x+y # 可以有多个参数,返回只能有一个式子</span><br><span class="line">可以作为一个函数的参数赋给另外一个参数</span><br><span class="line">当然普通函数也可以作为参数传入</span><br><span class="line">a = [&#123;&apos;name&apos;:&apos;ss&apos;,&apos;age&apos;:10&#125;,&#123;&apos;name&apos;:&apos;yy&apos;,&apos;age&apos;:7&#125;,&#123;&apos;name&apos;:&apos;zz&apos;,&apos;age&apos;:15&#125;] # 将匿名函数作为参数传入第三方函数的参数</span><br><span class="line">a.sort(key=lambda x:x[&apos;age&apos;]) # sort方法需要传入一个key，这个key可以作为排序依据，lambda可以提取每个元素，并对元素排列</span><br><span class="line">a.sort(key=lambda x:x[&apos;age&apos;]， reverse=True) # 降序</span><br></pre></td></tr></table></figure><h2 id="包模块"><a href="#包模块" class="headerlink" title="包模块"></a>包模块</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">包是一个文件夹</span><br><span class="line">模块是不同的python文件</span><br><span class="line">import package.module.func()</span><br><span class="line">import package1.module1, package2.module1 # 多个模块调用</span><br><span class="line">import package1.module1 as p1m1 # 对模块进行重命名使用</span><br><span class="line">from package.module import func1 # 调用某个包某个模块的某个函数</span><br><span class="line">import sys;sys.path # 搜索模块路径，包含当前文件夹</span><br><span class="line">package.module.__file__ # 可以确定当前的模块所在的路径</span><br><span class="line">__init__.py  #在包被加载的时候，会被执行。在一个包下面可以有也可以没有__init__.py</span><br><span class="line">from package import * # 引用包下面所有的模块都加载，自动搜索是不会发生的，</span><br><span class="line">    需要我们在__init__.py下进行定义才可以实现,定义的内容是__all__=[&quot;module1&quot;,&quot;module2&quot;],</span><br><span class="line">    将package下的module1和module2都加载进来</span><br><span class="line">    如果想直接加载某个函数，在__init__.py里面加入from .module1 import func1, __all__=[&quot;func1&quot;]</span><br><span class="line">    这样修改完之后，可以直接from package import *，然后直接调用func1即可，不用带package.module</span><br><span class="line">    restart kernel</span><br><span class="line">如果想要直接引用包,如：import package,这样的话，需要一定要有__init__.py，否则会在打印package.__file__的时候报错。</span><br><span class="line">注意import package和from package import *效果相同</span><br></pre></td></tr></table></figure><h2 id="Counter"><a href="#Counter" class="headerlink" title="Counter"></a>Counter</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">字典的继承类</span><br><span class="line">set dict list tuple 作为key</span><br><span class="line">from collection import Counter # 导入</span><br><span class="line">cnt = Counter()</span><br><span class="line">for i in [1,1,2,2,2,3]:</span><br><span class="line">    cnt[i] += 1</span><br><span class="line">print cnt</span><br><span class="line">如果用key的话会报错先做第一次初始化才行</span><br><span class="line">cnt2 = Counter(alist)  #可以统计每个元素出现的次数（字符串，set,list,）</span><br><span class="line">Counter(cat=4,dogs=8,abc=-1) # 初始化counter次数，或者用dictionary构建</span><br><span class="line">Counter(&#123;&apos;cat&apos;:4,&apos;dogs&apos;:8,&apos;abc&apos;:-1&#125;)</span><br><span class="line">Counter返回一个字典，如果缺失的话会返回0</span><br><span class="line">del cnt2[&apos;xx&apos;]</span><br><span class="line">.values()</span><br><span class="line">list(cnt),set(cnt),dict(cnt) # 前两个只返回key</span><br><span class="line">cnt.most_common()[0] # 对出现次数排序</span><br><span class="line">cnt.clear()</span><br><span class="line">cnt1+cnt2 # 对于key相同的value做加法，如果为0则不保留</span><br><span class="line">cnt1-cnt2 # 对于key相同的value做减法</span><br><span class="line">&amp; # 求key相同value的最小值</span><br><span class="line">| # 求key相同value的最大值</span><br></pre></td></tr></table></figure><h2 id="random"><a href="#random" class="headerlink" title="random"></a>random</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import random #引入</span><br><span class="line">random.random() # 0-1</span><br><span class="line">random.uniform(1,10) # 包含1，10的浮点数</span><br><span class="line">random.randint(1,10)  # 包含1，10的整数</span><br><span class="line">random.randrange(0,20,3) # 0-20能被3整除的数</span><br><span class="line">random.choice([1,2,3]) # 随机取元素</span><br><span class="line">random.choice(&quot;qwdwq&quot;) # 随机取元素</span><br><span class="line">random.shuffle([12,3,1,4,2,3]) # 混洗</span><br><span class="line">random.sample([1,2,3,4,5], 3) # 从前面的list中选3个</span><br></pre></td></tr></table></figure><h2 id="编码和解码"><a href="#编码和解码" class="headerlink" title="编码和解码"></a>编码和解码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import chardet</span><br><span class="line">chardet.detect(s)</span><br><span class="line">检测字符串的编码方式</span><br></pre></td></tr></table></figure><h2 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">try:</span><br><span class="line">statement</span><br><span class="line">except:</span><br><span class="line">pass</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">statement</span><br><span class="line">except Exception as e:</span><br><span class="line">print(e)</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">statement</span><br><span class="line">except (Exception1,Exception2) as e:</span><br><span class="line">statement1</span><br><span class="line">else:</span><br><span class="line">statement2</span><br><span class="line">finally:</span><br><span class="line">statement3#无论对错都运行</span><br><span class="line">抛出异常</span><br><span class="line">raise Exception(&apos;Oops！&apos;)</span><br><span class="line">assert statement,e#若继续运行代码、否则抛出e的错误提示信息</span><br></pre></td></tr></table></figure><h2 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">raw_s=r&apos;\d&#123;17&#125;[\d|x]|\d&#123;15&#125;&apos;</span><br><span class="line">pattern=re.compile(raw_s)</span><br><span class="line">re.search(pattern,s)</span><br><span class="line">用于匹配身份证号</span><br><span class="line">首先使用原始字符串定义正则表达式；然后编译原始字符为正则表达式Pattern对象；最后对整个字符串s进行模式搜索，如果模式匹配，则返回MatchObject的实例，如果该字符串没有模式匹配，则返回弄none</span><br><span class="line">re.search(r&apos;\d&#123;17&#125;[\d|x]|\d&#123;15&#125;&apos;,s)将Pattern编译过程与搜索过程合二为一</span><br><span class="line">re.match(pattern,s)从字符串s的起始位置匹配一个模式，如果匹配不成功返回None</span><br><span class="line">re.findall(pattern,s)返回一个包含所有满足条件的字串列表</span><br><span class="line">re.sub(pattern,repl,s)使用替换字符串repl替换匹配到的字符串</span><br><span class="line">re.split(pattern,s)利用满足匹配模式的字符串将字符串s分隔开，并返回一个列表</span><br><span class="line"></span><br><span class="line">1.正则表达式的match与search区别</span><br><span class="line">    https://segmentfault.com/a/1190000006736033</span><br><span class="line"></span><br><span class="line">2.贪婪匹配与非贪婪匹配的区别</span><br><span class="line">    https://segmentfault.com/a/1190000002640851</span><br><span class="line">    https://blog.csdn.net/lxcnn/article/details/4756030</span><br><span class="line"></span><br><span class="line">3. 练习网站</span><br><span class="line">    https://alf.nu/RegexGolf 一个正则表达式练习网站</span><br><span class="line">    https://regexr.com/ 验证网站</span><br><span class="line"></span><br><span class="line">4. 单字符匹配</span><br><span class="line">    . # 匹配出点换行符之外的任意字符</span><br><span class="line">    \. # 匹配单个.字符</span><br><span class="line">    [abd] # 匹配a/b/d单个字符</span><br><span class="line">    \d # 匹配数字, 相当于[1,2,3,4,5,6,7,8,9]</span><br><span class="line">    \D # 所有非字符</span><br><span class="line">    \s # 空白符,空格 tab等等</span><br><span class="line">    \S # 所有非空格</span><br><span class="line">    \w # a-z,A-Z,0-9,_</span><br><span class="line">    \W # 除了 a-z,A-Z,0-9</span><br><span class="line"></span><br><span class="line">5. 数量词用来多匹配</span><br><span class="line">    m&#123;2&#125; # 表示匹配两个m</span><br><span class="line">    m&#123;2,4&#125; # 表示匹配2/3/4个m,贪婪匹配</span><br><span class="line">    m* # 0个或者更多个,贪婪匹配</span><br><span class="line">    m+ # 1个或者更多个,贪婪匹配</span><br><span class="line">    m? # 0个或者1个</span><br><span class="line">    ^xx # 文本开头是xx进行匹配</span><br><span class="line">    xxx$ # 对结尾进行匹配</span><br><span class="line">    (re)su(lt) # group</span><br><span class="line"></span><br><span class="line">6. python中的正则表达式步骤</span><br><span class="line">    写一个文本pattern</span><br><span class="line">    进行匹配</span><br><span class="line">    对匹配的文本进行后续操作</span><br><span class="line">    例子:</span><br><span class="line">        import re</span><br><span class="line">        pattern = re.compile(r&apos;hello.*\!&apos;) # hello后面有若干个字符串直到有!</span><br><span class="line">        match = pattern.match(&apos;hello, xxx! how are you?&apos;) # 对文本进行匹配</span><br><span class="line">        if match: # 是否匹配</span><br><span class="line">            print match.group() # 如果匹配上了返回相应的匹配到的部分</span><br><span class="line"></span><br><span class="line">7. 使用实例</span><br><span class="line">    import re</span><br><span class="line">    re.compile(r&quot;&quot;&quot;</span><br><span class="line">    \d+ # 数字部分</span><br><span class="line">    \. # 小数点</span><br><span class="line">    \d # 小数部分</span><br><span class="line">    &quot;&quot;&quot;, re.X)</span><br><span class="line">    这种模式下可以写注解</span><br><span class="line">    re.compile(r&quot;\d+\.\d&quot;) # 与这个模式结果一样</span><br><span class="line"></span><br><span class="line">8.一些命令</span><br><span class="line">    match # 一次匹配结果,从头匹配,开头没有就匹配不上了</span><br><span class="line">    search # 所有匹配到的</span><br><span class="line">    findall # search返回第一个匹配的结果,findall会返回所有的结果</span><br><span class="line">    m=re.match()</span><br><span class="line">    m.string # 匹配的字符串</span><br><span class="line">    m.group(1,2) # 匹配1和2处字符串</span><br><span class="line"></span><br><span class="line">9. 替换和分割</span><br><span class="line">     split也可以使用正则表达式进行分割</span><br><span class="line">        p = re.compile(r&apos;\d+&apos;)</span><br><span class="line">        p.split(&apos;adwdwad1dawwd23dwadw&apos;) # 字符串复杂分割</span><br><span class="line">    sub # 用来替换</span><br><span class="line">        p = re.compile(r&apos;(\w+) (\w+)&apos;)</span><br><span class="line">        p.sub(r&apos;\2 \1&apos;, s) # 匹配字符串并且在匹配到的字符串处进行前后颠倒 </span><br><span class="line">    subn # 和sub类似,只不过除了返回替换的远足之外,还返回相应的替换次数,可以p.subn(afunc, s), afunc可以自己定义</span><br></pre></td></tr></table></figure><h2 id="日期处理"><a href="#日期处理" class="headerlink" title="日期处理"></a>日期处理</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from datetime import datetimes</span><br><span class="line">format=&quot;%Y-%m-%d %H:%M:%S&quot;指定日期格式</span><br><span class="line">date_s=datetime.striptime(s,format)</span><br><span class="line">date_s.year</span><br><span class="line">date_s.month</span><br><span class="line">date_s.now()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
