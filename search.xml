<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>扩展内容</title>
      <link href="/2020/06/09/%E6%89%A9%E5%B1%95%E5%86%85%E5%AE%B9/"/>
      <url>/2020/06/09/%E6%89%A9%E5%B1%95%E5%86%85%E5%AE%B9/</url>
      
        <content type="html"><![CDATA[<h3 id="结构化预测"><a href="#结构化预测" class="headerlink" title="结构化预测"></a><a href="https://shimo.im/docs/t9hjVGDx33vyTDjD" target="_blank" rel="noopener">结构化预测</a></h3><ul><li>隐马尔科夫模型 <a href="http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf" target="_blank" rel="noopener">http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf</a></li><li>最大熵与词性标注</li><li>条件随机场</li></ul><h3 id="中文分词-Chinese-Word-Segmentation"><a href="#中文分词-Chinese-Word-Segmentation" class="headerlink" title="中文分词 Chinese Word Segmentation"></a>中文分词 Chinese Word Segmentation</h3><ul><li><a href="https://www.aclweb.org/anthology/D18-1529.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D18-1529.pdf</a></li></ul><h3 id="Parsing与Recursive-Neural-Networks"><a href="#Parsing与Recursive-Neural-Networks" class="headerlink" title="Parsing与Recursive Neural Networks"></a>Parsing与Recursive Neural Networks</h3><ul><li><a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture18-TreeRNNs.pdf" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture18-TreeRNNs.pdf</a></li><li><a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture05-dep-parsing.pdf" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture05-dep-parsing.pdf</a></li></ul><h3 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h3><ul><li>GCN: <a href="https://arxiv.org/pdf/1609.02907.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1609.02907.pdf</a></li><li>GCN for relational graph: <a href="https://arxiv.org/pdf/1703.06103.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1703.06103.pdf</a></li></ul><h3 id="Data-to-Text-文本生成"><a href="#Data-to-Text-文本生成" class="headerlink" title="Data to Text 文本生成"></a>Data to Text 文本生成</h3><ul><li>GCN生成文本 <a href="https://arxiv.org/pdf/1810.09995v1.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1810.09995v1.pdf</a></li></ul><h3 id="知识图谱相关问题"><a href="#知识图谱相关问题" class="headerlink" title="知识图谱相关问题"></a><a href="https://shimo.im/docs/9pwCHPwXxcGHRrxh" target="_blank" rel="noopener">知识图谱相关问题</a></h3>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 中文分词 </tag>
            
            <tag> 图神经网络 </tag>
            
            <tag> Recursive Neural Networks </tag>
            
            <tag> Parsing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>XLNet</title>
      <link href="/2020/06/09/XLNet/"/>
      <url>/2020/06/09/XLNet/</url>
      
        <content type="html"><![CDATA[<h2 id="Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context"><a href="#Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context" class="headerlink" title="Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"></a>Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</h2><p><a href="https://arxiv.org/pdf/1901.02860.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.02860.pdf</a></p><p>相较于传统transformer decoder，引入两个新模块</p><ul><li>segment-level recurrence mechanism</li></ul><p><img src="https://uploader.shimo.im/f/DpNe30kuahkbOeW5.png!thumbnail" alt="img"></p><ul><li><p>a novel positional encoding scheme</p></li><li><p>考虑我们在attention机制中如何使用positional encoding</p></li></ul><p>(E_{x_i}^T+U_i^T)W_q^TW_kE_{x_j}U_j</p><p><img src="https://uploader.shimo.im/f/5zNU9yZQtQMClNiY.png!thumbnail" alt="img"></p><ul><li><p>R他们采用的是transformer当中的positional encoding</p></li><li><p>u和v是需要训练的模型参数</p></li></ul><p>最终Transformer XL模型</p><p><img src="https://uploader.shimo.im/f/Nm1uk49MIjUys1aK.png!thumbnail" alt="img"></p><p>代码</p><p><a href="https://github.com/kimiyoung/transformer-xl" target="_blank" rel="noopener">https://github.com/kimiyoung/transformer-xl</a></p><h2 id="XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding"><a href="#XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding" class="headerlink" title="XLNet: Generalized Autoregressive Pretraining for Language Understanding"></a>XLNet: Generalized Autoregressive Pretraining for Language Understanding</h2><p><a href="https://arxiv.org/pdf/1906.08237.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1906.08237.pdf</a></p><p>背景知识</p><ul><li><p>自回归语言模型（Autoregressive Language Model）：采用从左往右或从右往左的语言模型，根据上文预测下文。</p></li><li><p>缺点：只利用了预测单词左边或右边的信息，无法同时利用两边的信息。ELMo在一定程度上解决了这个问题。</p></li><li><p><img src="https://uploader.shimo.im/f/cpfGbeRfzf8c1ga8.png!thumbnail" alt="img"></p></li><li><p>自编码模型（Denoising Auto Encoder, DAE）：在输入中随机mask一些单词，利用上下文来预测被mask掉的单词。BERT采用了这一思路。</p></li><li><p><img src="https://uploader.shimo.im/f/za1FnG3zHdsbm5gD.png!thumbnail" alt="img"></p></li></ul><p>两个模型的问题</p><p><img src="https://uploader.shimo.im/f/A1rO6rAR1nAQqqvu.png!thumbnail" alt="img"></p><p>XLNet的目标是融合以上两种模型的优点，解决它们各自存在的问题。</p><p>XLNet模型：Permutation Language Modeling</p><p><img src="https://uploader.shimo.im/f/LdaKeEgG8XwH3iNj.png!thumbnail" alt="img"></p><p>Two-Stream Self-Attention</p><p><img src="https://uploader.shimo.im/f/TdQVsxOeYMoakBW0.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/iLMqF1WinQI6wOsW.png!thumbnail" alt="img"></p><p>参考资料</p><p><a href="https://zhuanlan.zhihu.com/p/70257427" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/70257427</a></p><p>代码</p><p><a href="https://github.com/zihangdai/xlnet" target="_blank" rel="noopener">https://github.com/zihangdai/xlnet</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> word-embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch</title>
      <link href="/2020/06/08/PyTorch/"/>
      <url>/2020/06/08/PyTorch/</url>
      
        <content type="html"><![CDATA[<h1 id="什么是PyTorch"><a href="#什么是PyTorch" class="headerlink" title="什么是PyTorch?"></a>什么是PyTorch?</h1><p>PyTorch是一个基于Python的科学计算库，它有以下特点:</p><ul><li>类似于NumPy，但是它可以使用GPU</li><li>可以用它定义深度学习模型，可以灵活地进行深度学习模型的训练和使用</li></ul><h2 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h2><p>Tensor类似与NumPy的ndarray，唯一的区别是Tensor可以在GPU上加速运算。</p><p>In [2]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br></pre></td></tr></table></figure><p>构造一个未初始化的5x3矩阵:</p><p>In [4]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(5,3)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>Out[4]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.0000e+00, -8.5899e+09,  0.0000e+00],</span><br><span class="line">        [-8.5899e+09,         nan,  0.0000e+00],</span><br><span class="line">        [ 2.7002e-06,  1.8119e+02,  1.2141e+01],</span><br><span class="line">        [ 7.8503e+02,  6.7504e-07,  6.5200e-10],</span><br><span class="line">        [ 2.9537e-06,  1.7186e-04,         nan]])</span><br></pre></td></tr></table></figure><p>构建一个随机初始化的矩阵:</p><p>In [5]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(5,3)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>Out[5]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.4628, 0.7432, 0.9785],</span><br><span class="line">        [0.2068, 0.4441, 0.9176],</span><br><span class="line">        [0.1027, 0.5275, 0.3884],</span><br><span class="line">        [0.9380, 0.2113, 0.2839],</span><br><span class="line">        [0.0094, 0.4001, 0.6483]])</span><br></pre></td></tr></table></figure><p>构建一个全部为0，类型为long的矩阵:</p><p>In [8]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(5,3,dtype=torch.long)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>Out[8]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0, 0, 0],</span><br><span class="line">        [0, 0, 0],</span><br><span class="line">        [0, 0, 0],</span><br><span class="line">        [0, 0, 0],</span><br><span class="line">        [0, 0, 0]])</span><br></pre></td></tr></table></figure><p>In [11]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(5,3).long()</span><br><span class="line">x.dtype</span><br></pre></td></tr></table></figure><p>Out[11]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.int64</span><br></pre></td></tr></table></figure><p>从数据直接直接构建tensor:</p><p>In [12]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([5.5,3])</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>Out[12]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([5.5000, 3.0000])</span><br></pre></td></tr></table></figure><p>也可以从一个已有的tensor构建一个tensor。这些方法会重用原来tensor的特征，例如，数据类型，除非提供新的数据。</p><p>In [16]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = x.new_ones(5,3, dtype=torch.double)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>Out[16]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p>In [17]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn_like(x, dtype=torch.float)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>Out[17]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.2411, -0.3961, -0.9206],</span><br><span class="line">        [-0.0508,  0.2653,  0.4685],</span><br><span class="line">        [ 0.5368, -0.3606, -0.0073],</span><br><span class="line">        [ 0.3383,  0.6826,  1.7368],</span><br><span class="line">        [-0.0811, -0.6957, -0.4566]])</span><br></pre></td></tr></table></figure><p>得到tensor的形状:</p><p>In [20]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure><p>Out[20]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([5, 3])</span><br></pre></td></tr></table></figure><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><p><code>torch.Size</code> 返回的是一个tuple</p><p>Operations</p><p>有很多种tensor运算。我们先介绍加法运算。</p><p>In [21]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = torch.rand(5,3)</span><br><span class="line">y</span><br></pre></td></tr></table></figure><p>Out[21]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.9456, 0.3996, 0.1981],</span><br><span class="line">        [0.8728, 0.7097, 0.3721],</span><br><span class="line">        [0.7489, 0.9502, 0.6241],</span><br><span class="line">        [0.5176, 0.0200, 0.5130],</span><br><span class="line">        [0.3552, 0.2710, 0.7392]])</span><br></pre></td></tr></table></figure><p>In [23]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x + y</span><br></pre></td></tr></table></figure><p>Out[23]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.1866,  0.0035, -0.7225],</span><br><span class="line">        [ 0.8220,  0.9750,  0.8406],</span><br><span class="line">        [ 1.2857,  0.5896,  0.6168],</span><br><span class="line">        [ 0.8559,  0.7026,  2.2498],</span><br><span class="line">        [ 0.2741, -0.4248,  0.2826]])</span><br></pre></td></tr></table></figure><p>另一种着加法的写法</p><p>In [24]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.add(x, y)</span><br></pre></td></tr></table></figure><p>Out[24]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.1866,  0.0035, -0.7225],</span><br><span class="line">        [ 0.8220,  0.9750,  0.8406],</span><br><span class="line">        [ 1.2857,  0.5896,  0.6168],</span><br><span class="line">        [ 0.8559,  0.7026,  2.2498],</span><br><span class="line">        [ 0.2741, -0.4248,  0.2826]])</span><br></pre></td></tr></table></figure><p>加法：把输出作为一个变量</p><p>In [26]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">result = torch.empty(5,3)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line"># result = x + y</span><br><span class="line">result</span><br></pre></td></tr></table></figure><p>Out[26]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.1866,  0.0035, -0.7225],</span><br><span class="line">        [ 0.8220,  0.9750,  0.8406],</span><br><span class="line">        [ 1.2857,  0.5896,  0.6168],</span><br><span class="line">        [ 0.8559,  0.7026,  2.2498],</span><br><span class="line">        [ 0.2741, -0.4248,  0.2826]])</span><br></pre></td></tr></table></figure><p>in-place加法</p><p>In [28]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.add_(x)</span><br><span class="line">y</span><br></pre></td></tr></table></figure><p>Out[28]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.1866,  0.0035, -0.7225],</span><br><span class="line">        [ 0.8220,  0.9750,  0.8406],</span><br><span class="line">        [ 1.2857,  0.5896,  0.6168],</span><br><span class="line">        [ 0.8559,  0.7026,  2.2498],</span><br><span class="line">        [ 0.2741, -0.4248,  0.2826]])</span><br></pre></td></tr></table></figure><h4 id="注意-1"><a href="#注意-1" class="headerlink" title="注意"></a>注意</h4><p>任何in-place的运算都会以<code>_</code>结尾。 举例来说：<code>x.copy_(y)</code>, <code>x.t_()</code>, 会改变 <code>x</code>。</p><p>各种类似NumPy的indexing都可以在PyTorch tensor上面使用。</p><p>In [31]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[1:, 1:]</span><br></pre></td></tr></table></figure><p>Out[31]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.2653,  0.4685],</span><br><span class="line">        [-0.3606, -0.0073],</span><br><span class="line">        [ 0.6826,  1.7368],</span><br><span class="line">        [-0.6957, -0.4566]])</span><br></pre></td></tr></table></figure><p>Resizing: 如果你希望resize/reshape一个tensor，可以使用<code>torch.view</code>：</p><p>In [39]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(4,4)</span><br><span class="line">y = x.view(16)</span><br><span class="line">z = x.view(-1,8)</span><br><span class="line">z</span><br></pre></td></tr></table></figure><p>Out[39]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.5683,  1.3885, -2.0829, -0.7613, -1.9115,  0.3732, -0.2055, -1.2300],</span><br><span class="line">        [-0.2612, -0.4682, -1.0596,  0.7447,  0.7603, -0.4281,  0.5495,  0.1025]])</span><br></pre></td></tr></table></figure><p>如果你有一个只有一个元素的tensor，使用<code>.item()</code>方法可以把里面的value变成Python数值。</p><p>In [40]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(1)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>Out[40]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([-1.1493])</span><br></pre></td></tr></table></figure><p>In [44]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.item()</span><br></pre></td></tr></table></figure><p>Out[44]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-1.1493233442306519</span><br></pre></td></tr></table></figure><p>In [48]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z.transpose(1,0)</span><br></pre></td></tr></table></figure><p>Out[48]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.5683, -0.2612],</span><br><span class="line">        [ 1.3885, -0.4682],</span><br><span class="line">        [-2.0829, -1.0596],</span><br><span class="line">        [-0.7613,  0.7447],</span><br><span class="line">        [-1.9115,  0.7603],</span><br><span class="line">        [ 0.3732, -0.4281],</span><br><span class="line">        [-0.2055,  0.5495],</span><br><span class="line">        [-1.2300,  0.1025]])</span><br></pre></td></tr></table></figure><p><strong>更多阅读</strong></p><p>各种Tensor operations, 包括transposing, indexing, slicing, mathematical operations, linear algebra, random numbers在<code>&lt;https://pytorch.org/docs/torch&gt;</code>.</p><h2 id="Numpy和Tensor之间的转化"><a href="#Numpy和Tensor之间的转化" class="headerlink" title="Numpy和Tensor之间的转化"></a>Numpy和Tensor之间的转化</h2><p>在Torch Tensor和NumPy array之间相互转化非常容易。</p><p>Torch Tensor和NumPy array会共享内存，所以改变其中一项也会改变另一项。</p><p>把Torch Tensor转变成NumPy Array</p><p>In [49]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(5)</span><br><span class="line">a</span><br></pre></td></tr></table></figure><p>Out[49]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1., 1., 1., 1., 1.])</span><br></pre></td></tr></table></figure><p>In [50]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = a.numpy()</span><br><span class="line">b</span><br></pre></td></tr></table></figure><p>Out[50]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([1., 1., 1., 1., 1.], dtype=float32)</span><br></pre></td></tr></table></figure><p>改变numpy array里面的值。</p><p>In [51]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b[1] = 2</span><br><span class="line">b</span><br></pre></td></tr></table></figure><p>Out[51]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([1., 2., 1., 1., 1.], dtype=float32)</span><br></pre></td></tr></table></figure><p>In [52]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a</span><br></pre></td></tr></table></figure><p>Out[52]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1., 2., 1., 1., 1.])</span><br></pre></td></tr></table></figure><p>把NumPy ndarray转成Torch Tensor</p><p>In [54]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br></pre></td></tr></table></figure><p>In [55]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = np.ones(5)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, 1, out=a)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2. 2. 2. 2. 2.]</span><br></pre></td></tr></table></figure><p>In [56]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b</span><br></pre></td></tr></table></figure><p>Out[56]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p>所有CPU上的Tensor都支持转成numpy或者从numpy转成Tensor。</p><h2 id="CUDA-Tensors"><a href="#CUDA-Tensors" class="headerlink" title="CUDA Tensors"></a>CUDA Tensors</h2><p>使用<code>.to</code>方法，Tensor可以被移动到别的device上。</p><p>In [60]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line">    y = torch.ones_like(x, device=device)</span><br><span class="line">    x = x.to(device)</span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))</span><br></pre></td></tr></table></figure><p>Out[60]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">False</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.to(<span class="string">"cpu"</span>).data.numpy()</span><br><span class="line">y.cpu().data.numpy()</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = model.cuda()</span><br></pre></td></tr></table></figure><h2 id="热身-用numpy实现两层神经网络"><a href="#热身-用numpy实现两层神经网络" class="headerlink" title="热身: 用numpy实现两层神经网络"></a>热身: 用numpy实现两层神经网络</h2><p>一个全连接ReLU神经网络，一个隐藏层，没有bias。用来从x预测y，使用L2 Loss。</p><ul><li>ℎ=𝑊1𝑋h=W1X</li><li>𝑎=𝑚𝑎𝑥(0,ℎ)a=max(0,h)</li><li>𝑦ℎ𝑎𝑡=𝑊2𝑎yhat=W2a</li></ul><p>这一实现完全使用numpy来计算前向神经网络，loss，和反向传播。</p><ul><li>forward pass</li><li>loss</li><li>backward pass</li></ul><p>numpy ndarray是一个普通的n维array。它不知道任何关于深度学习或者梯度(gradient)的知识，也不知道计算图(computation graph)，只是一种用来计算数学运算的数据结构。</p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机创建一些训练数据</span></span><br><span class="line">x = np.random.randn(N, D_in)</span><br><span class="line">y = np.random.randn(N, D_out)</span><br><span class="line"></span><br><span class="line">w1 = np.random.randn(D_in, H)</span><br><span class="line">w2 = np.random.randn(H, D_out)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    h = x.dot(w1) <span class="comment"># N * H</span></span><br><span class="line">    h_relu = np.maximum(h, <span class="number">0</span>) <span class="comment"># N * H</span></span><br><span class="line">    y_pred = h_relu.dot(w2) <span class="comment"># N * D_out</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute loss</span></span><br><span class="line">    loss = np.square(y_pred - y).sum()</span><br><span class="line">    print(it, loss)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    <span class="comment"># compute the gradient</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</span><br><span class="line">    grad_h = grad_h_relu.copy()</span><br><span class="line">    grad_h[h&lt;<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.T.dot(grad_h)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update weights of w1 and w2</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure><h2 id="PyTorch-Tensors"><a href="#PyTorch-Tensors" class="headerlink" title="PyTorch: Tensors"></a>PyTorch: Tensors</h2><p>这次我们使用PyTorch tensors来创建前向神经网络，计算损失，以及反向传播。</p><p>一个PyTorch Tensor很像一个numpy的ndarray。但是它和numpy ndarray最大的区别是，PyTorch Tensor可以在CPU或者GPU上运算。如果想要在GPU上运算，就需要把Tensor换成cuda类型。</p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机创建一些训练数据</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line">w1 = torch.randn(D_in, H)</span><br><span class="line">w2 = torch.randn(H, D_out)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    h = x.mm(w1) <span class="comment"># N * H</span></span><br><span class="line">    h_relu = h.clamp(min=<span class="number">0</span>) <span class="comment"># N * H</span></span><br><span class="line">    y_pred = h_relu.mm(w2) <span class="comment"># N * D_out</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum().item()</span><br><span class="line">    print(it, loss)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    <span class="comment"># compute the gradient</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.t().mm(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.mm(w2.t())</span><br><span class="line">    grad_h = grad_h_relu.clone()</span><br><span class="line">    grad_h[h&lt;<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.t().mm(grad_h)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update weights of w1 and w2</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure><p>简单的autograd</p><p>In [72]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">1.</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w = torch.tensor(<span class="number">2.</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = w*x + b <span class="comment"># y = 2*1+3</span></span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># dy / dw = x</span></span><br><span class="line">print(w.grad)</span><br><span class="line">print(x.grad)</span><br><span class="line">print(b.grad)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor(1.)</span><br><span class="line">tensor(2.)</span><br><span class="line">tensor(1.)</span><br></pre></td></tr></table></figure><h2 id="PyTorch-Tensor和autograd"><a href="#PyTorch-Tensor和autograd" class="headerlink" title="PyTorch: Tensor和autograd"></a>PyTorch: Tensor和autograd</h2><p>PyTorch的一个重要功能就是autograd，也就是说只要定义了forward pass(前向神经网络)，计算了loss之后，PyTorch可以自动求导计算模型所有参数的梯度。</p><p>一个PyTorch的Tensor表示计算图中的一个节点。如果<code>x</code>是一个Tensor并且<code>x.requires_grad=True</code>那么<code>x.grad</code>是另一个储存着<code>x</code>当前梯度(相对于一个scalar，常常是loss)的向量。</p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机创建一些训练数据</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line">w1 = torch.randn(D_in, H, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn(H, D_out, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = x.mm(w1).clamp(min=<span class="number">0</span>).mm(w2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum() <span class="comment"># computation graph</span></span><br><span class="line">    print(it, loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update weights of w1 and w2</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        w1 -= learning_rate * w1.grad</span><br><span class="line">        w2 -= learning_rate * w2.grad</span><br><span class="line">        w1.grad.zero_()</span><br><span class="line">        w2.grad.zero_()</span><br></pre></td></tr></table></figure><h2 id="PyTorch-nn"><a href="#PyTorch-nn" class="headerlink" title="PyTorch: nn"></a>PyTorch: nn</h2><p>这次我们使用PyTorch中nn这个库来构建网络。 用PyTorch autograd来构建计算图和计算gradients， 然后PyTorch会帮我们自动计算gradient。</p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机创建一些训练数据</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H, bias=<span class="literal">False</span>), <span class="comment"># w_1 * x + b_1</span></span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out, bias=<span class="literal">False</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">torch.nn.init.normal_(model[<span class="number">0</span>].weight)</span><br><span class="line">torch.nn.init.normal_(model[<span class="number">2</span>].weight)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model = model.cuda()</span></span><br><span class="line"></span><br><span class="line">loss_fn = nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(x) <span class="comment"># model.forward() </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y) <span class="comment"># computation graph</span></span><br><span class="line">    print(it, loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update weights of w1 and w2</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters(): <span class="comment"># param (tensor, grad)</span></span><br><span class="line">            param -= learning_rate * param.grad</span><br><span class="line">            </span><br><span class="line">    model.zero_grad()</span><br></pre></td></tr></table></figure><p>In [113]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model[0].weight</span><br></pre></td></tr></table></figure><p>Out[113]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[-0.0218,  0.0212,  0.0243,  ...,  0.0230,  0.0247,  0.0168],</span><br><span class="line">        [-0.0144,  0.0177, -0.0221,  ...,  0.0161,  0.0098, -0.0172],</span><br><span class="line">        [ 0.0086, -0.0122, -0.0298,  ..., -0.0236, -0.0187,  0.0295],</span><br><span class="line">        ...,</span><br><span class="line">        [ 0.0266, -0.0008, -0.0141,  ...,  0.0018,  0.0319, -0.0129],</span><br><span class="line">        [ 0.0296, -0.0005,  0.0115,  ...,  0.0141, -0.0088, -0.0106],</span><br><span class="line">        [ 0.0289, -0.0077,  0.0239,  ..., -0.0166, -0.0156, -0.0235]],</span><br><span class="line">       requires_grad=True)</span><br></pre></td></tr></table></figure><h2 id="PyTorch-optim"><a href="#PyTorch-optim" class="headerlink" title="PyTorch: optim"></a>PyTorch: optim</h2><p>这一次我们不再手动更新模型的weights,而是使用optim这个包来帮助我们更新参数。 optim这个package提供了各种不同的模型优化方法，包括SGD+momentum, RMSProp, Adam等等。</p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机创建一些训练数据</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H, bias=<span class="literal">False</span>), <span class="comment"># w_1 * x + b_1</span></span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out, bias=<span class="literal">False</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">torch.nn.init.normal_(model[<span class="number">0</span>].weight)</span><br><span class="line">torch.nn.init.normal_(model[<span class="number">2</span>].weight)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model = model.cuda()</span></span><br><span class="line"></span><br><span class="line">loss_fn = nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line"><span class="comment"># learning_rate = 1e-4</span></span><br><span class="line"><span class="comment"># optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span></span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(x) <span class="comment"># model.forward() </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y) <span class="comment"># computation graph</span></span><br><span class="line">    print(it, loss.item())</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update model parameters</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><h2 id="PyTorch-自定义-nn-Modules"><a href="#PyTorch-自定义-nn-Modules" class="headerlink" title="PyTorch: 自定义 nn Modules"></a>PyTorch: 自定义 nn Modules</h2><p>我们可以定义一个模型，这个模型继承自nn.Module类。如果需要定义一个比Sequential模型更加复杂的模型，就需要定义nn.Module模型。</p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机创建一些训练数据</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></span><br><span class="line">        super(TwoLayerNet, self).__init__()</span><br><span class="line">        <span class="comment"># define the model architecture</span></span><br><span class="line">        self.linear1 = torch.nn.Linear(D_in, H, bias=<span class="literal">False</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(H, D_out, bias=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        y_pred = self.linear2(self.linear1(x).clamp(min=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line">model = TwoLayerNet(D_in, H, D_out)</span><br><span class="line">loss_fn = nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(x) <span class="comment"># model.forward() </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y) <span class="comment"># computation graph</span></span><br><span class="line">    print(it, loss.item())</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update model parameters</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>朴素贝叶斯</title>
      <link href="/2020/06/08/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
      <url>/2020/06/08/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</url>
      
        <content type="html"><![CDATA[<h1 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h1><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>贝叶斯方法是一个历史悠久，有着坚实的理论基础的方法，同时处理很多问题时直接而又高效，很多高级自然语言处理模型也可以从它演化而来。因此，学习贝叶斯方法，是研究自然语言处理问题的一个非常好的切入口。</p><h2 id="2-贝叶斯公式"><a href="#2-贝叶斯公式" class="headerlink" title="2. 贝叶斯公式"></a>2. 贝叶斯公式</h2><p>贝叶斯公式就一行：</p><blockquote><p>$$<br>P(Y|X)=P(X|Y)P(Y)/P(X)<br>$$</p></blockquote><p>而它其实是由以下的联合概率公式推导出来：<br>$$<br>P(Y,X)=P(Y|X)P(X)=P(X|Y)P(Y)<br>$$<br>其中P(Y)叫做先验概率，P(Y|X)叫做后验概率，P(Y,X)叫做联合概率。</p><p>没了，贝叶斯最核心的公式就这么些。</p><h2 id="3-用机器学习的视角理解贝叶斯公式"><a href="#3-用机器学习的视角理解贝叶斯公式" class="headerlink" title="3. 用机器学习的视角理解贝叶斯公式"></a>3. 用机器学习的视角理解贝叶斯公式</h2><p>在机器学习的视角下，我们把X理解成<strong>“具有某特征”</strong>，把Y理解成<strong>“类别标签”</strong>(一般机器学习为题中都是<code>X=&gt;特征</code>, <code>Y=&gt;结果</code>对吧)。在最简单的二分类问题(<code>是</code>与<code>否</code>判定)下，我们将Y理解成<strong>“属于某类</strong>”的标签。于是贝叶斯公式就变形成了下面的样子:</p><blockquote><p>P(“属于某类”|“具有某特征”)=P(“具有某特征”|“属于某类”)P(“属于某类”)P(“具有某特征”)</p></blockquote><p>我们简化解释一下上述公式：</p><blockquote><p>P(“属于某类”|“具有某特征”)=在已知某样本“具有某特征”的条件下，该样本“属于某类”的概率。所以叫做<strong>『后验概率』</strong>。<br>P(“具有某特征”|“属于某类”)=在已知某样本“属于某类”的条件下，该样本“具有某特征”的概率。<br>P(“属于某类”)=（在未知某样本具有该“具有某特征”的条件下，）该样本“属于某类”的概率。所以叫做<strong>『先验概率』</strong>。<br>P(“具有某特征”)=(在未知某样本“属于某类”的条件下，)该样本“具有某特征”的概率。</p></blockquote><p>而我们二分类问题的最终目的就是要<strong>判断P(“属于某类”|“具有某特征”)是否大于1/2</strong>就够了。贝叶斯方法把计算<strong>“具有某特征的条件下属于某类”</strong>的概率转换成需要计算<strong>“属于某类的条件下具有某特征”</strong>的概率，而后者获取方法就简单多了，我们只需要找到一些包含已知特征标签的样本，即可进行训练。而样本的类别标签都是明确的，所以贝叶斯方法在机器学习里属于有监督学习方法。</p><p>这里再补充一下，一般<strong>『先验概率』、『后验概率』是相对</strong>出现的，比如P(Y)与P(Y|X)是关于Y的先验概率与后验概率，P(X)与P(X|Y)是关于X的先验概率与后验概率。</p><h2 id="4-垃圾邮件识别"><a href="#4-垃圾邮件识别" class="headerlink" title="4. 垃圾邮件识别"></a>4. 垃圾邮件识别</h2><p>举个例子好啦，我们现在要对邮件进行分类，识别垃圾邮件和普通邮件，如果我们选择使用朴素贝叶斯分类器，那目标就是<strong>判断P(“垃圾邮件”|“具有某特征”)是否大于1/2</strong>。现在假设我们有垃圾邮件和正常邮件各1万封作为训练集。需要判断以下这个邮件是否属于垃圾邮件：</p><blockquote><p>“我司可办理正规发票（保真）17%增值税发票点数优惠！”</p></blockquote><p>也就是<strong>判断概率P(“垃圾邮件”|“我司可办理正规发票（保真）17%增值税发票点数优惠！”)是否大于1/2</strong>。</p><p>咳咳，有木有发现，转换成的这个概率，计算的方法：就是写个计数器，然后+1 +1 +1统计出所有垃圾邮件和正常邮件中出现这句话的次数啊！！！好，具体点说：</p><blockquote><p>P(“垃圾邮件”|“我司可办理正规发票（保真）17%增值税发票点数优惠！”) =垃圾邮件中出现这句话的次数垃圾邮件中出现这句话的次数+正常邮件中出现这句话的次数</p></blockquote><h2 id="5-分词"><a href="#5-分词" class="headerlink" title="5. 分词"></a>5. 分词</h2><p>一个很悲哀但是很现实的结论： <strong>训练集是有限的，而句子的可能性则是无限的。所以覆盖所有句子可能性的训练集是不存在的。</strong></p><p>所以解决方法是？ <strong>句子的可能性无限，但是词语就那么些！！</strong>汉语常用字2500个，常用词语也就56000个(你终于明白小学语文老师的用心良苦了)。按人们的经验理解，两句话意思相近并不强求非得每个字、词语都一样。比如<strong>“我司可办理正规发票，17%增值税发票点数优惠！”</strong>，这句话就比之前那句话少了<strong>“（保真）”</strong>这个词，但是意思基本一样。如果把这些情况也考虑进来，那样本数量就会增加，这就方便我们计算了。</p><p>于是，我们可以不拿句子作为特征，而是拿句子里面的词语（组合）作为特征去考虑。比如<strong>“正规发票”</strong>可以作为一个单独的词语，<strong>“增值税”</strong>也可以作为一个单独的词语等等。</p><blockquote><p>句子<strong>“我司可办理正规发票，17%增值税发票点数优惠！”就可以变成（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)）</strong>。</p></blockquote><p>于是你接触到了中文NLP中，最最最重要的技术之一：<strong>分词</strong>！！！也就是<strong>把一整句话拆分成更细粒度的词语来进行表示</strong>。另外，分词之后<strong>去除标点符号、数字甚至无关成分(停用词)是特征预处理中的一项技术</strong>。</p><p><strong>中文分词是一个专门的技术领域(我不会告诉你某搜索引擎厂码砖工有专门做分词的！！！)，上过之前课程的同学都知道python有一个非常方便的分词工具jieba，假定我们已经完成分词工作：</strong></p><p>我们观察（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)，<strong>这可以理解成一个向量：向量的每一维度都表示着该特征词在文本中的特定位置存在。这种将特征拆分成更小的单元，依据这些更灵活、更细粒度的特征进行判断的思维方式，在自然语言处理与机器学习中都是非常常见又有效的。</strong></p><p>因此贝叶斯公式就变成了：</p><blockquote><p>P(“垃圾邮件”|（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)） =P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|”垃圾邮件”）P(“垃圾邮件”)P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”))</p><p>P(“正常邮件”|（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)） =P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|”正常邮件”）P(“正常邮件”)P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”))</p></blockquote><h2 id="6-条件独立假设"><a href="#6-条件独立假设" class="headerlink" title="6. 条件独立假设"></a>6. 条件独立假设</h2><p>下面我们马上会看到一个非常简单粗暴的假设。</p><p>概率P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|”垃圾邮件”）依旧不够好求，我们引进一个<strong>很朴素的近似</strong>。为了让公式显得更加紧凑，我们令字母S表示“垃圾邮件”,令字母H表示“正常邮件”。近似公式如下：</p><blockquote><p>P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|S）<br>=P(“我”|S）×P(“司”|S）×P(“可”|S）×P(“办理”|S）×P(“正规发票”|S） ×P(“保真”|S）×P(“增值税”|S）×P(“发票”|S）×P(“点数”|S）×P(“优惠”|S)</p></blockquote><p>这就是传说中的<strong>条件独立假设</strong>。基于“正常邮件”的条件独立假设的式子与上式类似，此处省去。接着，将条件独立假设代入上面两个相反事件的贝叶斯公式。</p><p>于是我们就只需要比较以下两个式子的大小：</p><blockquote><p>C=P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S) ×P(“保真”|S)P(“增值税”|S)P(“发票”|S)P(“点数”|S)P(“优惠”|S)P(“垃圾邮件”) C⎯⎯⎯⎯=P(“我”|H)P(“司”|H)P(“可”|H)P(“办理”|H)P(“正规发票”|H) ×P(“保真”|H)P(“增值税”|H)P(“发票”|H)P(“点数”|H)P(“优惠”|H)P(“正常邮件”)</p></blockquote><p>厉(wo)害(cao)！酱紫处理后<strong>式子中的每一项都特别好求</strong>！只需要<strong>分别统计各类邮件中该关键词出现的概率</strong>就可以了！！！比如：</p><blockquote><p>P(“发票”|S）=垃圾邮件中所有“发票”的次数垃圾邮件中所有词语的次数</p></blockquote><p>统计次数非常方便，而且样本数量足够大，算出来的概率比较接近真实。于是垃圾邮件识别的问题就可解了。</p><h2 id="7-朴素贝叶斯-Naive-Bayes-，“Naive”在何处？"><a href="#7-朴素贝叶斯-Naive-Bayes-，“Naive”在何处？" class="headerlink" title="7. 朴素贝叶斯(Naive Bayes)，“Naive”在何处？"></a>7. 朴素贝叶斯(Naive Bayes)，“Naive”在何处？</h2><p><strong>加上条件独立假设的贝叶斯方法就是朴素贝叶斯方法（Naive Bayes）。</strong> Naive的发音是“乃一污”，意思是“朴素的”、“幼稚的”、<strong>“蠢蠢的”</strong>。咳咳，也就是说，大神们取名说该方法是一种比较萌蠢的方法，为啥？</p><p>将句子（“我”,“司”,“可”,“办理”,“正规发票”) 中的 （“我”,“司”）与（“正规发票”）调换一下顺序，就变成了一个新的句子（“正规发票”,“可”,“办理”, “我”, “司”)。新句子与旧句子的意思完全不同。<strong>但由于乘法交换律，朴素贝叶斯方法中算出来二者的条件概率完全一样！</strong>计算过程如下：</p><blockquote><p>P(（“我”,“司”,“可”,“办理”,“正规发票”)|S) =P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S) =P(“正规发票”|S)P(“可”|S)P(“办理”|S)P(“我”|S)P(“司”|S） =P(（“正规发票”,“可”,“办理”,“我”,“司”)|S)</p></blockquote><p><strong>也就是说，在朴素贝叶斯眼里，“我司可办理正规发票”与“正规发票可办理我司”完全相同。朴素贝叶斯失去了词语之间的顺序信息。</strong>这就相当于把所有的词汇扔进到一个袋子里随便搅和，贝叶斯都认为它们一样。因此这种情况也称作<strong>词袋子模型(bag of words)</strong>。</p><p><img src="blob:file:///f3e451e8-1f8f-4c4c-b15f-25793dff88ca" alt="词袋子配图"></p><p>词袋子模型与人们的日常经验完全不同。比如，在条件独立假设的情况下，<strong>“武松打死了老虎”与“老虎打死了武松”被它认作一个意思了。</strong>恩，朴素贝叶斯就是这么单纯和直接，对比于其他分类器，好像是显得有那么点萌蠢。</p><h2 id="8-简单高效，吊丝逆袭"><a href="#8-简单高效，吊丝逆袭" class="headerlink" title="8. 简单高效，吊丝逆袭"></a>8. 简单高效，吊丝逆袭</h2><p>虽然说朴素贝叶斯方法萌蠢萌蠢的，但实践证明在垃圾邮件识别的应用还<strong>令人诧异地好</strong>。Paul Graham先生自己简单做了一个朴素贝叶斯分类器，<strong>“1000封垃圾邮件能够被过滤掉995封，并且没有一个误判”。</strong>（Paul Graham《黑客与画家》）</p><p>那个…效果为啥好呢？</p><p>“有人对此提出了一个理论解释，并且建立了什么时候朴素贝叶斯的效果能够等价于非朴素贝叶斯的充要条件，这个解释的核心就是：有些独立假设在各个分类之间的分布都是均匀的所以对于似然的相对大小不产生影响；即便不是如此，也有很大的可能性<strong>各个独立假设所产生的消极影响或积极影响互相抵消，最终导致结果受到的影响不大</strong>。具体的数学公式请参考<a href="http://www.cs.unb.ca/profs/hzhang/publications/FLAIRS04ZhangH.pdf" target="_blank" rel="noopener">这篇 paper</a>。”（刘未鹏《：平凡而又神奇的贝叶斯方法》）</p><p>恩，这个分类器中最简单直接看似萌蠢的小盆友『朴素贝叶斯』，实际上却是<strong>简单、实用、且强大</strong>的。</p><h2 id="9-处理重复词语的三种方式"><a href="#9-处理重复词语的三种方式" class="headerlink" title="9. 处理重复词语的三种方式"></a>9. 处理重复词语的三种方式</h2><p>我们<strong>之前的垃圾邮件向量（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)，其中每个词都不重复。</strong>而这在现实中其实很少见。因为如果文本长度增加，或者分词方法改变，<strong>必然会有许多词重复出现</strong>，因此需要对这种情况进行进一步探讨。比如以下这段邮件：</p><blockquote><p>“代开发票。增值税发票，正规发票。” 分词后为向量： （“代开”,“发票”,“增值税”,“发票”,“正规”,“发票”）</p></blockquote><p>其中“发票”重复了三次。</p><h3 id="9-1-多项式模型："><a href="#9-1-多项式模型：" class="headerlink" title="9.1 多项式模型："></a>9.1 多项式模型：</h3><p>如果我们考虑重复词语的情况，也就是说，<strong>重复的词语我们视为其出现多次</strong>，直接按条件独立假设的方式推导，则有</p><blockquote><p>P(（“代开”,“发票”,“增值税”,“发票”,“正规”,“发票”)|S） =P(“代开””|S)P(“发票”|S)P(“增值税”|S)P(“发票”|S)P(“正规”|S)P(“发票”|S）=P(“代开””|S)P3(“发票”|S)P(“增值税”|S)P(“正规”|S) <strong>注意这一项</strong>:P3(“发票”|S）。</p></blockquote><p>在统计计算P(“发票”|S）时，每个被统计的垃圾邮件样本中重复的词语也统计多次。</p><blockquote><p>P(“发票”|S）=每封垃圾邮件中出现“发票”的次数的总和每封垃圾邮件中所有词出现次数（计算重复次数）的总和</p></blockquote><p>你看这个多次出现的结果，出现在概率的指数/次方上，因此这样的模型叫作<strong>多项式模型</strong>。</p><h3 id="9-2-伯努利模型"><a href="#9-2-伯努利模型" class="headerlink" title="9.2 伯努利模型"></a>9.2 伯努利模型</h3><p>另一种更加简化的方法是<strong>将重复的词语都视为其只出现1次</strong>，</p><blockquote><p>P(（“代开”,“发票”,“增值税”,“发票”,“正规”,“发票”)|S） =P(“发票”|S)P(“代开””|S)P(“增值税”|S)P(“正规”|S）</p></blockquote><p>统计计算P(“词语”|S）时也是如此。</p><blockquote><p>P(“发票”|S）=出现“发票”的垃圾邮件的封数每封垃圾邮件中所有词出现次数（出现了只计算一次）的总和</p></blockquote><p>这样的模型叫作<strong>伯努利模型</strong>（又称为<strong>二项独立模型</strong>）。这种方式更加简化与方便。当然它丢失了词频的信息，因此效果可能会差一些。</p><h3 id="9-3-混合模型"><a href="#9-3-混合模型" class="headerlink" title="9.3 混合模型"></a>9.3 混合模型</h3><p>第三种方式是在计算句子概率时，不考虑重复词语出现的次数，但是在统计计算词语的概率P(“词语”|S）时，却考虑重复词语的出现次数，这样的模型可以叫作<strong>混合模型</strong>。</p><p>我们通过下图展示三种模型的关系。</p><p><img src="blob:file:///157f8870-f4d9-4b18-9922-0c1e7a18074b" alt="三种形态"></p><p>具体实践中采用那种模型，关键看具体的业务场景，一个简单经验是，<strong>对于垃圾邮件识别，混合模型更好些</strong>。</p><h2 id="10-去除停用词与选择关键词"><a href="#10-去除停用词与选择关键词" class="headerlink" title="10. 去除停用词与选择关键词"></a>10. 去除停用词与选择关键词</h2><p>我们继续观察<strong>（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)</strong> 这句话。其实，像<strong>“我”、“可”</strong>之类词其实非常中性，无论其是否出现在垃圾邮件中都无法帮助判断的有用信息。所以可以直接不考虑这些典型的词。这些无助于我们分类的词语叫作<strong>“停用词”（Stop Words）</strong>。这样可以<strong>减少我们训练模型、判断分类的时间</strong>。 于是之前的句子就变成了<strong>（“司”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)</strong> 。</p><p>我们进一步分析。以人类的经验，其实<strong>“正规发票”、“发票”</strong>这类的词如果出现的话，邮件作为垃圾邮件的概率非常大，可以作为我们区分垃圾邮件的<strong>“关键词”</strong>。而像<strong>“司”、“办理”、“优惠”</strong>这类的词则有点鸡肋，可能有助于分类，但又不那么强烈。如果想省事做个简单的分类器的话，则可以直接采用“关键词”进行统计与判断，剩下的词就可以先不管了。于是之前的垃圾邮件句子就变成了<strong>（“正规发票”,“发票”)</strong> 。这样就更加减少了我们训练模型、判断分类的时间，速度非常快。</p><p><strong>“停用词”和“关键词”一般都可以提前靠人工经验指定</strong>。不同的“停用词”和“关键词”训练出来的分类器的效果也会有些差异。</p><h2 id="11-浅谈平滑技术"><a href="#11-浅谈平滑技术" class="headerlink" title="11. 浅谈平滑技术"></a>11. 浅谈平滑技术</h2><p>我们来说个问题(中文NLP里问题超级多，哭瞎T_T)，比如在计算以下独立条件假设的概率：</p><blockquote><p>P(（“我”,“司”,“可”,“办理”,“正规发票”)|S) =P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S）</p></blockquote><p>我们扫描一下训练集，发现<strong>“正规发票”这个词从出现过！！！*，于是P(“正规发票”|S）=0…问题严重了，整个概率都变成0了！！！朴素贝叶斯方法面对一堆0，很凄惨地失效了…更残酷的是</strong>这种情况其实很常见<strong>，因为哪怕训练集再大，也可能有覆盖不到的词语。本质上还是</strong>样本数量太少，不满足大数定律，计算出来的概率失真**。为了解决这样的问题，一种分析思路就是直接不考虑这样的词语，但这种方法就相当于默认给P(“正规发票”|S）赋值为1。其实效果不太好，大量的统计信息给浪费掉了。我们进一步分析，既然可以默认赋值为1，为什么不能默认赋值为一个很小的数？这就是平滑技术的基本思路，依旧保持着一贯的作风，<code>朴实/土</code>但是<code>直接而有效</code>。</p><p>对于伯努利模型，P(“正规发票”|S）的一种平滑算法是：</p><blockquote><p>P(“正规发票”|S）=出现“正规发票”的垃圾邮件的封数+1每封垃圾邮件中所有词出现次数（出现了只计算一次）的总和+2</p></blockquote><p>对于多项式模型，P(“正规发票”| S）的一种平滑算法是：</p><blockquote><p>P(“发票”|S）=每封垃圾邮件中出现“发票”的次数的总和+1每封垃圾邮件中所有词出现次数（计算重复次数）的总和+被统计的词表的词语数量</p></blockquote><p>说起来，平滑技术的种类其实非常多，有兴趣的话回头我们专门拉个专题讲讲好了。这里只提一点，就是所有的<strong>平滑技术都是给未出现在训练集中的词语一个估计的概率，而相应地调低其他已经出现的词语的概率</strong>。</p><p>平滑技术是因为数据集太小而产生的现实需求。<strong>如果数据集足够大，平滑技术对结果的影响将会变小。</strong></p><h2 id="12-内容小结"><a href="#12-内容小结" class="headerlink" title="12. 内容小结"></a>12. 内容小结</h2><p>我们找了个最简单常见的例子：垃圾邮件识别，说明了一下朴素贝叶斯进行文本分类的思路过程。基本思路是先区分好训练集与测试集，对文本集合进行分词、去除标点符号等特征预处理的操作，然后使用条件独立假设，将原概率转换成词概率乘积，再进行后续的处理。</p><blockquote><p>贝叶斯公式 + 条件独立假设 = 朴素贝叶斯方法</p></blockquote><p>基于对重复词语在训练阶段与判断（测试）阶段的三种不同处理方式，我们相应的有伯努利模型、多项式模型和混合模型。在训练阶段，如果样本集合太小导致某些词语并未出现，我们可以采用平滑技术对其概率给一个估计值。而且并不是所有的词语都需要统计，我们可以按相应的“停用词”和“关键词”对模型进行进一步简化，提高训练和判断速度。</p><h2 id="13-为什么不直接匹配关键词来识别垃圾邮件？"><a href="#13-为什么不直接匹配关键词来识别垃圾邮件？" class="headerlink" title="13. 为什么不直接匹配关键词来识别垃圾邮件？"></a>13. 为什么不直接匹配关键词来识别垃圾邮件？</h2><p>有同学可能会问：“何必费这么大劲算那么多词的概率？直接看邮件中有没有‘代开发票’、‘转售发票’之类的关键词不就得了？如果关键词比较多就认为是垃圾邮件呗。”</p><p>其实关键词匹配的方法如果有效的话真不必用朴素贝叶斯。毕竟这种方法简单嘛，<strong>就是一个字符串匹配</strong>。从历史来看，之前没有贝叶斯方法的时候主要也是用关键词匹配。<strong>但是这种方法准确率太低</strong>。我们在工作项目中也尝试过用关键词匹配的方法去进行文本分类，发现大量误报。感觉就像扔到垃圾箱的邮件99%都是正常的！这样的效果不忍直视。而加一个朴素贝叶斯方法就可能把误报率拉低近一个数量级，体验好得不要不要的。</p><p><strong>另一个原因是词语会随着时间不断变化</strong>。发垃圾邮件的人也不傻，当他们发现自己的邮件被大量屏蔽之后，也会考虑采用新的方式，<strong>如变换文字、词语、句式、颜色等方式来绕过反垃圾邮件系统</strong>。比如对于垃圾邮件“我司可办理正规发票，17%增值税发票点数优惠”,他们采用火星文：<strong>“涐司岢办理㊣規髮票，17%增値稅髮票嚸數優蕙”</strong>，那么字符串匹配的方法又要重新找出这些火星文，一个一个找出关键词，重新写一些匹配规则。更可怕的是，这些规则可能相互之间的耦合关系异常复杂，要把它们梳理清楚又是大一个数量级的工作量。等这些规则失效了又要手动更新新的规则……<strong>无穷无尽猫鼠游戏最终会把猫给累死</strong>。</p><p>而朴素贝叶斯方法却显示出无比的优势。因为它是<strong>基于统计方法</strong>的，只要训练样本中有更新的垃圾邮件的新词语，哪怕它们是火星文，<strong>都能自动地把哪些更敏感的词语（如“髮”、“㊣”等）给凸显出来，并根据统计意义上的敏感性给他们分配适当的权重</strong> ，这样就不需要什么人工了，非常省事。<strong>你只需要时不时地拿一些最新的样本扔到训练集中，重新训练一次即可</strong>。</p><p>小补充一下，对于火星文、同音字等替代语言，一般的分词技术可能会分得不准，最终可能只把一个一个字给分出来，成为“分字”。效果可能不会太好。也可以用过n-gram之类的语言模型，拿到最常见短语。当然，对于英文等天生自带空格来间隔单词的语言，分词则不是什么问题，使用朴素贝叶斯方法将会更加顺畅。</p><h2 id="14-实际工程的tricks"><a href="#14-实际工程的tricks" class="headerlink" title="14.实际工程的tricks"></a>14.实际工程的tricks</h2><p>应用朴素贝叶斯方法的过程中，一些tricks能显著帮助工程解决问题。我们毕竟经验有限，无法将它们全都罗列出来，只能就所知的一点点经验与大家分享，欢迎批评指正。</p><h3 id="14-1-trick1：取对数"><a href="#14-1-trick1：取对数" class="headerlink" title="14.1 trick1：取对数"></a>14.1 trick1：取对数</h3><p>我们提到用来识别垃圾邮件的方法是比较以下两个概率的大小（字母S表示“垃圾邮件”,字母H表示“正常邮件”）：</p><blockquote><p>C=P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S)</p><p>×P(“保真”|S)P(“增值税”|S)P(“发票”|S)P(“点数”|S)P(“优惠”|S)P(“垃圾邮件”)</p><p>C⎯⎯⎯⎯=P(“我”|H)P(“司”|H)P(“可”|H)P(“办理”|H)P(“正规发票”|H)</p><p>×P(“保真”|H)P(“增值税”|H)P(“发票”|H)P(“点数”|H)P(“优惠”|H)P(“正常邮件”)</p></blockquote><p>但这里进行了<strong>很多乘法运算，计算的时间开销比较大</strong>。尤其是对于篇幅比较长的邮件，几万个数相乘起来还是非常花时间的。如果能<strong>把这些乘法变成加法则方便得多</strong>。刚好数学中的对数函数log就可以实现这样的功能。两边同时取对数（本文统一取底数为2），则上面的公式变为：</p><blockquote><p>logC=logP(“我”|S)+logP(“司”|S)+logP(“可”|S)+logP(“办理”|S)+logP(“正规发票”|S)</p><p>+logP(“保真”|S)+logP(“增值税”|S)+logP(“发票”|S)+logP(“点数”|S)+logP(“优惠”|S)+logP(“垃圾邮件”)</p><p>logC⎯⎯⎯⎯=logP(“我”|H)+logP(“司”|H)+logP(“可”|H)+logP(“办理”|H)+logP(“正规发票”|H)</p><p>+logP(“保真”|H)+logP(“增值税”|H)+logP(“发票”|H)+logP(“点数”|H)+logP(“优惠”|H)+logP(“正常邮件”)</p></blockquote><p>有同学可能要叫了：“做对数运算岂不会也很花时间？”的确如此，但是可以在训练阶段直接计算 logP ，然后把他们存在一张大的hash表里。<strong>在判断的时候直接提取hash表中已经计算好的对数概率，然后相加即可。这样使得判断所需要的计算时间被转移到了训练阶段</strong>，实时运行的时候速度就比之前快得多，这可不止几个数量级的提升。</p><h3 id="14-2-trick2：转换为权重"><a href="#14-2-trick2：转换为权重" class="headerlink" title="14.2 trick2：转换为权重"></a>14.2 trick2：转换为权重</h3><p>对于二分类，我们还可以继续提高判断的速度。既然要比较logC 和logC⎯⎯⎯⎯ 的大小，那就可以直接将上下两式相减，并继续化简：</p><blockquote><p>logCC⎯⎯⎯⎯⎯=logP(“我”|S)P(“我”|H)+logP(“司”|S)P(“司”|H)+logP(“可”|S)P(“可”|H)+logP(“办理”|S)P(“办理”|H)+logP(“正规发票”|S)P(“正规发票”|H)</p><p>+logP(“保真”|S)P(“保真”|H)+logP(“增值税”|S)P(“增值税”|H)+logP(“发票”|S)P(“发票”|H)+logP(“点数”|S)P(“点数”|H)+logP(“优惠”|S)P(“优惠”|H)+logP(“正常邮件”|S)P(“正常邮件”)</p></blockquote><p><strong>logCC⎯⎯⎯⎯⎯ 如果大于0则属于垃圾邮件。我们可以把其中每一项作为其对应词语的权重</strong>，比如logP(“发票”|S)P(“发票”|H) 就可以作为词语“发票”的权重，权重越大就越说明“发票”更可能是与“垃圾邮件”相关的特征。<strong>这样可以根据权重的大小来评估和筛选显著的特征，比如关键词。而这些权重值可以直接提前计算好而存在hash表中</strong> 。判断的时候直接将权重求和即可。</p><p>关键词hash表的样子如下，左列是权重，右列是其对应的词语，权重越高的说明越“关键”：</p><p><img src="blob:file:///6f29d16a-1075-45cb-9c7e-206aefcf4e4a" alt="hash"></p><h3 id="14-3-trick3：选取topk的关键词"><a href="#14-3-trick3：选取topk的关键词" class="headerlink" title="14.3 trick3：选取topk的关键词"></a>14.3 trick3：选取topk的关键词</h3><p>前文说过可以通过提前选取关键词来提高判断的速度。有一种方法可以省略提前选取关键词的步骤，<strong>就是直接选取一段文本中权重最高的K个词语，将其权重进行加和</strong>。比如Paul Graham 在《黑客与画家》中是选取邮件中权重最高的15个词语计算的。</p><p>通过权重hash表可知，如果是所有词语的权重，则权重有正有负。如果只选择权重最高的K个词语，则它们的权重基本都是正的。所以就不能像之前那样判断logCC⎯⎯⎯⎯⎯ 是否大于0来区分邮件了。而这<strong>需要依靠经验选定一个正数的阈值（门槛值）</strong> ，依据logCC⎯⎯⎯⎯⎯ 与该门槛值的大小来识别垃圾邮件。</p><p>如下图所示，蓝色点代表垃圾邮件，绿色点代表正常邮件，横坐标为计算出来的logCC⎯⎯⎯⎯⎯ 值，中间的红线代表阈值。</p><p><img src="blob:file:///fa290902-0e88-4f3b-8a65-79c442c05c05" alt="权重"></p><h3 id="14-4-trick4：分割样本"><a href="#14-4-trick4：分割样本" class="headerlink" title="14.4 trick4：分割样本"></a>14.4 trick4：分割样本</h3><p>选取topk个词语的方法对于篇幅变动不大的邮件样本比较有效。但是对篇幅过大或者过小的邮件则会有判断误差。</p><p>比如这个垃圾邮件的例子：（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)。分词出了10个词语，其中有“正规发票”、“发票”2个关键词。关键词的密度还是蛮大的，应该算是敏感邮件。但因为采用最高15个词语的权重求和，并且相应的阈值是基于15个词的情况有效，可能算出来的结果还小于之前的阈值，这就造成漏判了。</p><p>类似的，如果一封税务主题的邮件有1000个词语，其中只有“正规发票”、“发票”、“避税方法”3个权重比较大的词语，它们只是在正文表述中顺带提到的内容。关键词的密度被较长的篇幅稀释了，应该算是正常邮件。但是却被阈值判断成敏感邮件，造成误判了。</p><p><strong>这两种情况都说明topk关键词的方法需要考虑篇幅的影响</strong>。这里有许多种处理方式，<strong>它们的基本思想都是选取词语的个数及对应的阈值要与篇幅的大小成正比</strong>，本文只介绍其中一种方方法：</p><ul><li>对于长篇幅邮件，按一定的大小，比如每500字，将其分割成小的文本段落，再对小文本段落采用topk关键词的方法。只要其中有一个小文本段落超过阈值就判断整封邮件是垃圾邮件。</li><li>对于超短篇幅邮件，比如50字，可以按篇幅与标准比较篇幅的比例来选取topk，以确定应该匹配关键词语的个数。比如选取 50500×15≈2 个词语进行匹配，相应的阈值可以是之前阈值的 215 。以此来判断则更合理。</li></ul><h3 id="14-5-trick5：位置权重"><a href="#14-5-trick5：位置权重" class="headerlink" title="14.5 trick5：位置权重"></a>14.5 trick5：位置权重</h3><p>到目前为止，我们对词语权重求和的过程都没有考虑邮件篇章结构的因素。比如“正规发票”如果出现在标题中应该比它出现在正文中对判断整个邮件的影响更大；而出现在段首句中又比其出现在段落正文中对判断整个邮件的影响更大。<strong>所以可以根据词语出现的位置，对其权重再乘以一个放大系数，以扩大其对整封邮件的影响，提高识别准确度</strong>。</p><p>比如一封邮件其标题是“正规发票”（假设标题的放大系数为2），段首句是“发票”,“点数”,“优惠”（假设段首的放大系数为1.5），剩下的句子是（“我”,“司”,“可”,“办理”,“保真”）。则计算logCC⎯⎯⎯⎯⎯ 时的公式就可以调整为：</p><blockquote><p>logCC⎯⎯⎯⎯⎯=2×logP(“正规发票”|S)P(“正规发票”|H)+1.5×logP(“发票”|S)P(“发票”|H)+1.5×logP(“点数”|S)P(“点数”|H)+1.5×logP(“优惠”|S)P(“优惠”|H)</p><p>+logP(“我”|S)P(“我”|H)+logP(“司”|S)P(“司”|H)+logP(“可”|S)P(“可”|H)+logP(“办理”|S)P(“办理”|H)+logP(“保真”|S)P(“保真”|H)+logP(“正常邮件”|S)P(“正常邮件”)</p></blockquote><h3 id="14-6-trick6：蜜罐"><a href="#14-6-trick6：蜜罐" class="headerlink" title="14.6 trick6：蜜罐"></a>14.6 trick6：蜜罐</h3><p>我们通过辛辛苦苦的统计与计算，好不容易得到了不同词语的权重。然而这并不是一劳永逸的。我们我们之前交代过，<strong>词语及其权重会随着时间不断变化，需要时不时地用最新的样本来训练以更新词语及其权重</strong>。</p><p>而搜集最新垃圾邮件有一个技巧，就是随便注册一些邮箱，然后将它们公布在各大论坛上。接下来就坐等一个月，到时候收到的邮件就绝大部分都是垃圾邮件了（好奸诈）。再找一些正常的邮件，基本就能够训练了。这些用于自动搜集垃圾邮件的邮箱叫做“蜜罐”。<strong>“蜜罐”是网络安全领域常用的手段，因其原理类似诱捕昆虫的装有蜜的罐子而得名</strong>。比如杀毒软件公司会利用蜜罐来监视或获得计算机网络中的病毒样本、攻击行为等。</p><h2 id="15-贝叶斯方法的思维方式"><a href="#15-贝叶斯方法的思维方式" class="headerlink" title="15. 贝叶斯方法的思维方式"></a>15. 贝叶斯方法的思维方式</h2><p>讲了这么多tricks，但这些手段都是建立在贝叶斯方法基础之上的。因此有必要探讨一下贝叶斯方法的思维方式，以便更好地应用这种方法解决实际问题。</p><h3 id="15-1-逆概问题"><a href="#15-1-逆概问题" class="headerlink" title="15.1 逆概问题"></a>15.1 逆概问题</h3><p>我们重新看一眼贝叶斯公式：</p><blockquote><p>P(Y|X)=P(X|Y)P(Y)P(X)</p></blockquote><p>先不考虑先验概率P(Y)与P(X)，观察两个后验概率P(Y|X)与P(X|Y)，可见贝叶斯公式能够揭示<strong>两个相反方向的条件概率之间的转换关系</strong>。</p><p>从贝叶斯公式的发现历史来看，其就是为了处理所谓“逆概”问题而诞生的。比如P(Y|X) 不能通过直接观测来得到结果，而P(X|Y) 却容易通过直接观测得到结果，就可以通过贝叶斯公式<strong>从间接地观测对象去推断不可直接观测的对象的情况</strong>。</p><p>好吧，我们说人话。基于邮件的文本内容判断其属于垃圾邮件的概率不好求（不可通过直接观测、统计得到），但是基于已经搜集好的垃圾邮件样本，去统计（直接观测）其文本内部各个词语的概率却非常方便。这就可以用贝叶斯方法。</p><p>引申一步，基于样本特征去判断其所属标签的概率不好求，但是基于已经搜集好的打上标签的样本（有监督），却可以直接统计属于同一标签的样本内部各个特征的概率分布。因此贝叶斯方法的理论视角适用于一切分类问题的求解。</p><h3 id="15-2-处理多分类问题"><a href="#15-2-处理多分类问题" class="headerlink" title="15.2 处理多分类问题"></a>15.2 处理多分类问题</h3><p>前面我们一直在探讨二分类（判断题）问题，现在可以引申到多分类（单选题）问题了。</p><p>还是用邮件分类的例子，这是现在不只要判断垃圾邮件，还要将正常邮件细分为私人邮件、工作邮件。现在有这3类邮件各1万封作为样本。需要训练出一个贝叶斯分类器。这里依次用Y1,Y2,Y3表示这三类邮件，用X表示被判断的邮件。套用贝叶斯公式有：</p><blockquote><p>P(Y1|X)=P(X|Y1)P(Y1)P(X)</p><p>P(Y2|X)=P(X|Y2)P(Y2)P(X)</p><p>P(Y3|X)=P(X|Y3)P(Y3)P(X)</p></blockquote><p>通过比较3个概率值的大小即可得到X所属的分类。发现三个式子的分母P(X) 一样，比较大小时可以忽略不计，于是就可以用下面这一个式子表达上面3式：</p><blockquote><p>P(Yi|X)∝P(X|Yi)P(Yi)；i=1,2,3</p></blockquote><p>其中 ∝ 表示“正比于”。而P(X|Yi) 则有个特别高逼格的名字叫做“<strong>似然函数</strong>”。我们上大学的时候也被这个名字搞得晕晕乎乎的，其实它也是个概率，直接理解成<strong>“P(Yi|X) 的逆反条件概率”</strong> 就方便了。</p><p>这里只是以垃圾邮件3分类问题举了个例子，<strong>对于任意多分类的问题都可以用这样的思路去理解。比如新闻分类、情感喜怒哀乐分类等等</strong>。</p><h3 id="15-3-先验概率的问题"><a href="#15-3-先验概率的问题" class="headerlink" title="15.3 先验概率的问题"></a>15.3 先验概率的问题</h3><p>在垃圾邮件的例子中，先验概率都相等，P(Y1)=P(Y2)=P(Y3)=10000/30000=1/3，所以上面是式子又可以进一步化简：</p><blockquote><p>P(Yi|X)∝P(X|Yi)；i=1,2,3</p></blockquote><p>只需比较右边式子（也就是“似然函数”）的大小就可以了。这种方法就是传说中的<strong>最大似然法</strong>:不考虑先验概率而直接比较似然函数。</p><p>关于选出最佳分类Yi是否要考虑先验概率P(Yi)的问题，曾经在频率学派和贝叶斯学派之间产生了激烈的教派冲突。统计学家（频率学派）说：我们让数据自己说话。言下之意就是要摒弃先验概率。而贝叶斯学派支持者则说：数据会有各种各样的偏差，而一个<strong>靠谱的先验概率</strong>则可以对这些随机噪音做到健壮。对此有兴趣的同学可以找更多资料进行了解，本文在此不做更多的引申，只基于垃圾邮件识别的例子进行探讨。</p><p>比如我们在采集垃圾邮件样本的时候，不小心delete掉了一半的数据，就剩下5000封邮件。则计算出来的先验概率为:</p><blockquote><p>P(Y1)=5000/25000=1/5，</p><p>P(Y2)=P(Y3)=10000/25000=2/5</p></blockquote><p>如果还用贝叶斯方法,就要在似然函数后面乘上先验概率。比如之前用最大似然法算出Y1 垃圾邮件的概率大，但是因为P(Y1)特别小，用贝叶斯方法得出的结果是Y2 私人邮件的概率大。那相信哪个呢？其实，我们删掉了部分带标签的样本，从计算结果看P(Y1)，P(Y2)，P(Y3)的概率分布变化了，但实际上<strong>这三个类别的真实分布应该是一个客观的状态，不应该因为我们的计算方法而发生变化</strong>。所以是我们计算出来的先验概率失真，应该放弃这样计算出来的先验概率，而用最大似然法。但即便我们不删掉一半垃圾邮件，这三类邮件的分布就真的是1:1:1那样平均吗？那也未必。<strong>我们只是按1:1:1这样的方式进行了抽样而已，真正在邮箱里收到的这三类邮件的分布可能并不是这样</strong>。也就是说，<strong>在我们对于先验概率一无所知时，只能假设每种猜测的先验概率是均等的（其实这也是人类经验的结果），这个时候就只有用最大似然了</strong>。在现实运用过程中如果发现最大似然法有偏差，可以考虑对不同的似然函数设定一些系数或者阈值，使其接近真实情况。</p><p>但是，<strong>如果我们有足够的自信，训练集中这三类的样本分布的确很接近真实的情况，这时就应该用贝叶斯方法</strong>。难怪前面的贝叶斯学派强调的是“靠谱的先验概率”。所以说<strong>贝叶斯学派的适用范围更广，关键要先验概率靠谱，而频率学派有效的前提也是他们的先验概率同样是经验统计的结果</strong>。</p><h2 id="16-朴素-贝叶斯方法的常见应用"><a href="#16-朴素-贝叶斯方法的常见应用" class="headerlink" title="16. (朴素)贝叶斯方法的常见应用"></a>16. (朴素)贝叶斯方法的常见应用</h2><p>说了这么多理论的问题，咱们就可以探讨一下(朴素)贝叶斯方法在自然语言处理中的一些常见应用了。以下只是从原理上进行探讨，对于具体的技术细节顾及不多。</p><h3 id="16-1-褒贬分析"><a href="#16-1-褒贬分析" class="headerlink" title="16.1 褒贬分析"></a>16.1 褒贬分析</h3><p>一个比较常见的应用场景是情感褒贬分析。比如你要统计微博上人们对一个新上映电影的褒贬程度评价：好片还是烂片。但是一条一条地看微博是根本看不过来，只能用自动化的方法。我们可以有一个很粗略的思路：</p><ul><li>首先是用爬虫将微博上提到这个电影名字的微博全都抓取下来，比如有10万条。</li><li>然后用训练好的朴素贝叶斯分类器分别判断这些微博对电影是好评还是差评。</li><li>最后统计出这些好评的影评占所有样本中的比例，就能形成微博网友对这个电影综合评价的大致估计。</li></ul><p>接下来的核心问题就是训练出一个靠谱的分类器。首先需要有打好标签的文本。这个好找，豆瓣影评上就有大量网友对之前电影的评价，并且对电影进行1星到5星的评价。我们可以认为3星以上的评论都是好评，3星以下的评论都是差评。这样就分别得到了好评差评两类的语料样本。剩下就可以用朴素贝叶斯方法进行训练了。基本思路如下：</p><ul><li>训练与测试样本：豆瓣影评的网友评论，用爬虫抓取下100万条。</li><li>标签：3星以上的是好评，3星以下的是差评。</li><li>特征：豆瓣评论分词后的词语。一个简单的方法是只选择其中的形容词，网上有大量的情绪词库可以为我们所用。</li><li>然后再用常规的朴素贝叶斯方法进行训练。</li></ul><p>但是由于自然语言的特点，在提取特征的过程当中，有一些tricks需要注意：</p><ul><li><strong>对否定句进行特别的处理</strong>。比如这句话“我不是很喜欢部电影，因为它让我开心不起来。”其中两个形容词“喜欢”、“开心”都是褒义词，但是因为句子的否定句，所以整体是贬义的。有一种比较简单粗暴的处理方式，就是<strong>“对否定词（“不”、“非”、“没”等）与句尾标点之间的所有形容词都采用其否定形式”</strong> 。则这句话中提取出来的形容词就应该是“不喜欢”和“不开心”。</li><li>一般说来，最相关的情感词在一些文本片段中仅仅出现一次，词频模型起得作用有限，甚至是负作用，<strong>则使用伯努利模型代替多项式模型</strong>。这种情况在微博这样的小篇幅文本中似乎不太明显，但是在博客、空间、论坛之类允许长篇幅文本出现的平台中需要注意。</li><li>其实，副词对情感的评价有一定影响。“不很喜欢”与“很不喜欢”的程度就有很大差异。但如果是朴素贝叶斯方法的话比较难处理这样的情况。我们可以考虑用语言模型或者加入词性标注的信息进行综合判断。这些内容我们将在之后的文章进行探讨。</li></ul><p>当然经过以上的处理，情感分析还是会有一部分误判。这里涉及到许多问题，都是情感分析的难点：</p><ul><li><strong>情绪表达的含蓄微妙</strong>：“导演你出来，我保证不打死你。”你让机器怎么判断是褒还是贬？</li><li><strong>转折性表达</strong>：“我非常喜欢这些大牌演员，非常崇拜这个导演，非常赞赏这个剧本，非常欣赏他们的预告片，我甚至为了这部影片整整期待了一年，最后进了电影院发现这是个噩梦。” 五个褒义的形容词、副词对一个不那么贬义的词。机器自然判断成褒义，但这句话是妥妥的贬义。</li></ul><h3 id="16-2-拼写纠错"><a href="#16-2-拼写纠错" class="headerlink" title="16.2 拼写纠错"></a>16.2 拼写纠错</h3><p>拼写纠错本质上也是一个分类问题。但按照错误类型不同，又分为两种情况：</p><ul><li>非词错误（Non-word Errors）：指那些拼写错误后的词本身就不合法，如将“wifi”写成“wify”；</li><li>真词错误（Real-word Errors）：指那些拼写错误后的词仍然是合法的情况，如将“wifi”写成“wife”。</li></ul><p>真词错误复杂一些，我们将在接下来的文章中进行探讨。而对于非词错误，就可以直接采用贝叶斯方法，其基本思路如下：</p><ul><li>标签：通过计算错误词语的最小编辑距离（之前咱们提到过的），获取最相似的候选词，每个候选词作为一个分类。</li><li>特征：拼写错误的词本身。因为它就一个特征，所以没有什么条件独立性假设、朴素贝叶斯啥的。它就是纯而又纯的贝叶斯方法。</li><li>判别公式:</li></ul><blockquote><p>P(候选词i|错误词)∝P(错误词|候选词i)P(候选词i)；i=1,2,3,…</p></blockquote><ul><li>训练样本1：该场景下的正常用词语料库，用于计算P(候选词i)。</li></ul><blockquote><p>P(候选词i)=候选词出现的次数所有词出现的次数</p></blockquote><ul><li>训练样本2：该场景下错误词与正确词对应关系的语料库，用于计算P(错误词|候选词i)</li></ul><blockquote><p>P(错误词|候选词i)=候选词被拼写成该“错误词”的次数候选词出现的次数</p></blockquote><p>由于自然语言的特点，有一些tricks需要注意：</p><ul><li>据统计，80%的拼写错误编辑距离为1，几乎所有的拼写错误编辑距离小于等于2。我们<strong>只选择编辑距离为1或2的词作为候选词，这样就可以减少大量不必要的计算</strong>。</li><li>由于我们只选择编辑距离为1或2的词，其差别只是一两个字母级别差别。因此计算似然函数的时候，<strong>可以只统计字母层面的编辑错误，这样搜集的样本更多，更满足大数定律，也更简单</strong>。对于编辑距离为1的似然函数计算公式可以进化为：</li></ul><blockquote><p>P(错误词|候选词i)=⎧⎩⎨⎪⎪⎪⎪⎪⎪字母“xy”被拼写成“y”的次数字母“xy”出现的次数,字母“x”被拼写成“xy”的次数字母“x”出现的次数,字母“x”被拼写成“y”的次数字母“x”出现的次数,字母“xy”被拼写成“yx的次数字母“xy”出现的次数,</p></blockquote><ul><li><strong>键盘上临近的按键更容易拼写错误，据此可以对上面这个条件概率进行加权</strong>。</li></ul><p><img src="blob:file:///3ecb40bd-0f8b-4742-ac50-e0912d5c6cc0" alt="\[键盘\]"></p><h2 id="17-内容小结"><a href="#17-内容小结" class="headerlink" title="17. 内容小结"></a>17. 内容小结</h2><p>从前面大家基本可以看出，工程应用不同于学术理论，有许多tricks需要考虑，而理论本质就是翻来倒去折腾贝叶斯公式，都快玩出花来了。</p><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 朴素贝叶斯 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GPT模型</title>
      <link href="/2020/05/30/GPT%E6%A8%A1%E5%9E%8B/"/>
      <url>/2020/05/30/GPT%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>文本分类：</p><p>数据集</p><p>THUCNews 数据集子集,链接: <a href="https://pan.baidu.com/s/1hugrfRu" target="_blank" rel="noopener">https://pan.baidu.com/s/1hugrfRu</a> 密码: qfud</p><p>Language Understanding</p><ul><li><p>intent classification</p></li><li><p>dialogue state tracking</p></li><li><p>sentiment classification</p></li></ul><p>Language Generation</p><ul><li>information, structured, sentiment –&gt; language</li></ul><h1 id="必读论文"><a href="#必读论文" class="headerlink" title="必读论文"></a>必读论文</h1><h1 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h1><p>Radford et. al., Improving Language Understanding by Generative Pre-Training</p><p><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</a></p><p>这篇文章推出了generative pre-training + discriminative fine-tuning的方法，后来也被BERT沿用。task-aware input transformation也是BERT借用的一个点。当年这篇文章刚出来的时候刷榜一波，不过离BERT太近，导致后来大家都不怎么关心这篇文章了。</p><p><s>  a b c d e f </s></p><p>|        |  |  |  |  |  |</p><p>a       b c d e f  </p><p>1 0 0 0 0 0 0</p><p>1 1 0 0 0 0 0</p><p>1 1 1 0 0 0 0</p><p>1 1 1 1 0 0 0</p><p>1 1 1 1 1 0 0</p><p>1 1 1 1 1 1 0</p><h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><p>语言模型objective</p><p><img src="https://uploader.shimo.im/f/jUXNKYwxjuUim5hM.png!thumbnail" alt="img"></p><p>Transformer Decoder</p><p><img src="https://uploader.shimo.im/f/2VVdkCN84SM8h2NA.png!thumbnail" alt="img"></p><p>训练使用BooksCorpus数据集，7000本书。</p><p>模型参数：</p><ul><li><p>12层transformer decoder</p></li><li><p>768 hidden states, 12 attention heads</p></li><li><p>FFN层有3072维度inner states</p></li></ul><h2 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine tuning"></a>Fine tuning</h2><p>使用最后一层最后一个token的representation来做task specific的模型fine tuning</p><p><img src="https://uploader.shimo.im/f/sMzH5VF5BcQsk8pY.png!thumbnail" alt="img"></p><p>依然使用Log loss</p><p><img src="https://uploader.shimo.im/f/odJ6a19y3swDAYcI.png!thumbnail" alt="img"></p><p>作者发现在fine tuning的时候继续使用语言模型的loss也有好处</p><p><img src="https://uploader.shimo.im/f/z2GX6ss3yL8CrvaN.png!thumbnail" alt="img"></p><h2 id="Task-specific-Input-Transformations"><a href="#Task-specific-Input-Transformations" class="headerlink" title="Task-specific Input Transformations"></a>Task-specific Input Transformations</h2><p>四种问题有四种不同的文本表示方法</p><p><img src="https://uploader.shimo.im/f/e3Ep9QOmlzI2YmM8.png!thumbnail" alt="img"></p><p>Natural Language Inference</p><ul><li><p>判断两句话的关系，entailment 承接关系，contradiction 矛盾关系，neutral 中立关系</p></li><li><p>在几个NLI任务上都有不小的提升</p></li></ul><p>Question Answering and Common Sense Reasoning</p><p>Semantic Similarity 语义相似度</p><ul><li><p>Microsoft Paraphrase Corpus</p></li><li><p>Quora Question Pairs</p></li></ul><p>分类问题 </p><ul><li><p>Corpus of Lingustic Acceptability，判断一句话的语法是不是正确。</p></li><li><p>Stanford Sentiment Treebank 情感分类</p></li></ul><h1 id="GPT2"><a href="#GPT2" class="headerlink" title="GPT2"></a>GPT2</h1><p>Radford et. al., <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener">Language Models are Unsupervised Multitask Learners</a></p><p><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener">https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</a></p><p>比GPT更大的训练数据集</p><ul><li>Common Crawl来自网页爬取，删除了Wikipedia，总共40GB数据。</li></ul><p>evaluation任务</p><ul><li><p>The Winograd Schema Challenge</p></li><li><p>LAMBADA dataset <a href="https://arxiv.org/pdf/1606.06031.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1606.06031.pdf</a></p></li></ul><p>关于文本生成</p><p><a href="https://arxiv.org/pdf/1904.09751.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.09751.pdf</a></p><h2 id="代码解读"><a href="#代码解读" class="headerlink" title="代码解读"></a>代码解读</h2><p>我对代码添加了一些注释</p><p><a href="https://github.com/ZeweiChu/gpt-2/blob/master/src/model.py" target="_blank" rel="noopener">https://github.com/ZeweiChu/gpt-2/blob/master/src/model.py</a></p><p>huggingface代码</p><p><a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_gpt2.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_gpt2.py</a></p><p>自动检测</p><p>def attention_mask(nd, ns, *, dtype):</p><p>​    “””1’s in the lower triangle, counting from the lower right corner.</p><p>​    左下角的三角形都是1，其余是0，用于生成mask。</p><p>​    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn’t produce garbage on TPUs.</p><p>​    “””</p><p>​    i = tf.range(nd)[:,None]</p><p>​    j = tf.range(ns)</p><p>​    m = i &gt;= j - ns + nd</p><p>​    return tf.cast(m, dtype)</p><p>0 0 0 0 0</p><p>1 1 1 1 1</p><p>2 2 2 2 2</p><p>3 3 3 3 3</p><p>4 4 4 4 4</p><p>0 1 2 3 4 </p><p>1 0 0 0 0</p><p>1 1 0 0 0</p><p>1 1 1 0 0</p><p>1 1 1 1 0</p><p>1 1 1 1 1</p><p>w00 w01-inf w02-inf w03-inf w04-inf</p><p>w10 w11 w12-inf w13-inf w14-inf</p><p>w20 w21 w22 w23-inf w24-inf</p><p>w30 w31 w32 w33 w34-inf</p><p>w40 w41 w42 w43 w44</p><p>阅读GPT2代码：<a href="https://github.com/ZeweiChu/gpt-2/blob/master/src/model.py" target="_blank" rel="noopener">https://github.com/ZeweiChu/gpt-2/blob/master/src/model.py</a></p><h1 id="Google-T5-Transformer预训练模型大总结"><a href="#Google-T5-Transformer预训练模型大总结" class="headerlink" title="Google T5: Transformer预训练模型大总结"></a>Google T5: Transformer预训练模型大总结</h1><p>论文地址：<a href="https://arxiv.org/pdf/1910.10683.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1910.10683.pdf</a></p><p>代码+预训练模型：<a href="https://github.com/google-research/text-to-text-transfer-transformer" target="_blank" rel="noopener">https://github.com/google-research/text-to-text-transfer-transformer</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GPT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BERT系列预训练模型</title>
      <link href="/2020/05/30/BERT%E7%B3%BB%E5%88%97%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
      <url>/2020/05/30/BERT%E7%B3%BB%E5%88%97%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>BERT：Masked Language Modeling预训练模型</p><p>论文地址：<a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1810.04805.pdf</a></p><p>中文翻译：<a href="https://zhuanlan.zhihu.com/p/59775981" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/59775981</a></p><p> Language modeling预训练任务</p><h2 id="Masked-Language-Model"><a href="#Masked-Language-Model" class="headerlink" title="Masked Language Model"></a>Masked Language Model</h2><p>完形填空</p><p>I study at Julyedu . </p><p>80% I study at [MASK] . </p><p>10% I study at Apple . </p><p>10% I study at Julyedu . </p><p>[CLS] I study at [MASK] .  [SEP] I love [MASK] language processing . [SEP]</p><p>–&gt; transformer encoder</p><p>o1, o2, o3, o4, o5, …., o_n</p><p>o5 –&gt; Julyedu  cross entropy</p><p>o10 –&gt; natural cross entropy</p><p>o1 –&gt; True cross entropy</p><p>BERT说：“我要用 transformer 的 encoders”</p><p>Ernie不屑道：“呵呵，你不能像Bi-Lstm一样考虑文章”</p><p>BERT自信回答道：“我们会用masks”</p><blockquote><p>解释一下Mask：</p></blockquote><blockquote></blockquote><blockquote><p>语言模型会根据前面单词来预测下一个单词，但是self-attention的注意力只会放在自己身上，那么这样100%预测到自己，毫无意义，所以用Mask，把需要预测的词给挡住。</p></blockquote><p>如下图：</p><p><img src="https://uploader.shimo.im/f/jvcJ8SPeBEwszR8M.png!thumbnail" alt="img"></p><h2 id="Two-sentence-Tasks"><a href="#Two-sentence-Tasks" class="headerlink" title="Two-sentence Tasks"></a>Two-sentence Tasks</h2><p>我们回顾一下OpenAI transformer处理不同任务的输入转换，你会发现在某些任务上我们需要2个句子作为输入，并做一些更为智能的判断，比如是否相似，比如 给出一个维基百科的内容作为输入，同时在放入一条针对该条目的问题，那么我们的算法模型能够处理这个问题吗？</p><p>为了使BERT更好的处理2个句子之间的关系，预训练的过程还有一个额外的任务：给定2个句子（A和B）,A与B是否相似？（0或者1）</p><h2 id="特殊NLP任务"><a href="#特殊NLP任务" class="headerlink" title="特殊NLP任务"></a>特殊NLP任务</h2><p>BERT的论文为我们介绍了几种BERT可以处理的NLP任务：</p><ol><li><p>短文本相似 </p></li><li><p>文本分类</p></li><li><p>QA机器人</p></li><li><p>语义标注</p></li></ol><p><img src="https://uploader.shimo.im/f/yKFxOevBvMQXvjnv.png!thumbnail" alt="img"></p><h2 id="BERT用做特征提取"><a href="#BERT用做特征提取" class="headerlink" title="BERT用做特征提取"></a>BERT用做特征提取</h2><p>微调方法并不是使用BERT的唯一方法，就像ELMo一样，你可以使用预选训练好的BERT来创建语境化词嵌入。然后你可以将这些嵌入提供给现有的模型。</p><p><img src="https://uploader.shimo.im/f/uKUkG73gELQGry4L.png!thumbnail" alt="img"></p><p>哪个向量最适合作为上下文嵌入？ 我认为这取决于任务。 本文考察了六种选择（与微调模型相比，得分为96.4）：</p><p><img src="https://uploader.shimo.im/f/bfpUyWE9YCEP9IU2.png!thumbnail" alt="img"></p><ul><li><p>Feature Extraction：特征提取</p></li><li><p>Finetune：微调</p></li></ul><h1 id="如何使用BERT"><a href="#如何使用BERT" class="headerlink" title="如何使用BERT"></a>如何使用BERT</h1><h2 id="BERT源码"><a href="#BERT源码" class="headerlink" title="BERT源码"></a>BERT源码</h2><p>查看<a href="https://github.com/google-research/bert" target="_blank" rel="noopener">BERT仓库</a>中的代码：</p><ol><li><p>该模型在modeling.py（BertModel类）中构建，与vanilla Transformer编码器完全相同。</p></li><li><p>run_classifier.py是微调过程的一个示例。它还构建了监督模型的分类层。如果要构建自己的分类器，请查看该文件中的create_model()方法。</p></li><li><p>可以下载几种预先训练的模型。涵盖102种语言的多语言模型，这些语言都是在维基百科的数据基础上训练而成的。</p></li><li><p>BERT不会将单词视为tokens。相反，它注重WordPieces。 tokenization.py是将你的单词转换为适合BERT的wordPieces的tokensizer。</p></li></ol><p>可以查看BERT的PyTorch实现 (<a href="https://github.com/huggingface/transformers)。" target="_blank" rel="noopener">https://github.com/huggingface/transformers)。</a> </p><ul><li><p>modeling: <a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py</a></p></li><li><p>BertEmbedding: wordpiece embedding + position embedding + token type embedding</p></li><li><p>BertSelfAttnetion: query, key, value的变换</p></li><li><p>BertSelfOutput: </p></li><li><p>BertIntermediate</p></li><li><p>BertOutput</p></li><li><p>BertForSequenceClassification</p></li><li><p>configuration: <a href="https://github.com/huggingface/transformers/blob/master/transformers/configuration_bert.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/transformers/configuration_bert.py</a></p></li><li><p>tokenization: <a href="https://github.com/huggingface/transformers/blob/master/transformers/tokenization_bert.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/transformers/tokenization_bert.py</a></p></li><li><p>DataProcessor: <a href="https://github.com/huggingface/transformers/blob/master/transformers/data/processors/glue.py#L194" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/transformers/data/processors/glue.py#L194</a></p></li></ul><h2 id="BERT模型的使用"><a href="#BERT模型的使用" class="headerlink" title="BERT模型的使用"></a>BERT模型的使用</h2><ul><li>文本分类：<a href="https://github.com/huggingface/transformers/blob/master/examples/run_glue.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/examples/run_glue.py</a></li></ul><h1 id="BERT升级版"><a href="#BERT升级版" class="headerlink" title="BERT升级版"></a>BERT升级版</h1><h2 id="RoBERTa：更强大的BERT"><a href="#RoBERTa：更强大的BERT" class="headerlink" title="RoBERTa：更强大的BERT"></a>RoBERTa：更强大的BERT</h2><p>论文地址：<a href="https://arxiv.org/pdf/1907.11692.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1907.11692.pdf</a></p><ul><li><p>加大训练数据 16GB -&gt; 160GB，更大的batch size，训练时间加长</p></li><li><p>不需要NSP Loss: natural inference </p></li><li><p>使用更长的训练 Sequence</p></li><li><p>Static vs. Dynamic Masking </p></li><li><p>模型训练成本在6万美金以上（估算）</p></li></ul><h2 id="ALBERT：参数更少的BERT"><a href="#ALBERT：参数更少的BERT" class="headerlink" title="ALBERT：参数更少的BERT"></a>ALBERT：参数更少的BERT</h2><p>论文地址：<a href="https://arxiv.org/pdf/1909.11942.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1909.11942.pdf</a></p><ul><li><p>一个轻量级的BERT模型</p></li><li><p>核心思想：</p></li><li><p>共享层与层之间的参数 （减少模型参数）</p></li><li><p>增加单层向量维度</p></li></ul><h2 id="DistilBERT：轻量版BERT"><a href="#DistilBERT：轻量版BERT" class="headerlink" title="DistilBERT：轻量版BERT"></a>DistilBERT：轻量版BERT</h2><p>MNIST</p><p>0, 1, 2, 3, …, 9</p><p>logits: [0.1, 0.6, …, 0.01] q</p><p><strong>label: 2 [0, 0, 1, …, 0] p</strong></p><p>loss: cross entropy loss -\sum_{i=1}^10 p_i*log q_i</p><p>loss: -log q_{label}</p><p>训练一个Student network，mimic the behavior of the teacher network</p><p>teacher network: [0.1, 0.6, …, 0.01] t</p><p><strong>student network</strong>: [s_1, s_2, .., s_10]</p><p>cross entropy loss: -sum_{i=1}^10 t_i * log s_i</p><p>4, 7</p><p><a href="https://arxiv.org/pdf/1910.01108.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1910.01108.pdf</a></p><ul><li><p>MLM, NSP</p></li><li><p>MLM: cross entropy loss: -\sum_{i=1}^k p_i log (q_i) = - log (q_{label})</p></li><li><p>teacher (MLM) = distribution</p></li><li><p>student: 学习distribution: -\sum_{i=1}^k p_teacher_i log (q_student_i)</p></li></ul><p>Patient Distillation</p><p><a href="https://arxiv.org/abs/1908.09355" target="_blank" rel="noopener">https://arxiv.org/abs/1908.09355</a></p><p><img src="https://uploader.shimo.im/f/FtKDArmN5UoEwpsF.png!thumbnail" alt="img"></p><h1 id="参考阅读资料"><a href="#参考阅读资料" class="headerlink" title="参考阅读资料"></a>参考阅读资料</h1><h3 id="BERT-Distillation"><a href="#BERT-Distillation" class="headerlink" title="BERT Distillation"></a>BERT Distillation</h3><p>对于BERT模型压缩感兴趣的同学可以参考以下资料</p><ul><li>Patient Knowledge Distillation for BERT Model Compression  <a href="https://www.aclweb.org/anthology/D19-1441.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D19-1441.pdf</a></li></ul><p>关于BERT模型压缩的一套方法</p><h3 id="ELECTRA"><a href="#ELECTRA" class="headerlink" title="ELECTRA"></a>ELECTRA</h3><p><a href="https://openreview.net/pdf?id=r1xMH1BtvB" target="_blank" rel="noopener">https://openreview.net/pdf?id=r1xMH1BtvB</a></p><p>使用GAN训练BERT模型</p><p><img src="https://uploader.shimo.im/f/PJ9RGb3HpgIA4WYN.png!thumbnail" alt="img"></p><ul><li><p>Generator针对[MASK]位置生成单词，Discriminator判断这些单词是由Generator (从[MASK]) 生成的还是原本就存在的。</p></li><li><p>Discriminator被用于downstream task finetuning。</p></li></ul><h3 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h3><p>我在上一期NLP就业班中介绍了XLNet，不过由于近些日子BERT的各种加强版层出不穷，XLNet显得并不特别突出。感兴趣的同学可以参考上一期的课件：<a href="https://shimo.im/docs/PHqcpWtYjJjW3yH3" target="_blank" rel="noopener">https://shimo.im/docs/PHqcpWtYjJjW3yH3</a></p><p>XLNet的代码和预训练模型也可以在Huggingface的版本中找到。</p><h3 id="NLP预训练模型串讲"><a href="#NLP预训练模型串讲" class="headerlink" title="NLP预训练模型串讲"></a>NLP预训练模型串讲</h3><p>我之前在七月在线的公开课中使用的PPT</p><p>NLP预训练模型.pdf1.9MB</p><h3 id="参考阅读：The-Illustrated-BERT-ELMo-and-co"><a href="#参考阅读：The-Illustrated-BERT-ELMo-and-co" class="headerlink" title="参考阅读：The Illustrated BERT, ELMo, and co."></a>参考阅读：The Illustrated BERT, ELMo, and co.</h3><p><a href="https://shimo.im/docs/Y6q3gX8yGGjpWqXx" target="_blank" rel="noopener">https://shimo.im/docs/Y6q3gX8yGGjpWqXx</a></p><ul><li><p>阅读BertSelfAttention代码 <a href="https://github.com/huggingface/transformers/blob/master/transformers/modeling_bert.py#L190" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/transformers/modeling_bert.py#L190</a></p></li><li><p>阅读run_glue.py <a href="https://github.com/huggingface/transformers/blob/master/examples/run_glue.py#L152" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/examples/run_glue.py#L152</a></p></li><li><p>阅读BertForSequenceClassification <a href="https://github.com/huggingface/transformers/blob/master/transformers/modeling_bert.py#L970" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/transformers/modeling_bert.py#L970</a></p></li><li><p>阅读glue.py <a href="https://github.com/huggingface/transformers/blob/master/transformers/data/processors/glue.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/transformers/data/processors/glue.py</a> 用来做文本预处理</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读理解</title>
      <link href="/2020/05/19/%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3/"/>
      <url>/2020/05/19/%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>NLP当中的阅读理解(Reading Comprehension, Question Answering)任务主要是以下形式：给定一些背景知识，主要是一篇文章，有时候也可能是一些结构化的知识图谱，然后回答与该背景知识的相关问题。</p><p>常见的问题和答案形式有：</p><ul><li><p>完形填空：在文章中给定一个空位和一些候选答案，我们需要把一个候选答案填充进去。</p></li><li><p>简答题：给定一篇文章和一个问题，我们需要从文章中去找答案，且这个答案一定在文章中出现过。SQuAD</p></li><li><p>选择题：给定一篇文章，一个问题和一些候选答案，选择一个正确答案。</p></li></ul><p>还有一些在上述问答任务基础上的拓展情况，例如有的任务需要在多篇文章的基础上作答，有的QA任务需要我们自己来推理和撰写答案 (open domain)，无法直接从文中找到答案。</p><p>整个QA领域的发展主要都是依靠一些数据集的提出和解决来推动的。往往是有人制作了一个数据集和一个QA任务，然后大家开始比赛谁能更好地解决它。</p><p>我认为最好的学习方法是去了解这些QA数据集（<a href="http://nlpprogress.com/english/question_answering.html），针对自己感兴趣的数据集去寻找相应的解决方案，在这个过程中了解QA的解决方法。将来如果在实际的应用场景中遇到类似的QA任务（例如一些客服机器人等），我们就可以寻找到比较相关的QA数据集，使用在这些数据集上最好的解决方案来解决自己的任务。" target="_blank" rel="noopener">http://nlpprogress.com/english/question_answering.html），针对自己感兴趣的数据集去寻找相应的解决方案，在这个过程中了解QA的解决方法。将来如果在实际的应用场景中遇到类似的QA任务（例如一些客服机器人等），我们就可以寻找到比较相关的QA数据集，使用在这些数据集上最好的解决方案来解决自己的任务。</a></p><h2 id><a href="#" class="headerlink" title=" "></a> </h2><h1 id="一些有名的阅读理解数据集和模型"><a href="#一些有名的阅读理解数据集和模型" class="headerlink" title="一些有名的阅读理解数据集和模型"></a>一些有名的阅读理解数据集和模型</h1><h2 id="SQuAD-1-0-2-0"><a href="#SQuAD-1-0-2-0" class="headerlink" title="SQuAD 1.0/2.0"></a>SQuAD 1.0/2.0</h2><p>从文章中找答案</p><p><a href="https://aclweb.org/anthology/D16-1264" target="_blank" rel="noopener">https://aclweb.org/anthology/D16-1264</a></p><p><img src="https://uploader.shimo.im/f/yqtsScSwls8OkJWs.png!thumbnail" alt="img"></p><h3 id="BiDAF模型"><a href="#BiDAF模型" class="headerlink" title="BiDAF模型"></a>BiDAF模型</h3><p>Bi-Directional Attention Fflow for Machine Comprehension</p><p><a href="https://arxiv.org/pdf/1611.01603.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1611.01603.pdf</a></p><p>2017年的模型，用于解决SQuAD之类的问题。后来的很多模型都参考了该模型的设计思想</p><p><img src="https://uploader.shimo.im/f/hL8lQitMAxMtYJ5w.png!thumbnail" alt="img"></p><p>预测： start_pos, end_pos</p><p>其他相关模型</p><p>Document Reader (single model)</p><p>r-net (single model)</p><p>QANet (single)</p><p><a href="https://github.com/allenai/bi-att-flow" target="_blank" rel="noopener">https://github.com/allenai/bi-att-flow</a></p><p>MCTest</p><p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/MCTest_EMNLP2013.pdf" target="_blank" rel="noopener">https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/MCTest_EMNLP2013.pdf</a></p><h3 id="BERT模型"><a href="#BERT模型" class="headerlink" title="BERT模型"></a>BERT模型</h3><p>模型 <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1810.04805.pdf</a></p><p>代码 <a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1402" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1402</a></p><p><a href="https://github.com/huggingface/transformers/blob/master/examples/run_squad.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/examples/run_squad.py</a></p><h2 id="CNN-Daily-Mail"><a href="#CNN-Daily-Mail" class="headerlink" title="CNN/Daily Mail"></a>CNN/Daily Mail</h2><p>完形填空类问题</p><p>Teaching Machines to Read and Comprehend</p><p><a href="https://arxiv.org/pdf/1506.03340.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.03340.pdf</a></p><p><img src="https://uploader.shimo.im/f/ppAcqx7DjtM3486H.png!thumbnail" alt="img"></p><h3 id="Attention-Sum模型"><a href="#Attention-Sum模型" class="headerlink" title="Attention Sum模型"></a>Attention Sum模型</h3><p><a href="https://arxiv.org/pdf/1603.01547.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.01547.pdf</a></p><p>Gated Attention Sum模型是该模型的一个拓展形式</p><p><a href="https://arxiv.org/abs/1606.01549" target="_blank" rel="noopener">https://arxiv.org/abs/1606.01549</a></p><h3 id="陈丹琦在CNN-Daily-Mail上的工作"><a href="#陈丹琦在CNN-Daily-Mail上的工作" class="headerlink" title="陈丹琦在CNN/Daily Mail上的工作"></a>陈丹琦在CNN/Daily Mail上的工作</h3><p>A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task</p><p><a href="https://www.aclweb.org/anthology/P16-1223" target="_blank" rel="noopener">https://www.aclweb.org/anthology/P16-1223</a></p><p><img src="https://uploader.shimo.im/f/BKipbLYDzic4bWbc.png!thumbnail" alt="img"></p><p>顺便介绍一下，<a href="https://www.cs.princeton.edu/~danqic/" target="_blank" rel="noopener">陈丹琦</a>在QA领域做了很多工作，对QA感兴趣的同学可以参考她的博士论文。</p><p><a href="https://www.cs.princeton.edu/~danqic/" target="_blank" rel="noopener">https://www.cs.princeton.edu/~danqic/</a></p><p><a href="https://www.cs.princeton.edu/~danqic/papers/thesis.pdf" target="_blank" rel="noopener">https://www.cs.princeton.edu/~danqic/papers/thesis.pdf</a></p><p><a href="https://arxiv.org/pdf/1506.03340.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.03340.pdf</a></p><p><a href="https://arxiv.org/pdf/1603.01547.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.01547.pdf</a></p><p><a href="http://www.cs.cmu.edu/~bdhingra/papers/ga_reader.pdf" target="_blank" rel="noopener">http://www.cs.cmu.edu/~bdhingra/papers/ga_reader.pdf</a></p><p><a href="https://arxiv.org/pdf/1611.07954.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1611.07954.pdf</a></p><h2 id="RACE数据集"><a href="#RACE数据集" class="headerlink" title="RACE数据集"></a>RACE数据集</h2><p>RACE: Large-scale ReAding Comprehension Dataset From Examinations</p><p><a href="https://arxiv.org/pdf/1704.04683.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1704.04683.pdf</a></p><p>RACE数据集来自中国的中高考英语阅读理解题。</p><p>代码：</p><p><a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1204" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1204</a></p><p><a href="https://github.com/huggingface/transformers/blob/master/examples/utils_multiple_choice.py#L36" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/examples/utils_multiple_choice.py#L36</a></p><p><a href="https://github.com/huggingface/transformers/blob/master/examples/run_multiple_choice.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/examples/run_multiple_choice.py</a></p><p>SWAG</p><p>后来出现了很多的不同方向的QA问题</p><ul><li><p>基于多文本的、长文章的问答</p></li><li><p>narrative qa <a href="https://arxiv.org/pdf/1712.07040.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1712.07040.pdf</a></p></li><li><p>基于维基百科，结合文本搜索系统的问答</p></li><li><p>Dr QA <a href="https://arxiv.org/pdf/1704.00051.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1704.00051.pdf</a></p></li><li><p>基于聊天记录的问答 </p></li><li><p>QuAC : Question Answering in Context <a href="https://arxiv.org/pdf/1808.07036.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1808.07036.pdf</a></p></li></ul><p>参考以下链接</p><ul><li><p><a href="https://github.com/karthikncode/nlp-datasets#question-answering" target="_blank" rel="noopener">https://github.com/karthikncode/nlp-datasets#question-answering</a></p></li><li><p><a href="http://nlpprogress.com/english/question_answering.html" target="_blank" rel="noopener">http://nlpprogress.com/english/question_answering.html</a></p></li></ul><h2 id="基于多文本的QA任务"><a href="#基于多文本的QA任务" class="headerlink" title="基于多文本的QA任务"></a>基于多文本的QA任务</h2><p>HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering</p><p><a href="https://www.aclweb.org/anthology/D18-1259" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D18-1259</a></p><p>HotpotQA的主要特点是，这是一个基于多文本的QA任务。给定一系列的文章和一个问题，我们需要给出该问题的答案，并且回答我们是从哪些相关的句子中得到问题的答案的。</p><p>已经公开的可参考论文</p><p>Hierarchical Graph Network for Multi-hop Question Answering <a href="https://arxiv.org/pdf/1911.00484.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1911.00484.pdf</a></p><p>Select, Answer and Explain: Interpretable Multi-hop Reading Comprehension over Multiple Documents <a href="https://arxiv.org/pdf/1911.03631.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1911.03631.pdf</a></p><h2 id="CoQA-基于对话的问答数据集"><a href="#CoQA-基于对话的问答数据集" class="headerlink" title="CoQA 基于对话的问答数据集"></a>CoQA 基于对话的问答数据集</h2><p>leaderboard <a href="https://stanfordnlp.github.io/coqa/" target="_blank" rel="noopener">https://stanfordnlp.github.io/coqa/</a></p><p>论文 <a href="https://arxiv.org/abs/1808.07042" target="_blank" rel="noopener">https://arxiv.org/abs/1808.07042</a></p><p>搜狗有一个BERT模型的实现</p><p><a href="https://github.com/sogou/SMRCToolkit" target="_blank" rel="noopener">https://github.com/sogou/SMRCToolkit</a></p><p>这位同学也实现了一些模型</p><p><a href="https://github.com/jayelm/dialog-qa" target="_blank" rel="noopener">https://github.com/jayelm/dialog-qa</a></p><h2 id="中文数据集"><a href="#中文数据集" class="headerlink" title="中文数据集"></a>中文数据集</h2><h3 id="法研杯-阅读理解数据集"><a href="#法研杯-阅读理解数据集" class="headerlink" title="法研杯 阅读理解数据集"></a>法研杯 阅读理解数据集</h3><p><a href="http://cail.cipsc.org.cn/" target="_blank" rel="noopener">http://cail.cipsc.org.cn/</a></p><h3 id="讯飞杯-中文阅读理解评测"><a href="#讯飞杯-中文阅读理解评测" class="headerlink" title="讯飞杯 中文阅读理解评测"></a>讯飞杯 中文阅读理解评测</h3><p><a href="https://hfl-rc.github.io/cmrc2017/" target="_blank" rel="noopener">https://hfl-rc.github.io/cmrc2017/</a></p><p><a href="https://hfl-rc.github.io/cmrc2018/" target="_blank" rel="noopener">https://hfl-rc.github.io/cmrc2018/</a></p><p><a href="https://hfl-rc.github.io/cmrc2019/" target="_blank" rel="noopener">https://hfl-rc.github.io/cmrc2019/</a></p><p>同学们可以找到这三次比赛的数据集和相应的表现最好的模型代码进行学习。</p><p>KBQA</p><p><a href="http://tcci.ccf.org.cn/conference/2018/papers/EV51.pdf" target="_blank" rel="noopener">http://tcci.ccf.org.cn/conference/2018/papers/EV51.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> QA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer模型解读</title>
      <link href="/2020/05/18/Transformer%E6%A8%A1%E5%9E%8B%E8%A7%A3%E8%AF%BB/"/>
      <url>/2020/05/18/Transformer%E6%A8%A1%E5%9E%8B%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p>contextualized word vectors</p><p>RNN, LSTM</p><p>RNN(I study at Julyedu.) –&gt; RNN(I)-&gt;h1, RNN(study, h1)-&gt;h2, RNN(at, h2)-&gt;h3. </p><p>Encoder. 我可以同时观看全局信息。</p><p>query, keys, values</p><p>q1, q2, .., q5</p><p>k1, k2, k3, k4, k5</p><p>score(q, k1), score(q, k2), …, score(q, k5)</p><p>v1, v2, v3, v4, v5</p><p>\sum_{i=1}^5 func(score_i) v_i</p><p>dot(a, b)</p><p>mean</p><p>var(dot(a, b))</p><p>dot(a, b) = a1<em>b1 + a2</em>b2. …. </p><p>E(dot(a, b)) = n * E(ai*bi)</p><p>var(dot(a, b)) = E(dot(a, b)^2) - E(dot(a, b))^2</p><p>affine transformation</p><p>WX+b</p><p>Attention(Q, K, V ) = softmax(QKT √ dk )V</p><p>Q : seq_len, hid_size</p><p>K^T:  hid_size, seq_len</p><p>V: seq_len, hid_size</p><p>QK^T : seq_len, seq_len</p><p>QK^T V: seq_len, hid_size</p><p>[emb_w(x), emb_p(i)]W –&gt; </p><p>近两年来，NLP领域的模型研究已经被transformer模型以及它的各种变种给占领了。Transformer模型的火爆有很多原因，例如：</p><ul><li><p>模型简单易懂，encoder和decoder模块高度相似且通用</p></li><li><p>（encoder）容易并行，模型训练速度快</p></li><li><p>效果拔群，在NMT等领域都取得了state-of-the-art的效果</p></li></ul><p>论文地址</p><ul><li><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a> </li></ul><p>下面的文章翻译自</p><ul><li><p><a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">The Illustrated Transformer</a></p></li><li><p><a href="https://blog.csdn.net/yujianmin1990/article/details/85221271" target="_blank" rel="noopener">中文翻译</a></p></li></ul><p>高屋建瓴地说，Transformer模型拿到一个序列，用来生成另一个序列。</p><p><img src="https://uploader.shimo.im/f/vkvOEopS6TMPw0SL.png!thumbnail" alt="img"></p><p>打开这个黑箱，我们会看到其中包含了两个部分，encoders和decoders。</p><p><img src="https://uploader.shimo.im/f/hraPVC4iek06oDwt.png!thumbnail" alt="img"></p><p>其中encoders和decoders都是两个堆叠架构。一层一层同质的结构堆叠到一起，组成了编码器和解码器。</p><p><img src="https://uploader.shimo.im/f/WFbnFyb8peoeJuXW.png!thumbnail" alt="img"></p><p>首先我们打开每个encoder来参观一下其中包含的内容：</p><p><img src="https://uploader.shimo.im/f/c7oNzYNSIoceXYFZ.png!thumbnail" alt="img"></p><p>每一个encoder都包含了一个自注意力（self-attention）层和一个Feed Forward Neural Network。</p><p>encoder的输入首先会经过一个self-attention层。self-attention的作用是让每个单词可以看到自己和其他单词的关系，并且将自己转换成一个与所有单词相关的，<strong>focus在自己身上的词向量(?)</strong>。</p><p>self-attention之后的输出会再经过一层feed-forward神经网络。每个位置的输出被同样的feed-forward network处理。</p><p>decoder也有同样的self-attention和feed-forward结构，但是在这两层之间还有一层encoder-decoder attention层，帮助decoder关注到某一些特别需要关注的encoder位置。</p><h2 id="Tensor的变化"><a href="#Tensor的变化" class="headerlink" title="Tensor的变化"></a>Tensor的变化</h2><p><img src="https://uploader.shimo.im/f/Hmbb5V4mEJkBYpFS.png!thumbnail" alt="img"></p><h2 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h2><p>下面我们来详细解读一下编码器的工作。</p><p><img src="https://uploader.shimo.im/f/MzJmdqVJiSUz4DT9.png!thumbnail" alt="img"></p><h3 id="Self-Attention机制"><a href="#Self-Attention机制" class="headerlink" title="Self-Attention机制"></a>Self-Attention机制</h3><p>我们考虑用Transformer模型翻译下面这一句话：</p><p>“The animal didn’t cross the street because it was too tired”。</p><p>当我们翻译到 it 的时候，我们知道 it 指代的是 animal 而不是 street。所以，如果有办法可以让 it 对应位置的 embedding 适当包含 animal 的信息，就会非常有用。self-attention的出现就是为了完成这一任务。</p><p>如下图所示，self attnetion会让单词 it 和 某些单词发生比较强的联系，得到比较搞的attention分数。</p><p><img src="https://uploader.shimo.im/f/UsNXjO1OpN0usuAg.png!thumbnail" alt="img"></p><p>weight(The) = softmax(v(it) * v(The) / \sqrt(d))</p><p>weight(The) = softmx(Query(It) * Key(The) / \sqrt(d))</p><p>\sum_{word} weight(word) * Value(word)</p><h3 id="Self-attention的细节"><a href="#Self-attention的细节" class="headerlink" title="Self-attention的细节"></a>Self-attention的细节</h3><p>为了实现 self-attention，每个输入的位置需要产生三个向量，分别是 <strong>Query 向量，Key 向量和 Value 向量</strong>。这些向量都是由输入 embedding 通过三个 matrices （也就是线性变化）产生的。</p><p>注意到在Transformer架构中，这些新的向量比原来的输入向量要小，原来的向量是512维，转变后的三个向量都是64维。</p><p><img src="https://uploader.shimo.im/f/MAqlj67rbPYBI7Ad.png!thumbnail" alt="img"></p><p>第二步是<strong>计算分数</strong>。当我们在用self-attention encode某个位置上的某个单词的时候，我们希望知道这个单词对应的句子上其他单词的分数。其他单词所得到的分数表示了当我们encode当前单词的时候，应该放多少的关注度在其余的每个单词上。又或者说，其他单词和我当前的单词有多大的相关性或者相似性。</p><p>在transformer模型中，这个分数是由query vector和key vector做点积（dot product）所得的结果。所以说，当我们在对第一个单词做self-attention处理的时候，第一个单词的分数是q_1和k_1的点积，第二个分数是q_1和k_2的分数。</p><p><img src="https://uploader.shimo.im/f/kW9cJM4TjTc9xtMV.png!thumbnail" alt="img"></p><p>第三步和第四步是将这些分数除以8。8这个数字是64的开方，也就是key vector的维度的开方。据说这么做可以稳定模型的gradient。然后我们将这些分数传入softmax层产生一些符合概率分布的probability scores。</p><p><img src="https://uploader.shimo.im/f/6kTtVymp0XgZCDh0.png!thumbnail" alt="img"></p><p>softmax = exp(x_i) / sum exp(x_i)</p><p>这些分数就表示了在处理当前单词的时候我们应该分配多少的关注度给其他单词。</p><p>第五步是将每个value vector乘以它们各自的attention score。第六步是把这些weighted value vectors相加，成为当前单词的vector表示。</p><p><img src="https://uploader.shimo.im/f/FrqMNrQrlo0tLBgV.png!thumbnail" alt="img"></p><p>得到了self-attention生成的词向量之后，我们就可以将它们传入feed-forward network了。</p><h3 id="Self-Attention中的矩阵运算"><a href="#Self-Attention中的矩阵运算" class="headerlink" title="Self-Attention中的矩阵运算"></a>Self-Attention中的矩阵运算</h3><p>首先，我们要对每一个词向量计算Query, Key和Value矩阵。我们把句子中的每个词向量拼接到一起变成一个矩阵X，然后乘以不同的矩阵做线性变换（WQ, WK, WV）。</p><p><img src="https://uploader.shimo.im/f/xRsGTXMRHTQsNPiL.png!thumbnail" alt="img"></p><p>然后我们就用矩阵乘法实现上面介绍过的Self-Attention机制了。</p><p><img src="https://uploader.shimo.im/f/S1IEPFyGeMUTWMBk.png!thumbnail" alt="img"></p><h3 id="Multi-headed-attention"><a href="#Multi-headed-attention" class="headerlink" title="Multi-headed attention"></a>Multi-headed attention</h3><p>在论文当中，每个embedding vector并不止产生一个key, value, query vectors，而是产生若干组这样的vectors，称之为”multi-headed” attention。这么做有几个好处：</p><ul><li><p>k: key, q: query, v: value</p></li><li><p>模型有更强的能力产生不同的attention机制，focus在不同的单词上。</p></li><li><p>attention layer有多个不同的”representation space”。</p></li></ul><p><img src="https://uploader.shimo.im/f/vQX0sIYIoqUNYO4J.png!thumbnail" alt="img"></p><p>每个attention head最终都产生了一个matrix表示这个句子中的所有词向量。在transformer模型中，我们产生了八个matrices。我们知道self attention之后就是一个feed-forward network。那么我们是否需要做8次feed-forward network运算呢？事实上是不用的。我们只需要将这8个matrices拼接到一起，然后做一次前向神经网络的运算就可以了。</p><p><img src="https://uploader.shimo.im/f/E4AxOnUs2JgGJ0bW.png!thumbnail" alt="img"></p><p>综合起来，我们可以用下面一张图表示Self-Attention模块所做的事情。</p><p><img src="https://uploader.shimo.im/f/YmfWxTGsc48tTbfi.png!thumbnail" alt="img"></p><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>thinking machine</p><p>w_1, w_2</p><p>p_1, p_2</p><p>positional_embedding = nn.Embedding(512, 300)</p><p>w_1 + p_1, w_2 + p_2, w_3 + p_3, …, w_n + p_n</p><p>到目前为止，我们的模型完全没有考虑单词的顺序。即使我们将句子中单词的顺序完全打乱，对于transformer这个模型来说，并没有什么区别。为了加入句子中单词的顺序信息，我们引入一个概念叫做positional encoding。</p><p><img src="https://uploader.shimo.im/f/1F6bv1ngvE4hEp99.png!thumbnail" alt="img"></p><p>如果我们假设输入的embedding是4个维度的，那么他们的position encodings大概长下面这样。</p><p><img src="https://uploader.shimo.im/f/1p4K2IclsvwWGw0Z.png!thumbnail" alt="img"></p><p>下面这张图的每一行表示一个positional encoding vector。第一行表示第一个单词的positional encoding，以此类推。每一行都有512个-1到1之间的数字。我们用颜色标记了这些vectors。</p><p><img src="https://uploader.shimo.im/f/HMQy3lipFooyu8rO.png!thumbnail" alt="img"></p><h3 id="Residuals"><a href="#Residuals" class="headerlink" title="Residuals"></a>Residuals</h3><p>另外一个细节是，encoder中的每一层都包含了一个residual connection和layer-normalization。如下图所示。</p><p><img src="https://uploader.shimo.im/f/1qIGhKLQLYkHahSn.png!thumbnail" alt="img"></p><p>下面这张图是更详细的vector表示。</p><p><img src="https://uploader.shimo.im/f/ivgMtxCc8CsI7lAF.png!thumbnail" alt="img"></p><p>decoder也是同样的架构。如果我们把encoder和decoder放到一起，他们就长这样。</p><p><img src="https://uploader.shimo.im/f/TumXWzLQ6XMjJneZ.png!thumbnail" alt="img"></p><h2 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h2><p>encoder最后一层会输出attention vectors K和V。K和V会被decoder用作解码的原材料。</p><p><img src="https://uploader.shimo.im/f/3AgIt6lqzgADLwuf.png!thumbnail" alt="img"></p><p>在解码的过程中，解码器每一步会输出一个token。一直循环往复，直到它输出了一个特殊的end of sequence token，表示解码结束了。</p><p><img src="https://uploader.shimo.im/f/ai444UV6eQ4E0f6O.png!thumbnail" alt="img"></p><p>decoder的self attention机制与encoder稍有不同。在decoder当中，self attention层只能看到之前已经解码的文字。我们只需要把当前输出位置之后的单词全都mask掉（softmax层之前全都设置成-inf）即可。</p><p>softmax(Q matmul K^T / sqrt(d)) matmul V</p><p>weights = Q matmul K^T: [seq_len, seq_len]</p><p>Masked Self Attention</p><p>q, k (<strong>100, 24</strong>, 35 - inf, 88 - inf, -55 - inf) –&gt; softmax –&gt; (0.9, 0.1, 0, 0, 0)</p><p>attention_mask</p><p>0, -inf, -inf, -inf</p><p>0, 0, -inf, -inf</p><p>0, 0, 0, -inf </p><p>0, 0, 0, 0</p><p>softmax(weights - attention_mask, -1)</p><p>训练</p><p>QKV, 并行训练</p><p>预测</p><p>一个单词一个单词解码</p><p>Encoder-Decoder Attention层和普通的multiheaded self-attention一样，除了它的Queries完全来自下面的decoder层，然后Key和Value来自encoder的输出向量。</p><p>batch_size * seq_length * hidden_size </p><p>padding_mask</p><p>tgt_mask</p><h3 id="最后的线性层和softmax层"><a href="#最后的线性层和softmax层" class="headerlink" title="最后的线性层和softmax层"></a>最后的线性层和softmax层</h3><p>解码器最后输出浮点向量，如何将它转成词？这是最后的线性层和softmax层的主要工作。</p><p>线性层是个简单的全连接层，将解码器的最后输出映射到一个非常大的logits向量上。假设模型已知有1万个单词（输出的词表）从训练集中学习得到。那么，logits向量就有1万维，每个值表示是某个词的可能倾向值。</p><p>softmax层将这些分数转换成概率值（都是正值，且加和为1），最高值对应的维上的词就是这一步的输出单词。</p><p><img src="https://uploader.shimo.im/f/7ffWFIfMqOgtsK22.png!thumbnail" alt="img"></p><h2 id="模型的训练"><a href="#模型的训练" class="headerlink" title="模型的训练"></a>模型的训练</h2><p>现在我们已经了解了一个训练完毕的Transformer的前向过程，顺道看下训练的概念也是非常有用的。在训练时，模型将经历上述的前向过程，当我们在标记训练集上训练时，可以对比预测输出与实际输出。为了可视化，假设输出一共只有6个单词（“a”, “am”, “i”, “thanks”, “student”, “”）</p><p><img src="https://uploader.shimo.im/f/FNgjBBm5gbUGYgbs.png!thumbnail" alt="img"></p><p>模型的词表是在训练之前的预处理中生成的</p><p>一旦定义了词表，我们就能够构造一个同维度的向量来表示每个单词，比如one-hot编码，下面举例编码“am”。</p><p><img src="https://uploader.shimo.im/f/feV2TQAHPF0z3Rr2.png!thumbnail" alt="img"></p><p>举例采用one-hot编码输出词表</p><p>下面让我们讨论下模型的loss损失，在训练过程中用来优化的指标，指导学习得到一个非常准确的模型。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>我们用一个简单的例子来示范训练，比如翻译“merci”为“thanks”。那意味着输出的概率分布指向单词“thanks”，但是由于模型未训练是随机初始化的，不太可能就是期望的输出。</p><p><img src="https://uploader.shimo.im/f/aWNgQPklQh8odGQP.png!thumbnail" alt="img"></p><p>由于模型参数是随机初始化的，未训练的模型输出随机值。我们可以对比真实输出，然后利用误差后传调整模型权重，使得输出更接近与真实输出。如何对比两个概率分布呢？简单采用 <a href="https://colah.github.io/posts/2015-09-Visual-Information/" target="_blank" rel="noopener">cross-entropy</a>或者<a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained" target="_blank" rel="noopener">Kullback-Leibler divergence</a>中的一种。鉴于这是个极其简单的例子，更真实的情况是，使用一个句子作为输入。比如，输入是“je suis étudiant”，期望输出是“i am a student”。在这个例子下，我们期望模型输出连续的概率分布满足如下条件：</p><ol><li><p>每个概率分布都与词表同维度</p></li><li><p>第一个概率分布对“i”具有最高的预测概率值。</p></li><li><p>第二个概率分布对“am”具有最高的预测概率值。</p></li><li><p>一直到第五个输出指向””标记。</p></li></ol><p><img src="https://uploader.shimo.im/f/rAnz8qY0eHgt2OAe.png!thumbnail" alt="img"></p><p>对一个句子而言，训练模型的目标概率分布</p><p>在足够大的训练集上训练足够时间之后，我们期望产生的概率分布如下所示：</p><p><img src="https://uploader.shimo.im/f/IyKk2fNcC3k4tgBt.png!thumbnail" alt="img"></p><p>训练好之后，模型的输出是我们期望的翻译。当然，这并不意味着这一过程是来自训练集。注意，每个位置都能有值，即便与输出近乎无关，这也是softmax对训练有帮助的地方。现在，因为模型每步只产生一组输出，假设模型选择最高概率，扔掉其他的部分，这是种产生预测结果的方法，叫做greedy 解码。另外一种方法是beam search，每一步仅保留最头部高概率的两个输出，根据这俩输出再预测下一步，再保留头部高概率的两个输出，重复直到预测结束</p><h2 id="更多资料"><a href="#更多资料" class="headerlink" title="更多资料"></a>更多资料</h2><ul><li><p><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a> </p></li><li><p>Transformer博客文章 <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank" rel="noopener">Transformer: A Novel Neural Network Architecture for Language Understanding</a></p></li><li><p><a href="https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html" target="_blank" rel="noopener">Tensor2Tensor announcement</a>.</p></li><li><p><a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb" target="_blank" rel="noopener">Jupyter Notebook provided as part of the Tensor2Tensor repo</a></p></li><li><p><a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener">Tensor2Tensor repo</a>.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer-XL</title>
      <link href="/2020/05/16/Transformer-XL/"/>
      <url>/2020/05/16/Transformer-XL/</url>
      
        <content type="html"><![CDATA[<h2 id="Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context"><a href="#Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context" class="headerlink" title="Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"></a>Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</h2><p><a href="https://arxiv.org/pdf/1901.02860.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.02860.pdf</a></p><p>相较于传统transformer decoder，引入两个新模块</p><ul><li>segment-level recurrence mechanism</li></ul><p><img src="https://uploader.shimo.im/f/DpNe30kuahkbOeW5.png!thumbnail" alt="img"></p><ul><li><p>a novel positional encoding scheme</p></li><li><p>考虑我们在attention机制中如何使用positional encoding</p></li></ul><p>(E_{x_i}^T+U_i^T)W_q^TW_kE_{x_j}U_j</p><p><img src="https://uploader.shimo.im/f/5zNU9yZQtQMClNiY.png!thumbnail" alt="img"></p><ul><li><p>R他们采用的是transformer当中的positional encoding</p></li><li><p>u和v是需要训练的模型参数</p></li></ul><p>最终Transformer XL模型</p><p><img src="https://uploader.shimo.im/f/Nm1uk49MIjUys1aK.png!thumbnail" alt="img"></p><p>代码</p><p><a href="https://github.com/kimiyoung/transformer-xl" target="_blank" rel="noopener">https://github.com/kimiyoung/transformer-xl</a></p><h2 id="XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding"><a href="#XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding" class="headerlink" title="XLNet: Generalized Autoregressive Pretraining for Language Understanding"></a>XLNet: Generalized Autoregressive Pretraining for Language Understanding</h2><p><a href="https://arxiv.org/pdf/1906.08237.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1906.08237.pdf</a></p><p>背景知识</p><ul><li><p>自回归语言模型（Autoregressive Language Model）：采用从左往右或从右往左的语言模型，根据上文预测下文。</p></li><li><p>缺点：只利用了预测单词左边或右边的信息，无法同时利用两边的信息。ELMo在一定程度上解决了这个问题。</p></li><li><p><img src="https://uploader.shimo.im/f/cpfGbeRfzf8c1ga8.png!thumbnail" alt="img"></p></li><li><p>自编码模型（Denoising Auto Encoder, DAE）：在输入中随机mask一些单词，利用上下文来预测被mask掉的单词。BERT采用了这一思路。</p></li><li><p><img src="https://uploader.shimo.im/f/za1FnG3zHdsbm5gD.png!thumbnail" alt="img"></p></li></ul><p>两个模型的问题</p><p><img src="https://uploader.shimo.im/f/A1rO6rAR1nAQqqvu.png!thumbnail" alt="img"></p><p>XLNet的目标是融合以上两种模型的优点，解决它们各自存在的问题。</p><p>XLNet模型：Permutation Language Modeling</p><p><img src="https://uploader.shimo.im/f/LdaKeEgG8XwH3iNj.png!thumbnail" alt="img"></p><p>Two-Stream Self-Attention</p><p><img src="https://uploader.shimo.im/f/TdQVsxOeYMoakBW0.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/iLMqF1WinQI6wOsW.png!thumbnail" alt="img"></p><p>参考资料</p><p><a href="https://zhuanlan.zhihu.com/p/70257427" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/70257427</a></p><p>代码</p><p><a href="https://github.com/zihangdai/xlnet" target="_blank" rel="noopener">https://github.com/zihangdai/xlnet</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer-XL </tag>
            
            <tag> XLNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>英文书籍word级别的文本生成代码注释</title>
      <link href="/2020/05/12/%E8%8B%B1%E6%96%87%E4%B9%A6%E7%B1%8Dword%E7%BA%A7%E5%88%AB%E7%9A%84%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E4%BB%A3%E7%A0%81%E6%B3%A8%E9%87%8A/"/>
      <url>/2020/05/12/%E8%8B%B1%E6%96%87%E4%B9%A6%E7%B1%8Dword%E7%BA%A7%E5%88%AB%E7%9A%84%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E4%BB%A3%E7%A0%81%E6%B3%A8%E9%87%8A/</url>
      
        <content type="html"><![CDATA[<p><strong>先看丘吉尔的人物传记char级别的文本生成</strong></p><p>举个小小的例子，来看看LSTM是怎么玩的</p><p>我们这里不再用char级别，我们用word级别来做。我们这里的文本预测就是，给了前面的单词以后，下一个单词是谁？</p><p>比如，hello from the other, 给出 side</p><p>第一步，一样，先导入各种库</p><h3 id="导入数据并分词"><a href="#导入数据并分词" class="headerlink" title="导入数据并分词"></a>导入数据并分词</h3><p>In [1]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dropout</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> LSTM</span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ModelCheckpoint</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> gensim.models.word2vec <span class="keyword">import</span> Word2Vec</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Using TensorFlow backend.</span><br></pre></td></tr></table></figure><p>In [8]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行资源充足的可以试试下面的代码</span></span><br><span class="line"><span class="comment"># raw_text = ''</span></span><br><span class="line"><span class="comment"># for file in os.listdir("./input/"):</span></span><br><span class="line"><span class="comment">#     # os.listdir列出路径下的所有文件的名字</span></span><br><span class="line"><span class="comment">#     if file.endswith(".txt"): # 取出后缀.txt的文件</span></span><br><span class="line"><span class="comment">#         raw_text += open("./input/"+file, errors='ignore').read() + '\n\n'</span></span><br><span class="line">raw_text = open(<span class="string">'./input/Winston_Churchil.txt'</span>).read()</span><br><span class="line"><span class="comment"># 我们仍用丘吉尔的语料生成文本</span></span><br><span class="line">raw_text = raw_text.lower()</span><br><span class="line">sentensor = nltk.data.load(<span class="string">'tokenizers/punkt/english.pickle'</span>)   </span><br><span class="line"><span class="comment"># 加载英文的划分句子的模型</span></span><br><span class="line">sents = sentensor.tokenize(raw_text)</span><br><span class="line"><span class="comment"># .tokenize对一段文本进行分句，分成各个句子组成的列表。详解看下这个博客，蛮有意思的</span></span><br><span class="line"><span class="comment"># https://blog.csdn.net/ustbbsy/article/details/80053307</span></span><br><span class="line">print(sents[:<span class="number">2</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&apos;\ufeffproject gutenberg’s real soldiers of fortune, by richard harding davis\n\nthis ebook is for the use of anyone anywhere at no cost and with\nalmost no restrictions whatsoever.&apos;, &apos;you may copy it, give it away or\nre-use it under the terms of the project gutenberg license included\nwith this ebook or online at www.gutenberg.org\n\n\ntitle: real soldiers of fortune\n\nauthor: richard harding davis\n\nposting date: february 22, 2009 [ebook #3029]\nlast updated: september 26, 2016\n\nlanguage: english\n\ncharacter set encoding: utf-8\n\n*** start of this project gutenberg ebook real soldiers of fortune ***\n\n\n\n\nproduced by david reed, and ronald j. wilson\n\n\n\n\n\nreal soldiers of fortune\n\n\nby richard harding davis\n\n\n\n\n\nmajor-general henry ronald douglas maciver\n\nany sunny afternoon, on fifth avenue, or at night in the _table d’hote_\nrestaurants of university place, you may meet the soldier of fortune who\nof all his brothers in arms now living is the most remarkable.&apos;]</span><br></pre></td></tr></table></figure><p>In [9]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">corpus = []</span><br><span class="line"><span class="keyword">for</span> sen <span class="keyword">in</span> sents: <span class="comment"># 针对每个句子，再次进行分词。</span></span><br><span class="line">    corpus.append(nltk.word_tokenize(sen))</span><br><span class="line"></span><br><span class="line">print(len(corpus))</span><br><span class="line">print(corpus[:<span class="number">2</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1792</span><br><span class="line">[[&apos;\ufeffproject&apos;, &apos;gutenberg&apos;, &apos;’&apos;, &apos;s&apos;, &apos;real&apos;, &apos;soldiers&apos;, &apos;of&apos;, &apos;fortune&apos;, &apos;,&apos;, &apos;by&apos;, &apos;richard&apos;, &apos;harding&apos;, &apos;davis&apos;, &apos;this&apos;, &apos;ebook&apos;, &apos;is&apos;, &apos;for&apos;, &apos;the&apos;, &apos;use&apos;, &apos;of&apos;, &apos;anyone&apos;, &apos;anywhere&apos;, &apos;at&apos;, &apos;no&apos;, &apos;cost&apos;, &apos;and&apos;, &apos;with&apos;, &apos;almost&apos;, &apos;no&apos;, &apos;restrictions&apos;, &apos;whatsoever&apos;, &apos;.&apos;], [&apos;you&apos;, &apos;may&apos;, &apos;copy&apos;, &apos;it&apos;, &apos;,&apos;, &apos;give&apos;, &apos;it&apos;, &apos;away&apos;, &apos;or&apos;, &apos;re-use&apos;, &apos;it&apos;, &apos;under&apos;, &apos;the&apos;, &apos;terms&apos;, &apos;of&apos;, &apos;the&apos;, &apos;project&apos;, &apos;gutenberg&apos;, &apos;license&apos;, &apos;included&apos;, &apos;with&apos;, &apos;this&apos;, &apos;ebook&apos;, &apos;or&apos;, &apos;online&apos;, &apos;at&apos;, &apos;www.gutenberg.org&apos;, &apos;title&apos;, &apos;:&apos;, &apos;real&apos;, &apos;soldiers&apos;, &apos;of&apos;, &apos;fortune&apos;, &apos;author&apos;, &apos;:&apos;, &apos;richard&apos;, &apos;harding&apos;, &apos;davis&apos;, &apos;posting&apos;, &apos;date&apos;, &apos;:&apos;, &apos;february&apos;, &apos;22&apos;, &apos;,&apos;, &apos;2009&apos;, &apos;[&apos;, &apos;ebook&apos;, &apos;#&apos;, &apos;3029&apos;, &apos;]&apos;, &apos;last&apos;, &apos;updated&apos;, &apos;:&apos;, &apos;september&apos;, &apos;26&apos;, &apos;,&apos;, &apos;2016&apos;, &apos;language&apos;, &apos;:&apos;, &apos;english&apos;, &apos;character&apos;, &apos;set&apos;, &apos;encoding&apos;, &apos;:&apos;, &apos;utf-8&apos;, &apos;***&apos;, &apos;start&apos;, &apos;of&apos;, &apos;this&apos;, &apos;project&apos;, &apos;gutenberg&apos;, &apos;ebook&apos;, &apos;real&apos;, &apos;soldiers&apos;, &apos;of&apos;, &apos;fortune&apos;, &apos;***&apos;, &apos;produced&apos;, &apos;by&apos;, &apos;david&apos;, &apos;reed&apos;, &apos;,&apos;, &apos;and&apos;, &apos;ronald&apos;, &apos;j.&apos;, &apos;wilson&apos;, &apos;real&apos;, &apos;soldiers&apos;, &apos;of&apos;, &apos;fortune&apos;, &apos;by&apos;, &apos;richard&apos;, &apos;harding&apos;, &apos;davis&apos;, &apos;major-general&apos;, &apos;henry&apos;, &apos;ronald&apos;, &apos;douglas&apos;, &apos;maciver&apos;, &apos;any&apos;, &apos;sunny&apos;, &apos;afternoon&apos;, &apos;,&apos;, &apos;on&apos;, &apos;fifth&apos;, &apos;avenue&apos;, &apos;,&apos;, &apos;or&apos;, &apos;at&apos;, &apos;night&apos;, &apos;in&apos;, &apos;the&apos;, &apos;_table&apos;, &apos;d&apos;, &apos;’&apos;, &apos;hote_&apos;, &apos;restaurants&apos;, &apos;of&apos;, &apos;university&apos;, &apos;place&apos;, &apos;,&apos;, &apos;you&apos;, &apos;may&apos;, &apos;meet&apos;, &apos;the&apos;, &apos;soldier&apos;, &apos;of&apos;, &apos;fortune&apos;, &apos;who&apos;, &apos;of&apos;, &apos;all&apos;, &apos;his&apos;, &apos;brothers&apos;, &apos;in&apos;, &apos;arms&apos;, &apos;now&apos;, &apos;living&apos;, &apos;is&apos;, &apos;the&apos;, &apos;most&apos;, &apos;remarkable&apos;, &apos;.&apos;]]</span><br></pre></td></tr></table></figure><h1 id="word2vec生成词向量"><a href="#word2vec生成词向量" class="headerlink" title="word2vec生成词向量"></a>word2vec生成词向量</h1><p>In [45]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">w2v_model = Word2Vec(corpus, size=<span class="number">128</span>, window=<span class="number">5</span>, min_count=<span class="number">2</span>, workers=<span class="number">4</span>)</span><br><span class="line"><span class="comment"># Word2Vec()参数看这个博客：https://www.cnblogs.com/pinard/p/7278324.html</span></span><br><span class="line"><span class="comment"># size：词向量的维度</span></span><br><span class="line"><span class="comment"># window：即词向量上下文最大距离，window越大，则和某一词较远的词也会产生上下文关系。默认值为5。</span></span><br><span class="line"><span class="comment"># min_count：需要计算词向量的最小词频。这个值可以去掉一些很生僻的低频词，默认是5。如果是小语料，可以调低这个值。</span></span><br><span class="line"><span class="comment"># workers：用于控制训练的并行数。</span></span><br><span class="line"></span><br><span class="line">print(w2v_model[<span class="string">'office'</span>][:<span class="number">20</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[-0.03379476 -0.22743131 -0.17660786 -0.00957653 -0.10752155 -0.14298159</span><br><span class="line">  0.02914934 -0.08970737 -0.15872304 -0.05246524 -0.00084796 -0.05634443</span><br><span class="line"> -0.1461402   0.03880814 -0.12331649 -0.06511988 -0.08555544 -0.2300725</span><br><span class="line"> -0.0083805   0.02204316]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).</span><br></pre></td></tr></table></figure><h3 id="构造训练集"><a href="#构造训练集" class="headerlink" title="构造训练集"></a>构造训练集</h3><p>In [46]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">raw_input = [item <span class="keyword">for</span> sublist <span class="keyword">in</span> corpus <span class="keyword">for</span> item <span class="keyword">in</span> sublist]</span><br><span class="line">print(len(raw_input)) <span class="comment"># 原始语料库里的词语总数</span></span><br><span class="line">text_stream = []</span><br><span class="line">vocab = w2v_model.wv.vocab <span class="comment"># 查看w2v_model生成的词向量</span></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> raw_input:</span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> vocab:</span><br><span class="line">        text_stream.append(word)</span><br><span class="line">print(len(text_stream))  </span><br><span class="line"><span class="comment"># 查看去掉低频词后的总的词数，因为min_count把低频词去掉了</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">55562</span><br><span class="line">51876</span><br></pre></td></tr></table></figure><p>In [47]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 处理方式同char级别的文本生成</span></span><br><span class="line">seq_length = <span class="number">10</span> </span><br><span class="line">x = []</span><br><span class="line">y = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(text_stream) - seq_length):</span><br><span class="line">    given = text_stream[i:i + seq_length]</span><br><span class="line">    predict = text_stream[i + seq_length]</span><br><span class="line">    x.append([w2v_model[word] <span class="keyword">for</span> word <span class="keyword">in</span> given])</span><br><span class="line">    y.append(w2v_model[predict])</span><br><span class="line"></span><br><span class="line">x = np.reshape(x, (<span class="number">-1</span>, seq_length, <span class="number">128</span>))</span><br><span class="line">y = np.reshape(y, (<span class="number">-1</span>,<span class="number">128</span>))</span><br><span class="line">print(x.shape)</span><br><span class="line">print(y.shape)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).</span><br><span class="line">  </span><br><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).</span><br><span class="line">  if __name__ == &apos;__main__&apos;:</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(51866, 10, 128)</span><br><span class="line">(51866, 128)</span><br></pre></td></tr></table></figure><h3 id="构建和训练模型"><a href="#构建和训练模型" class="headerlink" title="构建和训练模型"></a>构建和训练模型</h3><p>In [53]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(LSTM(<span class="number">256</span>, input_shape=(seq_length, <span class="number">128</span>),dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.2</span>))</span><br><span class="line"><span class="comment"># 第一个dropout是x和hidden之间的dropout</span></span><br><span class="line"><span class="comment"># 第二个recurrent_dropout，这里我理解为是横向不同时刻隐藏层之间的dropout</span></span><br><span class="line">model.add(Dropout(<span class="number">0.2</span>)) <span class="comment"># 第三个，这里我理解为纵向层与层之间的dropout</span></span><br><span class="line">model.add(Dense(<span class="number">128</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.compile(loss=<span class="string">'mse'</span>, optimizer=<span class="string">'adam'</span>)</span><br><span class="line"><span class="comment"># 损失用的均方差损失，优化器adam</span></span><br></pre></td></tr></table></figure><p>In [54]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(x, y, nb_epoch=<span class="number">10</span>, batch_size=<span class="number">4096</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.</span><br><span class="line">  &quot;&quot;&quot;Entry point for launching an IPython kernel.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/10</span><br><span class="line">51866/51866 [==============================] - 28s 539us/step - loss: 0.3177</span><br><span class="line">Epoch 2/10</span><br><span class="line">51866/51866 [==============================] - 28s 542us/step - loss: 0.1405</span><br><span class="line">Epoch 3/10</span><br><span class="line">51866/51866 [==============================] - 29s 560us/step - loss: 0.1329</span><br><span class="line">Epoch 4/10</span><br><span class="line">51866/51866 [==============================] - 30s 584us/step - loss: 0.1318</span><br><span class="line">Epoch 5/10</span><br><span class="line">51866/51866 [==============================] - 28s 548us/step - loss: 0.1313</span><br><span class="line">Epoch 6/10</span><br><span class="line">51866/51866 [==============================] - 30s 574us/step - loss: 0.1309</span><br><span class="line">Epoch 7/10</span><br><span class="line">51866/51866 [==============================] - 30s 570us/step - loss: 0.1306</span><br><span class="line">Epoch 8/10</span><br><span class="line">51866/51866 [==============================] - 29s 551us/step - loss: 0.1303</span><br><span class="line">Epoch 9/10</span><br><span class="line">51866/51866 [==============================] - 27s 524us/step - loss: 0.1299</span><br><span class="line">Epoch 10/10</span><br><span class="line">51866/51866 [==============================] - 27s 512us/step - loss: 0.1296</span><br></pre></td></tr></table></figure><p>Out[54]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;keras.callbacks.History at 0x1a32c9a2b0&gt;</span><br></pre></td></tr></table></figure><h3 id="预测模型"><a href="#预测模型" class="headerlink" title="预测模型"></a>预测模型</h3><p>In [55]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 代码注释同丘吉尔的人物传记char级别的文本生成</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_next</span><span class="params">(input_array)</span>:</span></span><br><span class="line">    x = np.reshape(input_array, (<span class="number">-1</span>,seq_length,<span class="number">128</span>))</span><br><span class="line">    y = model.predict(x)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">string_to_index</span><span class="params">(raw_input)</span>:</span></span><br><span class="line">    raw_input = raw_input.lower()</span><br><span class="line">    input_stream = nltk.word_tokenize(raw_input)</span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> input_stream[(len(input_stream)-seq_length):]:</span><br><span class="line">        res.append(w2v_model[word])</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">y_to_word</span><span class="params">(y)</span>:</span></span><br><span class="line">    word = w2v_model.most_similar(positive=y, topn=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> word</span><br></pre></td></tr></table></figure><p>In [56]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_article</span><span class="params">(init, rounds=<span class="number">30</span>)</span>:</span></span><br><span class="line">    in_string = init.lower()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(rounds):</span><br><span class="line">        n = y_to_word(predict_next(string_to_index(in_string)))</span><br><span class="line">        in_string += <span class="string">' '</span> + n[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> in_string</span><br></pre></td></tr></table></figure><p>In [58]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">init = <span class="string">'His object in coming to New York was to engage officers for that service. He came at an  moment'</span></span><br><span class="line">article = generate_article(init)</span><br><span class="line">print(article) <span class="comment"># 语料库较小，可以看到重复了</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).</span><br><span class="line">  if sys.path[0] == &apos;&apos;:</span><br><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).</span><br><span class="line">  app.launch_new_instance()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">his object in coming to new york was to engage officers for that service. he came at an  moment battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LSTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文本生成任务</title>
      <link href="/2020/05/10/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1/"/>
      <url>/2020/05/10/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1/</url>
      
        <content type="html"><![CDATA[<p>主要讨论</p><ul><li><p>文本生成的方法：<strong>inference</strong></p></li><li><p>增加文本生成的多样性：<strong>variational auto encoder</strong></p></li><li><p>可以<strong>控制的文本生成、文本风格迁移</strong></p></li><li><p>Generative Adversarial Networks</p></li><li><p>Data to text</p></li></ul><p>log loss:</p><ul><li><p>[s1, s2, …, s_n] –&gt; softmax(s) = exp(s_i) / sum_i exp(s_i)  </p></li><li><p>p_i log q_i</p></li></ul><h1 id="关于文本生成"><a href="#关于文本生成" class="headerlink" title="关于文本生成"></a>关于文本生成</h1><p>之前的课程中，我们主要讨论了Natural Language Understanding，也就是给你一段文字，如何从各个方面去理解它。常见的NLU任务有：文本分类，情感分类，<strong>命名实体识别（Named Entity Recognition, NER），Relation Extraction</strong>等等。也就是说，从文字中提取出我们想要了解的关键信息。</p><p>这节课我们来讨论文本生成的一些方法。</p><p>对于文本生成，我们关心哪些问题？</p><ul><li><p>与文本理解相反，我们有一些想要表达的信息，这些信息可能来自于对话的历史，可能来自于结构化的数据 (structured data, data-to-text generation)。现在我们要考虑的是如何把这些我们想要表达的信息转换成自然语言的方式。这一任务在构建聊天机器人中显得尤为重要。目前看来，基于<strong>模板 (template)</strong> 的方法仍然是最保险的，但是在研究领域中，人们越来越关注<strong>基于神经网络的文本生成方法</strong>。</p></li><li><p>基于上文的文本补全任务，故事生成，生成式聊天机器人</p></li><li><p>人们一直希望计算机可以完成一些人类才可以完成的创造性任务，例如作画。AI作画实际上已经不是什么新闻了，Portrait of Edmond de Belamy，一幅AI创作的画像，拍卖出了43.2万美金的高价。</p></li><li><p>那么AI能不能写文章讲故事呢？关于文本生成的研究相对来说没有特别客观的评价指标，所以很多时候人们会按照自己的主观评价来判断模型的好坏。例如给定故事的上文，AI系统能不能很好地补全这个故事呢？</p></li><li><p>文本补全这个任务本质上就是训练一个语言模型，当然也有人尝试使用Seq2Seq的方法做文本生成。目前看来最强的模型是基于GPT-2预训练的语言模型。很多研究者使用GPT-2来进行文本生成相关的实验。由于训练GPT-2这样规模的语言模型需要大量的算力和数据资源，所以大部分的研究都关注在如何使用模型，也就是inference的步骤，而不在于模型的训练环节。</p></li></ul><h2 id="Greedy-Decoding"><a href="#Greedy-Decoding" class="headerlink" title="Greedy Decoding"></a>Greedy Decoding</h2><p><strong>autoregressive</strong>: 基于之前生成的文字来生成后续的文字? </p><p>P(y_i | y_1, … y_{i-1})</p><p>parallel generation</p><p>大部分基于神经网络的文本生成模型采用的是一种条件语言模型的方法，也就是说，我们有一些先决条件，例如 auto encoder 中的隐向量，然后我们基于这个隐向量来生成句子。</p><p>大部分语言模型的基本假设是从左往右的条件概率模型，也就是说，给定了单词1至n-1，我们希望生成第n个单词。假设我们现在采用一个基于LSTM的语言模型，在当前第i个位置上，我们预测下一个生成单词的概率分布为 p = (p_1, <strong>p_2</strong>, … p_|V|)，那么在当前位置上我们应该生成什么单词呢？</p><p>argmax_i p_i = 2</p><p>一个最简单的方法是使用Greedy Decoding，也就是说，我们直接采用 argmax_i (p_i) 即可。当然，同学们很容易联想到，这种decoding的方法是有问题的，因为每次都选择最大概率的单词并不能保证我们生成出来的句子的总体概率分布是最大的。事实上，大部分时候这样生成的句子其实是不好的。然而我们没有办法遍历所有可能的句子：首先句子的长度是不确定的；即使我们假定自己知道句子的长度 l，如果在每个位置上考虑每个可能的单词，我们需要考虑 |V|^l 种可能的情况，在计算资源上也是不现实的。</p><p>一种妥协的方法是采用 <strong>Beam Search</strong> （<a href="https://shimo.im/docs/rHwdq8wd8txyXjP6）。也就是说，在decoding的每个步骤，我们都保留着" target="_blank" rel="noopener">https://shimo.im/docs/rHwdq8wd8txyXjP6）。也就是说，在decoding的每个步骤，我们都保留着</a> <strong>top K</strong> 个可能的候选单词，然后到了下一个步骤的时候，我们对这 K 个单词都做下一步 decoding，分别选出 top K，然后对这 K^2 个候选句子再挑选出 <strong>top K 个句子</strong>。以此类推一直到 decoding 结束为止。当然 Beam Search 本质上也是一个 greedy decoding 的方法，所以我们无法保证自己一定可以得到最好的 decoding 结果。</p><p>p(x_1, x_2, …, x_n) = log (p(x_1) * p(x_2 | x_1) … p(x_n | x_1, …, x_{n-1})) / n</p><p><strong>Greedy Decoding</strong>的问题</p><ul><li><p>容易出现很无聊的回答：I don’t know. </p></li><li><p>容易重复自己：I don’t know. I don’t know. I don’t know. I don’t know. I don’t know. I don’t know. </p></li><li><p>Beam search K = 200</p></li></ul><h2 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h2><p>argmax 不一定是最好的</p><p>vocab(y_i) = [<strong>0.9</strong>, 0.05, 0.01, 0.01, 0.01, …., 0.01]  softmax(logits/temperature)</p><p>sample(vocab(y_i))</p><p>sample很多个句子，然后用另一个模型来打分，找出最佳generated text</p><p>sampling over the full vocabulary：我们可以在生成文本的时候引入一些随机性。例如现在语言模型告诉我们下一个单词在整个单词表上的概率分布是 p = (p_1, p_2, … p_|V|)，那么我们就可以按照这个概率分布进行随机采样，然后决定下一个单词生成什么。采样相对于greedy方法的好处是，我们生成的文字开始有了一些随机性，不会总是生成很机械的回复了。</p><p>1 - 0.98^n</p><p>Sampling的问题</p><ul><li><p>生成的话容易不连贯，上下文比较矛盾。</p></li><li><p>容易生成奇怪的话，出现<strong>罕见词</strong>。</p></li></ul><p>top-k sampling 可以缓解生成罕见单词的问题。比如说，我们可以每次只在概率最高的50个单词中按照概率分布做采样。</p><p>我只保留top-k个probability的单词，然后在这些单词中根据概率做sampling</p><h2 id="Neucleus-Sampling"><a href="#Neucleus-Sampling" class="headerlink" title="Neucleus Sampling"></a>Neucleus Sampling</h2><h3 id="The-Curious-Case-of-Neural-Text-Degeneration"><a href="#The-Curious-Case-of-Neural-Text-Degeneration" class="headerlink" title="The Curious Case of Neural Text Degeneration"></a>The Curious Case of Neural Text Degeneration</h3><p><a href="https://arxiv.org/pdf/1904.09751.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.09751.pdf</a></p><p><img src="https://uploader.shimo.im/f/8IjYXmjBFmAwnJy3.png!thumbnail" alt="img"></p><p>这篇文章在前些日子引起了不小的关注。文章提出了一种做sampling的方法，叫做 Neucleus Sampling。</p><p>Neucleus Sampling的基本思想是，我们不做beam search，而是做top p sampling。</p><p>设置一个threshold，p=0.95</p><p>top-k sampling 和 neucleus sampling 的代码：<a href="https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317" target="_blank" rel="noopener">https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317</a></p><h1 id="Variational-Auto-Encoder-VAE"><a href="#Variational-Auto-Encoder-VAE" class="headerlink" title="Variational Auto Encoder (VAE)"></a>Variational Auto Encoder (VAE)</h1><h2 id="Auto-Encoder-自编码器"><a href="#Auto-Encoder-自编码器" class="headerlink" title="Auto Encoder 自编码器"></a>Auto Encoder 自编码器</h2><p>NLP中的一个重要问题是获得一种语言的表示，无论是单词的表示还是句子的表示。为了获得句子的表示，一种直观的思路是训练一个auto encoder，也就是说一个encoder用来编码一个句子，把一个句子转换成一个vector；另一个decoder用来解码一个句子，也就是说把一个vector解码成一个句子。auto encoder 事实上是一种数据压缩的方法。</p><p>Encoder(text) –&gt; vector</p><p>Decoder(vector) –&gt; text</p><p>Encoder：得到很好的文本表示，这个文本表示你可用用于任何其他的任务。</p><p>Decoder: conditional language model</p><p>generalize能力不一定好。过拟合。</p><p>预期：希望类似的句子，能够变成比较相近的vector。不类似的句子，能够距离比较远。</p><p>Decoder(0,200,-23, 122) –&gt; text?</p><p>我爱[MASK]然语[MASK]处理 –&gt; vector –&gt; 我爱自然语言处理</p><p>在 auto encoder 的基础上又衍生出了各种类型的 auto encoder，例如 <strong>denoising</strong> auto encoder （<a href="https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf）。denoising" target="_blank" rel="noopener">https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf）。denoising</a> auto encoder 的基本思想是要加强 auto encoder 的 robustness。也就是说，我们希望把输入句子的一部分给“污染” (corrupt) 了，但是我们希望在经过编码和解码的过程之后，我们能够得到原来的正确的句子。事实上 BERT 的 masking 就是一种“污染”的手段。</p><p>Encoder(corrupt(text)) –&gt; vector</p><p>Decoder(vector) –&gt; text</p><p>随机产生一个vector –&gt; decoder –&gt; 生成一个句子</p><p>mapping </p><p>N(0, 1) –&gt; 各种各样的文字</p><p>从一个分布去生成一些东西</p><p>为了训练出可以用来sample文字的模型，人们发明了variational auto encoder (VAE)。VAE与普通auto encoder的不同之处在于，我们添加了一个constraint，希望encoder编码的每个句子都能够局限在某些特定的位置。例如，我们可以要求每个句子的encoding在空间上满足一个多维标准高斯分布。</p><p><strong>vector ~ N(0, 1)</strong></p><h2 id="什么是VAE？"><a href="#什么是VAE？" class="headerlink" title="什么是VAE？"></a>什么是VAE？</h2><p>网上有很多VAE的论文，博客，建议感兴趣的同学可以选择性阅读。我们这节课不会讨论太多的数学公式，而是从比较high level的层面介绍一下VAE模型以及它所解决的一些问题。</p><p>简单来说，VAE本质上是一种生成模型，我们希望能够通过隐向量z生成数据样本x。在文本生成的问题中，这个x往往表示的是一些文本/句子等内容。</p><p><img src="https://uploader.shimo.im/f/OoUmk9RItWAALQ69.png!thumbnail" alt="img"></p><p>下面是 Kingma 在 <strong>VAE</strong> 论文中定义的优化目标。</p><p>文本–&gt; 向量表示 –&gt; 文本</p><p>auto encoder: sentence –&gt; vector –&gt; sentence</p><p>Loss = -log P_{sentence}(dec(enc(sentence)))</p><p><img src="https://uploader.shimo.im/f/1HQEntdhGB8sSbbd.png!thumbnail" alt="img"></p><p>z -&gt; z’ -&gt; decoder(z) –&gt; 一个句子</p><p><strong>crossentropyloss(decoder(encoder(x)), x)</strong></p><p><strong>我们对z没有任何的约束条件</strong></p><p>q: encoder</p><p>p: decoder</p><p>KL divergence: 计算两个概率分布的差值</p><p>z: 把句子变成一个概率分布</p><p>z: (\mu, \sigma) –&gt; 正态分布的参数</p><p>用z做采样</p><p>KL Divergence的定义</p><p><img src="https://uploader.shimo.im/f/YQtU3e2EeBc0e4VH.png!thumbnail" alt="img"></p><p>sampling</p><p>N(0,1): sampling: 0.1, 0.05, 0.2, -0.1, -100</p><p>我们可以发现，VAE模型本质上就是要最大化样本的生成概率，并且最小化样本encode之后的参数表示与某种分布(正态分布)的KL散度。之所以我们会限制数据被编码后的向量服从某个局部的正态分布，是因为我们不希望这些数据被编码之后杂乱地散布在一个空间上，而是希望信息能够得到一定程度上的压缩。之所以让他们服从一个分布而不是一些固定的值，是因为我们希望模型中能够有一些随机性，好让模型的解码器能够生成各种各样的句子。</p><p>有了这个VAE模型的架构之后，人们就可以在各种任务上玩出各种不同的花样了。</p><p>例如对于图像来说，这里的<img src="https://uploader.shimo.im/f/FSNiIgsmVLc0HyUH.png!thumbnail" alt="img">和<img src="https://uploader.shimo.im/f/XpeVKp5c144d4RWl.png!thumbnail" alt="img">可能是CNN模型，对于自然语言来说，它们可能是一些RNN/LSTM之类的模型。</p><p>下面我们来看一些VAE在NLP领域的具体模型。</p><h3 id="Generating-Sentences-from-a-Continuous-Space"><a href="#Generating-Sentences-from-a-Continuous-Space" class="headerlink" title="Generating Sentences from a Continuous Space"></a>Generating Sentences from a Continuous Space</h3><p><a href="https://arxiv.org/pdf/1511.06349.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1511.06349.pdf</a></p><p><img src="https://uploader.shimo.im/f/LeCZ7dpW9JcvYC6T.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/qAE9oBPi2Lg7LJro.png!thumbnail" alt="img"></p><p>从上图可以看到，这篇论文的思路非常简单，就是把一个句子用RNN编码起来，编码之后得到的隐向量输出两个信息\mu和\simga，分别表示一个正太分布的平均值和标准差。然后这个分布应该尽可能地接近标准正态分布，在KL散度的表示下。并且如果我们用这个分布去采样得到新的向量表示，那么decoder应该要尽可能好地复原我们原来的这个句子。</p><p>具体的实验细节我们就不展开了，但是我们看一些论文中展示的生成的句子。</p><p><img src="https://uploader.shimo.im/f/WTJqo8usWYYxMR8k.png!thumbnail" alt="img"></p><p>下面看看VAE当中编码的空间是否具有某种连续性。</p><p><img src="https://uploader.shimo.im/f/uLmDTf81yPUZJbae.png!thumbnail" alt="img"></p><p>代码阅读：</p><ul><li><p><a href="https://github.com/timbmg/Sentence-VAE/blob/master/model.py" target="_blank" rel="noopener">https://github.com/timbmg/Sentence-VAE/blob/master/model.py</a></p></li><li><p><strong>练习</strong>：这份代码已经一年多没有更新了，感兴趣的同学可以把它更新到最新版本的PyTorch上，作为写代码练习，并且在自己的数据集上做一些实验，看看能否得到与论文中类似的效果（sentence interpolation）。</p></li></ul><p>GAN: generative adversarial networks</p><ul><li><p>generator: G(z) –&gt; x 一张逼真的汽车照片</p></li><li><p>discriminator: D(x) –&gt; 这个到底是不是一张汽车的照片 二分类</p></li></ul><p>Discriminator的目标</p><p>D(G(z)) –&gt; False</p><p>D(true photo) –&gt; True</p><p>Generator 的目标 D(G(z)) –&gt; True</p><h2 id="可控制的文本生成"><a href="#可控制的文本生成" class="headerlink" title="可控制的文本生成"></a>可控制的文本生成</h2><h3 id="Toward-Controlled-Generation-of-Text"><a href="#Toward-Controlled-Generation-of-Text" class="headerlink" title="Toward Controlled Generation of Text"></a>Toward Controlled Generation of Text</h3><p><a href="https://arxiv.org/pdf/1703.00955.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1703.00955.pdf</a></p><ul><li><p>Controlled Text Generation: 控制生成文本的一些特征</p></li><li><p>Learning disentangled latent representations: 对于文本不同的特征有不同的向量表示</p></li></ul><p>模型</p><p><img src="https://uploader.shimo.im/f/NejrcnoWDRgz7k58.png!thumbnail" alt="img"></p><p>To model and control the attributes of interest in an interpretable way, we augment the unstructured variables z with a set of structured variables c each of which targets a salient and independent semantic feature of sentences.</p><p>这篇文章试图解决这样一个问题，能不能把一句话编码成几个向量(z和c)。z和c分别包含了一些不同的关于句子的信息。</p><p><img src="https://uploader.shimo.im/f/LWIaBAzxmwkLY0pj.png!thumbnail" alt="img"></p><p>模型包含几个部分，一个generator可以基于若干个向量(z和c)生成句子，几个encoder可以从句子生成z和c的分布，几个discriminator用来判断模型编码出的向量(c)是否符合example的正确分类。这个模型的好处是，我们在某种程度上分离了句子的信息。例如如果向量c用来表示的是句子的情感正负，那么模型就具备了生成正面情感的句子和负面情感句子的能力。</p><p><img src="https://uploader.shimo.im/f/IGw674vLSf4tiqHe.png!thumbnail" alt="img"></p><p>参考代码</p><p><a href="https://github.com/wiseodd/controlled-text-generation" target="_blank" rel="noopener">https://github.com/wiseodd/controlled-text-generation</a></p><p>更多阅读</p><p>VAE论文：Auto-Encoding Variational Bayes <a href="https://arxiv.org/pdf/1312.6114.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1312.6114.pdf</a></p><p>An Introduction to Variational Autoencoders <a href="https://arxiv.org/pdf/1906.02691.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1906.02691.pdf</a></p><p>Stype Transfer</p><p>文本 –&gt; 内容z，风格c</p><p>z, 换一个风格c’ –&gt; 同样内容，不同风格的文本</p><h1 id="文本生成的应用：文本风格迁移"><a href="#文本生成的应用：文本风格迁移" class="headerlink" title="文本生成的应用：文本风格迁移"></a>文本生成的应用：文本风格迁移</h1><h3 id="Style-Transfer-from-Non-Parallel-Text-by-Cross-Alignment"><a href="#Style-Transfer-from-Non-Parallel-Text-by-Cross-Alignment" class="headerlink" title="Style Transfer from Non-Parallel Text by Cross-Alignment"></a>Style Transfer from Non-Parallel Text by Cross-Alignment</h3><p>论文：<a href="https://papers.nips.cc/paper/7259-style-transfer-from-non-parallel-text-by-cross-alignment.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/7259-style-transfer-from-non-parallel-text-by-cross-alignment.pdf</a></p><p>代码：<a href="https://github.com/shentianxiao/language-style-transfer/blob/master/code/style_transfer.py" target="_blank" rel="noopener">https://github.com/shentianxiao/language-style-transfer/blob/master/code/style_transfer.py</a></p><p>style transfer 其实也是controlled text generation的一种，只是它control的是文本的风格。文本风格有很多种，例如情感的正负面，文章是随意的还是严肃的。</p><p><img src="https://uploader.shimo.im/f/2BUGTCxcajoerOmj.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/fE6u9Ap33JIRO8So.png!thumbnail" alt="img"></p><p>一个很好的repo，总结了文本风格迁移领域的paper</p><p><a href="https://github.com/fuzhenxin/Style-Transfer-in-Text" target="_blank" rel="noopener">https://github.com/fuzhenxin/Style-Transfer-in-Text</a></p><h1 id="Generative-Adversarial-Networks-GAN-在NLP上的应用"><a href="#Generative-Adversarial-Networks-GAN-在NLP上的应用" class="headerlink" title="Generative Adversarial Networks (GAN) 在NLP上的应用"></a>Generative Adversarial Networks (GAN) 在NLP上的应用</h1><p>最早Ian Goodfellow的关于GAN的文章，其基本做法就是一个<strong>generator</strong>和一个<strong>discriminator</strong>(辅助角色)，然后让两个模型互相竞争对抗，在对抗的过程中逐渐提升各自的模型能力。而其中的generator就是我们希望能够最终optimize并且被拿来使用的模型。</p><p>早期GAN主要成功应用都在于图像领域。其关键原因在于，图像的每个像素都是三个连续的RGB数值。discriminator如果给图像计算一个概率分数，当我们在优化generator希望提高这个分数的时候，我们可以使用Back Propagation算法计算梯度，然后做梯度上升/下降来完成我们想要优化的目标。</p><p>discriminator: 二分类问题 图片–&gt;分类</p><p>D(G(z)) –&gt; cross entropyloss –&gt; backprop 到generator</p><p>文本–&gt; </p><p>LSTM –&gt; P_vocab() –&gt; <strong>argmax 文字</strong> –&gt; discriminator</p><p>LSTM –&gt; P_vocab() –&gt; discriminator</p><p>而文本生成是一个不同的问题，其特殊之处在于我们在做文本生成的时候有一步<strong>argmax</strong>的操作，也就是说当我们做inference生成文字的时候，在输出层使用了argmax或者sampling的操作。当我们把argmax或者sampling得到的文字传给discriminator打分的时候，我们无法用这个分数做<strong>back propagation</strong>对生成器做优化操作。</p><p>真正的sample –&gt; one hot vector ([1, 0, 0, 0, 0, 0])</p><p>预测一个输出单词的时候：([0.8, 0.1, 0, 0.05, 0, 0.05]) –&gt; gumbel_softmax –&gt; discriminator判断一下</p><p>为了解决这个问题，人们大致走了两条路线，一条是将普通的argmax转变成可导的Gumbel-softmax，然后我们就可以同时优化generator和discriminator了。</p><p>预测一个输出单词的时候：([0.8, 0.1, 0, 0.05, 0, 0.05]) –&gt; gumbel_softmax –&gt; discriminator判断一下</p><p><a href="https://arxiv.org/pdf/1611.04051.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1611.04051.pdf</a></p><p><a href="https://www.zhihu.com/question/62631725" target="_blank" rel="noopener">https://www.zhihu.com/question/62631725</a></p><p>另外一种方法是使用<strong>Reinforcement Learning</strong>中的<strong>Policy Gradient</strong>来估算模型的gradient，并做优化。</p><p>根据当前的policy来<strong>sample</strong> steps。</p><p>NLP： policy就是我们的<strong>语言模型</strong>，也就是说根据当前的hidden state, 决定我下一步要生成什么单词。</p><p>P_vocab –&gt; argmax</p><p>P_vocab –&gt; sampling</p><p>backpropagation –&gt; 没有办法更新模型</p><p>文本翻译 – 优化<strong>BLEU?</strong></p><p>训练？ cross entropy loss</p><p>policy gradient直接优化BLEU</p><p>可以不可以找个方法估算gradient。</p><p>Policy: 当前执行的策略,在文本生成模型中，这个Policy一般就是指我们的decoder(LSTM)</p><p>Policy Gradient: 根据当前的policy执行任务，然后得到reward，并估算每个参数的gradient, SGD</p><p>这里就涉及到一些Reinforcement Learning当中的基本知识。我们可以认为一个语言模型，例如LSTM，是在做一连串连续的决策。每一个decoding的步骤，每个hidden state对应一个<strong>状态state</strong>，每个输出对应一个<strong>observation</strong>。如果我们每次输出一个文字的时候使用sampling的方法，Reinforcement Learning有一套成熟的算法可以帮助我们估算模型的梯度，这种算法叫做policy gradient。如果采用这种方法，我们也可以对模型进行优化。</p><p><a href="https://arxiv.org/pdf/1609.05473.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1609.05473.pdf</a></p><p>这一套policy gradient的做法在很多文本生成（例如翻译，image captioning）的优化问题上也经常见到。</p><p>翻译：优化BLEU</p><p>Improved Image Captioning via Policy Gradient optimization of SPIDEr</p><p><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Improved_Image_Captioning_ICCV_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Improved_Image_Captioning_ICCV_2017_paper.pdf</a></p><p>还有一些方法是，我们不做最终的文本采样，我们直接使用模型输出的在单词表上的输出分布，或者是使用LSTM中的一些hidden vector来传给discriminator，并直接优化语言模型。</p><p>我个人的看法是GAN在文本生成上的作用大小还不明确，一部分原因在于我们没有一种很好的机制去评估文本生成的好坏。我们看到很多论文其实对模型的好坏没有明确的评价，很多时候是随机产生几个句子，然后由作者来评价一下生成句子的好坏。</p><h1 id="Data-to-text"><a href="#Data-to-text" class="headerlink" title="Data-to-text"></a>Data-to-text</h1><p><img src="https://uploader.shimo.im/f/Fo4ylW4dnbgm68U9.png!thumbnail" alt="img"></p><ul><li><p>Content selection: 选择什么数据需要进入到我们的文本之中</p></li><li><p>Sentence planning: 决定句子的结构</p></li><li><p>Surface realization: 把句子结构转化成具体的字符串</p></li></ul><p><img src="https://uploader.shimo.im/f/9QQQLucVUMsIVS6P.png!thumbnail" alt="img"></p><p>问题定义</p><ul><li><p>输入: A table of records。每个record包含四个features: type, entity, value, home or away</p></li><li><p>输出: 一段文字描述</p></li></ul><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="https://github.com/Morde-kaiser/LearningNotes/blob/master/GAN-Overview-Chinese.pdf" target="_blank" rel="noopener">https://github.com/Morde-kaiser/LearningNotes/blob/master/GAN-Overview-Chinese.pdf</a></p><p>William Wang关于GAN in NLP的slides: <a href="http://sameersingh.org/files/ppts/naacl19-advnlp-part1-william-slides.pdf" target="_blank" rel="noopener">http://sameersingh.org/files/ppts/naacl19-advnlp-part1-william-slides.pdf</a></p><p>这篇博文也讲的很好</p><p><a href="https://zhuanlan.zhihu.com/p/29168803" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29168803</a></p><p>参考该知乎专栏文章 <a href="https://zhuanlan.zhihu.com/p/36880287" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/36880287</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> inference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BERT&amp;ELMo&amp;co</title>
      <link href="/2020/05/09/%E5%B8%B8%E8%A7%81%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
      <url>/2020/05/09/%E5%B8%B8%E8%A7%81%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="【译】The-Illustrated-BERT-ELMo-and-co"><a href="#【译】The-Illustrated-BERT-ELMo-and-co" class="headerlink" title="【译】The Illustrated BERT, ELMo, and co."></a>【译】The Illustrated BERT, ELMo, and co.</h2><p><a href="https://www.mihaileric.com/posts/deep-contextualized-word-representations-elmo/" target="_blank" rel="noopener">ELMo: Contextualized Word Vectors</a></p><p>本文由Adam Liu授权转载，源链接 <a href="https://blog.csdn.net/qq_41664845/article/details/84787969#comments" target="_blank" rel="noopener">https://blog.csdn.net/qq_41664845/article/details/84787969</a></p><p>原文链接：The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</p><p>作者：Jay Alammar</p><p>修改：褚则伟 <a href="mailto:zeweichu@gmail.com" target="_blank" rel="noopener">zeweichu@gmail.com</a></p><p>BERT论文地址：BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding <a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">https://arxiv.org/abs/1810.04805</a></p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>2018年可谓是自然语言处理（NLP）的元年，在我们如何以最能捕捉潜在语义关系的方式  来辅助计算机对的句子概念性的理解 这方面取得了极大的发展进步。此外， NLP领域的一些开源社区已经发布了很多强大的组件，我们可以在自己的模型训练过程中免费的下载使用。（可以说今年是NLP的ImageNet时刻，因为这和几年前计算机视觉的发展很相似）</p><p><img src="https://uploader.shimo.im/f/Z0tgsQt24GAoKjkj.png!thumbnail" alt="img"></p><p>上图中，最新发布的BERT是一个NLP任务的里程碑式模型，它的发布势必会带来一个NLP的新时代。BERT是一个算法模型，它的出现打破了大量的自然语言处理任务的记录。在BERT的论文发布不久后，Google的研发团队还开放了该模型的代码，并提供了一些在大量数据集上预训练好的算法模型下载方式。Goole开源这个模型，并提供预训练好的模型，这使得所有人都可以通过它来构建一个涉及NLP的算法模型，节约了大量训练语言模型所需的时间，精力，知识和资源。</p><p><img src="https://uploader.shimo.im/f/6xxJC31NvvYDCGFQ.png!thumbnail" alt="img"></p><p>BERT集成了最近一段时间内NLP领域中的一些顶尖的思想，包括但不限于 Semi-supervised Sequence Learning (by Andrew Dai and Quoc Le), ELMo (by Matthew Peters and researchers from AI2 and UW CSE), ULMFiT (by fast.ai founder Jeremy Howard and Sebastian Ruder), and the OpenAI transformer (by OpenAI researchers Radford, Narasimhan, Salimans, and Sutskever), and the Transformer (Vaswani et al).。</p><p>你需要注意一些事情才能恰当的理解BERT的内容，不过，在介绍模型涉及的概念之前可以使用BERT的方法。 </p><h2 id="示例：句子分类"><a href="#示例：句子分类" class="headerlink" title="示例：句子分类"></a>示例：句子分类</h2><p>使用BERT最简单的方法就是做一个文本分类模型，这样的模型结构如下图所示：</p><p><img src="https://uploader.shimo.im/f/8T7zkJ6MWgwE98oi.png!thumbnail" alt="img"></p><p>为了训练一个这样的模型，（主要是训练一个分类器），在训练阶段BERT模型发生的变化很小。该训练过程称为微调，并且源于 Semi-supervised Sequence Learning 和 ULMFiT.。</p><p>为了更方便理解，我们下面举一个分类器的例子。分类器是属于监督学习领域的，这意味着你需要一些标记的数据来训练这些模型。对于垃圾邮件分类器的示例，标记的数据集由邮件的内容和邮件的类别2部分组成（类别分为“垃圾邮件”或“非垃圾邮件”）。</p><h1 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h1><p>现在您已经了解了如何使用BERT的示例，让我们仔细了解一下他的工作原理。</p><p><img src="https://uploader.shimo.im/f/1jNYmPwPIDEzhsLv.png!thumbnail" alt="img"></p><p>BERT的论文中介绍了2种版本：</p><ul><li><p>BERT BASE - 与OpenAI Transformer的尺寸相当，以便比较性能</p></li><li><p>BERT LARGE - 一个非常庞大的模型，它完成了本文介绍的最先进的结果。</p></li></ul><p>BERT的基础集成单元是Transformer的Encoder。关于Transformer的介绍可以阅读作者之前的文章：The Illustrated Transformer，该文章解释了Transformer模型 - BERT的基本概念以及我们接下来要讨论的概念。</p><p>2个BERT的模型都有一个很大的编码器层数，（论文里面将此称为Transformer Blocks） - 基础版本就有12层，进阶版本有24层。同时它也有很大的前馈神经网络（ 768和1024个隐藏层神经元），还有很多attention heads（12-16个）。这超过了Transformer论文中的参考配置参数（6个编码器层，512个隐藏层单元，和8个注意头）</p><h2 id="模型输入"><a href="#模型输入" class="headerlink" title="模型输入"></a>模型输入</h2><p>输入的第一个字符为[CLS]，在这里字符[CLS]表达的意思很简单 - Classification （分类）。</p><p>BERT与Transformer 的编码方式一样。将固定长度的字符串作为输入，数据由下而上传递计算，每一层都用到了self attention，并通过前馈神经网络传递其结果，将其交给下一个编码器。</p><p><img src="https://uploader.shimo.im/f/4VwjFpmDltoJInZh.png!thumbnail" alt="img"></p><p>这样的架构，似乎是沿用了Transformer 的架构（除了层数，不过这是我们可以设置的参数）。那么BERT与Transformer 不同之处在哪里呢？可能在模型的输出上，我们可以发现一些端倪。</p><h2 id="模型输出"><a href="#模型输出" class="headerlink" title="模型输出"></a>模型输出</h2><p>每个位置返回的输出都是一个隐藏层大小的向量（基本版本BERT为768）。以文本分类为例，我们重点关注第一个位置上的输出（第一个位置是分类标识[CLS]） 。如下图</p><p>该向量现在可以用作我们选择的分类器的输入，在论文中指出使用单层神经网络作为分类器就可以取得很好的效果。原理如下：</p><p><img src="https://uploader.shimo.im/f/X6bsq7gTFDERfO9s.png!thumbnail" alt="img"></p><p>例子中只有垃圾邮件和非垃圾邮件，如果你有更多的label，你只需要增加输出神经元的个数即可，另外把最后的激活函数换成softmax即可。</p><h2 id="Parallels-with-Convolutional-Nets（BERT-VS卷积神经网络）"><a href="#Parallels-with-Convolutional-Nets（BERT-VS卷积神经网络）" class="headerlink" title="Parallels with Convolutional Nets（BERT VS卷积神经网络）"></a>Parallels with Convolutional Nets（BERT VS卷积神经网络）</h2><p>对于那些具有计算机视觉背景的人来说，这个矢量切换应该让人联想到VGGNet等网络的卷积部分与网络末端的完全连接的分类部分之间发生的事情。你可以这样理解，实质上这样理解也很方便。</p><p><img src="https://uploader.shimo.im/f/SIlXQTqB9vM4DEDi.png!thumbnail" alt="img"></p><h1 id="词嵌入的新时代〜"><a href="#词嵌入的新时代〜" class="headerlink" title="词嵌入的新时代〜"></a>词嵌入的新时代〜</h1><p>BERT的开源随之而来的是一种词嵌入的更新。到目前为止，词嵌入已经成为NLP模型处理自然语言的主要组成部分。诸如Word2vec和Glove 等方法已经广泛的用于处理这些问题，在我们使用新的词嵌入之前，我们有必要回顾一下其发展。</p><h2 id="Word-Embedding-Recap"><a href="#Word-Embedding-Recap" class="headerlink" title="Word Embedding Recap"></a>Word Embedding Recap</h2><p>为了让机器可以学习到文本的特征属性，我们需要一些将文本数值化的表示的方式。Word2vec算法通过使用一组固定维度的向量来表示单词，计算其方式可以捕获到单词的语义及单词与单词之间的关系。使用Word2vec的向量化表示方式可以用于判断单词是否相似，对立，或者说判断“男人‘与’女人”的关系就如同“国王”与“王后”。（这些话是不是听腻了〜 emmm水文必备）。另外还能捕获到一些语法的关系，这个在英语中很实用。例如“had”与“has”的关系如同“was”与“is”的关系。</p><p>这样的做法，我们可以使用大量的文本数据来预训练一个词嵌入模型，而这个词嵌入模型可以广泛用于其他NLP的任务，这是个好主意，这使得一些初创公司或者计算资源不足的公司，也能通过下载已经开源的词嵌入模型来完成NLP的任务。</p><h2 id="ELMo：语境问题"><a href="#ELMo：语境问题" class="headerlink" title="ELMo：语境问题"></a>ELMo：语境问题</h2><p>上面介绍的词嵌入方式有一个很明显的问题，因为使用预训练好的词向量模型，那么无论上下文的语境关系如何，每个单词都只有一个唯一的且已经固定保存的向量化形式“。Wait a minute “ - 出自(Peters et. al., 2017, McCann et. al., 2017, and yet again Peters et. al., 2018 in the ELMo paper )</p><blockquote><p>“ Wait a minute ”这是一个欧美日常梗，示例：</p></blockquote><blockquote><p>​                         我：兄弟，你认真学习深度，没准能拿80W年薪啊。</p></blockquote><blockquote><p>​                         你：Wait a minute，这么好，你为啥不做。 </p></blockquote><p>这和中文的同音字其实也类似，用这个举一个例子吧， ‘长’ 这个字，在 ‘长度’ 这个词中表示度量，在 ‘长高’ 这个词中表示增加。那么为什么我们不通过”长’周围是度或者是高来判断它的读音或者它的语义呢？嗖嘎，这个问题就派生出语境化的词嵌入模型。</p><p><img src="https://uploader.shimo.im/f/AahBpyq3tDodAsMn.png!thumbnail" alt="img"></p><p>EMLo改变Word2vec类的将单词固定为指定长度的向量的处理方式，它是在为每个单词分配词向量之前先查看整个句子，然后使用bi-LSTM来训练它对应的词向量。</p><p><img src="https://uploader.shimo.im/f/IPV3LOYXmr8m8GN7.png!thumbnail" alt="img"></p><p>ELMo为解决NLP的语境问题作出了重要的贡献，它的LSTM可以使用与我们任务相关的大量文本数据来进行训练，然后将训练好的模型用作其他NLP任务的词向量的基准。</p><p>ELMo的秘密是什么？</p><p>ELMo会训练一个模型，这个模型接受一个句子或者单词的输入,输出最有可能出现在后面的一个单词。想想输入法，对啦，就是这样的道理。这个在NLP中我们也称作Language Modeling。这样的模型很容易实现，因为我们拥有大量的文本数据且我们可以在不需要标签的情况下去学习。</p><p><img src="https://uploader.shimo.im/f/7z7sv9ALI24kQSst.png!thumbnail" alt="img"></p><p>上图介绍了ELMo预训练的过程的步骤的一部分：</p><p>我们需要完成一个这样的任务：输入“Lets stick to”，预测下一个最可能出现的单词，如果在训练阶段使用大量的数据集进行训练，那么在预测阶段我们可能准确的预测出我们期待的下一个单词。比如输入“机器”，在‘’学习‘和‘买菜’中它最有可能的输出会是‘学习’而不是‘买菜’。</p><p>从上图可以发现，每个展开的LSTM都在最后一步完成预测。</p><p>对了真正的ELMo会更进一步，它不仅能判断下一个词，还能预测前一个词。（Bi-Lstm）</p><p><img src="https://uploader.shimo.im/f/HWw1FQCwDbUJkIi5.png!thumbnail" alt="img"></p><p>ELMo通过下图的方式将hidden states（的初始的嵌入）组合咋子一起来提炼出具有语境意义的词嵌入方式（全连接后加权求和）</p><p><img src="https://uploader.shimo.im/f/ZldUQJvmyjsiR5fx.png!thumbnail" alt="img"></p><p>ELMo pretrained embedding可以在AllenNLP的repo下找到</p><p><a href="https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md" target="_blank" rel="noopener">https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md</a></p><p>顺便说一下AllenNLP有个非常不错的关于NLP的教程</p><p><a href="https://github.com/allenai/writing-code-for-nlp-research-emnlp2018" target="_blank" rel="noopener">https://github.com/allenai/writing-code-for-nlp-research-emnlp2018</a></p><p>ELMo的几位作者都是NLP圈内的知名人士</p><ul><li><p><a href="https://people.cs.umass.edu/~miyyer/" target="_blank" rel="noopener">Mohit Iyyer: UMass</a></p></li><li><p><a href="https://www.cs.washington.edu/people/faculty/lsz/" target="_blank" rel="noopener">Luke Zettlemoyer: UWashington</a></p></li><li><p><a href="https://matt-gardner.github.io/" target="_blank" rel="noopener">Matt Gardner: Allan AI</a></p></li></ul><h3 id="更多ELMo的模型图片"><a href="#更多ELMo的模型图片" class="headerlink" title="更多ELMo的模型图片"></a>更多ELMo的模型图片</h3><p><img src="https://uploader.shimo.im/f/khgCdxx0pNIaAVe3.png!thumbnail" alt="img"></p><p>图片来源（<a href="https://tsenghungchen.github.io/posts/elmo/）" target="_blank" rel="noopener">https://tsenghungchen.github.io/posts/elmo/）</a></p><p><img src="https://uploader.shimo.im/f/TId9a8gwTE0DTjua.png!thumbnail" alt="img"></p><p>图片来源（<a href="https://www.mihaileric.com/posts/deep-contextualized-word-representations-elmo/）" target="_blank" rel="noopener">https://www.mihaileric.com/posts/deep-contextualized-word-representations-elmo/）</a></p><h2 id="ULM-FiT：NLP领域应用迁移学习"><a href="#ULM-FiT：NLP领域应用迁移学习" class="headerlink" title="ULM-FiT：NLP领域应用迁移学习"></a>ULM-FiT：NLP领域应用迁移学习</h2><p>ULM-FiT机制让模型的预训练参数得到更好的利用。所利用的参数不仅限于embeddings，也不仅限于语境embedding，ULM-FiT引入了Language Model和一个有效微调该Language Model来执行各种NLP任务的流程。这使得NLP任务也能像计算机视觉一样方便的使用迁移学习。</p><h2 id="The-Transformer：超越LSTM的结构"><a href="#The-Transformer：超越LSTM的结构" class="headerlink" title="The Transformer：超越LSTM的结构"></a>The Transformer：超越LSTM的结构</h2><p>Transformer论文和代码的发布，以及其在机器翻译等任务上取得的优异成果，让一些研究人员认为它是LSTM的替代品，事实上却是Transformer比LSTM更好的处理long-term dependancies（长程依赖）问题。Transformer Encoding和Decoding的结构非常适合机器翻译，但是怎么利用他来做文本分类的任务呢？实际上你只用使用它来预训练可以针对其他任务微调的语言模型即可。</p><h2 id="OpenAI-Transformer：用于语言模型的Transformer解码器预训练"><a href="#OpenAI-Transformer：用于语言模型的Transformer解码器预训练" class="headerlink" title="OpenAI Transformer：用于语言模型的Transformer解码器预训练"></a>OpenAI Transformer：用于语言模型的Transformer解码器预训练</h2><p>事实证明，我们并不需要一个完整的transformer结构来使用迁移学习和一个很好的语言模型来处理NLP任务。我们只需要Transformer的解码器就行了。The decoder is a good choice because it’s a natural choice for language modeling (predicting the next word) since it’s built to mask future tokens – a valuable feature when it’s generating a translation word by word.</p><p><img src="https://uploader.shimo.im/f/cBZwrEweFIQLuP0O.png!thumbnail" alt="img"></p><p>该模型堆叠了十二个Decoder层。 由于在该设置中没有Encoder，因此这些Decoder将不具有Transformer Decoder层具有的Encoder - Decoder attention层。 然而，取而代之的是一个self attention层（masked so it doesn’t peak at future tokens）。</p><p>通过这种结构调整，我们可以继续在相似的语言模型任务上训练模型：使用大量的未标记数据集训练，来预测下一个单词。举个列子：你那7000本书喂给你的模型，（书籍是极好的训练样本~比博客和推文好很多。）训练框架如下：</p><p><img src="https://uploader.shimo.im/f/KdcfSdkeNBIb5iRT.png!thumbnail" alt="img"></p><h2 id="Transfer-Learning-to-Downstream-Tasks"><a href="#Transfer-Learning-to-Downstream-Tasks" class="headerlink" title="Transfer Learning to Downstream Tasks"></a>Transfer Learning to Downstream Tasks</h2><p>通过OpenAI的transformer的预训练和一些微调后，我们就可以将训练好的模型，用于其他下游NLP任务啦。（比如训练一个语言模型，然后拿他的hidden state来做分类。），下面就介绍一下这个骚操作。（还是如上面例子：分为垃圾邮件和非垃圾邮件）</p><p><img src="https://uploader.shimo.im/f/7x6X4ngskaEUd6sY.png!thumbnail" alt="img"></p><p>OpenAI论文概述了许多Transformer使用迁移学习来处理不同类型NLP任务的例子。如下图例子所示：</p><p><img src="https://uploader.shimo.im/f/P4V9NbGQz9Q2k213.png!thumbnail" alt="img"></p><h2 id="BERT-From-Decoders-to-Encoders"><a href="#BERT-From-Decoders-to-Encoders" class="headerlink" title="BERT: From Decoders to Encoders"></a>BERT: From Decoders to Encoders</h2><p>OpenAI transformer为我们提供了基于Transformer的精密的预训练模型。但是从LSTM到Transformer的过渡中，我们发现少了些东西。ELMo的语言模型是双向的，但是OpenAI的transformer是前向训练的语言模型。我们能否让我们的Transformer模型也具有Bi-Lstm的特性呢？</p><p>R-BERT：“Hold my beer”</p><h2 id="Masked-Language-Model"><a href="#Masked-Language-Model" class="headerlink" title="Masked Language Model"></a>Masked Language Model</h2><p>BERT说：“我要用 transformer 的 encoders”</p><p>Ernie不屑道：“呵呵，你不能像Bi-Lstm一样考虑文章”</p><p>BERT自信回答道：“我们会用masks”</p><blockquote><p>解释一下Mask：</p></blockquote><blockquote></blockquote><blockquote><p>语言模型会根据前面单词来预测下一个单词，但是self-attention的注意力只会放在自己身上，那么这样100%预测到自己，毫无意义，所以用Mask，把需要预测的词给挡住。</p></blockquote><p>如下图：</p><p><img src="https://uploader.shimo.im/f/jvcJ8SPeBEwszR8M.png!thumbnail" alt="img"></p><h2 id="Two-sentence-Tasks"><a href="#Two-sentence-Tasks" class="headerlink" title="Two-sentence Tasks"></a>Two-sentence Tasks</h2><p>我们回顾一下OpenAI transformer处理不同任务的输入转换，你会发现在某些任务上我们需要2个句子作为输入，并做一些更为智能的判断，比如是否相似，比如 给出一个维基百科的内容作为输入，同时在放入一条针对该条目的问题，那么我们的算法模型能够处理这个问题吗？</p><p>为了使BERT更好的处理2个句子之间的关系，预训练的过程还有一个额外的任务：给定2个句子（A和B）,A与B是否相似？（0或者1）</p><h2 id="特殊NLP任务"><a href="#特殊NLP任务" class="headerlink" title="特殊NLP任务"></a>特殊NLP任务</h2><p>BERT的论文为我们介绍了几种BERT可以处理的NLP任务：</p><ol><li><p>短文本相似 </p></li><li><p>文本分类</p></li><li><p>QA机器人</p></li><li><p>语义标注</p></li></ol><p><img src="https://uploader.shimo.im/f/yKFxOevBvMQXvjnv.png!thumbnail" alt="img"></p><h2 id="BERT用做特征提取"><a href="#BERT用做特征提取" class="headerlink" title="BERT用做特征提取"></a>BERT用做特征提取</h2><p>微调方法并不是使用BERT的唯一方法，就像ELMo一样，你可以使用预选训练好的BERT来创建语境化词嵌入。然后你可以将这些嵌入提供给现有的模型。</p><p><img src="https://uploader.shimo.im/f/uKUkG73gELQGry4L.png!thumbnail" alt="img"></p><p>哪个向量最适合作为上下文嵌入？ 我认为这取决于任务。 本文考察了六种选择（与微调模型相比，得分为96.4）：</p><p><img src="https://uploader.shimo.im/f/bfpUyWE9YCEP9IU2.png!thumbnail" alt="img"></p><h1 id="如何使用BERT"><a href="#如何使用BERT" class="headerlink" title="如何使用BERT"></a>如何使用BERT</h1><p>使用BERT的最佳方式是通过 BERT FineTuning with Cloud TPUs (<a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb" target="_blank" rel="noopener">https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb</a>) 谷歌云上托管的笔记。如果你未使用过谷歌云TPU可以试试看，这是个不错的尝试。另外BERT也适用于TPU，CPU和GPU</p><p>下一步是查看BERT仓库中的代码：</p><ol><li><p>该模型在modeling.py（BertModel类）中构建，与vanilla Transformer编码器完全相同。</p></li><li><p>run_classifier.py是微调过程的一个示例。它还构建了监督模型的分类层。如果要构建自己的分类器，请查看该文件中的create_model()方法。</p></li><li><p>可以下载几种预先训练的模型。涵盖102种语言的多语言模型，这些语言都是在维基百科的数据基础上训练而成的。</p></li><li><p>BERT不会将单词视为tokens。相反，它注重WordPieces。 tokenization.py是将你的单词转换为适合BERT的wordPieces的tokensizer。</p></li></ol><p>我自己给BERT的代码增加了一些注解</p><p><a href="https://github.com/ZeweiChu/bert/blob/master/modeling.py" target="_blank" rel="noopener">https://github.com/ZeweiChu/bert/blob/master/modeling.py</a></p><p>重点关注其中的：</p><ul><li><p>attention_layer: <a href="https://github.com/ZeweiChu/bert/blob/master/modeling.py#L638" target="_blank" rel="noopener">https://github.com/ZeweiChu/bert/blob/master/modeling.py#L638</a></p></li><li><p>transformer_model: <a href="https://github.com/ZeweiChu/bert/blob/master/modeling.py#L868" target="_blank" rel="noopener">https://github.com/ZeweiChu/bert/blob/master/modeling.py#L868</a></p></li></ul><p>BERT的很多任务基于GLUE benchmark</p><p><a href="https://gluebenchmark.com/tasks/" target="_blank" rel="noopener">https://gluebenchmark.com/tasks/</a></p><p><a href="https://openreview.net/pdf?id=rJ4km2R5t7" target="_blank" rel="noopener">https://openreview.net/pdf?id=rJ4km2R5t7</a></p><p>最近还有一个SuperGLUE</p><p><a href="https://w4ngatang.github.io/static/papers/superglue.pdf" target="_blank" rel="noopener">https://w4ngatang.github.io/static/papers/superglue.pdf</a></p><p>您还可以查看BERT的PyTorch实现 (<a href="https://github.com/huggingface/pytorch-transformers)。" target="_blank" rel="noopener">https://github.com/huggingface/pytorch-transformers)。</a> AllenNLP库使用此实现允许将BERT嵌入与任何模型一起使用。</p><p>最近NVIDIA开源了他们53分钟训练BERT的代码</p><p><a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT" target="_blank" rel="noopener">https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT</a></p><hr><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>BERT全文翻译成中文</p><p><a href="https://zhuanlan.zhihu.com/p/59775981" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/59775981</a></p><p>图解 BERT 模型：从零开始构建 BERT</p><p><a href="https://flashgene.com/archives/20062.html" target="_blank" rel="noopener">https://flashgene.com/archives/20062.html</a></p><p>NLP必读：十分钟读懂谷歌BERT模型</p><p><a href="https://zhuanlan.zhihu.com/p/51413773" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/51413773</a></p><p>BERT Explained: State of the art language model for NLP</p><p><a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270" target="_blank" rel="noopener">https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BERT </tag>
            
            <tag> ELMo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大规模无监督预训练语言模型与应用上</title>
      <link href="/2020/05/01/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%97%A0%E7%9B%91%E7%9D%A3%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%BA%94%E7%94%A8%E4%B8%8A/"/>
      <url>/2020/05/01/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%97%A0%E7%9B%91%E7%9D%A3%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%BA%94%E7%94%A8%E4%B8%8A/</url>
      
        <content type="html"><![CDATA[<h3 id="Subword-Modeling"><a href="#Subword-Modeling" class="headerlink" title="Subword Modeling"></a>Subword Modeling</h3><p>以单词作为模型的基本单位有一些问题：</p><ul><li><p>单词量有限，我们一般会把单词量固定在50k-300k，然后没有见过的单词只能用<strong>UNK</strong>表示</p></li><li><p>zipf distribution: given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.: the rank-frequency distribution is an inverse relation. </p></li><li><p>模型参数量太大，100K * 300 = 30M个参数，仅仅是embedding层</p></li><li><p>对于很多语言，例如英语来说，很多时候单词是由几个subword拼接而成的</p></li><li><p>对于中文来说，很多常用的模型会采用分词后得到的词语作为模型的基本单元，同样存在上述问题</p></li></ul><p>可能的解决方案：</p><ul><li><p>使用subword information，例如字母作为语言的基本单元 Char-CNN</p></li><li><p>用wordpiece</p></li></ul><h2 id="解决方案：character-level-modeling"><a href="#解决方案：character-level-modeling" class="headerlink" title="解决方案：character level modeling"></a>解决方案：character level modeling</h2><ul><li>使用字母作为模型的基本输入单元</li></ul><h3 id="Ling-et-al-Finding-Function-in-Form-Compositional-Character-Models-for-Open-Vocabulary-Word-Representation"><a href="#Ling-et-al-Finding-Function-in-Form-Compositional-Character-Models-for-Open-Vocabulary-Word-Representation" class="headerlink" title="Ling et. al, Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation"></a>Ling et. al, <a href="https://aclweb.org/anthology/D15-1176" target="_blank" rel="noopener">Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation</a></h3><p>用BiLSTM把单词中的每个字母encode到一起</p><p><img src="https://uploader.shimo.im/f/V49ti0noVOsqeLRH.png!thumbnail" alt="img"></p><h3 id="Yoon-Kim-et-al-Character-Aware-Neural-Language-Models"><a href="#Yoon-Kim-et-al-Character-Aware-Neural-Language-Models" class="headerlink" title="Yoon Kim et. al, Character-Aware Neural Language Models"></a>Yoon Kim et. al, <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/viewFile/12489/12017" target="_blank" rel="noopener">Character-Aware Neural Language Models</a></h3><p><img src="https://uploader.shimo.im/f/bsR8NzGROvs0scpq.png!thumbnail" alt="img"></p><p>根据以上模型示意图思考以下问题：</p><ul><li><p>character emebdding的的维度是多少？4</p></li><li><p>有几个character 4-gram的filter？filter-size=4? 红色的 5个filter</p></li><li><p>max-over-time pooling: 3-gram 4维， 2-gram 3维 4-gram 55维</p></li><li><p>为什么不同的filter (kernel size)长度会导致不同长度的feature map?  seq_length - kernel_size + 1</p></li></ul><p>fastText</p><ul><li>与word2vec类似，但是每个单词是它的character n-gram embeddings + word emebdding</li></ul><h2 id="解决方案：使用subword作为模型的基本单元"><a href="#解决方案：使用subword作为模型的基本单元" class="headerlink" title="解决方案：使用subword作为模型的基本单元"></a>解决方案：使用subword作为模型的基本单元</h2><h3 id="Botha-amp-Blunsom-2014-Composional-Morphology-for-Word-Representations-and-Language-Modelling"><a href="#Botha-amp-Blunsom-2014-Composional-Morphology-for-Word-Representations-and-Language-Modelling" class="headerlink" title="Botha &amp; Blunsom (2014): Composional Morphology for Word Representations    and    Language Modelling"></a>Botha &amp; Blunsom (2014): <a href="http://proceedings.mlr.press/v32/botha14.pdf" target="_blank" rel="noopener">Composional Morphology for Word Representations    and    Language Modelling</a></h3><p><img src="https://uploader.shimo.im/f/FplKX422O5owOuVV.png!thumbnail" alt="img"></p><p>subword embedding</p><p><img src="https://uploader.shimo.im/f/3nQUw9cZGvwNSCyx.png!thumbnail" alt="img"></p><h3 id="Byte-Pair-Encoding-需要知道什么是BPE"><a href="#Byte-Pair-Encoding-需要知道什么是BPE" class="headerlink" title="Byte Pair Encoding (需要知道什么是BPE)"></a>Byte Pair Encoding (需要知道什么是BPE)</h3><p><a href="https://www.aclweb.org/anthology/P16-1162" target="_blank" rel="noopener">Neural Machine Translation of Rare Words with Subword Units</a></p><p>关于什么是BPE可以参考下面的文章</p><p><a href="https://www.cnblogs.com/huangyc/p/10223075.html" target="_blank" rel="noopener">https://www.cnblogs.com/huangyc/p/10223075.html</a></p><p><a href="https://leimao.github.io/blog/Byte-Pair-Encoding/" target="_blank" rel="noopener">https://leimao.github.io/blog/Byte-Pair-Encoding/</a></p><ul><li><p>首先定义所有可能的基本字符（abcde…）</p></li><li><p>然后开始循环数出最经常出现的pairs，加入到我们的候选字符（基本组成单元）中去</p></li></ul><p>a, b, c, d, …, z, A, B, …., Z.. !, @, ?, st, est, lo, low, </p><p>控制单词表的大小</p><ul><li>我只要确定iteration的次数 30000个iteartion，30000+原始字母表当中的字母数 个单词</li></ul><p>happiest</p><p>h a p p i est</p><p>LSTM</p><p>emb(h), emb(a), emb(p), emb(p), emb(i), emb(est)</p><p>happ, iest</p><p>emb(happ), emb(iest)</p><p><img src="https://uploader.shimo.im/f/0zx2ooI2uzoWLfOg.png!thumbnail" alt="img"></p><p><a href="https://www.aclweb.org/anthology/P16-1162.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/P16-1162.pdf</a></p><h2 id="中文词向量"><a href="#中文词向量" class="headerlink" title="中文词向量"></a>中文词向量</h2><h3 id="Meng-et-al-Is-Word-Segmentation-Necessary-for-Deep-Learning-of-Chinese-Representations"><a href="#Meng-et-al-Is-Word-Segmentation-Necessary-for-Deep-Learning-of-Chinese-Representations" class="headerlink" title="Meng et. al, Is Word Segmentation Necessary for Deep Learning of Chinese Representations?"></a>Meng et. al, <a href="https://arxiv.org/pdf/1905.05526.pdf" target="_blank" rel="noopener">Is Word Segmentation Necessary for Deep Learning of Chinese Representations?</a></h3><p>简单来说，这篇文章的作者生成通过他们的实验发现Chinese Word Segmentation对于语言模型、文本分类，翻译和文本关系分类并没有什么帮助，直接使用单个字作为模型的输入可以达到更好的效果。</p><blockquote><p>We benchmark neural word-based models which rely on word segmentation against neural char-based models which do not involve word segmentation in four end-to-end NLP benchmark tasks: language modeling, machine translation, sentence matching/paraphrase and text classification. Through direct comparisons between these two types of models, we find that charbased models consistently outperform wordbased models.</p></blockquote><blockquote></blockquote><blockquote><p>word-based models are more vulnerable to data sparsity and the presence of out-of-vocabulary (OOV) words, and thus more prone to overfitting</p></blockquote><p>Jiwei Li</p><p><a href="https://nlp.stanford.edu/~bdlijiwei/" target="_blank" rel="noopener">https://nlp.stanford.edu/~bdlijiwei/</a></p><h3 id="中文分词工具"><a href="#中文分词工具" class="headerlink" title="中文分词工具"></a>中文分词工具</h3><p>建议同学们可以在自己的项目中尝试以下工具</p><ul><li><p>北大中文分词工具 </p></li><li><p><a href="https://github.com/lancopku/pkuseg-python" target="_blank" rel="noopener">https://github.com/lancopku/pkuseg-python</a> </p></li><li><p>机器之心报道 <a href="https://www.jiqizhixin.com/articles/2019-01-09-12" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2019-01-09-12</a></p></li><li><p>清华分词工具 <a href="https://github.com/thunlp/THULAC-Python" target="_blank" rel="noopener">https://github.com/thunlp/THULAC-Python</a></p></li><li><p>结巴 <a href="https://github.com/fxsjy/jieba" target="_blank" rel="noopener">https://github.com/fxsjy/jieba</a></p></li></ul><h1 id="预训练句子-文档向量"><a href="#预训练句子-文档向量" class="headerlink" title="预训练句子/文档向量"></a>预训练句子/文档向量</h1><p>既然有词向量，那么我们是否可以更进一步，把句子甚至一整个文档也编码成一个向量呢？</p><p>在之前的课程中我们已经涉及到了一些句子级别的任务，例如文本分类，常常就是把一句或者若干句文本分类成一定的类别。此类模型的一般实现方式是首先把文本编码成某种文本表示方式，例如averaged word embeddings，或者双向LSTM头尾拼接，或者CNN模型等等。</p><p>文本分类</p><ul><li><p>文本通过某种方式变成一个向量</p></li><li><p>WORDAVG</p></li><li><p>LSTM</p></li><li><p>CNN</p></li><li><p>最后是一个linear layer 300维句子向量 –》 2 情感分类</p></li></ul><p>猫图片/狗图片</p><p>图片 –&gt; <strong>ResNet</strong> –&gt; 2048维向量 –&gt; (2, 2048) –&gt; 2维向量 binary cross entropy loss</p><p><strong>ResNet</strong> 预训练模型</p><p>文本 –&gt; TextResNet –&gt; 2048维向量</p><p>apply to any downstream tasks</p><p>TextResNet：LSTM模型</p><p>不同的任务（例如不同的文本分类：情感分类，话题分类）虽然最终的输出不同，但是往往拥有着相似甚至完全一样的编码层。如果我们能够预训练一个非常好的编码层，那么后续模型的负担就可以在一定程度上得到降低。这样的思想很多是来自图像处理的相关工作。例如人们在各类图像任务中发现，如果使用在ImageNet上预训练过的深层CNN网络（例如ResNet），只把最终的输出层替换成自己需要的样子，往往可以取得非常好的效果，且可以在少量数据的情况下训练出优质的模型。</p><p>在句子/文本向量预训练的领域涌现出了一系列的工作，下面我们选取一些有代表性的工作供大家学习参考。</p><h2 id="Skip-Thought"><a href="#Skip-Thought" class="headerlink" title="Skip-Thought"></a>Skip-Thought</h2><p>Kiros et. al, <a href="https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf" target="_blank" rel="noopener">Skip-Thought Vectors</a></p><p>skip-gram: distributional semantics of words 用中心词–》周围词</p><p>skip-thought: distributional semantics of sentences 用中心句–》周围句</p><p>两个句子如果总是在同一个环境下出现，那么这两个句子可能有某种含义上的联系</p><p>如何把句子map成一个向量：compositional model，RNN, LSTM, CNN, WordAvg, <strong>GRU</strong></p><p>Skip-thought 模型的思想非常简单，我们训练一个基于GRU的模型作为句子的编码器。事实上Skip-thought这个名字与Skip-gram有着千丝万缕的联系，它们基于一个共同的思想，就是一句话（一个单词）的含义与它所处的环境（context，周围句子/单词）高度相关。</p><p>如下图所示，Skipthought采用一个GRU encoder，使用编码器最后一个hidden state来表示整个句子。然后使用这个hidden state作为初始状态来解码它之前和之后的句子。</p><p>decoder: 两个conditional语言模型。</p><p>基于中心句的句子向量，优化conditional log likelihood</p><p><img src="https://uploader.shimo.im/f/keW15vUeJX4E7gam.png!thumbnail" alt="img"></p><p>一个encoder GRU</p><p><img src="https://uploader.shimo.im/f/XbXbCNlxSpk5PLuh.png!thumbnail" alt="img"></p><p>两个decoder GRU</p><p><img src="https://uploader.shimo.im/f/atuHcc6hYNIE2QOd.png!thumbnail" alt="img"></p><p>训练目标</p><p><img src="https://uploader.shimo.im/f/XCVPs561UVADzFkO.png!thumbnail" alt="img"></p><p>然后我们就可以把encoder当做feature extractor了。</p><p>类似的工作还有<a href="https://arxiv.org/pdf/1602.03483.pdf" target="_blank" rel="noopener">FastSent</a>。FastSent直接使用词向量之和来表示整个句子，然后用该句子向量来解码周围句子中的单个单词们。</p><h2 id="InferSent"><a href="#InferSent" class="headerlink" title="InferSent"></a>InferSent</h2><p><a href="https://www.aclweb.org/anthology/D17-1070" target="_blank" rel="noopener">Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</a></p><p>Natural Language Inference (NLI)</p><ul><li><p>给定两个句子，判断这两个句子之间的关系</p></li><li><p>entailment 承接关系</p></li><li><p>neutral 没有关系</p></li><li><p>contradiction 矛盾</p></li><li><p>(non_entailment)</p></li></ul><h3 id="SNLI任务"><a href="#SNLI任务" class="headerlink" title="SNLI任务"></a>SNLI任务</h3><p>给定两个句子，预测这两个句子的关系是entailment, contradiction，还是neutral  </p><p>一个简单有效的模型</p><p><img src="https://uploader.shimo.im/f/bdClv9vmULUCXgcc.png!thumbnail" alt="img"></p><p>Encoder是BiLSTM + max pooling</p><p><img src="https://uploader.shimo.im/f/Uiw1y8KX5pEw5Lri.png!thumbnail" alt="img"></p><p>模型效果</p><p><img src="https://uploader.shimo.im/f/kzvejsQ7DMI3369N.png!thumbnail" alt="img"></p><h2 id="SentEval"><a href="#SentEval" class="headerlink" title="SentEval"></a>SentEval</h2><p><a href="https://www.aclweb.org/anthology/L18-1269" target="_blank" rel="noopener">SentEval: An Evaluation Toolkit for Universal Sentence Representations</a></p><p>一个非常通用的benchmark，用来评估句子embedding是否能够很好地应用于downstream tasks。</p><p>Github: <a href="https://github.com/facebookresearch/SentEval" target="_blank" rel="noopener">https://github.com/facebookresearch/SentEval</a></p><h2 id="Document-Vector"><a href="#Document-Vector" class="headerlink" title="Document Vector"></a>Document Vector</h2><p>事实上研究者在句子向量上的各种尝试是不太成功的。主要体现在这些预训练向量并不能非常好地提升模型在各种下游任务上的表现，人们大多数时候还是从头开始训练模型。</p><p>在document vector上的尝试就更不尽如人意了，因为一个文本往往包含非常丰富的信息，而一个向量能够编码的信息量实在太小。</p><p>Learning Deep Structured Semantic Models for Web Search using Clickthrough Data</p><p><a href="https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/" target="_blank" rel="noopener">https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/</a></p><p>Hierarchical Attention Networks for Document Classification</p><p><a href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf" target="_blank" rel="noopener">https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf</a></p><h1 id="ELMo-BERT"><a href="#ELMo-BERT" class="headerlink" title="ELMo, BERT"></a><a href="https://shimo.im/docs/Y6q3gX8yGGjpWqXx" target="_blank" rel="noopener">ELMo, BERT</a></h1><p>ELMO paper: <a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1802.05365.pdf</a></p><h1 id="Transformer中的Encoder"><a href="#Transformer中的Encoder" class="headerlink" title="Transformer中的Encoder"></a><a href="https://shimo.im/docs/gPwkqCXrkJyRW89V" target="_blank" rel="noopener">Transformer中的Encoder</a></h1>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BERT </tag>
            
            <tag> Transformer </tag>
            
            <tag> ELMo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>word2vec</title>
      <link href="/2020/04/24/word2vec/"/>
      <url>/2020/04/24/word2vec/</url>
      
        <content type="html"><![CDATA[<h4 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h4><ul><li>有一个很大的词表库</li><li>在词表中的每个词都可以通过向量表征</li><li>有一个中心词c，有一个输出词o</li><li>用词c和o的相似度来计算他们之间同时出现的概率</li><li>调整这个词向量来获得最大输出概率</li></ul>]]></content>
      
      
      <categories>
          
          <category> word2vec </category>
          
      </categories>
      
      
        <tags>
            
            <tag> skip-gram </tag>
            
            <tag> cbow </tag>
            
            <tag> hierarchical softmax </tag>
            
            <tag> negative sampling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>特征工程与模型调优</title>
      <link href="/2020/04/20/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B8%8E%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98/"/>
      <url>/2020/04/20/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B8%8E%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<h2 id="机器学习特征工程"><a href="#机器学习特征工程" class="headerlink" title="机器学习特征工程"></a>机器学习特征工程</h2><h3 id="机器学习流程与概念"><a href="#机器学习流程与概念" class="headerlink" title="机器学习流程与概念"></a>机器学习流程与概念</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBncGV.jpg" alt></p><h3 id="机器学习建模流程"><a href="#机器学习建模流程" class="headerlink" title="机器学习建模流程"></a>机器学习建模流程</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBn2xU.png" alt></p><h3 id="机器学习特征工程一览"><a href="#机器学习特征工程一览" class="headerlink" title="机器学习特征工程一览"></a>机器学习特征工程一览</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBnOMD.jpg" alt></p><h3 id="机器学习特征工程介绍"><a href="#机器学习特征工程介绍" class="headerlink" title="机器学习特征工程介绍"></a>机器学习特征工程介绍</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBnjqH.jpg" alt></p><h3 id="特征清洗"><a href="#特征清洗" class="headerlink" title="特征清洗"></a>特征清洗</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBumon.jpg" alt><br><img src="https://s1.ax1x.com/2020/04/24/JBuKJ0.jpg" alt><br><img src="https://s1.ax1x.com/2020/04/24/JBu3yF.jpg" alt></p><h3 id="数值型数据上的特征工程"><a href="#数值型数据上的特征工程" class="headerlink" title="数值型数据上的特征工程"></a>数值型数据上的特征工程</h3><p>数值型数据通常以标量的形式表示数据，描述观测值、记录或者测量值。本文的数值型数据是指连续型数据而不是离散型数据，表示不同类目的数据就是后者。数值型数据也可以用向量来表示，向量的每个值或分量代表一个特征。整数和浮点数是连续型数值数据中最常见也是最常使用的数值型数据类型。即使数值型数据可以直接输入到机器学习模型中，你仍需要在建模前设计与场景、问题和领域相关的特征。因此仍需要特征工程。让我们利用 python 来看看在数值型数据上做特征工程的一些策略。我们首先加载下面一些必要的依赖（通常在 <a href="http://jupyter.org/" target="_blank" rel="noopener"><strong>Jupyter</strong> </a> botebook 上）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">&gt;</span><br><span class="line">&gt; <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">&gt;</span><br><span class="line">&gt; <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">&gt;</span><br><span class="line">&gt; <span class="keyword">import</span> scipy.stats <span class="keyword">as</span> spstats</span><br><span class="line">&gt;</span><br><span class="line">&gt; %matplotlib inline</span><br></pre></td></tr></table></figure><p>原始度量</p><p>正如我们先前提到的，根据上下文和数据的格式，原始数值型数据通常可直接输入到机器学习模型中。原始的度量方法通常用数值型变量来直接表示为特征，而不需要任何形式的变换或特征工程。通常这些特征可以表示一些值或总数。让我们加载四个数据集之一的 <a href="https://www.kaggle.com/abcsds/pokemon/data" target="_blank" rel="noopener">Pokemon </a>数据集，该数据集也在 <a href="https://www.kaggle.com/abcsds/pokemon/data" target="_blank" rel="noopener">Kaggle </a>上公布了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">poke_df = pd.read_csv(<span class="string">'datasets/Pokemon.csv'</span>, encoding=<span class="string">'utf-8'</span>) </span><br><span class="line"></span><br><span class="line">poke_df.head()</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f55514768e.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="我们的Pokemon数据集截图"><a href="#我们的Pokemon数据集截图" class="headerlink" title="我们的Pokemon数据集截图"></a>我们的Pokemon数据集截图</h5><p>Pokemon 是一个大型多媒体游戏，包含了各种口袋妖怪（Pokemon）角色。简而言之，你可以认为他们是带有超能力的动物！这些数据集由这些口袋妖怪角色构成，每个角色带有各种统计信息。</p><h4 id="数值"><a href="#数值" class="headerlink" title="数值"></a>数值</h4><p>如果你仔细地观察上图中这些数据，你会看到几个代表数值型原始值的属性，它可以被直接使用。下面的这行代码挑出了其中一些重点特征。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poke_df[[&apos;HP&apos;, &apos;Attack&apos;, &apos;Defense&apos;]].head()</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f557552811.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="带（连续型）数值数据的特征"><a href="#带（连续型）数值数据的特征" class="headerlink" title="带（连续型）数值数据的特征"></a>带（连续型）数值数据的特征</h5><p>这样，你可以直接将这些属性作为特征，如上图所示。这些特征包括 Pokemon 的 HP（血量），Attack（攻击）和 Defense（防御）状态。事实上，我们也可以基于这些字段计算出一些基本的统计量。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poke_df[[&apos;HP&apos;, &apos;Attack&apos;, &apos;Defense&apos;]].describe()</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f559f61c14.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p><strong>数值特征形式的基本描述性统计量</strong></p><p>这样你就对特征中的统计量如总数、平均值、标准差和四分位数有了一个很好的印象。</p><h4 id="记数"><a href="#记数" class="headerlink" title="记数"></a>记数</h4><p>原始度量的另一种形式包括代表频率、总数或特征属性发生次数的特征。让我们看看 <a href="https://labrosa.ee.columbia.edu/millionsong/" target="_blank" rel="noopener"><strong>millionsong</strong></a> <strong>数据集</strong>中的一个例子，其描述了某一歌曲被各种用户收听的总数或频数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">popsong_df = pd.read_csv(&apos;datasets/song_views.csv&apos;,encoding=&apos;utf-8&apos;)</span><br><span class="line"></span><br><span class="line">popsong_df.head(10)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f55bf6176f.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="数值特征形式的歌曲收听总数"><a href="#数值特征形式的歌曲收听总数" class="headerlink" title="数值特征形式的歌曲收听总数"></a>数值特征形式的歌曲收听总数</h5><p>根据这张截图，显而易见 listen_count 字段可以直接作为基于数值型特征的频数或总数。</p><h4 id="二值化"><a href="#二值化" class="headerlink" title="二值化"></a>二值化</h4><p>基于要解决的问题构建模型时，通常原始频数或总数可能与此不相关。比如如果我要建立一个推荐系统用来推荐歌曲，我只希望知道一个人是否感兴趣或是否听过某歌曲。我不需要知道一首歌被听过的次数，因为我更关心的是一个人所听过的各种各样的歌曲。在这个例子中，二值化的特征比基于计数的特征更合适。我们二值化 listen_count 字段如下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; watched = np.array(popsong_df[&apos;listen_count&apos;])</span><br><span class="line">&gt;</span><br><span class="line">&gt; watched[watched &gt;= 1] = 1</span><br><span class="line">&gt;</span><br><span class="line">&gt; popsong_df[&apos;watched&apos;] = watched</span><br></pre></td></tr></table></figure><p>你也可以使用 scikit-learn 中 preprocessing 模块的 Binarizer 类来执行同样的任务，而不一定使用 numpy 数组。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import Binarizer</span><br><span class="line"></span><br><span class="line">bn = Binarizer(threshold=0.9)</span><br><span class="line"></span><br><span class="line">pd_watched =bn.transform([popsong_df[&apos;listen_count&apos;]])[0]</span><br><span class="line"></span><br><span class="line">popsong_df[&apos;pd_watched&apos;] = pd_watched</span><br><span class="line"></span><br><span class="line">popsong_df.head(11)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56505e8ff.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="歌曲收听总数的二值化结构"><a href="#歌曲收听总数的二值化结构" class="headerlink" title="歌曲收听总数的二值化结构"></a>歌曲收听总数的二值化结构</h5><p>你可以从上面的截图中清楚地看到，两个方法得到了相同的结果。因此我们得到了一个二值化的特征来表示一首歌是否被每个用户听过，并且可以在相关的模型中使用它。</p><h4 id="数据舍入"><a href="#数据舍入" class="headerlink" title="数据舍入"></a>数据舍入</h4><p>处理连续型数值属性如比例或百分比时，我们通常不需要高精度的原始数值。因此通常有必要将这些高精度的百分比舍入为整数型数值。这些整数可以直接作为原始数值甚至分类型特征（基于离散类的）使用。让我们试着将这个观念应用到一个虚拟数据集上，该数据集描述了库存项和他们的流行度百分比。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">items_popularity =pd.read_csv(<span class="string">'datasets/item_popularity.csv'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">items_popularity[<span class="string">'popularity_scale_10'</span>] = np.array(np.round((items_popularity[<span class="string">'pop_percent'</span>] * <span class="number">10</span>)),dtype=<span class="string">'int'</span>)</span><br><span class="line"></span><br><span class="line">items_popularity[<span class="string">'popularity_scale_100'</span>] = np.array(np.round((items_popularity[<span class="string">'pop_percent'</span>] * <span class="number">100</span>)),dtype=<span class="string">'int'</span>)</span><br><span class="line"></span><br><span class="line">items_popularity</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f566e30ad2.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="不同尺度下流行度舍入结果"><a href="#不同尺度下流行度舍入结果" class="headerlink" title="不同尺度下流行度舍入结果"></a>不同尺度下流行度舍入结果</h5><p>基于上面的输出，你可能猜到我们试了两种不同的舍入方式。这些特征表明项目流行度的特征现在既有 1-10 的尺度也有 1-100 的尺度。基于这个场景或问题你可以使用这些值同时作为数值型或分类型特征。</p><h4 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h4><p>高级机器学习模型通常会对作为输入特征变量函数的输出响应建模（离散类别或连续数值）。例如，一个简单的线性回归方程可以表示为</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56ab22fb7.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>其中输入特征用变量表示为</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56c69ac66.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>权重或系数可以分别表示为</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56de74ee7.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>目标是预测响应 <strong>*y*</strong>.</p><p>在这个例子中，仅仅根据单个的、分离的输入特征，这个简单的线性模型描述了输出与输入之间的关系。</p><p>然而，在一些真实场景中，有必要试着捕获这些输入特征集一部分的特征变量之间的相关性。上述带有相关特征的线性回归方程的展开式可以简单表示为</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f5701419ee.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>此处特征可表示为</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f57162d4f7.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>表示了相关特征。现在让我们试着在 Pokemon 数据集上设计一些相关特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">atk_def = poke_df[[<span class="string">'Attack'</span>, <span class="string">'Defense'</span>]]</span><br><span class="line"></span><br><span class="line">atk_def.head()</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f572bad2cc.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>从输出数据框中，我们可以看到我们有两个数值型（连续的）特征，Attack 和 Defence。现在我们可以利用 scikit-learn 建立二度特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">pf = PolynomialFeatures(degree=<span class="number">2</span>,</span><br><span class="line"></span><br><span class="line">interaction_only=<span class="literal">False</span>,include_bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">res = pf.fit_transform(atk_def)</span><br><span class="line"></span><br><span class="line">res</span><br><span class="line"></span><br><span class="line">**Output**</span><br><span class="line"></span><br><span class="line">**------**</span><br><span class="line"></span><br><span class="line">array([[ <span class="number">49.</span>, <span class="number">49.</span>, <span class="number">2401.</span>, <span class="number">2401.</span>, <span class="number">2401.</span>],</span><br><span class="line"></span><br><span class="line">  [ <span class="number">62.</span>, <span class="number">63.</span>, <span class="number">3844.</span>, <span class="number">3906.</span>, <span class="number">3969.</span>],</span><br><span class="line"></span><br><span class="line">  [ <span class="number">82.</span>, <span class="number">83.</span>, <span class="number">6724.</span>, <span class="number">6806.</span>, <span class="number">6889.</span>],</span><br><span class="line"></span><br><span class="line">  ...,</span><br><span class="line"></span><br><span class="line">  [ <span class="number">110.</span>, <span class="number">60.</span>, <span class="number">12100.</span>, <span class="number">6600.</span>, <span class="number">3600.</span>],</span><br><span class="line"></span><br><span class="line">  [ <span class="number">160.</span>, <span class="number">60.</span>, <span class="number">25600.</span>, <span class="number">9600.</span>, <span class="number">3600.</span>],</span><br><span class="line"></span><br><span class="line">[ <span class="number">110.</span>, <span class="number">120.</span>, <span class="number">12100.</span>, <span class="number">13200.</span>, <span class="number">14400.</span>]])</span><br></pre></td></tr></table></figure><p>上面的特征矩阵一共描述了 5 个特征，其中包括新的相关特征。我们可以看到上述矩阵中每个特征的度，如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.DataFrame(pf.powers_, columns=[<span class="string">'Attack_degree'</span>,<span class="string">'Defense_degree'</span>])</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f575a65683.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>基于这个输出，现在我们可以通过每个特征的度知道它实际上代表什么。在此基础上，现在我们可以对每个特征进行命名如下。这仅仅是为了便于理解，你可以给这些特征取更好的、容易使用和简单的名字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">intr_features = pd.DataFrame(res, columns=[<span class="string">'Attack'</span>,<span class="string">'Defense'</span>,<span class="string">'Attack^2'</span>,<span class="string">'Attack x Defense'</span>,<span class="string">'Defense^2'</span>])</span><br><span class="line"></span><br><span class="line">intr_features.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f576e91376.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="数值型特征及其相关特征"><a href="#数值型特征及其相关特征" class="headerlink" title="数值型特征及其相关特征"></a>数值型特征及其相关特征</h5><p>因此上述数据代表了我们原始的特征以及它们的相关特征。</p><h4 id="分区间处理数据"><a href="#分区间处理数据" class="headerlink" title="分区间处理数据"></a>分区间处理数据</h4><p>处理原始、连续的数值型特征问题通常会导致这些特征值的分布被破坏。这表明有些值经常出现而另一些值出现非常少。除此之外，另一个问题是这些特征的值的变化范围。比如某个音乐视频的观看总数会非常大（<a href="https://www.youtube.com/watch?v=kJQP7kiw5Fk" target="_blank" rel="noopener">Despacito</a>，说你呢）而一些值会非常小。直接使用这些特征会产生很多问题，反而会影响模型表现。因此出现了处理这些问题的技巧，包括分区间法和变换。</p><p>分区间（Bining），也叫做量化，用于将连续型数值特征转换为离散型特征（类别）。可以认为这些离散值或数字是类别或原始的连续型数值被分区间或分组之后的数目。每个不同的区间大小代表某种密度，因此一个特定范围的连续型数值会落在里面。对数据做分区间的具体技巧包括等宽分区间以及自适应分区间。我们使用从 <a href="https://github.com/freeCodeCamp/2016-new-coder-survey" target="_blank" rel="noopener">2016 年 FreeCodeCamp 开发者和编码员调查报告</a>中抽取出来的一个子集中的数据，来讨论各种针对编码员和软件开发者的属性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df =pd.read_csv(<span class="string">'datasets/fcc_2016_coder_survey_subset.csv'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'EmploymentField'</span>, <span class="string">'Age'</span>,<span class="string">'Income'</span>]].head()</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f578e01139.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="来自FCC编码员调查数据集的样本属性"><a href="#来自FCC编码员调查数据集的样本属性" class="headerlink" title="来自FCC编码员调查数据集的样本属性"></a>来自FCC编码员调查数据集的样本属性</h5><p>对于每个参加调查的编码员或开发者，ID.x 变量基本上是一个唯一的标识符而其他字段是可自我解释的。</p><h4 id="等宽分区间"><a href="#等宽分区间" class="headerlink" title="等宽分区间"></a>等宽分区间</h4><p>就像名字表明的那样，在等宽分区间方法中，每个区间都是固定宽度的，通常可以预先分析数据进行定义。基于一些领域知识、规则或约束，每个区间有个预先固定的值的范围，只有处于范围内的数值才被分配到该区间。基于数据舍入操作的分区间是一种方式，你可以使用数据舍入操作来对原始值进行分区间，我们前面已经讲过。</p><p>现在我们分析编码员调查报告数据集的 Age 特征并看看它的分布。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Age'</span>].hist(color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Age Histogram'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Age'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f57b05846b.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="描述开发者年龄分布的直方图"><a href="#描述开发者年龄分布的直方图" class="headerlink" title="描述开发者年龄分布的直方图"></a>描述开发者年龄分布的直方图</h5><p>上面的直方图表明，如预期那样，开发者年龄分布仿佛往左侧倾斜（上年纪的开发者偏少）。现在我们根据下面的模式，将这些原始年龄值分配到特定的区间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Age Range: Bin</span><br><span class="line"></span><br><span class="line">\---------------</span><br><span class="line"></span><br><span class="line"><span class="number">0</span> - <span class="number">9</span> : <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="number">10</span> - <span class="number">19</span> : <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="number">20</span> - <span class="number">29</span> : <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="number">30</span> - <span class="number">39</span> : <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="number">40</span> - <span class="number">49</span> : <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="number">50</span> - <span class="number">59</span> : <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="number">60</span> - <span class="number">69</span> : <span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="meta">... </span><span class="keyword">and</span> so on</span><br></pre></td></tr></table></figure><p>我们可以简单地使用我们先前学习到的数据舍入部分知识，先将这些原始年龄值除以 10，然后通过 floor 函数对原始年龄数值进行截断。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df[<span class="string">'Age_bin_round'</span>] = np.array(np.floor(np.array(fcc_survey_df[<span class="string">'Age'</span>]) / <span class="number">10.</span>))</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>,<span class="string">'Age_bin_round'</span>]].iloc[<span class="number">1071</span>:<span class="number">1076</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f57d916a6f.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="通过舍入法分区间"><a href="#通过舍入法分区间" class="headerlink" title="通过舍入法分区间"></a>通过舍入法分区间</h5><p>你可以看到基于数据舍入操作的每个年龄对应的区间。但是如果我们需要更灵活的操作怎么办？如果我们想基于我们的规则或逻辑，确定或修改区间的宽度怎么办？基于常用范围的分区间方法将帮助我们完成这个。让我们来定义一些通用年龄段位，使用下面的方式来对开发者年龄分区间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Age Range : Bin</span><br><span class="line"></span><br><span class="line">\---------------</span><br><span class="line"></span><br><span class="line"><span class="number">0</span> - <span class="number">15</span> : <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="number">16</span> - <span class="number">30</span> : <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="number">31</span> - <span class="number">45</span> : <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="number">46</span> - <span class="number">60</span> : <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="number">61</span> - <span class="number">75</span> : <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="number">75</span> - <span class="number">100</span> : <span class="number">6</span></span><br></pre></td></tr></table></figure><p>基于这些常用的分区间方式，我们现在可以对每个开发者年龄值的区间打标签，我们将存储区间的范围和相应的标签。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">bin_ranges = [<span class="number">0</span>, <span class="number">15</span>, <span class="number">30</span>, <span class="number">45</span>, <span class="number">60</span>, <span class="number">75</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">bin_names = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Age_bin_custom_range'</span>] = pd.cut(np.array(fcc_survey_df[<span class="string">'Age'</span>]),bins=bin_ranges)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Age_bin_custom_label'</span>] = pd.cut(np.array(fcc_survey_df[<span class="string">'Age'</span>]),bins=bin_ranges, labels=bin_names)</span><br><span class="line"></span><br><span class="line">\<span class="comment"># view the binned features</span></span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Age_bin_round'</span>,<span class="string">'Age_bin_custom_range'</span>,<span class="string">'Age_bin_custom_label'</span>]].iloc[<span class="number">10</span>a71:<span class="number">1076</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58143c35f.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="开发者年龄的常用分区间方式"><a href="#开发者年龄的常用分区间方式" class="headerlink" title="开发者年龄的常用分区间方式"></a>开发者年龄的常用分区间方式</h5><h4 id="自适应分区间"><a href="#自适应分区间" class="headerlink" title="自适应分区间"></a>自适应分区间</h4><p>使用等宽分区间的不足之处在于，我们手动决定了区间的值范围，而由于落在某个区间中的数据点或值的数目是不均匀的，因此可能会得到不规则的区间。一些区间中的数据可能会非常的密集，一些区间会非常稀疏甚至是空的！自适应分区间方法是一个更安全的策略，在这些场景中，我们让数据自己说话！这样，我们使用数据分布来决定区间的范围。</p><p>基于分位数的分区间方法是自适应分箱方法中一个很好的技巧。量化对于特定值或切点有助于将特定数值域的连续值分布划分为离散的互相挨着的区间。因此 q 分位数有助于将数值属性划分为 q 个相等的部分。关于量化比较流行的例子包括 2 分位数，也叫中值，将数据分布划分为2个相等的区间；4 分位数，也简称分位数，它将数据划分为 4 个相等的区间；以及 10 分位数，也叫十分位数，创建 10 个相等宽度的区间，现在让我们看看开发者数据集的 Income 字段的数据分布。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>].hist(bins=<span class="number">30</span>, color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Income Histogram'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Developer Income'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f583631eff.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p><strong>描述开发者收入分布的直方图</strong></p><p>上述的分布描述了一个在收入上右歪斜的分布，少数人赚更多的钱，多数人赚更少的钱。让我们基于自适应分箱方式做一个 4-分位数或分位数。我们可以很容易地得到如下的分位数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">quantile_list = [<span class="number">0</span>, <span class="number">.25</span>, <span class="number">.5</span>, <span class="number">.75</span>, <span class="number">1.</span>]</span><br><span class="line"></span><br><span class="line">quantiles =</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>].quantile(quantile_list)</span><br><span class="line"></span><br><span class="line">quantiles</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**Output**</span><br><span class="line"></span><br><span class="line">**------**</span><br><span class="line"></span><br><span class="line"><span class="number">0.00</span> <span class="number">6000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">0.25</span> <span class="number">20000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">0.50</span> <span class="number">37000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">0.75</span> <span class="number">60000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">1.00</span> <span class="number">200000.0</span></span><br><span class="line"></span><br><span class="line">Name: Income, dtype: float64</span><br></pre></td></tr></table></figure><p>现在让我们在原始的分布直方图中可视化下这些分位数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>].hist(bins=<span class="number">30</span>, color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> quantile <span class="keyword">in</span> quantiles:</span><br><span class="line"></span><br><span class="line">qvl = plt.axvline(quantile, color=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">ax.legend([qvl], [<span class="string">'Quantiles'</span>], fontsize=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Income Histogram with Quantiles'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Developer Income'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f5853f1a2c.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="带分位数形式描述开发者收入分布的直方图"><a href="#带分位数形式描述开发者收入分布的直方图" class="headerlink" title="带分位数形式描述开发者收入分布的直方图"></a>带分位数形式描述开发者收入分布的直方图</h5><p>上面描述的分布中红色线代表了分位数值和我们潜在的区间。让我们利用这些知识来构建我们基于分区间策略的分位数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">quantile_labels = [<span class="string">'0-25Q'</span>, <span class="string">'25-50Q'</span>, <span class="string">'50-75Q'</span>, <span class="string">'75-100Q'</span>]</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_quantile_range'</span>] = pd.qcut(</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>],q=quantile_list)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_quantile_label'</span>] = pd.qcut(</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>],q=quantile_list,labels=quantile_labels)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Income'</span>,<span class="string">'Income_quantile_range'</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">'Income_quantile_label'</span>]].iloc[<span class="number">4</span>:<span class="number">9</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f586dbd8f4.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="基于分位数的开发者收入的区间范围和标签"><a href="#基于分位数的开发者收入的区间范围和标签" class="headerlink" title="基于分位数的开发者收入的区间范围和标签"></a>基于分位数的开发者收入的区间范围和标签</h5><p>通过这个例子，你应该对如何做基于分位数的自适应分区间法有了一个很好的认识。一个需要重点记住的是，分区间的结果是离散值类型的分类特征，当你在模型中使用分类数据之前，可能需要额外的特征工程相关步骤。我们将在接下来的部分简要地讲述分类数据的特征工程技巧。</p><h4 id="统计变换"><a href="#统计变换" class="headerlink" title="统计变换"></a>统计变换</h4><p>我们讨论下先前简单提到过的数据分布倾斜的负面影响。现在我们可以考虑另一个特征工程技巧，即利用统计或数学变换。我们试试看 Log 变换和 Box-Cox 变换。这两种变换函数都属于幂变换函数簇，通常用来创建单调的数据变换。它们的主要作用在于它能帮助稳定方差，始终保持分布接近于正态分布并使得数据与分布的平均值无关。</p><h4 id="Log变换"><a href="#Log变换" class="headerlink" title="Log变换"></a>Log变换</h4><p>log 变换属于幂变换函数簇。该函数用数学表达式表示为</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f588a0f6a5.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>读为以 b 为底 x 的对数等于 y。这可以变换为</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f589e77242.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>表示以b为底指数必须达到多少才等于x。自然对数使用 b=e，e=2.71828，通常叫作欧拉常数。你可以使用通常在十进制系统中使用的 b=10 作为底数。</p><p><strong>当应用于倾斜分布时 Log 变换是很有用的，因为他们倾向于拉伸那些落在较低的幅度范围内自变量值的范围，倾向于压缩或减少更高幅度范围内的自变量值的范围</strong>。从而使得倾斜分布尽可能的接近正态分布。让我们对先前使用的开发者数据集的 Income 特征上使用log变换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df[<span class="string">'Income_log'</span>] = np.log((<span class="number">1</span>+fcc_survey_df[<span class="string">'Income'</span>]))</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Income'</span>,<span class="string">'Income_log'</span>]].iloc[<span class="number">4</span>:<span class="number">9</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58b3ed249.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="开发者收入log变换后结构"><a href="#开发者收入log变换后结构" class="headerlink" title="开发者收入log变换后结构"></a>开发者收入log变换后结构</h5><p>Income_log 字段描述了经过 log 变换后的特征。现在让我们来看看字段变换后数据的分布。</p><p>基于上面的图，我们可以清楚地看到与先前倾斜分布相比，该分布更加像正态分布或高斯分布。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">income_log_mean =np.round(np.mean(fcc_survey_df[<span class="string">'Income_log'</span>]), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_log'</span>].hist(bins=<span class="number">30</span>,color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">plt.axvline(income_log_mean, color=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Income Histogram after Log Transform'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Developer Income (log scale)'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.text(<span class="number">11.5</span>, <span class="number">450</span>, <span class="string">r'$\mu$='</span>+str(income_log_mean),fontsize=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58cdaf02a.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p><strong>经过log变换后描述开发者收入分布的直方图</strong></p><h4 id="Box-Cox变换"><a href="#Box-Cox变换" class="headerlink" title="Box-Cox变换"></a>Box-Cox变换</h4><p>Box-Cox 变换是另一个流行的幂变换函数簇中的一个函数。该函数有一个前提条件，即数值型值必须先变换为正数（与 log 变换所要求的一样）。万一出现数值是负的，使用一个常数对数值进行偏移是有帮助的。数学上，Box-Cox 变换函数可以表示如下。</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58e556c08.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p>生成的变换后的输出y是输入 x 和变换参数的函数；当 λ=0 时，该变换就是自然对数 log 变换，前面我们已经提到过了。λ 的最佳取值通常由最大似然或最大对数似然确定。现在让我们在开发者数据集的收入特征上应用 Box-Cox 变换。首先我们从数据分布中移除非零值得到最佳的值，结果如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">income = np.array(fcc_survey_df[<span class="string">'Income'</span>])</span><br><span class="line"></span><br><span class="line">income_clean = income[~np.isnan(income)]</span><br><span class="line"></span><br><span class="line">l, opt_lambda = spstats.boxcox(income_clean)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Optimal lambda value:'</span>, opt_lambda)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**Output**</span><br><span class="line"></span><br><span class="line">**------**</span><br><span class="line"></span><br><span class="line">Optimal <span class="keyword">lambda</span> value: <span class="number">0.117991239456</span></span><br></pre></td></tr></table></figure><p>现在我们得到了最佳的值，让我们在取值为 0 和 λ（最佳取值 λ ）时使用 Box-Cox 变换对开发者收入特征进行变换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df[<span class="string">'Income_boxcox_lambda_0'</span>] = spstats.boxcox((<span class="number">1</span>+fcc_survey_df[<span class="string">'Income'</span>]),lmbda=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_boxcox_lambda_opt'</span>] = spstats.boxcox(fcc_survey_df[<span class="string">'Income'</span>],lmbda=opt_lambda)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Income'</span>, <span class="string">'Income_log'</span>,<span class="string">'Income_boxcox_lambda_0'</span>,<span class="string">'Income_boxcox_lambda_opt'</span>]].iloc[<span class="number">4</span>:<span class="number">9</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58fd7fd5e.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><h5 id="经过-Box-Cox-变换后开发者的收入分布"><a href="#经过-Box-Cox-变换后开发者的收入分布" class="headerlink" title="经过 Box-Cox 变换后开发者的收入分布"></a>经过 Box-Cox 变换后开发者的收入分布</h5><p>变换后的特征在上述数据框中描述了。就像我们期望的那样，Income_log 和 Income_boxcox_lamba_0具有相同的取值。让我们看看经过最佳λ变换后 Income 特征的分布。</p><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;income_boxcox_mean = np.round(np.mean(fcc_survey_df[<span class="string">'Income_boxcox_lambda_opt'</span>]),<span class="number">2</span>)</span><br><span class="line">&gt; </span><br><span class="line">&gt;fig, ax = plt.subplots()</span><br><span class="line">&gt; </span><br><span class="line">&gt;fcc_survey_df[<span class="string">'Income_boxcox_lambda_opt'</span>].hist(bins=<span class="number">30</span>,  color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>, grid=<span class="literal">False</span>)</span><br><span class="line">&gt;    plt.axvline(income_boxcox_mean, color=<span class="string">'r'</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.set_title(<span class="string">'Developer Income Histogram after Box–Cox Transform'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.set_xlabel(<span class="string">'Developer Income (Box–Cox transform)'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.text(<span class="number">24</span>, <span class="number">450</span>, <span class="string">r'$\mu$='</span>+str(income_boxcox_mean),fontsize=<span class="number">10</span>)       </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f591679bfb.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p><p><strong>经过Box-Cox变换后描述开发者收入分布的直方图</strong></p><p> 分布看起来更像是正态分布，与我们经过 log 变换后的分布相似。</p><h3 id="类别型数据上的特征工程"><a href="#类别型数据上的特征工程" class="headerlink" title="类别型数据上的特征工程"></a>类别型数据上的特征工程</h3><p>在深入研究特征工程之前，让我们先了解一下分类数据。通常，在<strong>自然界中可分类的任意数据属性都是离散值，这意味着它们属于某一特定的有限类别</strong>。在模型预测的属性或者变量（通常被称为<strong>响应变量 response variables</strong>）中，这些也经常被称为类别或者标签。这些离散值在自然界中可以是文本或者数字（甚至是诸如图像这样的非结构化数据）。分类数据有两大类——<strong>定类（Nominal）和定序（Ordinal）</strong>。</p><p>在任意定类分类数据属性中，这些属性值之间<strong>没有顺序的概念</strong>。如下图所示，举个简单的例子，天气分类。我们可以看到，在这个特定的场景中，主要有六个大类，而这些类之间没有任何顺序上的关系（刮风天并不总是发生在晴天之前，并且也不能说比晴天来的更小或者更大）</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af6bc87b4e.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>将天气作为分类属性</p><p>与天气相类似的属性还有很多，比如电影、音乐、电子游戏、国家、食物和美食类型等等，这些都属于定类分类属性。</p><p>定序分类的属性值则存在着一定的顺序意义或概念。例如，下图中的字母标识了衬衫的大小。显而易见的是，当我们考虑衬衫的时候，它的“大小”属性是很重要的（S 码比 M 码来的小，而 M 码又小于 L 码等等）。</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af6d3b83ac.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>衬衫大小作为定序分类属性</p><p>鞋号、受教育水平和公司职位则是定序分类属性的一些其它例子。既然已经对分类数据有了一个大致的理解之后，接下来我们来看看一些特征工程的策略。</p><p>在接受像文本标签这样复杂的分类数据类型问题上，各种机器学习框架均已取得了许多的进步。通常，特征工程中的任意标准工作流都涉及将这些分类值转换为数值标签的某种形式，然后对这些值应用一些<strong>编码方案</strong>。我们将在开始之前导入必要的工具包。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h4 id="定类属性转换-LabelEncoding"><a href="#定类属性转换-LabelEncoding" class="headerlink" title="定类属性转换(LabelEncoding)"></a>定类属性转换(LabelEncoding)</h4><p><strong>定类属性由离散的分类值组成，它们没有先后顺序概念</strong>。这里的思想是将这些属性转换成更具代表性的数值格式，这样可以很容易被下游的代码和流水线所理解。我们来看一个关于视频游戏销售的新数据集。这个数据集也可以在 <a href="https://www.kaggle.com/gregorut/videogamesales" target="_blank" rel="noopener">Kaggle</a> 和我的 <a href="https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/notebooks/Ch04_Feature_Engineering_and_Selection" target="_blank" rel="noopener">GitHub</a> 仓库中找到。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vg_df = pd.read_csv(<span class="string">'datasets/vgsales.csv'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">vg_df[[<span class="string">'Name'</span>, <span class="string">'Platform'</span>, <span class="string">'Year'</span>, <span class="string">'Genre'</span>, <span class="string">'Publisher'</span>]].iloc[<span class="number">1</span>:<span class="number">7</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af756b687d.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>游戏销售数据</p><p>让我们首先专注于上面数据框中“视频游戏风格（Genre）”属性。显而易见的是，这是一个类似于“发行商（Publisher）”和“平台（Platform）”属性一样的定类分类属性。我们可以很容易得到一个独特的视频游戏风格列表，如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">genres = np.unique(vg_df[<span class="string">'Genre'</span>])</span><br><span class="line"></span><br><span class="line">genres</span><br><span class="line"></span><br><span class="line">Output</span><br><span class="line"></span><br><span class="line">\------</span><br><span class="line"></span><br><span class="line">array([<span class="string">'Action'</span>, <span class="string">'Adventure'</span>, <span class="string">'Fighting'</span>, <span class="string">'Misc'</span>, <span class="string">'Platform'</span>, <span class="string">'Puzzle'</span>, <span class="string">'Racing'</span>, <span class="string">'Role-Playing'</span>, <span class="string">'Shooter'</span>, <span class="string">'Simulation'</span>, <span class="string">'Sports'</span>, <span class="string">'Strategy'</span>], dtype=object)</span><br></pre></td></tr></table></figure><p>输出结果表明，我们有 12 种不同的视频游戏风格。我们现在可以生成一个标签编码方法，即利用 scikit-learn 将每个类别映射到一个数值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line">gle = LabelEncoder()</span><br><span class="line"></span><br><span class="line">genre_labels = gle.fit_transform(vg_df[<span class="string">'Genre'</span>])</span><br><span class="line"></span><br><span class="line">genre_mappings = &#123;index: label <span class="keyword">for</span> index, label <span class="keyword">in</span> enumerate(gle.classes_)&#125;</span><br><span class="line"></span><br><span class="line">genre_mappings</span><br><span class="line"></span><br><span class="line">Output</span><br><span class="line"></span><br><span class="line">\------</span><br><span class="line"></span><br><span class="line">&#123;<span class="number">0</span>: <span class="string">'Action'</span>, <span class="number">1</span>: <span class="string">'Adventure'</span>, <span class="number">2</span>: <span class="string">'Fighting'</span>, <span class="number">3</span>: <span class="string">'Misc'</span>, <span class="number">4</span>: <span class="string">'Platform'</span>, <span class="number">5</span>: <span class="string">'Puzzle'</span>, <span class="number">6</span>: <span class="string">'Racing'</span>, <span class="number">7</span>: <span class="string">'Role-Playing'</span>, <span class="number">8</span>: <span class="string">'Shooter'</span>, <span class="number">9</span>: <span class="string">'Simulation'</span>, <span class="number">10</span>: <span class="string">'Sports'</span>, <span class="number">11</span>: <span class="string">'Strategy'</span>&#125;</span><br></pre></td></tr></table></figure><p>因此，在 <em>LabelEncoder</em> 类的实例对象 <em>gle</em> 的帮助下生成了一个映射方案，成功地将每个风格属性映射到一个数值。转换后的标签存储在 <em>genre_labels</em> 中，该变量允许我们将其写回数据表中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vg_df[<span class="string">'GenreLabel'</span>] = genre_labels</span><br><span class="line"></span><br><span class="line">vg_df[[<span class="string">'Name'</span>, <span class="string">'Platform'</span>, <span class="string">'Year'</span>, <span class="string">'Genre'</span>, <span class="string">'GenreLabel'</span>]].iloc[<span class="number">1</span>:<span class="number">7</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af8164e6db.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>视频游戏风格及其编码标签</p><p>如果你打算将它们用作预测的响应变量，那么这些标签通常可以直接用于诸如 sikit-learn 这样的框架。但是如前所述，我们还需要额外的编码步骤才能将它们用作特征。</p><h4 id="定序属性编码"><a href="#定序属性编码" class="headerlink" title="定序属性编码"></a>定序属性编码</h4><p><strong>定序属性是一种带有先后顺序概念的分类属性</strong>。这里我将以本系列文章第一部分所使用的<a href="https://www.kaggle.com/abcsds/pokemon/data" target="_blank" rel="noopener">神奇宝贝数据集</a>进行说明。让我们先专注于 「世代（Generation）」 属性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; poke_df = pd.read_csv(<span class="string">'datasets/Pokemon.csv'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; poke_df = poke_df.sample(random_state=<span class="number">1</span>, frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; np.unique(poke_df[<span class="string">'Generation'</span>])</span><br><span class="line">&gt;</span><br><span class="line">&gt; Output</span><br><span class="line">&gt;</span><br><span class="line">&gt; \------</span><br><span class="line">&gt;</span><br><span class="line">&gt; array([<span class="string">'Gen 1'</span>, <span class="string">'Gen 2'</span>, <span class="string">'Gen 3'</span>, <span class="string">'Gen 4'</span>, <span class="string">'Gen 5'</span>, <span class="string">'Gen 6'</span>], dtype=object)</span><br></pre></td></tr></table></figure><p>根据上面的输出，我们可以看到一共有 6 代，并且每个神奇宝贝通常属于视频游戏的特定世代（依据发布顺序），而且电视系列也遵循了相似的时间线。这个属性通常是定序的（需要相关的领域知识才能理解），因为属于第一代的大多数神奇宝贝在第二代的视频游戏或者电视节目中也会被更早地引入。神奇宝贝的粉丝们可以看下下图，然后记住每一代中一些比较受欢迎的神奇宝贝（不同的粉丝可能有不同的看法）。</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af8f58f535.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>基于不同类型和世代选出的一些受欢迎的神奇宝贝</p><p>因此，它们之间存在着先后顺序。一般来说，没有通用的模块或者函数可以根据这些顺序自动将这些特征转换和映射到数值表示。因此，我们可以使用自定义的编码\映射方案。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">gen_ord_map = &#123;<span class="string">'Gen 1'</span>: <span class="number">1</span>, <span class="string">'Gen 2'</span>: <span class="number">2</span>, <span class="string">'Gen 3'</span>: <span class="number">3</span>, <span class="string">'Gen 4'</span>: <span class="number">4</span>, <span class="string">'Gen 5'</span>: <span class="number">5</span>, <span class="string">'Gen 6'</span>: <span class="number">6</span>&#125; </span><br><span class="line"></span><br><span class="line">poke_df[<span class="string">'GenerationLabel'</span>] = poke_df[<span class="string">'Generation'</span>].map(gen_ord_map)</span><br><span class="line"></span><br><span class="line">poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'GenerationLabel'</span>]].iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af95f94dc8.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>神奇宝贝世代编码</p><p>从上面的代码中可以看出，来自 <em>pandas</em> 库的 <em>map(…)</em> 函数在转换这种定序特征的时候非常有用。</p><h4 id="编码分类属性–独热编码方案（One-hot-Encoding-Scheme）"><a href="#编码分类属性–独热编码方案（One-hot-Encoding-Scheme）" class="headerlink" title="编码分类属性–独热编码方案（One-hot Encoding Scheme）"></a>编码分类属性–独热编码方案（One-hot Encoding Scheme）</h4><p>如果你还记得我们之前提到过的内容，通常对分类数据进行特征工程就涉及到一个转换过程，我们在前一部分描述了一个转换过程，还有一个强制编码过程，我们应用特定的编码方案为特定的每个类别创建虚拟变量或特征分类属性。</p><p>你可能想知道，我们刚刚在上一节说到将类别转换为数字标签，为什么现在我们又需要这个？原因很简单。考虑到视频游戏风格，如果我们直接将 <em>GenereLabel</em> 作为属性特征提供给机器学习模型，则模型会认为它是一个连续的数值特征，从而认为值 10 （体育）要大于值 6 （赛车），然而事实上这种信息是毫无意义的，因为<em>体育类型</em>显然并不大于或者小于<em>赛车类型</em>，这些不同值或者类别无法直接进行比较。因此我们需要另一套编码方案层，它要能为每个属性的所有不同类别中的每个唯一值或类别创建虚拟特征。</p><p>考虑到任意具有 m 个标签的分类属性（变换之后）的数字表示，独热编码方案将该属性编码或变换成 m 个二进制特征向量（向量中的每一维的值只能为 0 或 1）。那么在这个分类特征中每个属性值都被转换成一个 m 维的向量，其中只有某一维的值为 1。让我们来看看神奇宝贝数据集的一个子集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Legendary'</span>]].iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af9b37bf97.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>神奇宝贝数据集子集</p><p>这里关注的属性是神奇宝贝的「世代（Generation）」和「传奇（Legendary）」状态。第一步是根据之前学到的将这些属性转换为数值表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder, LabelEncoder</span><br><span class="line"></span><br><span class="line">\<span class="comment"># transform and map pokemon generations</span></span><br><span class="line"></span><br><span class="line">gen_le = LabelEncoder()</span><br><span class="line"></span><br><span class="line">gen_labels = gen_le.fit_transform(poke_df[<span class="string">'Generation'</span>])</span><br><span class="line"></span><br><span class="line">poke_df[<span class="string">'Gen_Label'</span>] = gen_labels</span><br><span class="line"></span><br><span class="line">\<span class="comment"># transform and map pokemon legendary status</span></span><br><span class="line"></span><br><span class="line">leg_le = LabelEncoder()</span><br><span class="line"></span><br><span class="line">leg_labels = leg_le.fit_transform(poke_df[<span class="string">'Legendary'</span>])</span><br><span class="line"></span><br><span class="line">poke_df[<span class="string">'Lgnd_Label'</span>] = leg_labels</span><br><span class="line"></span><br><span class="line">poke_df_sub = poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>, <span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>]]</span><br><span class="line"></span><br><span class="line">poke_df_sub.iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afa18d27fc.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>转换后的标签属性</p><p><em>Gen_Label</em> 和 <em>Lgnd_Label</em> 特征描述了我们分类特征的数值表示。现在让我们在这些特征上应用独热编码方案。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encode generation labels using one-hot encoding scheme</span></span><br><span class="line"></span><br><span class="line">gen_ohe = OneHotEncoder()</span><br><span class="line"></span><br><span class="line">gen_feature_arr = gen_ohe.fit_transform(poke_df[[<span class="string">'Gen_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">gen_feature_labels = list(gen_le.classes_)</span><br><span class="line"></span><br><span class="line">gen_features = pd.DataFrame(gen_feature_arr, columns=gen_feature_labels)</span><br><span class="line"></span><br><span class="line">\<span class="comment"># encode legendary status labels using one-hot encoding scheme</span></span><br><span class="line"></span><br><span class="line">leg_ohe = OneHotEncoder()</span><br><span class="line"></span><br><span class="line">leg_feature_arr = leg_ohe.fit_transform(poke_df[[<span class="string">'Lgnd_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">leg_feature_labels = [<span class="string">'Legendary_'</span>+str(cls_label) <span class="keyword">for</span> cls_label <span class="keyword">in</span> leg_le.classes_]</span><br><span class="line"></span><br><span class="line">leg_features = pd.DataFrame(leg_feature_arr, columns=leg_feature_labels)</span><br></pre></td></tr></table></figure><p>通常来说，你可以使用 <em>fit_transform</em> 函数将两个特征一起编码（通过将两个特征的二维数组一起传递给函数，详情<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" target="_blank" rel="noopener">查看文档</a>）。但是我们分开编码每个特征，这样可以更易于理解。除此之外，我们还可以创建单独的数据表并相应地标记它们。现在让我们链接这些特征表（Feature frames）然后看看最终的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">poke_df_ohe = pd.concat([poke_df_sub, gen_features, leg_features], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">columns = sum([[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>], gen_feature_labels, [<span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>], leg_feature_labels], [])</span><br><span class="line"></span><br><span class="line">poke_df_ohe[columns].iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afab9940ae.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>神奇宝贝世代和传奇状态的独热编码特征</p><p>此时可以看到已经为「世代（Generation）」生成 6 个虚拟变量或者二进制特征，并为「传奇（Legendary）」生成了 2 个特征。这些特征数量是这些属性中不同类别的总数。<strong>某一类别的激活状态通过将对应的虚拟变量置 1 来表示</strong>，这从上面的数据表中可以非常明显地体现出来。</p><p>考虑你在训练数据上建立了这个编码方案，并建立了一些模型，现在你有了一些新的数据，这些数据必须在预测之前进行如下设计。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_poke_df = pd.DataFrame([[<span class="string">'PikaZoom'</span>, <span class="string">'Gen 3'</span>, <span class="literal">True</span>], [<span class="string">'CharMyToast'</span>, <span class="string">'Gen 4'</span>, <span class="literal">False</span>]], columns=[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Legendary'</span>])</span><br><span class="line"></span><br><span class="line">new_poke_df</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afaf0cb42e.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>新数据</p><p>你可以通过调用之前构建的 <em>LabelEncoder</em> 和 <em>OneHotEncoder</em> 对象的 <em>transform()</em> 方法来处理新数据。请记得我们的工作流程，首先我们要做转换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">new_gen_labels = gen_le.transform(new_poke_df[<span class="string">'Generation'</span>])</span><br><span class="line"></span><br><span class="line">new_poke_df[<span class="string">'Gen_Label'</span>] = new_gen_labels</span><br><span class="line"></span><br><span class="line">new_leg_labels = leg_le.transform(new_poke_df[<span class="string">'Legendary'</span>])</span><br><span class="line"></span><br><span class="line">new_poke_df[<span class="string">'Lgnd_Label'</span>] = new_leg_labels</span><br><span class="line"></span><br><span class="line">new_poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>, <span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>]]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afb428f0dc.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>转换之后的分类属性</p><p>在得到了数值标签之后，接下来让我们应用编码方案吧！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">new_gen_feature_arr = gen_ohe.transform(new_poke_df[[<span class="string">'Gen_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">new_gen_features = pd.DataFrame(new_gen_feature_arr, columns=gen_feature_labels)</span><br><span class="line"></span><br><span class="line">new_leg_feature_arr = leg_ohe.transform(new_poke_df[[<span class="string">'Lgnd_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">new_leg_features = pd.DataFrame(new_leg_feature_arr, columns=leg_feature_labels)</span><br><span class="line"></span><br><span class="line">new_poke_ohe = pd.concat([new_poke_df, new_gen_features, new_leg_features], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">columns = sum([[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>], gen_feature_labels, [<span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>], leg_feature_labels], [])</span><br><span class="line"></span><br><span class="line">new_poke_ohe[columns]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afb91bb3be.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>独热编码之后的分类属性</p><p>因此，通过利用 scikit-learn 强大的 API，我们可以很容易将编码方案应用于新数据。</p><p>你也可以通过利用来自 pandas 的 <em>to_dummies()</em> 函数轻松应用独热编码方案。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gen_onehot_features = pd.get_dummies(poke_df[<span class="string">'Generation'</span>])</span><br><span class="line"></span><br><span class="line">pd.concat([poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>]], gen_onehot_features], axis=<span class="number">1</span>).iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afbcc0ff2b.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>使用 pandas 实现的独热编码特征</p><p>上面的数据表描述了应用在「世代（Generation）」属性上的独热编码方案，结果与之前的一致。</p><h4 id="区间计数方案（Bin-counting-Scheme）"><a href="#区间计数方案（Bin-counting-Scheme）" class="headerlink" title="区间计数方案（Bin-counting Scheme）"></a>区间计数方案（Bin-counting Scheme）</h4><p>到目前为止，我们所讨论的编码方案在分类数据方面效果还不错，但是当任意特征的不同类别数量变得很大的时候，问题开始出现。对于具有 m 个不同标签的任意分类特征这点非常重要，你将得到 m 个独立的特征。这会很容易地增加特征集的大小，从而导致在时间、空间和内存方面出现存储问题或者模型训练问题。除此之外，我们还必须处理“<a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" target="_blank" rel="noopener">维度诅咒</a>”问题，通常指的是拥有大量的特征，却缺乏足够的代表性样本，然后模型的性能开始受到影响并导致过拟合。</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afd0459749.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>因此，我们需要针对那些可能具有非常多种类别的特征（如 IP 地址），研究其它分类数据特征工程方案。区间计数方案是处理具有多个类别的分类变量的有效方案。在这个方案中，我们使用<strong>基于概率的统计信息和在建模过程中所要预测的实际目标或者响应值</strong>，而不是使用实际的标签值进行编码。一个简单的例子是，基于过去的 IP 地址历史数据和 DDOS 攻击中所使用的历史数据，我们可以为任一 IP 地址会被 DDOS 攻击的可能性建立概率模型。使用这些信息，我们可以对输入特征进行编码，该输入特征描述了如果将来出现相同的 IP 地址，则引起 DDOS 攻击的概率值是多少。<strong>这个方案需要历史数据作为先决条件，并且要求数据非常详尽。</strong></p><h4 id="特征哈希方案"><a href="#特征哈希方案" class="headerlink" title="特征哈希方案"></a>特征哈希方案</h4><p>特征哈希方案（Feature Hashing Scheme）是处理大规模分类特征的另一个有用的特征工程方案。在该方案中，哈希函数通常与预设的编码特征的数量（作为预定义长度向量）一起使用，使得特征的哈希值被用作这个预定义向量中的索引，并且值也要做相应的更新。由于哈希函数将大量的值映射到一个小的有限集合中，因此<strong>多个不同值可能会创建相同的哈希</strong>，这一现象称为<strong>冲突</strong>。典型地，使用带符号的哈希函数，使得从哈希获得的值的符号被用作那些在适当的索引处存储在最终特征向量中的值的符号。这样能够确保实现较少的冲突和由于冲突导致的误差累积。</p><p>哈希方案适用于字符串、数字和其它结构（如向量）。你可以将哈希输出看作一个有限的 <em>b bins</em> 集合，以便于当将哈希函数应用于相同的值\类别时，哈希函数能根据哈希值将其分配到 <em>b bins</em> 中的同一个 bin（或者 bins 的子集）。我们可以预先定义 <em>b</em> 的值，它成为我们使用特征哈希方案编码的每个分类属性的编码特征向量的最终尺寸。</p><p>因此，即使我们有一个特征拥有超过 <strong>1000</strong> 个不同的类别，我们设置 <strong>b = 10</strong> 作为最终的特征向量长度，那么最终输出的特征将只有 10 个特征。而采用独热编码方案则有 1000 个二进制特征。我们来考虑下视频游戏数据集中的「风格（Genre）」属性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">unique_genres = np.unique(vg_df[[<span class="string">'Genre'</span>]])</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Total game genres:"</span>, len(unique_genres))</span><br><span class="line"></span><br><span class="line">print(unique_genres)</span><br><span class="line"></span><br><span class="line">Output</span><br><span class="line"></span><br><span class="line">\------</span><br><span class="line"></span><br><span class="line">Total game genres: <span class="number">12</span></span><br><span class="line"></span><br><span class="line">[<span class="string">'Action'</span> <span class="string">'Adventure'</span> <span class="string">'Fighting'</span> <span class="string">'Misc'</span> <span class="string">'Platform'</span> <span class="string">'Puzzle'</span> <span class="string">'Racing'</span> <span class="string">'Role-Playing'</span> <span class="string">'Shooter'</span> <span class="string">'Simulation'</span> <span class="string">'Sports'</span> <span class="string">'Strategy'</span>]</span><br></pre></td></tr></table></figure><p>我们可以看到，总共有 12 中风格的游戏。如果我们在“风格”特征中采用独热编码方案，则将得到 12 个二进制特征。而这次，我们将通过 scikit-learn 的 <em>FeatureHasher</em> 类来使用特征哈希方案，该类使用了一个有符号的 32 位版本的 <em>Murmurhash3</em> 哈希函数。在这种情况下，我们将预先定义最终的特征向量大小为 6。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> FeatureHasher</span><br><span class="line"></span><br><span class="line">fh = FeatureHasher(n_features=<span class="number">6</span>, input_type=<span class="string">'string'</span>)</span><br><span class="line"></span><br><span class="line">hashed_features = fh.fit_transform(vg_df[<span class="string">'Genre'</span>])</span><br><span class="line"></span><br><span class="line">hashed_features = hashed_features.toarray()pd.concat([vg_df[[<span class="string">'Name'</span>, <span class="string">'Genre'</span>]], pd.DataFrame(hashed_features)], axis=<span class="number">1</span>).iloc[<span class="number">1</span>:<span class="number">7</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afd62f2a51.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p><p>风格属性的特征哈希</p><p>基于上述输出，「风格（Genre）」属性已经使用哈希方案编码成 6 个特征而不是 12 个。我们还可以看到，第 1 行和第 6 行表示相同风格的游戏「平台（Platform）」，而它们也被正确编码成了相同的特征向量。</p><h3 id="时间型"><a href="#时间型" class="headerlink" title="时间型"></a>时间型</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBQ8OS.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQrOU.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQcTJ.jpg" alt="avatar"></p><h3 id="文本型"><a href="#文本型" class="headerlink" title="文本型"></a>文本型</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBQhSx.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQIOO.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQzX8.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBlC7Q.jpg" alt="avatar"></p>]]></content>
      
      
      <categories>
          
          <category> 特征工程 </category>
          
          <category> 模型调优 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模型调优 </tag>
            
            <tag> python </tag>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>语言模型</title>
      <link href="/2020/04/18/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
      <url>/2020/04/18/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h1><p>学习目标</p><ul><li>学习语言模型，以及如何训练一个语言模型</li><li>学习torchtext的基本使用方法<ul><li>构建 vocabulary</li><li>word to inde 和 index to word</li></ul></li><li>学习torch.nn的一些基本模型<ul><li>Linear</li><li>RNN</li><li>LSTM</li><li>GRU</li></ul></li><li>RNN的训练技巧<ul><li>Gradient Clipping</li></ul></li><li>如何保存和读取模型</li></ul><p>我们会使用 <a href="https://github.com/pytorch/text" target="_blank" rel="noopener">torchtext</a> 来创建vocabulary, 然后把数据读成batch的格式。请大家自行阅读README来学习torchtext。</p><p><strong>先了解下torchtext库：<a href="https://blog.csdn.net/u012436149/article/details/79310176" target="_blank" rel="noopener">torchtext介绍和使用教程</a></strong></p><p>In [1]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">USE_CUDA = torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了保证实验结果可以复现，我们经常会把各种random seed固定在某一个值</span></span><br><span class="line">random.seed(<span class="number">53113</span>)</span><br><span class="line">np.random.seed(<span class="number">53113</span>)</span><br><span class="line">torch.manual_seed(<span class="number">53113</span>)</span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    torch.cuda.manual_seed(<span class="number">53113</span>)</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">32</span> <span class="comment">#一个batch多少个句子</span></span><br><span class="line">EMBEDDING_SIZE = <span class="number">650</span>  <span class="comment">#每个单词多少维</span></span><br><span class="line">MAX_VOCAB_SIZE = <span class="number">50000</span>  <span class="comment">#单词总数</span></span><br></pre></td></tr></table></figure><ul><li>我们会继续使用上次的text8作为我们的训练，验证和测试数据</li><li>torchtext提供了LanguageModelingDataset这个class来帮助我们处理语言模型数据集</li><li>BPTTIterator可以连续地得到连贯的句子</li></ul><p>In [2]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">TEXT = torchtext.data.Field(lower=<span class="literal">True</span>) </span><br><span class="line"><span class="comment"># .Field这个对象包含了我们打算如何预处理文本数据的信息，这里定义单词全部小写</span></span><br><span class="line"></span><br><span class="line">train, val, test = \</span><br><span class="line">torchtext.datasets.LanguageModelingDataset.splits(</span><br><span class="line">    path=<span class="string">"."</span>, </span><br><span class="line">    train=<span class="string">"text8.train.txt"</span>, </span><br><span class="line">    validation=<span class="string">"text8.dev.txt"</span>, </span><br><span class="line">    test=<span class="string">"text8.test.txt"</span>, </span><br><span class="line">    text_field=TEXT)</span><br><span class="line"><span class="comment"># torchtext提供了LanguageModelingDataset这个class来帮助我们处理语言模型数据集</span></span><br><span class="line"></span><br><span class="line">TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)</span><br><span class="line"><span class="comment"># build_vocab可以根据我们提供的训练数据集来创建最高频单词的单词表，max_size帮助我们限定单词总量。</span></span><br><span class="line">print(<span class="string">"vocabulary size: &#123;&#125;"</span>.format(len(TEXT.vocab)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vocabulary size: 50002</span><br></pre></td></tr></table></figure><p>In [4]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test</span><br></pre></td></tr></table></figure><p>Out[4]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;torchtext.data.example.Example at 0x121738b00&gt;</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In [9]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(TEXT.vocab.itos[<span class="number">0</span>:<span class="number">50</span>]) </span><br><span class="line"><span class="comment"># 这里越靠前越常见，增加了两个特殊的token，&lt;unk&gt;表示未知的单词，&lt;pad&gt;表示padding。</span></span><br><span class="line">print(<span class="string">"------"</span>*<span class="number">10</span>)</span><br><span class="line">print(list(TEXT.vocab.stoi.items())[<span class="number">0</span>:<span class="number">50</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&apos;&lt;unk&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;the&apos;, &apos;of&apos;, &apos;and&apos;, &apos;one&apos;, &apos;in&apos;, &apos;a&apos;, &apos;to&apos;, &apos;zero&apos;, &apos;nine&apos;, &apos;two&apos;, &apos;is&apos;, &apos;as&apos;, &apos;eight&apos;, &apos;for&apos;, &apos;s&apos;, &apos;five&apos;, &apos;three&apos;, &apos;was&apos;, &apos;by&apos;, &apos;that&apos;, &apos;four&apos;, &apos;six&apos;, &apos;seven&apos;, &apos;with&apos;, &apos;on&apos;, &apos;are&apos;, &apos;it&apos;, &apos;from&apos;, &apos;or&apos;, &apos;his&apos;, &apos;an&apos;, &apos;be&apos;, &apos;this&apos;, &apos;he&apos;, &apos;at&apos;, &apos;which&apos;, &apos;not&apos;, &apos;also&apos;, &apos;have&apos;, &apos;were&apos;, &apos;has&apos;, &apos;but&apos;, &apos;other&apos;, &apos;their&apos;, &apos;its&apos;, &apos;first&apos;, &apos;they&apos;, &apos;had&apos;]</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">[(&apos;&lt;unk&gt;&apos;, 0), (&apos;&lt;pad&gt;&apos;, 1), (&apos;the&apos;, 2), (&apos;of&apos;, 3), (&apos;and&apos;, 4), (&apos;one&apos;, 5), (&apos;in&apos;, 6), (&apos;a&apos;, 7), (&apos;to&apos;, 8), (&apos;zero&apos;, 9), (&apos;nine&apos;, 10), (&apos;two&apos;, 11), (&apos;is&apos;, 12), (&apos;as&apos;, 13), (&apos;eight&apos;, 14), (&apos;for&apos;, 15), (&apos;s&apos;, 16), (&apos;five&apos;, 17), (&apos;three&apos;, 18), (&apos;was&apos;, 19), (&apos;by&apos;, 20), (&apos;that&apos;, 21), (&apos;four&apos;, 22), (&apos;six&apos;, 23), (&apos;seven&apos;, 24), (&apos;with&apos;, 25), (&apos;on&apos;, 26), (&apos;are&apos;, 27), (&apos;it&apos;, 28), (&apos;from&apos;, 29), (&apos;or&apos;, 30), (&apos;his&apos;, 31), (&apos;an&apos;, 32), (&apos;be&apos;, 33), (&apos;this&apos;, 34), (&apos;he&apos;, 35), (&apos;at&apos;, 36), (&apos;which&apos;, 37), (&apos;not&apos;, 38), (&apos;also&apos;, 39), (&apos;have&apos;, 40), (&apos;were&apos;, 41), (&apos;has&apos;, 42), (&apos;but&apos;, 43), (&apos;other&apos;, 44), (&apos;their&apos;, 45), (&apos;its&apos;, 46), (&apos;first&apos;, 47), (&apos;they&apos;, 48), (&apos;had&apos;, 49)]</span><br></pre></td></tr></table></figure><p>In [10]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">VOCAB_SIZE = len(TEXT.vocab) <span class="comment"># 50002</span></span><br><span class="line">train_iter, val_iter, test_iter = \</span><br><span class="line">torchtext.data.BPTTIterator.splits(</span><br><span class="line">    (train, val, test), </span><br><span class="line">    batch_size=BATCH_SIZE, </span><br><span class="line">    device=<span class="number">-1</span>, </span><br><span class="line">    bptt_len=<span class="number">50</span>, <span class="comment"># 反向传播往回传的长度，这里我暂时理解为一个样本有多少个单词传入模型</span></span><br><span class="line">    repeat=<span class="literal">False</span>, </span><br><span class="line">    shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># BPTTIterator可以连续地得到连贯的句子，BPTT的全称是back propagation through time。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Iterator：标准迭代器</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">BucketIerator：相比于标准迭代器，会将类似长度的样本当做一批来处理，</span></span><br><span class="line"><span class="string">因为在文本处理中经常会需要将每一批样本长度补齐为当前批中最长序列的长度，</span></span><br><span class="line"><span class="string">因此当样本长度差别较大时，使用BucketIerator可以带来填充效率的提高。</span></span><br><span class="line"><span class="string">除此之外，我们还可以在Field中通过fix_length参数来对样本进行截断补齐操作。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">BPTTIterator: 基于BPTT(基于时间的反向传播算法)的迭代器，一般用于语言模型中。</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.</span><br><span class="line">The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.</span><br><span class="line">The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.</span><br></pre></td></tr></table></figure><p>Out[10]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;\nIterator：标准迭代器\n\nBucketIerator：相比于标准迭代器，会将类似长度的样本当做一批来处理，\n因为在文本处理中经常会需要将每一批样本长度补齐为当前批中最长序列的长度，\n因此当样本长度差别较大时，使用BucketIerator可以带来填充效率的提高。\n除此之外，我们还可以在Field中通过fix_length参数来对样本进行截断补齐操作。\n\nBPTTIterator: 基于BPTT(基于时间的反向传播算法)的迭代器，一般用于语言模型中。\n&apos;</span><br></pre></td></tr></table></figure><p>In [11]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(next(iter(train_iter))) <span class="comment"># 一个batch训练集维度</span></span><br><span class="line">print(next(iter(val_iter))) <span class="comment"># 一个batch验证集维度</span></span><br><span class="line">print(next(iter(test_iter))) <span class="comment"># 一个batch测试集维度</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[torchtext.data.batch.Batch of size <span class="number">32</span>]</span><br><span class="line">[.text]:[torch.LongTensor of size <span class="number">50</span>x32]</span><br><span class="line">[.target]:[torch.LongTensor of size <span class="number">50</span>x32]</span><br><span class="line"></span><br><span class="line">[torchtext.data.batch.Batch of size <span class="number">32</span>]</span><br><span class="line">[.text]:[torch.LongTensor of size <span class="number">50</span>x32]</span><br><span class="line">[.target]:[torch.LongTensor of size <span class="number">50</span>x32]</span><br><span class="line"></span><br><span class="line">[torchtext.data.batch.Batch of size <span class="number">32</span>]</span><br><span class="line">[.text]:[torch.LongTensor of size <span class="number">50</span>x32]</span><br><span class="line">[.target]:[torch.LongTensor of size <span class="number">50</span>x32]</span><br></pre></td></tr></table></figure><p>模型的输入是一串文字，模型的输出也是一串文字，他们之间相差一个位置，因为语言模型的目标是根据之前的单词预测下一个单词。</p><p>In [12]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">it = iter(train_iter)</span><br><span class="line">batch = next(it)</span><br><span class="line">print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.text[:,<span class="number">1</span>].data])) <span class="comment"># 打印一个输入的句子</span></span><br><span class="line">print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.target[:,<span class="number">1</span>].data])) <span class="comment"># 打印一个输出的句子</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">combine in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most typical forms of matter they had also suggested the possibility</span><br><span class="line">in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most typical forms of matter they had also suggested the possibility of</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In [13]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">5</span>): <span class="comment"># 这种取法是在一个固定的batch里取数据，发现一个batch里的数据是连不起来的。</span></span><br><span class="line">    print(j)</span><br><span class="line">    print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.text[:,j].data]))</span><br><span class="line">    print(j)</span><br><span class="line">    print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.target[:,j].data]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">0</span><br><span class="line">anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans &lt;unk&gt; of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the</span><br><span class="line">0</span><br><span class="line">originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans &lt;unk&gt; of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization</span><br><span class="line">1</span><br><span class="line">combine in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most typical forms of matter they had also suggested the possibility</span><br><span class="line">1</span><br><span class="line">in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most typical forms of matter they had also suggested the possibility of</span><br><span class="line">2</span><br><span class="line">culture few living ainu settlements exist many authentic ainu villages advertised in hokkaido are simply tourist attractions language the ainu language is significantly different from japanese in its syntax phonology morphology and vocabulary although there have been attempts to show that they are related the vast majority of modern scholars</span><br><span class="line">2</span><br><span class="line">few living ainu settlements exist many authentic ainu villages advertised in hokkaido are simply tourist attractions language the ainu language is significantly different from japanese in its syntax phonology morphology and vocabulary although there have been attempts to show that they are related the vast majority of modern scholars reject</span><br><span class="line">3</span><br><span class="line">zero the apple iie card an expansion card for the lc line of macintosh computers was released essentially a miniaturized apple iie computer on a card utilizing the mega ii chip from the apple iigs it allowed the macintosh to run eight bit apple iie software through hardware emulation although</span><br><span class="line">3</span><br><span class="line">the apple iie card an expansion card for the lc line of macintosh computers was released essentially a miniaturized apple iie computer on a card utilizing the mega ii chip from the apple iigs it allowed the macintosh to run eight bit apple iie software through hardware emulation although video</span><br><span class="line">4</span><br><span class="line">in papers have been written arguing that the anthropic principle would explain the physical constants such as the fine structure constant the number of dimensions in the universe and the cosmological constant the three primary versions of the principle as stated by john d barrow and frank j &lt;unk&gt; one</span><br><span class="line">4</span><br><span class="line">papers have been written arguing that the anthropic principle would explain the physical constants such as the fine structure constant the number of dimensions in the universe and the cosmological constant the three primary versions of the principle as stated by john d barrow and frank j &lt;unk&gt; one nine</span><br></pre></td></tr></table></figure><p>In [14]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>): <span class="comment"># 这种取法是在每个batch里取某一个相同位置数据，发现不同batch间相同位置的数据是可以连起来的。这里有点小疑问。</span></span><br><span class="line">    batch = next(it)</span><br><span class="line">    print(i)</span><br><span class="line">    print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.text[:,<span class="number">2</span>].data]))</span><br><span class="line">    print(i)</span><br><span class="line">    print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.target[:,<span class="number">2</span>].data]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">0</span><br><span class="line">reject that the relationship goes beyond contact i e mutual borrowing of words between japanese and ainu in fact no attempt to show a relationship with ainu to any other language has gained wide acceptance and ainu is currently considered to be a language isolate culture traditional ainu culture is</span><br><span class="line">0</span><br><span class="line">that the relationship goes beyond contact i e mutual borrowing of words between japanese and ainu in fact no attempt to show a relationship with ainu to any other language has gained wide acceptance and ainu is currently considered to be a language isolate culture traditional ainu culture is quite</span><br><span class="line">1</span><br><span class="line">quite different from japanese culture never shaving after a certain age the men had full beards and &lt;unk&gt; men and women alike cut their hair level with the shoulders at the sides of the head but trimmed it &lt;unk&gt; behind the women tattooed their mouths arms &lt;unk&gt; and sometimes their</span><br><span class="line">1</span><br><span class="line">different from japanese culture never shaving after a certain age the men had full beards and &lt;unk&gt; men and women alike cut their hair level with the shoulders at the sides of the head but trimmed it &lt;unk&gt; behind the women tattooed their mouths arms &lt;unk&gt; and sometimes their &lt;unk&gt;</span><br><span class="line">2</span><br><span class="line">&lt;unk&gt; starting at the onset of puberty the soot deposited on a pot hung over a fire of birch bark was used for colour their traditional dress is a robe spun from the bark of the elm tree it has long sleeves reaches nearly to the feet is folded round</span><br><span class="line">2</span><br><span class="line">starting at the onset of puberty the soot deposited on a pot hung over a fire of birch bark was used for colour their traditional dress is a robe spun from the bark of the elm tree it has long sleeves reaches nearly to the feet is folded round the</span><br><span class="line">3</span><br><span class="line">the body and is tied with a girdle of the same material women also wear an &lt;unk&gt; of japanese cloth in winter the skins of animals were worn with &lt;unk&gt; of &lt;unk&gt; and boots made from the skin of dogs or salmon both sexes are fond of earrings which are</span><br><span class="line">3</span><br><span class="line">body and is tied with a girdle of the same material women also wear an &lt;unk&gt; of japanese cloth in winter the skins of animals were worn with &lt;unk&gt; of &lt;unk&gt; and boots made from the skin of dogs or salmon both sexes are fond of earrings which are said</span><br><span class="line">4</span><br><span class="line">said to have been made of grapevine in former times as also are bead necklaces called &lt;unk&gt; which the women prized highly their traditional cuisine consists of the flesh of bear fox wolf badger ox or horse as well as fish fowl millet vegetables herbs and roots they never ate</span><br><span class="line">4</span><br><span class="line">to have been made of grapevine in former times as also are bead necklaces called &lt;unk&gt; which the women prized highly their traditional cuisine consists of the flesh of bear fox wolf badger ox or horse as well as fish fowl millet vegetables herbs and roots they never ate raw</span><br></pre></td></tr></table></figure><h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><ul><li>继承nn.Module</li><li>初始化函数</li><li>forward函数</li><li>其余可以根据模型需要定义相关的函数</li></ul><p>In [15]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" 一个简单的循环神经网络"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        <span class="comment"># rnn_type；有两个层供选择'LSTM', 'GRU'</span></span><br><span class="line">        <span class="comment"># ntoken：VOCAB_SIZE=50002</span></span><br><span class="line">        <span class="comment"># ninp：EMBEDDING_SIZE = 650，输入层维度</span></span><br><span class="line">        <span class="comment"># nhid：EMBEDDING_SIZE = 1000，隐藏层维度，这里是我自己设置的，用于区分ninp层。</span></span><br><span class="line">        <span class="comment"># nlayers：纵向有多少层神经网络</span></span><br><span class="line"></span><br><span class="line">        <span class="string">''' 该模型包含以下几层:</span></span><br><span class="line"><span class="string">            - 词嵌入层</span></span><br><span class="line"><span class="string">            - 一个循环神经网络层(RNN, LSTM, GRU)</span></span><br><span class="line"><span class="string">            - 一个线性层，从hidden state到输出单词表</span></span><br><span class="line"><span class="string">            - 一个dropout层，用来做regularization</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super(RNNModel, self).__init__()</span><br><span class="line">        self.drop = nn.Dropout(dropout)</span><br><span class="line">        self.encoder = nn.Embedding(ntoken, ninp)</span><br><span class="line">        <span class="comment"># 定义输入的Embedding层，用来把每个单词转化为词向量</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> rnn_type <span class="keyword">in</span> [<span class="string">'LSTM'</span>, <span class="string">'GRU'</span>]: <span class="comment"># 下面代码以LSTM举例</span></span><br><span class="line">            </span><br><span class="line">            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)</span><br><span class="line">            <span class="comment"># getattr(nn, rnn_type) 相当于 nn.rnn_type</span></span><br><span class="line">            <span class="comment"># nlayers代表纵向有多少层。还有个参数是bidirectional: 是否是双向LSTM，默认false</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                nonlinearity = &#123;<span class="string">'RNN_TANH'</span>: <span class="string">'tanh'</span>, <span class="string">'RNN_RELU'</span>: <span class="string">'relu'</span>&#125;[rnn_type]</span><br><span class="line">            <span class="keyword">except</span> KeyError:</span><br><span class="line">                <span class="keyword">raise</span> ValueError( <span class="string">"""An invalid option for `--model` was supplied,</span></span><br><span class="line"><span class="string">                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']"""</span>)</span><br><span class="line">            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)</span><br><span class="line">        self.decoder = nn.Linear(nhid, ntoken)</span><br><span class="line">        <span class="comment"># 最后线性全连接隐藏层的维度(1000,50002)</span></span><br><span class="line">      </span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">        self.rnn_type = rnn_type</span><br><span class="line">        self.nhid = nhid</span><br><span class="line">        self.nlayers = nlayers</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        initrange = <span class="number">0.1</span></span><br><span class="line">        self.encoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        self.decoder.bias.data.zero_()</span><br><span class="line">        self.decoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span> </span><br><span class="line">        </span><br><span class="line">        <span class="string">''' Forward pass:</span></span><br><span class="line"><span class="string">            - word embedding</span></span><br><span class="line"><span class="string">            - 输入循环神经网络</span></span><br><span class="line"><span class="string">            - 一个线性层从hidden state转化为输出单词表</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># input.shape = seg_length * batch = torch.Size([50, 32])</span></span><br><span class="line">        <span class="comment"># 如果觉得想变成32*50格式，可以在LSTM里定义batch_first = True</span></span><br><span class="line">        <span class="comment"># hidden = (nlayers * 32 * hidden_size, nlayers * 32 * hidden_size)</span></span><br><span class="line">        <span class="comment"># hidden是个元组，输入有两个参数，一个是刚开始的隐藏层h的维度，一个是刚开始的用于记忆的c的维度，</span></span><br><span class="line">        <span class="comment"># 这两个层的维度一样，并且需要先初始化，hidden_size的维度和上面nhid的维度一样 =1000，我理解这两个是同一个东西。</span></span><br><span class="line">        emb = self.drop(self.encoder(input)) <span class="comment"># </span></span><br><span class="line">        <span class="comment"># emb.shape=torch.Size([50, 32, 650]) # 输入数据的维度</span></span><br><span class="line">        <span class="comment"># 这里进行了运算（50，50002，650）*(50, 32，50002)</span></span><br><span class="line">        output, hidden = self.rnn(emb, hidden)</span><br><span class="line">        <span class="comment"># output.shape = 50 * 32 * hidden_size # 最终输出数据的维度，</span></span><br><span class="line">        <span class="comment"># hidden是个元组，输出有两个参数，一个是最后的隐藏层h的维度，一个是最后的用于记忆的c的维度，这两个层维度相同 </span></span><br><span class="line">        <span class="comment"># hidden = (h层维度：nlayers * 32 * hidden_size, c层维度：nlayers * 32 * hidden_size)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        output = self.drop(output)</span><br><span class="line">        decoded = self.decoder(output.view(output.size(<span class="number">0</span>)*output.size(<span class="number">1</span>), output.size(<span class="number">2</span>)))</span><br><span class="line">        <span class="comment"># output最后的输出层一定要是二维的，只是为了能进行全连接层的运算，所以把前两个维度拼到一起，（50*32,hidden_size)</span></span><br><span class="line">        <span class="comment"># decoded.shape=（50*32,hidden_size)*(hidden_size,50002)=torch.Size([1600, 50002])</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> decoded.view(output.size(<span class="number">0</span>), output.size(<span class="number">1</span>), decoded.size(<span class="number">1</span>)), hidden</span><br><span class="line">               <span class="comment"># 我们要知道每一个位置预测的是哪个单词，所以最终输出要恢复维度 = (50,32,50002)</span></span><br><span class="line">               <span class="comment"># hidden = (h层维度：2 * 32 * 1000, c层维度：2 * 32 * 1000)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, bsz, requires_grad=True)</span>:</span></span><br><span class="line">        <span class="comment"># 这步我们初始化下隐藏层参数</span></span><br><span class="line">        weight = next(self.parameters())</span><br><span class="line">        <span class="comment"># weight = torch.Size([50002, 650])是所有参数的第一个参数</span></span><br><span class="line">        <span class="comment"># 所有参数self.parameters()，是个生成器，LSTM所有参数维度种类如下：</span></span><br><span class="line">        <span class="comment"># print(list(iter(self.parameters())))</span></span><br><span class="line">        <span class="comment"># torch.Size([50002, 650])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000, 650])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000, 1000])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000]) # 偏置项</span></span><br><span class="line">        <span class="comment"># torch.Size([4000])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000, 1000])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000, 1000])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000])</span></span><br><span class="line">        <span class="comment"># torch.Size([50002, 1000])</span></span><br><span class="line">        <span class="comment"># torch.Size([50002])</span></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type == <span class="string">'LSTM'</span>:</span><br><span class="line">            <span class="keyword">return</span> (weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad),</span><br><span class="line">                    weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad))</span><br><span class="line">                   <span class="comment"># return = (2 * 32 * 1000, 2 * 32 * 1000)</span></span><br><span class="line">                   <span class="comment"># 这里不明白为什么需要weight.new_zeros，我估计是想整个计算图能链接起来</span></span><br><span class="line">                   <span class="comment"># 这里特别注意hidden的输入不是model的参数，不参与更新，就跟输入数据x一样</span></span><br><span class="line">                   </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad)</span><br><span class="line">            <span class="comment"># GRU神经网络把h层和c层合并了，所以这里只有一层。</span></span><br></pre></td></tr></table></figure><p>初始化一个模型</p><p>In [16]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nhid = <span class="number">1000</span> <span class="comment"># 我自己设置的维度，用于区分embeding_size=650</span></span><br><span class="line">model = RNNModel(<span class="string">"LSTM"</span>, VOCAB_SIZE, EMBEDDING_SIZE, nhid, <span class="number">2</span>, dropout=<span class="number">0.5</span>)</span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    model = model.cuda()</span><br></pre></td></tr></table></figure><p>In [17]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model</span><br></pre></td></tr></table></figure><p>Out[17]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">RNNModel(</span><br><span class="line">  (drop): Dropout(p=<span class="number">0.5</span>)</span><br><span class="line">  (encoder): Embedding(<span class="number">50002</span>, <span class="number">650</span>)</span><br><span class="line">  (rnn): LSTM(<span class="number">650</span>, <span class="number">1000</span>, num_layers=<span class="number">2</span>, dropout=<span class="number">0.5</span>)</span><br><span class="line">  (decoder): Linear(in_features=<span class="number">1000</span>, out_features=<span class="number">50002</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>In [23]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list(model.parameters())[0].shape</span><br></pre></td></tr></table></figure><p>Out[23]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([50002, 650])</span><br></pre></td></tr></table></figure><ul><li>我们首先定义评估模型的代码。</li><li>模型的评估和模型的训练逻辑基本相同，唯一的区别是我们只需要forward pass，不需要backward pass</li></ul><p>In [68]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先从下面训练模式看起，在看evaluate</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, data)</span>:</span></span><br><span class="line">    model.eval() <span class="comment"># 预测模式</span></span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    it = iter(data)</span><br><span class="line">    total_count = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        hidden = model.init_hidden(BATCH_SIZE, requires_grad=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 这里不管是训练模式还是预测模式，h层的输入都是初始化为0，hidden的输入不是model的参数</span></span><br><span class="line"><span class="comment"># 这里model里的model.parameters()已经是训练过的参数。</span></span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(it):</span><br><span class="line">            data, target = batch.text, batch.target</span><br><span class="line">            <span class="comment"># # 取出验证集的输入的数据和输出的数据，相当于特征和标签</span></span><br><span class="line">            <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">                data, target = data.cuda(), target.cuda()</span><br><span class="line">            hidden = repackage_hidden(hidden) <span class="comment"># 截断计算图</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad(): <span class="comment"># 验证阶段不需要更新梯度</span></span><br><span class="line">                output, hidden = model(data, hidden)</span><br><span class="line">                <span class="comment">#调用model的forward方法进行一次前向传播，得到return输出值</span></span><br><span class="line">            loss = loss_fn(output.view(<span class="number">-1</span>, VOCAB_SIZE), target.view(<span class="number">-1</span>))</span><br><span class="line">            <span class="comment"># 计算交叉熵损失</span></span><br><span class="line">            </span><br><span class="line">            total_count += np.multiply(*data.size()) </span><br><span class="line"><span class="comment"># 上面计算交叉熵的损失是平均过的，这里需要计算下总的损失</span></span><br><span class="line"><span class="comment"># total_count先计算验证集样本的单词总数，一个样本有50个单词，一个batch32个样本</span></span><br><span class="line"><span class="comment"># np.multiply(*data.size()) =50*32=1600</span></span><br><span class="line">            total_loss += loss.item()*np.multiply(*data.size())</span><br><span class="line"><span class="comment"># 每次batch平均后的损失乘以每次batch的样本的总的单词数 = 一次batch总的损失</span></span><br><span class="line">            </span><br><span class="line">    loss = total_loss / total_count <span class="comment"># 整个验证集总的损失除以总的单词数</span></span><br><span class="line">    model.train() <span class="comment"># 训练模式</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>In [9]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = torch.ones((<span class="number">5</span>,<span class="number">3</span>))</span><br><span class="line">print(a.size())</span><br><span class="line">np.multiply(*a.size())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([5, 3])</span><br></pre></td></tr></table></figure><p>Out[9]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">15</span><br></pre></td></tr></table></figure><p>我们需要定义下面的一个function，帮助我们把一个hidden state和计算图之前的历史分离。</p><p>In [69]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Remove this part</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">repackage_hidden</span><span class="params">(h)</span>:</span></span><br><span class="line">    <span class="string">"""Wraps hidden states in new Tensors, to detach them from their history."""</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(h, torch.Tensor): </span><br><span class="line">        <span class="comment"># 这个是GRU的截断，因为只有一个隐藏层</span></span><br><span class="line">        <span class="comment"># 判断h是不是torch.Tensor</span></span><br><span class="line">        <span class="keyword">return</span> h.detach() <span class="comment"># 截断计算图，h是全的计算图的开始，只是保留了h的值</span></span><br><span class="line">    <span class="keyword">else</span>: <span class="comment"># 这个是LSTM的截断，有两个隐藏层，格式是元组</span></span><br><span class="line">        <span class="keyword">return</span> tuple(repackage_hidden(v) <span class="keyword">for</span> v <span class="keyword">in</span> h)</span><br></pre></td></tr></table></figure><p>定义loss function和optimizer</p><p>In [70]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss() <span class="comment"># 交叉熵损失</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, <span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># 每调用一次这个函数，lenrning_rate就降一半，0.5就是一半的意思</span></span><br></pre></td></tr></table></figure><p>训练模型：</p><ul><li>模型一般需要训练若干个epoch</li><li>每个epoch我们都把所有的数据分成若干个batch</li><li>把每个batch的输入和输出都包装成cuda tensor</li><li>forward pass，通过输入的句子预测每个单词的下一个单词</li><li>用模型的预测和正确的下一个单词计算cross entropy loss</li><li>清空模型当前gradient</li><li>backward pass</li><li>gradient clipping，防止梯度爆炸</li><li>更新模型参数</li><li>每隔一定的iteration输出模型在当前iteration的loss，以及在验证集上做模型的评估</li></ul><p>In [13]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">GRAD_CLIP = <span class="number">1.</span></span><br><span class="line">NUM_EPOCHS = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">val_losses = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    model.train() <span class="comment"># 训练模式</span></span><br><span class="line">    it = iter(train_iter) </span><br><span class="line">    <span class="comment"># iter,生成迭代器,这里train_iter也是迭代器，不用iter也可以</span></span><br><span class="line">    hidden = model.init_hidden(BATCH_SIZE) </span><br><span class="line">    <span class="comment"># 得到hidden初始化后的维度</span></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(it):</span><br><span class="line">        data, target = batch.text, batch.target</span><br><span class="line">        <span class="comment"># 取出训练集的输入的数据和输出的数据，相当于特征和标签</span></span><br><span class="line">        <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">            data, target = data.cuda(), target.cuda()</span><br><span class="line">        hidden = repackage_hidden(hidden)</span><br><span class="line"><span class="comment"># 语言模型每个batch的隐藏层的输出值是要继续作为下一个batch的隐藏层的输入的</span></span><br><span class="line"><span class="comment"># 因为batch数量很多，如果一直往后传，会造成整个计算图很庞大，反向传播会内存崩溃。</span></span><br><span class="line"><span class="comment"># 所有每次一个batch的计算图迭代完成后，需要把计算图截断，只保留隐藏层的输出值。</span></span><br><span class="line"><span class="comment"># 不过只有语言模型才这么干，其他比如翻译模型不需要这么做。</span></span><br><span class="line"><span class="comment"># repackage_hidden自定义函数用来截断计算图的。</span></span><br><span class="line">        model.zero_grad() <span class="comment"># 梯度归零，不然每次迭代梯度会累加</span></span><br><span class="line">        output, hidden = model(data, hidden)</span><br><span class="line">        <span class="comment"># output = (50,32,50002)</span></span><br><span class="line">        loss = loss_fn(output.view(<span class="number">-1</span>, VOCAB_SIZE), target.view(<span class="number">-1</span>))</span><br><span class="line"><span class="comment"># output.view(-1, VOCAB_SIZE) = (1600,50002)</span></span><br><span class="line"><span class="comment"># target.view(-1) =(1600),关于pytorch中交叉熵的计算公式请看下面链接。</span></span><br><span class="line"><span class="comment"># https://blog.csdn.net/geter_CS/article/details/84857220</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)</span><br><span class="line">        <span class="comment"># 防止梯度爆炸，设定阈值，当梯度大于阈值时，更新的梯度为阈值</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"epoch"</span>, epoch, <span class="string">"iter"</span>, i, <span class="string">"loss"</span>, loss.item())</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            val_loss = evaluate(model, val_iter)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> len(val_losses) == <span class="number">0</span> <span class="keyword">or</span> val_loss &lt; min(val_losses):</span><br><span class="line">                <span class="comment"># 如果比之前的loss要小，就保存模型</span></span><br><span class="line">                print(<span class="string">"best model, val loss: "</span>, val_loss)</span><br><span class="line">                torch.save(model.state_dict(), <span class="string">"lm-best.th"</span>)</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 否则loss没有降下来，需要优化</span></span><br><span class="line">                scheduler.step() <span class="comment"># 自动调整学习率</span></span><br><span class="line">                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line">                <span class="comment"># 学习率调整后需要更新optimizer，下次训练就用更新后的</span></span><br><span class="line">            val_losses.append(val_loss) <span class="comment"># 保存每10000次迭代后的验证集损失损失</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">epoch 0 iter 0 loss 10.821578979492188</span><br><span class="line">best model, val loss:  10.782116411285918</span><br><span class="line">epoch 0 iter 1000 loss 6.5122528076171875</span><br><span class="line">epoch 0 iter 2000 loss 6.3599748611450195</span><br><span class="line">epoch 0 iter 3000 loss 6.13856315612793</span><br><span class="line">epoch 0 iter 4000 loss 5.473214626312256</span><br><span class="line">epoch 0 iter 5000 loss 5.901871204376221</span><br><span class="line">epoch 0 iter 6000 loss 5.85321569442749</span><br><span class="line">epoch 0 iter 7000 loss 5.636535167694092</span><br><span class="line">epoch 0 iter 8000 loss 5.7489800453186035</span><br><span class="line">epoch 0 iter 9000 loss 5.464158058166504</span><br><span class="line">epoch 0 iter 10000 loss 5.554863452911377</span><br><span class="line">best model, val loss:  5.264891533569864</span><br><span class="line">epoch 0 iter 11000 loss 5.703625202178955</span><br><span class="line">epoch 0 iter 12000 loss 5.6448974609375</span><br><span class="line">epoch 0 iter 13000 loss 5.372857570648193</span><br><span class="line">epoch 0 iter 14000 loss 5.2639479637146</span><br><span class="line">epoch 1 iter 0 loss 5.696778297424316</span><br><span class="line">best model, val loss:  5.124550380139679</span><br><span class="line">epoch 1 iter 1000 loss 5.534722805023193</span><br><span class="line">epoch 1 iter 2000 loss 5.599489212036133</span><br><span class="line">epoch 1 iter 3000 loss 5.459986686706543</span><br><span class="line">epoch 1 iter 4000 loss 4.927192211151123</span><br><span class="line">epoch 1 iter 5000 loss 5.435710906982422</span><br><span class="line">epoch 1 iter 6000 loss 5.4059576988220215</span><br><span class="line">epoch 1 iter 7000 loss 5.308575630187988</span><br><span class="line">epoch 1 iter 8000 loss 5.405811786651611</span><br><span class="line">epoch 1 iter 9000 loss 5.1389055252075195</span><br><span class="line">epoch 1 iter 10000 loss 5.226413726806641</span><br><span class="line">best model, val loss:  4.946829228873176</span><br><span class="line">epoch 1 iter 11000 loss 5.379891395568848</span><br><span class="line">epoch 1 iter 12000 loss 5.360724925994873</span><br><span class="line">epoch 1 iter 13000 loss 5.176026344299316</span><br><span class="line">epoch 1 iter 14000 loss 5.110936641693115</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载保存好的模型参数</span></span><br><span class="line">best_model = RNNModel(<span class="string">"LSTM"</span>, VOCAB_SIZE, EMBEDDING_SIZE, nhid, <span class="number">2</span>, dropout=<span class="number">0.5</span>)</span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    best_model = best_model.cuda()</span><br><span class="line">best_model.load_state_dict(torch.load(<span class="string">"lm-best.th"</span>))</span><br><span class="line"><span class="comment"># 把模型参数load到best_model里</span></span><br></pre></td></tr></table></figure><h3 id="使用最好的模型在valid数据上计算perplexity"><a href="#使用最好的模型在valid数据上计算perplexity" class="headerlink" title="使用最好的模型在valid数据上计算perplexity"></a>使用最好的模型在valid数据上计算perplexity</h3><p>In [15]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val_loss = evaluate(best_model, val_iter)</span><br><span class="line">print(<span class="string">"perplexity: "</span>, np.exp(val_loss))</span><br><span class="line"><span class="comment"># 这里不清楚语言模型的评估指标perplexity = np.exp(val_loss)</span></span><br><span class="line"><span class="comment"># 清楚的朋友欢迎交流下</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">perplexity:  140.72803934425724</span><br></pre></td></tr></table></figure><h3 id="使用最好的模型在测试数据上计算perplexity"><a href="#使用最好的模型在测试数据上计算perplexity" class="headerlink" title="使用最好的模型在测试数据上计算perplexity"></a>使用最好的模型在测试数据上计算perplexity</h3><p>In [16]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_loss = evaluate(best_model, test_iter)</span><br><span class="line">print(<span class="string">"perplexity: "</span>, np.exp(test_loss))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">perplexity:  178.54742013696125</span><br></pre></td></tr></table></figure><p>使用训练好的模型生成一些句子。</p><p>In [18]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">hidden = best_model.init_hidden(<span class="number">1</span>) <span class="comment"># batch_size = 1</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">input = torch.randint(VOCAB_SIZE, (<span class="number">1</span>, <span class="number">1</span>), dtype=torch.long).to(device)</span><br><span class="line"><span class="comment"># (1,1)表示输出格式是1行1列的2维tensor，VOCAB_SIZE表示随机取的值小于VOCAB_SIZE=50002</span></span><br><span class="line"><span class="comment"># 我们input相当于取的是一个单词</span></span><br><span class="line">words = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    output, hidden = best_model(input, hidden)</span><br><span class="line">    <span class="comment"># output.shape = 1 * 1 * 50002</span></span><br><span class="line">    <span class="comment"># hidden = (2 * 1 * 1000, 2 * 1 * 1000)</span></span><br><span class="line">    word_weights = output.squeeze().exp().cpu()</span><br><span class="line">    <span class="comment"># .exp()的两个作用：一是把概率更大的变得更大，二是把负数经过e后变成正数，下面.multinomial参数需要正数</span></span><br><span class="line">    word_idx = torch.multinomial(word_weights, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 按照word_weights里面的概率随机的取值，概率大的取到的机会大。</span></span><br><span class="line">    <span class="comment"># torch.multinomial看这个博客理解：https://blog.csdn.net/monchin/article/details/79787621</span></span><br><span class="line">    <span class="comment"># 这里如果选择概率最大的，会每次生成重复的句子。</span></span><br><span class="line">    input.fill_(word_idx) <span class="comment"># 预测的单词index是word_idx，然后把word_idx作为下一个循环预测的input输入</span></span><br><span class="line">    word = TEXT.vocab.itos[word_idx] <span class="comment"># 根据word_idx取出对应的单词</span></span><br><span class="line">    words.append(word) </span><br><span class="line">print(<span class="string">" "</span>.join(words))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s influence clinton decision de gaulle is himself sappho s iv one family banquet was made published by paul &lt;unk&gt; and by a persuaded to prevent arcane of animate poverty based at copernicus bachelor in search services and in a cruise corps references eds the robin series july four one nine zero eight summer gutenberg one nine six four births one nine two eight deaths timeline of this method by the fourth amendment the german ioc known for his &lt;unk&gt; from &lt;unk&gt; one eight nine eight one seven eight nine management was established in one nine seven zero they had</span><br></pre></td></tr></table></figure><p>In [42]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.randint(50002, (1, 1))</span><br></pre></td></tr></table></figure><p>Out[42]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[11293]])</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RNN </tag>
            
            <tag> Linear </tag>
            
            <tag> Gradient Clipping </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SQuAD-BiDAF</title>
      <link href="/2020/04/17/SQuAD-BiDAF/"/>
      <url>/2020/04/17/SQuAD-BiDAF/</url>
      
        <content type="html"><![CDATA[<p>代码是在github<a href="https://github.com/galsang/BiDAF-pytorch" target="_blank" rel="noopener">BiDAF-pytorch</a>上下载的，我把代码弄成了下面jupyter notebook格式，代码是在kaggle GPU跑的，</p><p>数据集如果不能下载的可以到我的网盘下载，包括数据集和训练好的模型，比较大：<a href="https://pan.baidu.com/s/1XCEUG6E2biCdqFyaxIGtBw" target="_blank" rel="noopener">百度网盘下载地址</a></p><p>整个代码跑下来，训练集可以跑通，测试集当时跑的时候kaggle内存不够了，报错了，有兴趣可以试下最终的效果。</p><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This Python 3 environment comes with many helpful analytics libraries installed</span></span><br><span class="line"><span class="comment"># It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python</span></span><br><span class="line"><span class="comment"># For example, here's several helpful packages to load in </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># linear algebra</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># data processing, CSV file I/O (e.g. pd.read_csv)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Input data files are available in the "../input/" directory.</span></span><br><span class="line"><span class="comment"># For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">print(os.listdir(<span class="string">"../input"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Any results you write to the current directory are saved as output.</span></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!cp -r /kaggle/input/bidaf-pytorch-master/BiDAF-pytorch-master /kaggle/working</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.chdir(&quot;BiDAF-pytorch-master&quot;)</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.chdir(&quot;..&quot;)</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!ls</span><br><span class="line">!pwd</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> copy, json, os</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> gmtime, strftime</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> GloVe</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br></pre></td></tr></table></figure><h1 id="一、定义初始变量参数"><a href="#一、定义初始变量参数" class="headerlink" title="一、定义初始变量参数"></a>一、定义初始变量参数</h1><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有关argparse的官方文档操作请查看：https://docs.python.org/3/library/argparse.html#module-argparse，</span></span><br><span class="line"><span class="comment"># 下面的参数代码注释，我也不是特别懂，仅供参考</span></span><br><span class="line"><span class="comment"># 关于parser.add_argument(）的详解请查看：https://blog.csdn.net/u013177568/article/details/62432761/</span></span><br><span class="line"><span class="comment"># 对于下面函数add_argument()第一个是选项是必须写的参数，该参数接受选项参数或者是位置参数（一串文件名）</span></span><br><span class="line"><span class="comment"># 第二个是default默认值，如果第一个选项参数没有单独指定，那选项参数的值就是默认值</span></span><br><span class="line"><span class="comment"># 第三个是参数数据类型，代表你的选项参数必须是是int还是float字符型数据。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_args</span><span class="params">()</span>:</span></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">'--char-dim'</span>, default=<span class="number">8</span>, type=int)</span><br><span class="line">    <span class="comment"># char-dim 默认值是8</span></span><br><span class="line">    parser.add_argument(<span class="string">'--char-channel-width'</span>, default=<span class="number">5</span>, type=int)</span><br><span class="line">    <span class="comment"># char-channel-width 默认值是5 以下类似</span></span><br><span class="line">    parser.add_argument(<span class="string">'--char-channel-size'</span>, default=<span class="number">100</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--context-threshold'</span>, default=<span class="number">400</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--dev-batch-size'</span>, default=<span class="number">100</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--dev-file'</span>, default=<span class="string">'dev-v1.1.json'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--dropout'</span>, default=<span class="number">0.2</span>, type=float)</span><br><span class="line">    parser.add_argument(<span class="string">'--epoch'</span>, default=<span class="number">12</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--exp-decay-rate'</span>, default=<span class="number">0.999</span>, type=float)</span><br><span class="line">    parser.add_argument(<span class="string">'--gpu'</span>, default=<span class="number">0</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--hidden-size'</span>, default=<span class="number">100</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--learning-rate'</span>, default=<span class="number">0.5</span>, type=float)</span><br><span class="line">    parser.add_argument(<span class="string">'--print-freq'</span>, default=<span class="number">250</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--train-batch-size'</span>, default=<span class="number">60</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--train-file'</span>, default=<span class="string">'train-v1.1.json'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--word-dim'</span>, default=<span class="number">100</span>, type=int)</span><br><span class="line">    args = parser.parse_args(args=[]) </span><br><span class="line"><span class="comment"># .parse_args()是将之前所有add_argument定义的参数在括号里进行赋值，没有赋值(args=[])，就返回参数各自default的默认值。</span></span><br><span class="line"><span class="comment"># 返回值args相当于是个参数命名空间的集合，可以调用上面第一项选项参数的名字，就可以得到default值了。</span></span><br><span class="line"><span class="comment"># 比如调用上面参数方式：args.char_dim,args.char_channel_width....默认情况下，中划线会转换为下划线.</span></span><br><span class="line">    <span class="keyword">return</span> args</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args = parse_args()</span><br></pre></td></tr></table></figure><h1 id="二、SQuAD问答数据预处理"><a href="#二、SQuAD问答数据预处理" class="headerlink" title="二、SQuAD问答数据预处理"></a>二、SQuAD问答数据预处理</h1><h2 id="1、查看数据集结构"><a href="#1、查看数据集结构" class="headerlink" title="1、查看数据集结构"></a>1、查看数据集结构</h2><p>SQuAD问答数据介绍：<a href="https://rajpurkar.github.io/SQuAD-explorer/" target="_blank" rel="noopener">https://rajpurkar.github.io/SQuAD-explorer/</a> 这个数据集有两个文件，验证集和测试集：train-v1.1.json，dev-v1.1.json</p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'data/squad/dev-v1.1.json'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                line = f.readline() <span class="comment"># 该方法每次读出一行内容</span></span><br><span class="line">                <span class="keyword">if</span> line:</span><br><span class="line">                    print(<span class="string">"type(line)"</span>,type(line)) <span class="comment"># 直接打印就是字符串格式</span></span><br><span class="line">                    r = json.loads(line)</span><br><span class="line">                    print(<span class="string">"type(r)"</span>,type(r)) <span class="comment"># 使用json.loads将字符串转化为字典</span></span><br><span class="line">                    print(r)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            f.close()</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据架构如下</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"data"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"title"</span>: <span class="string">"Super_Bowl_50"</span>, <span class="comment"># 第一个主题</span></span><br><span class="line">            <span class="string">"paragraphs"</span>: [</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">"context"</span>: <span class="string">" numerals 50......."</span>, <span class="comment"># 每个主题会有很多context短文,这里只列出一个</span></span><br><span class="line">                    <span class="string">"qas"</span>: [  <span class="comment"># 这个列表里放问题和答案的位置，每篇context会有很有很多answer和question，这里只列出一个</span></span><br><span class="line">                        &#123;</span><br><span class="line">                            <span class="string">"answers"</span>: [  <span class="comment"># 一个问题会有三个答案，三个答案都是对的，只是在context不同或相同位置</span></span><br><span class="line">                                &#123;         <span class="comment"># 下面三个答案都在相同的位置</span></span><br><span class="line">                                    <span class="string">"answer_start"</span>: <span class="number">177</span>,  <span class="comment"># 答案在文中的起始位置是第177的字符。</span></span><br><span class="line">                                    <span class="string">"text"</span>: <span class="string">"Denver Broncos"</span></span><br><span class="line">                                &#125;,</span><br><span class="line">                                &#123;</span><br><span class="line">                                    <span class="string">"answer_start"</span>: <span class="number">177</span>,</span><br><span class="line">                                    <span class="string">"text"</span>: <span class="string">"Denver Broncos"</span></span><br><span class="line">                                &#125;,</span><br><span class="line">                                &#123;</span><br><span class="line">                                    <span class="string">"answer_start"</span>: <span class="number">177</span>,</span><br><span class="line">                                    <span class="string">"text"</span>: <span class="string">"Denver Broncos"</span></span><br><span class="line">                                &#125;</span><br><span class="line">                            ],</span><br><span class="line">                            <span class="string">"question"</span>: <span class="string">"Which NFL team represented the AFC at Super Bowl 50?"</span>,</span><br><span class="line">                            <span class="string">"id"</span>: <span class="string">"56be4db0acb8001400a502ec"</span></span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                    ]</span><br><span class="line">                &#125;</span><br><span class="line">                </span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"title"</span>: <span class="string">"Warsaw"</span>, <span class="comment"># 第二个主题</span></span><br><span class="line">            <span class="string">"paragraphs"</span>:   </span><br><span class="line">        &#125;,</span><br><span class="line">        </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"title"</span>: <span class="string">"Normans"</span>, <span class="comment"># 第三个主题</span></span><br><span class="line">            <span class="string">"paragraphs"</span>: </span><br><span class="line">        &#125;,</span><br><span class="line">        </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"title"</span>: <span class="string">"Nikola_Tesla"</span>, <span class="comment"># 第四个主题</span></span><br><span class="line">            <span class="string">"paragraphs"</span>: </span><br><span class="line">        &#125;,</span><br><span class="line">        ........... <span class="comment"># 还有很多</span></span><br><span class="line">        </span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"version"</span>: <span class="string">"1.1"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2、定义分词方法"><a href="#2、定义分词方法" class="headerlink" title="2、定义分词方法"></a>2、定义分词方法</h2><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_tokenize</span><span class="params">(tokens)</span>:</span></span><br><span class="line">    tokens = [token.replace(<span class="string">"''"</span>, <span class="string">'"'</span>).replace(<span class="string">"``"</span>, <span class="string">'"'</span>) <span class="keyword">for</span> token <span class="keyword">in</span> nltk.word_tokenize(tokens)]</span><br><span class="line">    <span class="comment"># nltk.word_tokenize(tokens)分词，replace规范化引号，方便后面处理</span></span><br><span class="line">    <span class="keyword">return</span> tokens</span><br></pre></td></tr></table></figure><h2 id="3、清洗数据，并生成数据迭代器"><a href="#3、清洗数据，并生成数据迭代器" class="headerlink" title="3、清洗数据，并生成数据迭代器"></a>3、清洗数据，并生成数据迭代器</h2><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SQuAD</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 以下定好中间输出缓存文件的路径</span></span><br><span class="line">        path = <span class="string">'data/squad'</span> </span><br><span class="line">        dataset_path = path + <span class="string">'/torch_text/'</span> </span><br><span class="line">        train_examples_path = dataset_path + <span class="string">'train_examples.pt'</span></span><br><span class="line">        dev_examples_path = dataset_path + <span class="string">'dev_examples.pt'</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">"preprocessing data files..."</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.train_file&#125;</span>l'</span>):</span><br><span class="line">            <span class="comment"># 字符串前以f开头表示在字符串内支持大括号内的 python 表达式</span></span><br><span class="line">            <span class="comment"># args.train_file = 'train-v1.1.json'</span></span><br><span class="line">            print(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.train_file&#125;</span>'</span>)</span><br><span class="line">            self.preprocess_file(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.train_file&#125;</span>'</span>)  </span><br><span class="line">            <span class="comment"># preprocess_file下面函数有定义，完成文件的预处理</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.dev_file&#125;</span>l'</span>):</span><br><span class="line">            <span class="comment"># args.dev_file = 'dev-v1.1.json'</span></span><br><span class="line">            self.preprocess_file(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.dev_file&#125;</span>'</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 下面是用torchtext处理数据的步骤看不懂了，有知道的可以交流下   </span></span><br><span class="line">        self.RAW = data.RawField()<span class="comment"># 这个是完全空白的field，意味着不经过任何处理</span></span><br><span class="line">        <span class="comment"># explicit declaration for torchtext compatibility</span></span><br><span class="line">        self.RAW.is_target = <span class="literal">False</span></span><br><span class="line">        self.CHAR_NESTING = data.Field(batch_first=<span class="literal">True</span>, tokenize=list, lower=<span class="literal">True</span>)</span><br><span class="line">        self.CHAR = data.NestedField(self.CHAR_NESTING, tokenize=word_tokenize)</span><br><span class="line">        self.WORD = data.Field(batch_first=<span class="literal">True</span>, tokenize=word_tokenize, lower=<span class="literal">True</span>, include_lengths=<span class="literal">True</span>)</span><br><span class="line">        self.LABEL = data.Field(sequential=<span class="literal">False</span>, unk_token=<span class="literal">None</span>, use_vocab=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        dict_fields = &#123;<span class="string">'id'</span>: (<span class="string">'id'</span>, self.RAW),</span><br><span class="line">                       <span class="string">'s_idx'</span>: (<span class="string">'s_idx'</span>, self.LABEL),</span><br><span class="line">                       <span class="string">'e_idx'</span>: (<span class="string">'e_idx'</span>, self.LABEL),</span><br><span class="line">                       <span class="string">'context'</span>: [(<span class="string">'c_word'</span>, self.WORD), (<span class="string">'c_char'</span>, self.CHAR)],</span><br><span class="line">                       <span class="string">'question'</span>: [(<span class="string">'q_word'</span>, self.WORD), (<span class="string">'q_char'</span>, self.CHAR)]&#125;</span><br><span class="line"></span><br><span class="line">        list_fields = [(<span class="string">'id'</span>, self.RAW), (<span class="string">'s_idx'</span>, self.LABEL), (<span class="string">'e_idx'</span>, self.LABEL),</span><br><span class="line">                       (<span class="string">'c_word'</span>, self.WORD), (<span class="string">'c_char'</span>, self.CHAR),</span><br><span class="line">                       (<span class="string">'q_word'</span>, self.WORD), (<span class="string">'q_char'</span>, self.CHAR)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(dataset_path):</span><br><span class="line">            print(<span class="string">"loading splits..."</span>)</span><br><span class="line">            train_examples = torch.load(train_examples_path)</span><br><span class="line">            dev_examples = torch.load(dev_examples_path)</span><br><span class="line"></span><br><span class="line">            self.train = data.Dataset(examples=train_examples, fields=list_fields)</span><br><span class="line">            self.dev = data.Dataset(examples=dev_examples, fields=list_fields)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"building splits..."</span>)</span><br><span class="line">             <span class="comment"># 划分训练集和验证集</span></span><br><span class="line">            self.train, self.dev = data.TabularDataset.splits(</span><br><span class="line">                path=path,</span><br><span class="line">                train=<span class="string">f'<span class="subst">&#123;args.train_file&#125;</span>l'</span>,</span><br><span class="line">                validation=<span class="string">f'<span class="subst">&#123;args.dev_file&#125;</span>l'</span>,</span><br><span class="line">                format=<span class="string">'json'</span>,</span><br><span class="line">                fields=dict_fields)</span><br><span class="line"></span><br><span class="line">            os.makedirs(dataset_path)</span><br><span class="line">            torch.save(self.train.examples, train_examples_path)</span><br><span class="line">            torch.save(self.dev.examples, dev_examples_path)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#cut too long context in the training set for efficiency.</span></span><br><span class="line">        <span class="keyword">if</span> args.context_threshold &gt; <span class="number">0</span>:</span><br><span class="line">            self.train.examples = [e <span class="keyword">for</span> e <span class="keyword">in</span> self.train.examples <span class="keyword">if</span> len(e.c_word) &lt;= args.context_threshold]</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"building vocab..."</span>)</span><br><span class="line">        self.CHAR.build_vocab(self.train, self.dev) <span class="comment"># 字符向量没有设置vector</span></span><br><span class="line">        self.WORD.build_vocab(self.train, self.dev, vectors=GloVe(name=<span class="string">'6B'</span>, dim=args.word_dim))</span><br><span class="line">        <span class="comment"># 加载Glove向量，args.word_dim = 100</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">"building iterators..."</span>)</span><br><span class="line">        device = torch.device(<span class="string">f"cuda:<span class="subst">&#123;args.gpu&#125;</span>"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">        <span class="comment"># 生成迭代器</span></span><br><span class="line">        self.train_iter, self.dev_iter = \</span><br><span class="line">            data.BucketIterator.splits((self.train, self.dev),</span><br><span class="line">                                       batch_sizes=[args.train_batch_size, args.dev_batch_size],</span><br><span class="line">                                       device=device,</span><br><span class="line">                                       sort_key=<span class="keyword">lambda</span> x: len(x.c_word))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preprocess_file</span><span class="params">(self,path)</span>:</span></span><br><span class="line">        dump = []</span><br><span class="line">        abnormals = [<span class="string">' '</span>, <span class="string">'\n'</span>, <span class="string">'\u3000'</span>, <span class="string">'\u202f'</span>, <span class="string">'\u2009'</span>]</span><br><span class="line">        <span class="comment"># 空白无效字符列表</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> open(path, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            data = json.load(f) <span class="comment"># 直接文件句柄转化为字典</span></span><br><span class="line">            data = data[<span class="string">'data'</span>] <span class="comment"># 返回值data是个列表，字典是列表的元素</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> article <span class="keyword">in</span> data:</span><br><span class="line">                <span class="comment"># 每个article是一个字典，一个字典包含一个title的信息</span></span><br><span class="line">                <span class="keyword">for</span> paragraph <span class="keyword">in</span> article[<span class="string">'paragraphs'</span>]:</span><br><span class="line">                    <span class="comment"># 每个paragraph是一个字典，一个字典里有一个context和qas的信息，qas是问题和答案。</span></span><br><span class="line">                    context = paragraph[<span class="string">'context'</span>]</span><br><span class="line">                    <span class="comment"># context的内容，是字符串，如：" numerals 50............."</span></span><br><span class="line">                    tokens = word_tokenize(context) <span class="comment"># 对context进行分词</span></span><br><span class="line">                    <span class="keyword">for</span> qa <span class="keyword">in</span> paragraph[<span class="string">'qas'</span>]:</span><br><span class="line">                        <span class="comment"># 每个qa是一个字典，一个字典包含一对answers和question的信息</span></span><br><span class="line">                        id = qa[<span class="string">'id'</span>]</span><br><span class="line">                        <span class="comment"># 取出这对answers和question的id信息，如："56be4db0acb8001400a502ec"</span></span><br><span class="line">                        question = qa[<span class="string">'question'</span>]</span><br><span class="line">                        <span class="comment"># 取出question，如："Which NFL team represented the AFC at Super Bowl 50?"</span></span><br><span class="line">                        <span class="keyword">for</span> ans <span class="keyword">in</span> qa[<span class="string">'answers'</span>]:</span><br><span class="line">                            <span class="comment"># ans为每个答案，共有三个标准答案，可以相同，可以不同，统一为3个。</span></span><br><span class="line">                            answer = ans[<span class="string">'text'</span>]</span><br><span class="line">                            <span class="comment"># 问题的每个回答，如："Denver Broncos"</span></span><br><span class="line">                            s_idx = ans[<span class="string">'answer_start'</span>]</span><br><span class="line">                            <span class="comment"># 每个回答的start位置，数值代表context中第几个字符，如：177</span></span><br><span class="line">                            e_idx = s_idx + len(answer)</span><br><span class="line">                            <span class="comment"># 每个回答的end位置</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                            <span class="comment"># 下面重新更新字符的起始位置，使用字符计算位置改为使用单词计算位置</span></span><br><span class="line">                            <span class="comment"># 请看下面单元格的示例输出有助理解。</span></span><br><span class="line">                            l = <span class="number">0</span></span><br><span class="line">                            s_found = <span class="literal">False</span></span><br><span class="line">                            <span class="keyword">for</span> i, t <span class="keyword">in</span> enumerate(tokens):</span><br><span class="line">                                <span class="comment"># 循环t次，t为分词后的单词数量</span></span><br><span class="line">                                <span class="keyword">while</span> l &lt; len(context):</span><br><span class="line">                                    <span class="keyword">if</span> context[l] <span class="keyword">in</span> abnormals:</span><br><span class="line">                                        <span class="comment"># context中有空白无效字符，就计数</span></span><br><span class="line">                                        l += <span class="number">1</span></span><br><span class="line">                                    <span class="keyword">else</span>:    <span class="comment"># 一碰到不是空白字符的就break</span></span><br><span class="line">                                        <span class="keyword">break</span></span><br><span class="line">                                <span class="comment"># exceptional cases</span></span><br><span class="line">                                <span class="keyword">if</span> t[<span class="number">0</span>] == <span class="string">'"'</span> <span class="keyword">and</span> context[l:l + <span class="number">2</span>] == <span class="string">'\'\''</span>:</span><br><span class="line">                                    <span class="comment"># 专门计算context=''an 这种长度，这个长度为4</span></span><br><span class="line">                                    t = <span class="string">'\'\''</span> + t[<span class="number">1</span>:] </span><br><span class="line">                                <span class="keyword">elif</span> t == <span class="string">'"'</span> <span class="keyword">and</span> context[l:l + <span class="number">2</span>] == <span class="string">'\'\''</span>:</span><br><span class="line">                                    <span class="comment"># 专门计算context='' 这种长度</span></span><br><span class="line">                                    <span class="comment"># 上面t[0] == '"'表达式包含了这种，所以我认为这个表达式没用上</span></span><br><span class="line">                                    t = <span class="string">'\'\''</span></span><br><span class="line"></span><br><span class="line">                                l += len(t)</span><br><span class="line">                                <span class="keyword">if</span> l &gt; s_idx <span class="keyword">and</span> s_found == <span class="literal">False</span>:</span><br><span class="line">                                    <span class="comment"># 只要计数超过起始位置值，这个单词就是start的单词</span></span><br><span class="line">                                    s_idx = i</span><br><span class="line">                                    s_found = <span class="literal">True</span></span><br><span class="line">                                <span class="keyword">if</span> l &gt;= e_idx:</span><br><span class="line">                                    <span class="comment"># 这里不出错的话，等于e_idx就是end的单词</span></span><br><span class="line">                                    e_idx = i</span><br><span class="line">                                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">                            <span class="comment"># 这里把三个answer分开，每个answer都放进字典中,并作为一个样本</span></span><br><span class="line">                            dump.append(dict([(<span class="string">'id'</span>, id),</span><br><span class="line">                                              (<span class="string">'context'</span>, context),</span><br><span class="line">                                              (<span class="string">'question'</span>, question),</span><br><span class="line">                                              (<span class="string">'answer'</span>, answer),</span><br><span class="line">                                              (<span class="string">'s_idx'</span>, s_idx),</span><br><span class="line">                                              (<span class="string">'e_idx'</span>, e_idx)]))</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">f'<span class="subst">&#123;path&#125;</span>l'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> dump:</span><br><span class="line">                <span class="comment"># line为字典，一个样本存储</span></span><br><span class="line">                json.dump(line, f)</span><br><span class="line">                <span class="comment">#dump：将dict类型转换为json字符串格式，写入到文件</span></span><br><span class="line">                print(<span class="string">''</span>, file=f) <span class="comment"># 这里print的作用就是换行用的。</span></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = SQuAD(args)</span><br></pre></td></tr></table></figure><h3 id="上面不太明白的举例子"><a href="#上面不太明白的举例子" class="headerlink" title="上面不太明白的举例子"></a>上面不太明白的举例子</h3><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 举例子</span></span><br><span class="line">a = <span class="string">" \u2009\n\u3000Super Bowl 50 was ''an'' American football     \u3000game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50."</span></span><br><span class="line">tokens = word_tokenize(a)</span><br><span class="line">print(nltk.word_tokenize(a)) <span class="comment"># 所有的“\u2009”，“\n”，“\u3000”等空白字符都去掉了</span></span><br><span class="line">print(<span class="string">"----"</span>*<span class="number">20</span>)</span><br><span class="line">print(tokens)</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># </span></span><br><span class="line">print(a[<span class="number">0</span>]) <span class="comment"># 空白字符打印不出来</span></span><br><span class="line">print(a[<span class="number">1</span>]) <span class="comment"># 空白字符打印不出来</span></span><br><span class="line">print(a[<span class="number">2</span>]) <span class="comment"># 空白字符打印不出来</span></span><br><span class="line">print(a[<span class="number">3</span>]) <span class="comment"># 空白字符打印不出来</span></span><br><span class="line">print(a[<span class="number">4</span>]) </span><br><span class="line">print(a[<span class="number">5</span>])</span><br><span class="line">a[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面特别注意</span></span><br><span class="line">print(tokens[<span class="number">4</span>][<span class="number">0</span>]== <span class="string">'"'</span>) <span class="comment"># 虽然切分后看起来是"''"，但实际上是'"'</span></span><br><span class="line">print(tokens[<span class="number">4</span>][<span class="number">0</span>]== <span class="string">"''"</span>) </span><br><span class="line">print(tokens[<span class="number">5</span>]== <span class="string">'"'</span>)</span><br><span class="line">print(len(<span class="string">'"'</span>)) <span class="comment"># 这种长度为1</span></span><br><span class="line">print(len(<span class="string">"''"</span>)) <span class="comment"># 这种长度为2</span></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看输出理解</span></span><br><span class="line">s_idx = <span class="number">177</span></span><br><span class="line">e_idx = s_idx + len(<span class="string">"Denver Broncos"</span>)</span><br><span class="line">l=<span class="number">0</span></span><br><span class="line">context = <span class="string">" \u2009\n\u3000Super Bowl 50 was ''an'' American football     \u3000game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50."</span></span><br><span class="line">tokens = word_tokenize(context)</span><br><span class="line">abnormals = [<span class="string">' '</span>, <span class="string">'\n'</span>, <span class="string">'\u3000'</span>, <span class="string">'\u202f'</span>, <span class="string">'\u2009'</span>]</span><br><span class="line">s_found = <span class="literal">False</span></span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> enumerate(tokens):</span><br><span class="line">    print(<span class="string">"t="</span>,t)</span><br><span class="line">    <span class="keyword">while</span> l &lt; len(context):</span><br><span class="line">        <span class="keyword">if</span> context[l] <span class="keyword">in</span> abnormals:</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    print(<span class="string">"l"</span>,l)</span><br><span class="line">    <span class="comment"># exceptional cases</span></span><br><span class="line">    <span class="keyword">if</span> t[<span class="number">0</span>] == <span class="string">'"'</span> <span class="keyword">and</span> context[l:l + <span class="number">2</span>] == <span class="string">'\'\''</span>:</span><br><span class="line">        print(<span class="string">"1111111111111111111"</span>)</span><br><span class="line">        print(t)</span><br><span class="line">        print(t[<span class="number">1</span>:])</span><br><span class="line">        t = <span class="string">'\'\''</span> + t[<span class="number">1</span>:]</span><br><span class="line">        print(t)</span><br><span class="line">    <span class="keyword">elif</span> t == <span class="string">'"'</span> <span class="keyword">and</span> context[l:l + <span class="number">2</span>] == <span class="string">'\'\''</span>:</span><br><span class="line">        <span class="comment"># 看输出结果，这个表达式没有用到</span></span><br><span class="line">        print(<span class="string">"22222222222222222222"</span>)</span><br><span class="line">        print(t)</span><br><span class="line">        t = <span class="string">'\'\''</span></span><br><span class="line">    print(<span class="string">"len(t)"</span>,len(t))</span><br><span class="line">    l += len(t)</span><br><span class="line">    print(<span class="string">"l"</span>,l)</span><br><span class="line">    <span class="keyword">if</span> l &gt; s_idx <span class="keyword">and</span> s_found == <span class="literal">False</span>:</span><br><span class="line">        s_idx = i</span><br><span class="line">        print(<span class="string">"s_idx"</span>,s_idx)</span><br><span class="line">        s_found = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> l &gt;= e_idx:</span><br><span class="line">        e_idx = i</span><br><span class="line">        print(<span class="string">"e_idx"</span>,e_idx)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch = next(iter(data.train_iter)) <span class="comment">#一个batch的信息</span></span><br><span class="line">print(batch)</span><br><span class="line"><span class="comment"># 训练集的batch_sizes=60</span></span><br><span class="line"><span class="comment"># batch.c_word = 60x293，293是60个样本中最长样本token的单词数</span></span><br><span class="line"><span class="comment"># batch.c_char = 60x293x25，25是某个单词字符的最大的数量</span></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(batch.q_word)</span><br><span class="line">print(batch.q_char[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面为args新增参数，并赋值</span></span><br><span class="line"><span class="comment"># hasattr() getattr() setattr() 函数使用方法详解https://www.cnblogs.com/cenyu/p/5713686.html</span></span><br><span class="line">setattr(args, <span class="string">'char_vocab_size'</span>, len(data.CHAR.vocab)) <span class="comment"># 设置属性args.char_vocab_size的值 = len(data.CHAR.vocab)</span></span><br><span class="line">setattr(args, <span class="string">'word_vocab_size'</span>, len(data.WORD.vocab))</span><br><span class="line">setattr(args, <span class="string">'dataset_file'</span>, <span class="string">f'data/squad/<span class="subst">&#123;args.dev_file&#125;</span>'</span>)</span><br><span class="line">setattr(args, <span class="string">'prediction_file'</span>, <span class="string">f'prediction<span class="subst">&#123;args.gpu&#125;</span>.out'</span>)</span><br><span class="line">setattr(args, <span class="string">'model_time'</span>, strftime(<span class="string">'%H:%M:%S'</span>, gmtime())) <span class="comment"># 时间</span></span><br><span class="line">print(<span class="string">'data loading complete!'</span>)</span><br></pre></td></tr></table></figure><h2 id="BIDAF"><a href="#BIDAF" class="headerlink" title="BIDAF"></a>BIDAF</h2><p><img src="https://s1.ax1x.com/2020/06/09/t4C2yd.jpg" alt="avatar"></p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTM</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, batch_first=False, num_layers=<span class="number">1</span>, bidirectional=False, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        <span class="comment"># input_size=args.hidden_size * 2 = 200,</span></span><br><span class="line">        <span class="comment"># hidden_size=args.hidden_size = 100,</span></span><br><span class="line">        <span class="comment"># bidirectional=True,</span></span><br><span class="line">        <span class="comment"># batch_first=True, </span></span><br><span class="line">        <span class="comment"># dropout=args.dropout = 0.2</span></span><br><span class="line">        super(LSTM, self).__init__()</span><br><span class="line">        self.rnn = nn.LSTM(input_size=input_size,</span><br><span class="line">                           hidden_size=hidden_size,</span><br><span class="line">                           num_layers=num_layers,</span><br><span class="line">                           bidirectional=bidirectional,</span><br><span class="line">                           batch_first=batch_first)</span><br><span class="line">        self.reset_params() <span class="comment"># 重置参数</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_params</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.rnn.num_layers):</span><br><span class="line">            nn.init.orthogonal_(getattr(self.rnn, <span class="string">f'weight_hh_l<span class="subst">&#123;i&#125;</span>'</span>)) <span class="comment"># hidden-hidden weights</span></span><br><span class="line">            <span class="comment"># weight_hh_l&#123;i&#125;、weight_ih_l&#123;i&#125;、bias_hh_l&#123;i&#125;、bias_ih_l&#123;i&#125; 都是nn.LSTM源码里的参数</span></span><br><span class="line">            <span class="comment"># getattr取出源码里参数的值，用nn.init.orthogonal_正交进行重新初始化</span></span><br><span class="line">            <span class="comment"># nn.init初始化方法看这个链接：https://www.aiuai.cn/aifarm613.html</span></span><br><span class="line">            nn.init.kaiming_normal_(getattr(self.rnn, <span class="string">f'weight_ih_l<span class="subst">&#123;i&#125;</span>'</span>)) <span class="comment"># input-hidden weights</span></span><br><span class="line">            nn.init.constant_(getattr(self.rnn, <span class="string">f'bias_hh_l<span class="subst">&#123;i&#125;</span>'</span>), val=<span class="number">0</span>) <span class="comment"># hidden-hidden bias</span></span><br><span class="line">            nn.init.constant_(getattr(self.rnn, <span class="string">f'bias_ih_l<span class="subst">&#123;i&#125;</span>'</span>), val=<span class="number">0</span>) <span class="comment"># input-hidden bias</span></span><br><span class="line">            getattr(self.rnn, <span class="string">f'bias_hh_l<span class="subst">&#123;i&#125;</span>'</span>).chunk(<span class="number">4</span>)[<span class="number">1</span>].fill_(<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># .chunk看下这个链接：https://blog.csdn.net/XuM222222/article/details/92380538</span></span><br><span class="line">            <span class="comment"># .fill_(1),下划线代表直接替换，看链接：https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.fill.html</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.rnn.bidirectional: <span class="comment"># 双向，需要初始化反向的参数</span></span><br><span class="line">                nn.init.orthogonal_(getattr(self.rnn, <span class="string">f'weight_hh_l<span class="subst">&#123;i&#125;</span>_reverse'</span>))</span><br><span class="line">                nn.init.kaiming_normal_(getattr(self.rnn, <span class="string">f'weight_ih_l<span class="subst">&#123;i&#125;</span>_reverse'</span>))</span><br><span class="line">                nn.init.constant_(getattr(self.rnn, <span class="string">f'bias_hh_l<span class="subst">&#123;i&#125;</span>_reverse'</span>), val=<span class="number">0</span>)</span><br><span class="line">                nn.init.constant_(getattr(self.rnn, <span class="string">f'bias_ih_l<span class="subst">&#123;i&#125;</span>_reverse'</span>), val=<span class="number">0</span>)</span><br><span class="line">                getattr(self.rnn, <span class="string">f'bias_hh_l<span class="subst">&#123;i&#125;</span>_reverse'</span>).chunk(<span class="number">4</span>)[<span class="number">1</span>].fill_(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># x是一个元组(c, c_lens)</span></span><br><span class="line">        x, x_len = x</span><br><span class="line">        <span class="comment"># x = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        <span class="comment"># x_len = (batch) 一个batch中所有context或question的样本长度</span></span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 下面一顿操作和第七课机器翻译的一样，</span></span><br><span class="line">        <span class="comment"># 看下这篇博客理解：https://www.cnblogs.com/sbj123456789/p/9834018.html</span></span><br><span class="line">        x_len_sorted, x_idx = torch.sort(x_len, descending=<span class="literal">True</span>)</span><br><span class="line">        x_sorted = x.index_select(dim=<span class="number">0</span>, index=x_idx)</span><br><span class="line">        _, x_ori_idx = torch.sort(x_idx)</span><br><span class="line"></span><br><span class="line">        x_packed = nn.utils.rnn.pack_padded_sequence(x_sorted, x_len_sorted, batch_first=<span class="literal">True</span>)</span><br><span class="line">        x_packed, (h, c) = self.rnn(x_packed)</span><br><span class="line"></span><br><span class="line">        x = nn.utils.rnn.pad_packed_sequence(x_packed, batch_first=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">        x = x.index_select(dim=<span class="number">0</span>, index=x_ori_idx)</span><br><span class="line">        h = h.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>).contiguous().view(<span class="number">-1</span>, h.size(<span class="number">0</span>) * h.size(<span class="number">2</span>)).squeeze()</span><br><span class="line">        h = h.index_select(dim=<span class="number">0</span>, index=x_ori_idx)</span><br><span class="line">        <span class="comment"># x = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        <span class="comment"># h = (1, batch, hidden_size * 2) 这个维度不用管</span></span><br><span class="line">        <span class="keyword">return</span> x, h</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features, dropout=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(Linear, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.linear = nn.Linear(in_features=in_features, out_features=out_features)</span><br><span class="line">        <span class="comment"># in_features = hidden_size * 2</span></span><br><span class="line">        <span class="comment"># out_features = hidden_size * 2</span></span><br><span class="line">        <span class="keyword">if</span> dropout &gt; <span class="number">0</span>:</span><br><span class="line">            self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        self.reset_params()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_params</span><span class="params">(self)</span>:</span></span><br><span class="line">        nn.init.kaiming_normal_(self.linear.weight)</span><br><span class="line">        nn.init.constant_(self.linear.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> hasattr(self, <span class="string">'dropout'</span>): <span class="comment"># 判断self有没有'dropout'这个参数，返回bool值</span></span><br><span class="line">            x = self.dropout(x)</span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args.char_dim</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 看英文论文或这篇博客理解模型：https://blog.csdn.net/u014665013/article/details/79793395</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiDAF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args, pretrained)</span>:</span></span><br><span class="line">        <span class="comment"># pretrained = data.WORD.vocab.vectors = (108777, 100)</span></span><br><span class="line">        super(BiDAF, self).__init__()</span><br><span class="line">        self.args = args</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. Character Embedding Layer 是模型示意图左边的层的名字，从下往上</span></span><br><span class="line">        <span class="comment"># 字符编码层</span></span><br><span class="line">        self.char_emb = nn.Embedding(args.char_vocab_size, args.char_dim, padding_idx=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># args.char_vocab_size = 1307，args.char_dim = 8</span></span><br><span class="line">        nn.init.uniform_(self.char_emb.weight, <span class="number">-0.001</span>, <span class="number">0.001</span>)</span><br><span class="line">        <span class="comment"># 初始化权重</span></span><br><span class="line"></span><br><span class="line">        self.char_conv = nn.Conv2d(<span class="number">1</span>, args.char_channel_size, (args.char_dim, args.char_channel_width))</span><br><span class="line">        <span class="comment"># args.char_channel_size = 100 卷积核数量 </span></span><br><span class="line">        <span class="comment"># (args.char_dim, args.char_channel_width) = (8,5) 过滤器大小</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. Word Embedding Layer</span></span><br><span class="line">        <span class="comment"># 单词编码层</span></span><br><span class="line">        <span class="comment"># initialize word embedding with GloVe</span></span><br><span class="line">        self.word_emb = nn.Embedding.from_pretrained(pretrained, freeze=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 初始化词向量权重，用的Glove向量</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># highway network</span></span><br><span class="line">        <span class="keyword">assert</span> self.args.hidden_size * <span class="number">2</span> == (self.args.char_channel_size + self.args.word_dim)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            setattr(self, <span class="string">f'highway_linear<span class="subst">&#123;i&#125;</span>'</span>,</span><br><span class="line">                    nn.Sequential(Linear(args.hidden_size * <span class="number">2</span>, args.hidden_size * <span class="number">2</span>),</span><br><span class="line">                                  nn.ReLU()))</span><br><span class="line">            <span class="comment"># 设置highway_linear0 = nn.Sequential(Linear(args.hidden_size * 2, args.hidden_size * 2)</span></span><br><span class="line">            <span class="comment"># 设置highway_linear1 = nn.Sequential(Linear(args.hidden_size * 2, args.hidden_size * 2)</span></span><br><span class="line">            <span class="comment"># args.hidden_size = 100</span></span><br><span class="line">                                </span><br><span class="line">            setattr(self, <span class="string">f'highway_gate<span class="subst">&#123;i&#125;</span>'</span>,</span><br><span class="line">                    nn.Sequential(Linear(args.hidden_size * <span class="number">2</span>, args.hidden_size * <span class="number">2</span>),</span><br><span class="line">                                  nn.Sigmoid()))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. Contextual Embedding Layer</span></span><br><span class="line">        <span class="comment"># 上下文，和答案嵌入层，用的LSTM</span></span><br><span class="line">        <span class="comment"># 下面LSTM定位到了自定义的class LSTM(nn.Module)。</span></span><br><span class="line">        self.context_LSTM = LSTM(input_size=args.hidden_size * <span class="number">2</span>,</span><br><span class="line">                                 hidden_size=args.hidden_size,</span><br><span class="line">                                 bidirectional=<span class="literal">True</span>,</span><br><span class="line">                                 batch_first=<span class="literal">True</span>,</span><br><span class="line">                                 dropout=args.dropout) </span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. Attention Flow Layer</span></span><br><span class="line">        <span class="comment"># 注意力层</span></span><br><span class="line">        self.att_weight_c = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.att_weight_q = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.att_weight_cq = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5. Modeling Layer</span></span><br><span class="line">        self.modeling_LSTM1 = LSTM(input_size=args.hidden_size * <span class="number">8</span>,</span><br><span class="line">                                   hidden_size=args.hidden_size,</span><br><span class="line">                                   bidirectional=<span class="literal">True</span>,</span><br><span class="line">                                   batch_first=<span class="literal">True</span>,</span><br><span class="line">                                   dropout=args.dropout)</span><br><span class="line"></span><br><span class="line">        self.modeling_LSTM2 = LSTM(input_size=args.hidden_size * <span class="number">2</span>,</span><br><span class="line">                                   hidden_size=args.hidden_size,</span><br><span class="line">                                   bidirectional=<span class="literal">True</span>,</span><br><span class="line">                                   batch_first=<span class="literal">True</span>,</span><br><span class="line">                                   dropout=args.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 6. Output Layer</span></span><br><span class="line">        self.p1_weight_g = Linear(args.hidden_size * <span class="number">8</span>, <span class="number">1</span>, dropout=args.dropout)</span><br><span class="line">        self.p1_weight_m = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>, dropout=args.dropout)</span><br><span class="line">        self.p2_weight_g = Linear(args.hidden_size * <span class="number">8</span>, <span class="number">1</span>, dropout=args.dropout)</span><br><span class="line">        self.p2_weight_m = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>, dropout=args.dropout)</span><br><span class="line"></span><br><span class="line">        self.output_LSTM = LSTM(input_size=args.hidden_size * <span class="number">2</span>,</span><br><span class="line">                                hidden_size=args.hidden_size,</span><br><span class="line">                                bidirectional=<span class="literal">True</span>,</span><br><span class="line">                                batch_first=<span class="literal">True</span>,</span><br><span class="line">                                dropout=args.dropout)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(p=args.dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, batch)</span>:</span></span><br><span class="line">        <span class="comment"># batch里面有'id','s_idx','e_idx', 'c_word','c_char','q_word', 'q_char'数据</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> More memory-efficient architecture</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">char_emb_layer</span><span class="params">(x)</span>:</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            :param x: (batch, seq_len, word_len)</span></span><br><span class="line"><span class="string">            :return: (batch, seq_len, char_channel_size)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            <span class="comment"># x = (batch_sizes,seq_len,word_len)</span></span><br><span class="line">            batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">            x = self.dropout(self.char_emb(x))</span><br><span class="line">            <span class="comment"># (batch, seq_len, word_len, char_dim)</span></span><br><span class="line">            x = x.view(<span class="number">-1</span>, self.args.char_dim, x.size(<span class="number">2</span>)).unsqueeze(<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># (batch * seq_len, 1, char_dim, word_len) 1是输入的channel的维度</span></span><br><span class="line">            x = self.char_conv(x).squeeze()</span><br><span class="line">            <span class="comment"># (batch * seq_len, char_channel_size, 1, conv_len) -&gt; </span></span><br><span class="line">            <span class="comment"># (batch * seq_len, char_channel_size, conv_len) conv_len不用管，下一步都会pool掉</span></span><br><span class="line">            x = F.max_pool1d(x, x.size(<span class="number">2</span>)).squeeze()</span><br><span class="line">            <span class="comment"># (batch * seq_len, char_channel_size, 1) -&gt; (batch * seq_len, char_channel_size)</span></span><br><span class="line">            x = x.view(batch_size, <span class="number">-1</span>, self.args.char_channel_size)</span><br><span class="line">            <span class="comment"># (batch, seq_len, char_channel_size)</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">highway_network</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            :param x1: (batch, seq_len, char_channel_size)</span></span><br><span class="line"><span class="string">            :param x2: (batch, seq_len, word_dim)</span></span><br><span class="line"><span class="string">            :return: (batch, seq_len, hidden_size * 2)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            </span><br><span class="line">            x = torch.cat([x1, x2], dim=<span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># x = (batch, seq_len, char_channel_size + word_dim)</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">                h = getattr(self, <span class="string">f'highway_linear<span class="subst">&#123;i&#125;</span>'</span>)(x) <span class="comment"># 调用Linear的forward方法</span></span><br><span class="line">                <span class="comment"># h = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">                g = getattr(self, <span class="string">f'highway_gate<span class="subst">&#123;i&#125;</span>'</span>)(x)</span><br><span class="line">                <span class="comment"># g = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">                x = g * h + (<span class="number">1</span> - g) * x</span><br><span class="line">            <span class="comment"># (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">att_flow_layer</span><span class="params">(c, q)</span>:</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            :param c: (batch, c_len, hidden_size * 2)</span></span><br><span class="line"><span class="string">            :param q: (batch, q_len, hidden_size * 2)</span></span><br><span class="line"><span class="string">            :return: (batch, c_len, q_len)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            c_len = c.size(<span class="number">1</span>)</span><br><span class="line">            q_len = q.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (batch, c_len, q_len, hidden_size * 2)</span></span><br><span class="line">            <span class="comment">#c_tiled = c.unsqueeze(2).expand(-1, -1, q_len, -1)</span></span><br><span class="line">            <span class="comment"># (batch, c_len, q_len, hidden_size * 2)</span></span><br><span class="line">            <span class="comment">#q_tiled = q.unsqueeze(1).expand(-1, c_len, -1, -1)</span></span><br><span class="line">            <span class="comment"># (batch, c_len, q_len, hidden_size * 2)</span></span><br><span class="line">            <span class="comment">#cq_tiled = c_tiled * q_tiled</span></span><br><span class="line">            <span class="comment">#cq_tiled = c.unsqueeze(2).expand(-1, -1, q_len, -1) * q.unsqueeze(1).expand(-1, c_len, -1, -1)</span></span><br><span class="line"><span class="comment">#        # 4. Attention Flow Layer</span></span><br><span class="line"><span class="comment">#         # 注意力层</span></span><br><span class="line"><span class="comment">#         self.att_weight_c = Linear(args.hidden_size * 2, 1)</span></span><br><span class="line"><span class="comment">#         self.att_weight_q = Linear(args.hidden_size * 2, 1)</span></span><br><span class="line"><span class="comment">#         self.att_weight_cq = Linear(args.hidden_size * 2, 1)</span></span><br><span class="line">            cq = []</span><br><span class="line">            <span class="comment"># 1、相似度计算方式，看下这篇博客理解：https://blog.csdn.net/u014665013/article/details/79793395</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(q_len):</span><br><span class="line">                qi = q.select(<span class="number">1</span>, i).unsqueeze(<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># (batch, 1, hidden_size * 2)</span></span><br><span class="line">                <span class="comment"># .select看这个：https://blog.csdn.net/hungryof/article/details/51802829</span></span><br><span class="line">                ci = self.att_weight_cq(c * qi).squeeze()</span><br><span class="line">                <span class="comment"># (batch, c_len, 1)</span></span><br><span class="line">                cq.append(ci)</span><br><span class="line">            cq = torch.stack(cq, dim=<span class="number">-1</span>) </span><br><span class="line">            <span class="comment"># (batch, c_len, q_len) cp是共享相似度矩阵</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 2、计算对每一个 context word 而言哪些 query words 和它最相关。</span></span><br><span class="line">            <span class="comment"># context-to-query attention(C2Q):</span></span><br><span class="line">            s = self.att_weight_c(c).expand(<span class="number">-1</span>, <span class="number">-1</span>, q_len) + \</span><br><span class="line">                self.att_weight_q(q).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).expand(<span class="number">-1</span>, c_len, <span class="number">-1</span>) + cq</span><br><span class="line">            <span class="comment"># (batch, c_len, q_len) </span></span><br><span class="line">            a = F.softmax(s, dim=<span class="number">2</span>) </span><br><span class="line">            <span class="comment"># (batch, c_len, q_len)</span></span><br><span class="line">            c2q_att = torch.bmm(a, q) </span><br><span class="line">            <span class="comment"># (batch, c_len, q_len) * (batch, q_len, hidden_size * 2) -&gt; (batch, c_len, hidden_size * 2)</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 3、计算对每一个 query word 而言哪些 context words 和它最相关</span></span><br><span class="line">            <span class="comment"># query-to-context attention(Q2C):</span></span><br><span class="line">            b = F.softmax(torch.max(s, dim=<span class="number">2</span>)[<span class="number">0</span>], dim=<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># (batch, 1, c_len)</span></span><br><span class="line">            q2c_att = torch.bmm(b, c).squeeze()</span><br><span class="line">            <span class="comment"># (batch, 1, c_len) * (batch, c_len, hidden_size * 2) -&gt; (batch, hidden_size * 2)</span></span><br><span class="line">            q2c_att = q2c_att.unsqueeze(<span class="number">1</span>).expand(<span class="number">-1</span>, c_len, <span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># (batch, c_len, hidden_size * 2) (tiled)</span></span><br><span class="line">            <span class="comment"># q2c_att = torch.stack([q2c_att] * c_len, dim=1)</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 4、最后将context embedding和C2Q、Q2C的结果（三个矩阵）拼接起来</span></span><br><span class="line">            x = torch.cat([c, c2q_att, c * c2q_att, c * q2c_att], dim=<span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># (batch, c_len, hidden_size * 8)</span></span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">output_layer</span><span class="params">(g, m, l)</span>:</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            :param g: (batch, c_len, hidden_size * 8)</span></span><br><span class="line"><span class="string">            :param m: (batch, c_len ,hidden_size * 2)</span></span><br><span class="line"><span class="string">             #  l = c_lens</span></span><br><span class="line"><span class="string">            :return: p1: (batch, c_len), p2: (batch, c_len)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            p1 = (self.p1_weight_g(g) + self.p1_weight_m(m)).squeeze()</span><br><span class="line">            <span class="comment"># (batch, c_len)</span></span><br><span class="line">            m2 = self.output_LSTM((m, l))[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># (batch, c_len, hidden_size * 2)</span></span><br><span class="line">            p2 = (self.p2_weight_g(g) + self.p2_weight_m(m2)).squeeze()</span><br><span class="line">            <span class="comment"># (batch, c_len)</span></span><br><span class="line">            <span class="keyword">return</span> p1, p2</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. Character Embedding Layer</span></span><br><span class="line">        <span class="comment"># 令:一个batch中单词数量最多的样本长度为seq_len</span></span><br><span class="line">        <span class="comment"># 令:一个batch中某个单词长度最长的单词长度为word_len</span></span><br><span class="line">        </span><br><span class="line">        c_char = char_emb_layer(batch.c_char) </span><br><span class="line">        <span class="comment"># batch.c_char = (batch,seq_len,word_len) 后两个维度对应context</span></span><br><span class="line">        <span class="comment"># c_char = (batch, seq_len, char_channel_size)</span></span><br><span class="line"></span><br><span class="line">        q_char = char_emb_layer(batch.q_char)</span><br><span class="line">        <span class="comment"># batch.c_char = (batch,seq_len,word_len) 后两个维度对应question</span></span><br><span class="line">        <span class="comment"># c_char = (batch, seq_len, char_channel_size)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. Word Embedding Layer</span></span><br><span class="line">        c_word = self.word_emb(batch.c_word[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># batch.c_word[0] = (batch,seq_len) 后一个维度对应context</span></span><br><span class="line">        <span class="comment"># c_word = (batch, seq_len, word_dim) word_dim是Glove词向量维度</span></span><br><span class="line">        q_word = self.word_emb(batch.q_word[<span class="number">0</span>]) </span><br><span class="line">        <span class="comment"># batch.q_word[0] = (batch,seq_len) 后一个维度对应question</span></span><br><span class="line">        <span class="comment"># q_word = (batch, seq_len, word_dim)</span></span><br><span class="line">        c_lens = batch.c_word[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># c_lens：一个batch中所有context的样本长度</span></span><br><span class="line">        q_lens = batch.q_word[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># q_lens：一个batch中所有question的样本长度</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Highway network</span></span><br><span class="line">        c = highway_network(c_char, c_word)</span><br><span class="line">        <span class="comment"># c = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        q = highway_network(q_char, q_word)</span><br><span class="line">        <span class="comment"># q = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. Contextual Embedding Layer</span></span><br><span class="line">        c = self.context_LSTM((c, c_lens))[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># c = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        q = self.context_LSTM((q, q_lens))[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># q = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 4. Attention Flow Layer</span></span><br><span class="line">        g = att_flow_layer(c, q)</span><br><span class="line">        <span class="comment"># (batch, c_len, hidden_size * 8)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 5. Modeling Layer</span></span><br><span class="line">        m = self.modeling_LSTM2((self.modeling_LSTM1((g, c_lens))[<span class="number">0</span>], c_lens))[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># self.modeling_LSTM1((g, c_lens))[0] = (batch, c_len, hidden_size * 2) # 2因为是双向</span></span><br><span class="line">        <span class="comment"># m = (batch, c_len, hidden_size * 2) 2因为是双向</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 6. Output Layer</span></span><br><span class="line">        p1, p2 = output_layer(g, m, c_lens) <span class="comment"># 预测开始位置和结束位置</span></span><br><span class="line">        <span class="comment"># (batch, c_len), (batch, c_len)</span></span><br><span class="line">        <span class="keyword">return</span> p1, p2</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand((<span class="number">2</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line">print(x)</span><br><span class="line">y = x.select(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(len(data.WORD.vocab)) <span class="comment"># 108777个单词</span></span><br><span class="line">print(data.WORD.vocab.vectors.shape) <span class="comment"># 词向量维度</span></span><br><span class="line"></span><br><span class="line">print(data.WORD.vocab.itos[:<span class="number">50</span>]) <span class="comment"># 前50个词频最高的单词</span></span><br><span class="line">print(<span class="string">"------"</span>*<span class="number">10</span>)</span><br><span class="line">print(list(data.WORD.vocab.stoi.items())[<span class="number">0</span>:<span class="number">50</span>]) <span class="comment"># 对应的索引</span></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(len(data.CHAR.vocab)) <span class="comment"># 1307个单词</span></span><br><span class="line">print(data.CHAR.vocab.itos[:<span class="number">50</span>]) <span class="comment"># 108777个单词</span></span><br><span class="line">print(<span class="string">"------"</span>*<span class="number">10</span>)</span><br><span class="line">print(list(data.CHAR.vocab.stoi.items())[<span class="number">0</span>:<span class="number">50</span>]) <span class="comment"># 对应的索引</span></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">f"cuda:<span class="subst">&#123;args.gpu&#125;</span>"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">model = BiDAF(args, data.WORD.vocab.vectors).to(device)</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EMA</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mu)</span>:</span></span><br><span class="line">        <span class="comment"># mu = args.exp_decay_rate = 0.999</span></span><br><span class="line">        self.mu = mu</span><br><span class="line">        self.shadow = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">register</span><span class="params">(self, name, val)</span>:</span></span><br><span class="line">        <span class="comment"># name:各个参数层的名字, param.data；参数层的数据</span></span><br><span class="line">        self.shadow[name] = val.clone() <span class="comment"># 建立字典</span></span><br><span class="line">        <span class="comment"># clone()得到的Tensor不仅拷贝了原始的value，而且会计算梯度传播信息，copy_()只拷贝数值</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.shadow[name]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, name, x)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> name <span class="keyword">in</span> self.shadow</span><br><span class="line">        new_average = (<span class="number">1.0</span> - self.mu) * x + self.mu * self.shadow[name]</span><br><span class="line">        self.shadow[name] = new_average.clone()</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(model, ema, args, data)</span>:</span></span><br><span class="line">    device = torch.device(<span class="string">f"cuda:<span class="subst">&#123;args.gpu&#125;</span>"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    answers = dict()</span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    backup_params = EMA(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">            backup_params.register(name, param.data) <span class="comment"># 重新建立字典</span></span><br><span class="line">            param.data.copy_(ema.get(name))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.set_grad_enabled(<span class="literal">False</span>):</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> iter(data.dev_iter):</span><br><span class="line">            p1, p2 = model(batch)</span><br><span class="line">            print(p1.shape,p2.shape)</span><br><span class="line">            print(batch.s_idx,batch.e_idx)</span><br><span class="line">            batch_loss = criterion(p1, batch.s_idx<span class="number">-1</span>) + criterion(p2, batch.e_idx<span class="number">-1</span>)</span><br><span class="line">            print(<span class="string">"batch_loss"</span>,batch_loss)</span><br><span class="line">            print(<span class="string">"----"</span>*<span class="number">40</span>)</span><br><span class="line">            loss += batch_loss.item()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (batch, c_len, c_len)</span></span><br><span class="line">            batch_size, c_len = p1.size()</span><br><span class="line">            ls = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">            mask = (torch.ones(c_len, c_len) * float(<span class="string">'-inf'</span>)).to(device).tril(<span class="number">-1</span>).unsqueeze(<span class="number">0</span>).expand(batch_size, <span class="number">-1</span>, <span class="number">-1</span>)</span><br><span class="line">            score = (ls(p1).unsqueeze(<span class="number">2</span>) + ls(p2).unsqueeze(<span class="number">1</span>)) + mask</span><br><span class="line">            score, s_idx = score.max(dim=<span class="number">1</span>)</span><br><span class="line">            score, e_idx = score.max(dim=<span class="number">1</span>)</span><br><span class="line">            s_idx = torch.gather(s_idx, <span class="number">1</span>, e_idx.view(<span class="number">-1</span>, <span class="number">1</span>)).squeeze()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">                id = batch.id[i]</span><br><span class="line">                answer = batch.c_word[<span class="number">0</span>][i][s_idx[i]:e_idx[i]+<span class="number">1</span>]</span><br><span class="line">                answer = <span class="string">' '</span>.join([data.WORD.vocab.itos[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> answer])</span><br><span class="line">                answers[id] = answer</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                param.data.copy_(backup_params.get(name))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(args.prediction_file, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        print(json.dumps(answers), file=f)</span><br><span class="line"></span><br><span class="line">    results = evaluate.main(args)</span><br><span class="line">    <span class="keyword">return</span> loss, results[<span class="string">'exact_match'</span>], results[<span class="string">'f1'</span>]</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    print(name)</span><br><span class="line">    print(param.requires_grad)</span><br><span class="line">    print(param.data.shape)</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">strftime(&apos;%H:%M:%S&apos;, gmtime())</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">iterator = data.train_iter</span><br><span class="line">n= <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    print(<span class="string">"j="</span>,j)</span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(iterator):</span><br><span class="line">        print(<span class="string">"当前epoch"</span>,int(iterator.epoch))</span><br><span class="line">        print(<span class="string">"-----"</span>*<span class="number">10</span>)</span><br><span class="line">        print(i)</span><br><span class="line">        print(batch)</span><br><span class="line">        n+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> n&gt;<span class="number">3</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(args, data)</span>:</span></span><br><span class="line">    device = torch.device(<span class="string">f"cuda:<span class="subst">&#123;args.gpu&#125;</span>"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">    model = BiDAF(args, data.WORD.vocab.vectors).to(device) <span class="comment"># 定义主模型类实例</span></span><br><span class="line"></span><br><span class="line">    ema = EMA(args.exp_decay_rate) <span class="comment"># args.exp_decay_rate = 0.999</span></span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters(): </span><br><span class="line">        <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">            ema.register(name, param.data) <span class="comment"># 参数名字和对应的参数数据形成字典</span></span><br><span class="line">    parameters = filter(<span class="keyword">lambda</span> p: p.requires_grad, model.parameters())</span><br><span class="line">    <span class="comment"># p.requires_grad = True or False 保留有梯度的参数</span></span><br><span class="line">    optimizer = optim.Adadelta(parameters, lr=args.learning_rate)</span><br><span class="line">    <span class="comment"># args.learning_rate = 0.5,优化器选用Adadelta</span></span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    <span class="comment"># 交叉熵损失</span></span><br><span class="line"></span><br><span class="line">    writer = SummaryWriter(log_dir=<span class="string">'runs/'</span> + args.model_time)</span><br><span class="line">    <span class="comment"># args.model_time = strftime('%H:%M:%S', gmtime()) 文件夹命名为写入文件的当地时间</span></span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line">    loss, last_epoch = <span class="number">0</span>, <span class="number">-1</span></span><br><span class="line">    max_dev_exact, max_dev_f1 = <span class="number">-1</span>, <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    iterator = data.train_iter</span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(iterator):</span><br><span class="line">        present_epoch = int(iterator.epoch) </span><br><span class="line">        <span class="comment">#print("当前epoch",present_epoch)# 这个我打印了下，一直是0，觉得有问题</span></span><br><span class="line">        <span class="keyword">if</span> present_epoch == args.epoch:</span><br><span class="line">            <span class="comment"># args.epoch=12</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> present_epoch &gt; last_epoch:</span><br><span class="line">            print(<span class="string">'epoch:'</span>, present_epoch + <span class="number">1</span>)</span><br><span class="line">        last_epoch = present_epoch</span><br><span class="line"></span><br><span class="line">        p1, p2 = model(batch)</span><br><span class="line">        <span class="comment"># (batch, c_len), (batch, c_len)</span></span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        batch_loss = criterion(p1, batch.s_idx) + criterion(p2, batch.e_idx)</span><br><span class="line">        <span class="comment"># 最后的目标函数：batch.s_idx是答案开始的位置，batch.e_idx是答案结束的位置</span></span><br><span class="line">        loss += batch_loss.item()</span><br><span class="line">        batch_loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                ema.update(name, param.data) <span class="comment"># 更新训练完后的的参数数据</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % args.print_freq == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"i"</span>,i)</span><br><span class="line">            dev_loss, dev_exact, dev_f1 = test(model, ema, args, data)</span><br><span class="line">            c = (i + <span class="number">1</span>) // args.print_freq</span><br><span class="line"></span><br><span class="line">            writer.add_scalar(<span class="string">'loss/train'</span>, loss, c)</span><br><span class="line">            writer.add_scalar(<span class="string">'loss/dev'</span>, dev_loss, c)</span><br><span class="line">            writer.add_scalar(<span class="string">'exact_match/dev'</span>, dev_exact, c)</span><br><span class="line">            writer.add_scalar(<span class="string">'f1/dev'</span>, dev_f1, c)</span><br><span class="line">            print(<span class="string">f'train loss: <span class="subst">&#123;loss:<span class="number">.3</span>f&#125;</span> / dev loss: <span class="subst">&#123;dev_loss:<span class="number">.3</span>f&#125;</span>'</span></span><br><span class="line">                  <span class="string">f' / dev EM: <span class="subst">&#123;dev_exact:<span class="number">.3</span>f&#125;</span> / dev F1: <span class="subst">&#123;dev_f1:<span class="number">.3</span>f&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> dev_f1 &gt; max_dev_f1:</span><br><span class="line">                max_dev_f1 = dev_f1</span><br><span class="line">                max_dev_exact = dev_exact</span><br><span class="line">                best_model = copy.deepcopy(model)</span><br><span class="line"></span><br><span class="line">            loss = <span class="number">0</span></span><br><span class="line">            model.train()</span><br><span class="line"></span><br><span class="line">    writer.close()</span><br><span class="line"><span class="comment">#     print(f'max dev EM: &#123;max_dev_exact:.3f&#125; / max dev F1: &#123;max_dev_f1:.3f&#125;')</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> best_model</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(&apos;training start!&apos;)</span><br><span class="line">best_model = train(args, data)</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/pytorch/pytorch/issues/4144</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQuAD-BiDAF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP中的ConvNet</title>
      <link href="/2020/04/15/NLP%E4%B8%AD%E7%9A%84ConvNet/"/>
      <url>/2020/04/15/NLP%E4%B8%AD%E7%9A%84ConvNet/</url>
      
        <content type="html"><![CDATA[<p>​        NLP/AI是近几年来飞速发展的领域，很多的模型和算法只能在论文、讲义和博客中找到，而不会出现在任何的教科书中。凡是课程中提到的论文，大家都能够阅读一遍。对于重要的论文（我会特别标明或者在课上强调，例如BERT, transformer等），建议认真阅读，搞清楚模型的细节。其余的论文，建议至少能够阅读，了解论文的创新点和中心思想。</p><h3 id="如何读论文？"><a href="#如何读论文？" class="headerlink" title="如何读论文？"></a>如何读论文？</h3><p>对于如何读论文，每个人有自己不同的方法。我的建议是：</p><ul><li><p>最快读论文的方法：上各大中文网站（知乎，CSDN，微信公众号等）寻找该论文的中文解读，大部分有名的论文都会有很多的解读文章。</p></li><li><p>读论文时候的重点章节：大部分NLP的论文的主要两个章节是，Model, Experiments。基本上看完这两个章节就了解了论文的核心思想。另外我也会特别关注论文使用的<strong>数据</strong>，因为这些数据我们可能可以拿来用在自己的项目上。</p></li><li><p>如果想要更加深入地学习该论文的内容，可以上网去寻找与该论文相关的资料，包括作者的个人主页，他/她发布的论文slides，论文代码等等。顺便说一下，如果你想要复现论文的结果，但是在网上找不到代码，不要急于自己实现，可以写邮件给论文的第一作者与通讯作者（最后一位），礼貌地询问对方是否可以将源码和数据提供给你，理论上论文作者有义务公开自己的代码和数据。如果没有代码可以公开，要不然可能是论文太新，还没有公开代码，要不然可能是论文中某些部分的实现有困难，不那么容易复现。</p></li><li><p>另外如果你想要更深入地学习这个论文相关的领域，可以读一下Related Work中提到的一些文章。</p></li></ul><h1 id="NLP中的-ConvNet-精选论文"><a href="#NLP中的-ConvNet-精选论文" class="headerlink" title="NLP中的 ConvNet 精选论文"></a>NLP中的 ConvNet 精选论文</h1><p>MNIST</p><p>convolutional kernel: local feature detector</p><p>图像：</p><ul><li><p>平移不变性</p></li><li><p>pixel features</p></li></ul><p>Hinton</p><ul><li><p>Capsule Network</p></li><li><p>ConvNet的缺陷：</p></li><li><p>没有处理旋转不变性</p></li><li><p>图片大小发生改变</p></li></ul><p>文本</p><ul><li><p>ngram</p></li><li><p>ngram 之间的联系 n-n-gram</p></li></ul><p>曾经有一段时间由于<strong>Yann Lecun</strong>加入Facebook AI Research担任Director的关系，FB投入了很多的精力研发把ConvNet用在Text问题上。ConvNet主打的一个强项就是速度比RNN快，Encoder可以并行。后来可能是由于Google的Transformer开始统治这个领域，导致大家慢慢在ConvNet上的关注度越来越小。</p><p>transformer (BERT) 就是 filter size 为 1 的 convolutional neural network 。</p><p>不过这一系列以ConvNet为核心的NLP模型依然非常值得学习。ConvNet的一个长处在于它可以很自然地得到 <strong>ngram</strong> 的表示。由于NLP最近的进展日新月异，可能几天或者几个月之后又有一系列基于ConvNet的模型重登SOTA，谁知道呢。</p><p>对于不了解什么是Convolutional Neural Network的同学，建议阅读斯坦福cs231的课程资料 <a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="noopener">http://cs231n.github.io/convolutional-networks/</a> 网上的中文翻译很多，例如：<a href="https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit</a></p><h2 id="Yoon-Kim-Convolutional-Neural-Networks-for-Sentence-Classification"><a href="#Yoon-Kim-Convolutional-Neural-Networks-for-Sentence-Classification" class="headerlink" title="Yoon Kim Convolutional Neural Networks for Sentence Classification"></a>Yoon Kim <a href="https://aclweb.org/anthology/D14-1181" target="_blank" rel="noopener">Convolutional Neural Networks for Sentence Classification</a></h2><p><a href="https://aclweb.org/anthology/D14-1181" target="_blank" rel="noopener">https://aclweb.org/anthology/D14-1181</a></p><p>这篇文章首次提出了在text上使用convolutional network，并且取得了不错的效果。后续很多把ConvNet用在NLP任务上都是基于这篇论文的模型改进。</p><h3 id="模型架构图"><a href="#模型架构图" class="headerlink" title="模型架构图"></a>模型架构图</h3><p><img src="https://uploader.shimo.im/f/bAD5TU2kjCQipLid.png!thumbnail" alt="img"></p><h3 id="embedding层"><a href="#embedding层" class="headerlink" title="embedding层"></a>embedding层</h3><p><img src="https://uploader.shimo.im/f/pOmG3eS8ntYi0dSQ.png!thumbnail" alt="img"></p><h3 id="convolution层"><a href="#convolution层" class="headerlink" title="convolution层"></a>convolution层</h3><p><img src="https://uploader.shimo.im/f/MlEd8ePXgDs7gaLg.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/T3pionftmyImwzPH.png!thumbnail" alt="img"></p><h3 id="Max-over-time-pooling"><a href="#Max-over-time-pooling" class="headerlink" title="Max over time pooling"></a>Max over time pooling</h3><p><img src="https://uploader.shimo.im/f/cM7DZvGSt3gNz8uL.png!thumbnail" alt="img"></p><h3 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h3><p>一个affine transformation加上dropout</p><p><img src="https://uploader.shimo.im/f/5ZrPtIuh6X4P9lQJ.png!thumbnail" alt="img"></p><h3 id="模型的效果"><a href="#模型的效果" class="headerlink" title="模型的效果"></a>模型的效果</h3><p>可以媲美当时的众多传统模型。从今天的眼光来看这个模型的思路还是挺简单的，不过当时大家开始探索把CNN用到text问题上的时候，这一系列模型架构的想法还是很新颖的。</p><p><img src="https://uploader.shimo.im/f/BxnIovJf5Pwv8RHZ.png!thumbnail" alt="img"></p><h3 id="我们的代码实现"><a href="#我们的代码实现" class="headerlink" title="我们的代码实现"></a>我们的代码实现</h3><p>用ConvNet做文本分类的部分代码。有些部分可能的实现可能和模型有一定出入，不过我的模型实现效果也很不错，仅供参考。</p><p><a href="https://github.com/ZeweiChu/PyTorch-Course/blob/master/notebooks/4.sentiment_with_mask.ipynb" target="_blank" rel="noopener">https://github.com/ZeweiChu/PyTorch-Course/blob/master/notebooks/4.sentiment_with_mask.ipynb</a></p><p>感兴趣的同学可以参考更多Yoon Kim的工作</p><p><a href="http://www.people.fas.harvard.edu/~yoonkim/" target="_blank" rel="noopener">http://www.people.fas.harvard.edu/~yoonkim/</a></p><p>Yoon Kim的导师Alex Rush</p><p><a href="http://nlp.seas.harvard.edu/rush.html" target="_blank" rel="noopener">http://nlp.seas.harvard.edu/rush.html</a></p><p>他们的一项工作OpenNMT-py</p><p><a href="https://github.com/OpenNMT/OpenNMT-py" target="_blank" rel="noopener">https://github.com/OpenNMT/OpenNMT-py</a></p><p>Alex Rush的一些优秀学生</p><p>Sam Wiseman <a href="https://swiseman.github.io/" target="_blank" rel="noopener">https://swiseman.github.io/</a> 他做了很多VAE的工作</p><h2 id="Zhang-et-al-Character-level-Convolutional-Networks-for-Text-Classification"><a href="#Zhang-et-al-Character-level-Convolutional-Networks-for-Text-Classification" class="headerlink" title="Zhang et. al., Character-level Convolutional Networks for Text Classification "></a>Zhang et. al., <a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf" target="_blank" rel="noopener">Character-level Convolutional Networks for Text Classification </a></h2><p><a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf</a></p><p>这篇文章在char层面上使用ConvNet，当时在分类任务上取得了SOTA的效果。后来人们经常把这套方法用来做单词表示的学习，例如ELMo就是用CharCNN来encode单词的。</p><h3 id="关键Modules"><a href="#关键Modules" class="headerlink" title="关键Modules"></a>关键Modules</h3><p>Convolutional Module</p><p><img src="https://uploader.shimo.im/f/24t3sOep2m8g4ls6.png!thumbnail" alt="img"></p><p>k是kernel size。</p><p>max pooling</p><p><img src="https://uploader.shimo.im/f/NzpvIEElx3UVKJyo.png!thumbnail" alt="img"></p><h3 id="模型架构图-1"><a href="#模型架构图-1" class="headerlink" title="模型架构图"></a>模型架构图</h3><h2 id><a href="#" class="headerlink" title></a><img src="https://uploader.shimo.im/f/WDfJgndz6HMQ5CTF.png!thumbnail" alt="img"></h2><p>在ELMo上的character embedding</p><p><img src="https://uploader.shimo.im/f/3aLnMpCpyUUQSGcQ.png!thumbnail" alt="img"></p><h3 id="模型代码"><a href="#模型代码" class="headerlink" title="模型代码"></a>模型代码</h3><p><a href="https://github.com/srviest/char-cnn-text-classification-pytorch/blob/master/model.py" target="_blank" rel="noopener">https://github.com/srviest/char-cnn-text-classification-pytorch/blob/master/model.py</a></p><h2 id="Gehring-et-al-Convolutional-Sequence-to-Sequence-Learning"><a href="#Gehring-et-al-Convolutional-Sequence-to-Sequence-Learning" class="headerlink" title="Gehring et. al., Convolutional Sequence to Sequence Learning"></a>Gehring et. al., <a href="https://arxiv.org/pdf/1705.03122.pdf" target="_blank" rel="noopener">Convolutional Sequence to Sequence Learning</a></h2><p><a href="https://arxiv.org/pdf/1705.03122.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1705.03122.pdf</a></p><p>参考博客资料</p><p><a href="https://ycts.github.io/weeklypapers/convSeq2seq/" target="_blank" rel="noopener">https://ycts.github.io/weeklypapers/convSeq2seq/</a></p><p>用ConvNet做Seq2Seq模型，其实这篇文章中有很多Transformer的影子，并且模型效果也很好。可能由于同时期的Transformer光芒过于耀眼，掩盖了这一篇同样非常重量级的文章。</p><p>我的建议是，这篇文章可以简要阅读，了解ConvNet可以怎么样被运用到Text Modeling问题上。由于现在学术界和工业界的主流是各种Transformer模型的变种，且Transformer的模型相对更简洁易懂，所以建议同学们在后面花更多的时间在Transformer上。最近很多NLP的面试都会问到一些与Transformer和BERT相关的问题，可能很多人不太了解这篇Conv Seq2Seq的论文。</p><h3 id="Positional-Embedddings"><a href="#Positional-Embedddings" class="headerlink" title="Positional Embedddings"></a>Positional Embedddings</h3><p><img src="https://uploader.shimo.im/f/3s1XVoEyWCA2091O.png!thumbnail" alt="img"></p><p>对每个单词分别做word embedding w_i和positional embedding p_i，然后单词的embedding的w_i + p_i。p_i是模型的参数，在训练中会被更新。</p><p>如果没有positional embedding，CNN是无法知晓单词的位置信息的。因为不同于LSTM，如果没有postional embedding，在CNN encoder中的单词位置其实没有区别。</p><h3 id="Convolutional-Block-Structure"><a href="#Convolutional-Block-Structure" class="headerlink" title="Convolutional Block Structure"></a>Convolutional Block Structure</h3><p>Encoder和Decoder第l层的输入</p><p><img src="https://uploader.shimo.im/f/8RwAjCP390sIoG1A.png!thumbnail" alt="img"></p><p>每一层都包含一个一维Convolution，以及一个non-linearity单元，其中conv block/layer的kernel宽度为k，其output包含k个输入元素的信息。参数为</p><p><img src="https://uploader.shimo.im/f/8tCfiuW0gHgwBKAi.png!thumbnail" alt="img"></p><p>输出为</p><p><img src="https://uploader.shimo.im/f/kDZ1ulm65h8m0dOX.png!thumbnail" alt="img"></p><p>然后使用一个Gated Linear Units作为non-linearity。</p><p><img src="https://uploader.shimo.im/f/MuogKhR9b48cABpI.png!thumbnail" alt="img"></p><p>encoder和decoder都有好多层，每一层都加上了residual connection。</p><p><img src="https://uploader.shimo.im/f/EdLJ369IWcQgqx0x.png!thumbnail" alt="img"></p><p>我们在encoder每一层的左右两边都添加padding，这样可以保证每一层经过convolution之后输出的长度和原来一样。decoder和encoder稍有不同，因为我们必须保证我们在decoder一个位置的单词的时候没有看到这个位置后面的单词。所以我们的做法是，在decoder每一层左右两边都加上k-1个padding，做完conv之后把右边的k个单位移除。</p><p>最后的一个标准套路是把hidden state做个affine transformation，然后Softmax变成单词表上的一个概率分布。</p><p><img src="https://uploader.shimo.im/f/yFXfmhqvzzcU8D7D.png!thumbnail" alt="img"></p><h3 id="Multi-step-Attention"><a href="#Multi-step-Attention" class="headerlink" title="Multi-step Attention"></a>Multi-step Attention</h3><p>Decoder的每一层都有单独的Attention。</p><p><img src="https://uploader.shimo.im/f/uQLvQ1erpmAJfLlP.png!thumbnail" alt="img"></p><p>g_i是当前单词的embedding，</p><p><img src="https://uploader.shimo.im/f/eHzdNjYIZBM93TQN.png!thumbnail" alt="img"></p><p>然后我们用这个新造的 d_i^l 对 encoder 的每个位置做attention。</p><p><img src="https://uploader.shimo.im/f/er1A7CMeiukGczjw.png!thumbnail" alt="img"></p><p>然后非常常规的，用attention score对encoder hidden states做加权平均。唯一不同的是，这里还直接加上了输入的embedding。</p><p><img src="https://uploader.shimo.im/f/BJXPoQQi9Xg4fJJa.png!thumbnail" alt="img"></p><p>作者说他们发现直接加上这个词向量的embedding还是很有用的。</p><h3 id="模型架构图-2"><a href="#模型架构图-2" class="headerlink" title="模型架构图"></a>模型架构图</h3><p><img src="https://uploader.shimo.im/f/r5pAK5SQWzQDyDFL.png!thumbnail" alt="img"></p><h3 id="Normalization策略"><a href="#Normalization策略" class="headerlink" title="Normalization策略"></a>Normalization策略</h3><p>为了保持模型训练的稳定性，我们希望模型中间的向量的variance不要太大。</p><ul><li>输出+residual之后乘以\sqrt{5}，这样可以让这些vector每个维度的variance减半。其实很多时候这些确保模型稳定度的细节挺关键的，大家可能也知道transformer中也增加了一些减少variance的方法。如果不是调模型专家就会忽视这些细节，然后模型就训练不好了。</li></ul><p><img src="https://uploader.shimo.im/f/IMLzC3rtvxkqLmuo.png!thumbnail" alt="img"></p><p>还有更多的模型参数初始化细节，感兴趣的同学可以自己去认真阅读paper。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="https://uploader.shimo.im/f/aNGZg3EjGyw1aWu7.png!thumbnail" alt="img"></p><p>在翻译任务上超越了GNMT (Google Neural Machine Translation)，其实这个比较能说明问题，因为当时的GNMT是State of the Art。</p><p><img src="https://uploader.shimo.im/f/VMfU3If3biw5286r.png!thumbnail" alt="img"></p><p>然后他们还展示了ConvS2S的速度比GNMT更快。</p><p>总结来说，ConvS2S其实是一篇很有价值的文章，Decoder的设计比较精致， 不知道这篇文章对后来的Transformer产生了多少的影响，当然他们可以说是同时期的作品。</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>主要代码在Fairseq的下面这个文件中</p><p><a href="https://github.com/ZeweiChu/fairseq/blob/master/fairseq/models/fconv.py" target="_blank" rel="noopener">https://github.com/ZeweiChu/fairseq/blob/master/fairseq/models/fconv.py</a></p><p>Fairseq是一个值得关注一波的工具包，由Facebook开发，主要开发者有 </p><ul><li>Myle Ott <a href="https://myleott.com/" target="_blank" rel="noopener">https://myleott.com/</a></li></ul><h1 id="关于文本分类的更多参考资料"><a href="#关于文本分类的更多参考资料" class="headerlink" title="关于文本分类的更多参考资料"></a>关于文本分类的更多参考资料</h1><p>基于深度学习的文本分类</p><p><a href="https://zhuanlan.zhihu.com/p/34212945" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34212945</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ConvNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>seq2seq</title>
      <link href="/2020/04/13/seq2seq/"/>
      <url>/2020/04/13/seq2seq/</url>
      
        <content type="html"><![CDATA[<h1 id="Seq2Seq-Attention"><a href="#Seq2Seq-Attention" class="headerlink" title="Seq2Seq, Attention"></a>Seq2Seq, Attention</h1><p>在这份notebook当中，我们会(尽可能)复现Luong的attention模型</p><p>由于我们的数据集非常小，只有一万多个句子的训练数据，所以训练出来的模型效果并不好。如果大家想训练一个好一点的模型，可以参考下面的资料。</p><h2 id="更多阅读"><a href="#更多阅读" class="headerlink" title="更多阅读"></a>更多阅读</h2><h4 id="课件"><a href="#课件" class="headerlink" title="课件"></a>课件</h4><ul><li><a href="http://cs224d.stanford.edu/lectures/CS224d-Lecture15.pdf" target="_blank" rel="noopener">cs224d</a></li></ul><h4 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h4><ul><li><a href="https://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></li><li><a href="https://arxiv.org/abs/1508.04025?context=cs" target="_blank" rel="noopener">Effective Approaches to Attention-based Neural Machine Translation</a></li><li><a href="https://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a></li></ul><h4 id="PyTorch代码"><a href="#PyTorch代码" class="headerlink" title="PyTorch代码"></a>PyTorch代码</h4><ul><li><a href="https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb" target="_blank" rel="noopener">seq2seq-tutorial</a></li><li><a href="https://github.com/bentrevett/pytorch-seq2seq" target="_blank" rel="noopener">Tutorial from Ben Trevett</a></li><li><a href="https://github.com/IBM/pytorch-seq2seq" target="_blank" rel="noopener">IBM seq2seq</a></li><li><a href="https://github.com/OpenNMT/OpenNMT-py" target="_blank" rel="noopener">OpenNMT-py</a></li></ul><h4 id="更多关于Machine-Translation"><a href="#更多关于Machine-Translation" class="headerlink" title="更多关于Machine Translation"></a>更多关于Machine Translation</h4><ul><li><a href="https://www.coursera.org/lecture/nlp-sequence-models/beam-search-4EtHZ" target="_blank" rel="noopener">Beam Search</a></li><li>Pointer network 文本摘要</li><li>Copy Mechanism 文本摘要</li><li>Converage Loss </li><li>ConvSeq2Seq</li><li>Transformer</li><li>Tensor2Tensor</li></ul><h4 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h4><ul><li>建议同学尝试对中文进行分词</li></ul><h4 id="NER"><a href="#NER" class="headerlink" title="NER"></a>NER</h4><ul><li><a href="https://github.com/allenai/allennlp/tree/master/allennlp" target="_blank" rel="noopener">https://github.com/allenai/allennlp/tree/master/allennlp</a></li></ul><p>In [137]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter <span class="comment">#计数器</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> nltk</span><br></pre></td></tr></table></figure><p>读入中英文数据</p><ul><li>英文我们使用nltk的word tokenizer来分词，并且使用小写字母</li><li>中文我们直接使用单个汉字作为基本单元</li></ul><p>In [138]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(in_file)</span>:</span></span><br><span class="line">    cn = []</span><br><span class="line">    en = []</span><br><span class="line">    num_examples = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> open(in_file, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            <span class="comment">#print(line) #Anyone can do that.任何人都可以做到。</span></span><br><span class="line">            line = line.strip().split(<span class="string">"\t"</span>) <span class="comment">#分词后用逗号隔开</span></span><br><span class="line">            <span class="comment">#print(line) #['Anyone can do that.', '任何人都可以做到。']</span></span><br><span class="line">            en.append([<span class="string">"BOS"</span>] + nltk.word_tokenize(line[<span class="number">0</span>].lower()) + [<span class="string">"EOS"</span>])</span><br><span class="line">            <span class="comment">#BOS:beginning of sequence EOS:end of</span></span><br><span class="line">            <span class="comment"># split chinese sentence into characters</span></span><br><span class="line">            cn.append([<span class="string">"BOS"</span>] + [c <span class="keyword">for</span> c <span class="keyword">in</span> line[<span class="number">1</span>]] + [<span class="string">"EOS"</span>])</span><br><span class="line">            <span class="comment">#中文一个一个字分词，可以尝试用分词器分词</span></span><br><span class="line">    <span class="keyword">return</span> en, cn</span><br><span class="line"></span><br><span class="line">train_file = <span class="string">"nmt/en-cn/train.txt"</span></span><br><span class="line">dev_file = <span class="string">"nmt/en-cn/dev.txt"</span></span><br><span class="line">train_en, train_cn = load_data(train_file)</span><br><span class="line">dev_en, dev_cn = load_data(dev_file)</span><br></pre></td></tr></table></figure><p>In [0]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(train_en[:10])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[&apos;BOS&apos;, &apos;anyone&apos;, &apos;can&apos;, &apos;do&apos;, &apos;that&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;how&apos;, &apos;about&apos;, &apos;another&apos;, &apos;piece&apos;, &apos;of&apos;, &apos;cake&apos;, &apos;?&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;she&apos;, &apos;married&apos;, &apos;him&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;i&apos;, &apos;do&apos;, &quot;n&apos;t&quot;, &apos;like&apos;, &apos;learning&apos;, &apos;irregular&apos;, &apos;verbs&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;it&apos;, &quot;&apos;s&quot;, &apos;a&apos;, &apos;whole&apos;, &apos;new&apos;, &apos;ball&apos;, &apos;game&apos;, &apos;for&apos;, &apos;me&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;he&apos;, &quot;&apos;s&quot;, &apos;sleeping&apos;, &apos;like&apos;, &apos;a&apos;, &apos;baby&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;he&apos;, &apos;can&apos;, &apos;play&apos;, &apos;both&apos;, &apos;tennis&apos;, &apos;and&apos;, &apos;baseball&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;we&apos;, &apos;should&apos;, &apos;cancel&apos;, &apos;the&apos;, &apos;hike&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;he&apos;, &apos;is&apos;, &apos;good&apos;, &apos;at&apos;, &apos;dealing&apos;, &apos;with&apos;, &apos;children&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;she&apos;, &apos;will&apos;, &apos;do&apos;, &apos;her&apos;, &apos;best&apos;, &apos;to&apos;, &apos;be&apos;, &apos;here&apos;, &apos;on&apos;, &apos;time&apos;, &apos;.&apos;, &apos;EOS&apos;]]</span><br></pre></td></tr></table></figure><p>In [0]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(train_cn[:10])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[&apos;BOS&apos;, &apos;任&apos;, &apos;何&apos;, &apos;人&apos;, &apos;都&apos;, &apos;可&apos;, &apos;以&apos;, &apos;做&apos;, &apos;到&apos;, &apos;。&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;要&apos;, &apos;不&apos;, &apos;要&apos;, &apos;再&apos;, &apos;來&apos;, &apos;一&apos;, &apos;塊&apos;, &apos;蛋&apos;, &apos;糕&apos;, &apos;？&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;她&apos;, &apos;嫁&apos;, &apos;给&apos;, &apos;了&apos;, &apos;他&apos;, &apos;。&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;我&apos;, &apos;不&apos;, &apos;喜&apos;, &apos;欢&apos;, &apos;学&apos;, &apos;习&apos;, &apos;不&apos;, &apos;规&apos;, &apos;则&apos;, &apos;动&apos;, &apos;词&apos;, &apos;。&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;這&apos;, &apos;對&apos;, &apos;我&apos;, &apos;來&apos;, &apos;說&apos;, &apos;是&apos;, &apos;個&apos;, &apos;全&apos;, &apos;新&apos;, &apos;的&apos;, &apos;球&apos;, &apos;類&apos;, &apos;遊&apos;, &apos;戲&apos;, &apos;。&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;他&apos;, &apos;正&apos;, &apos;睡&apos;, &apos;着&apos;, &apos;，&apos;, &apos;像&apos;, &apos;个&apos;, &apos;婴&apos;, &apos;儿&apos;, &apos;一&apos;, &apos;样&apos;, &apos;。&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;他&apos;, &apos;既&apos;, &apos;会&apos;, &apos;打&apos;, &apos;网&apos;, &apos;球&apos;, &apos;，&apos;, &apos;又&apos;, &apos;会&apos;, &apos;打&apos;, &apos;棒&apos;, &apos;球&apos;, &apos;。&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;我&apos;, &apos;們&apos;, &apos;應&apos;, &apos;該&apos;, &apos;取&apos;, &apos;消&apos;, &apos;這&apos;, &apos;次&apos;, &apos;遠&apos;, &apos;足&apos;, &apos;。&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;他&apos;, &apos;擅&apos;, &apos;長&apos;, &apos;應&apos;, &apos;付&apos;, &apos;小&apos;, &apos;孩&apos;, &apos;子&apos;, &apos;。&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;她&apos;, &apos;会&apos;, &apos;尽&apos;, &apos;量&apos;, &apos;按&apos;, &apos;时&apos;, &apos;赶&apos;, &apos;来&apos;, &apos;的&apos;, &apos;。&apos;, &apos;EOS&apos;]]</span><br></pre></td></tr></table></figure><p>In [0]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>构建单词表</p><p>In [139]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">UNK_IDX = <span class="number">0</span></span><br><span class="line">PAD_IDX = <span class="number">1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dict</span><span class="params">(sentences, max_words=<span class="number">50000</span>)</span>:</span></span><br><span class="line">    word_count = Counter()</span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> sentence:</span><br><span class="line">            word_count[s] += <span class="number">1</span>  <span class="comment">#word_count这里应该是个字典</span></span><br><span class="line">    ls = word_count.most_common(max_words) </span><br><span class="line">    <span class="comment">#按每个单词数量排序前50000个,这个数字自己定的，不重复单词数没有50000</span></span><br><span class="line">    print(len(ls)) <span class="comment">#train_en：5491</span></span><br><span class="line">    total_words = len(ls) + <span class="number">2</span></span><br><span class="line">    <span class="comment">#加的2是留给"unk"和"pad"</span></span><br><span class="line">    <span class="comment">#ls = [('BOS', 14533), ('EOS', 14533), ('.', 12521), ('i', 4045), .......</span></span><br><span class="line">    word_dict = &#123;w[<span class="number">0</span>]: index+<span class="number">2</span> <span class="keyword">for</span> index, w <span class="keyword">in</span> enumerate(ls)&#125;</span><br><span class="line">    <span class="comment">#加的2是留给"unk"和"pad",转换成字典格式。</span></span><br><span class="line">    word_dict[<span class="string">"UNK"</span>] = UNK_IDX</span><br><span class="line">    word_dict[<span class="string">"PAD"</span>] = PAD_IDX</span><br><span class="line">    <span class="keyword">return</span> word_dict, total_words</span><br><span class="line"></span><br><span class="line">en_dict, en_total_words = build_dict(train_en)</span><br><span class="line">cn_dict, cn_total_words = build_dict(train_cn)</span><br><span class="line">inv_en_dict = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> en_dict.items()&#125;</span><br><span class="line"><span class="comment">#en_dict.items()把字典转换成可迭代对象，取出键值，并调换键值的位置。</span></span><br><span class="line">inv_cn_dict = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> cn_dict.items()&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">5491</span><br><span class="line">3193</span><br></pre></td></tr></table></figure><p>In [1]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># print(en_dict)</span><br><span class="line"># print(en_total_words)</span><br></pre></td></tr></table></figure><p>In [3]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(cn_dict)</span><br><span class="line">print(cn_total_words)</span><br></pre></td></tr></table></figure><p>In [4]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(inv_en_dict)</span><br></pre></td></tr></table></figure><p>In [5]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(inv_cn_dict)</span><br></pre></td></tr></table></figure><p>把单词全部转变成数字</p><p>In [140]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        Encode the sequences. </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    length = len(en_sentences)</span><br><span class="line">    <span class="comment">#en_sentences=[['BOS', 'anyone', 'can', 'do', 'that', '.', 'EOS'],....</span></span><br><span class="line">    </span><br><span class="line">    out_en_sentences = [[en_dict.get(w, <span class="number">0</span>) <span class="keyword">for</span> w <span class="keyword">in</span> sent] <span class="keyword">for</span> sent <span class="keyword">in</span> en_sentences]</span><br><span class="line">    <span class="comment">#out_en_sentences=[[2, 328, 43, 14, 28, 4, 3], ....</span></span><br><span class="line">    <span class="comment">#.get(w, 0)，返回w对应的值，没有就为0.因题库比较小，这里所有的单词向量都有非零索引。</span></span><br><span class="line">    </span><br><span class="line"> </span><br><span class="line">    out_cn_sentences = [[cn_dict.get(w, <span class="number">0</span>) <span class="keyword">for</span> w <span class="keyword">in</span> sent] <span class="keyword">for</span> sent <span class="keyword">in</span> cn_sentences]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sort sentences by english lengths</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">len_argsort</span><span class="params">(seq)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> sorted(range(len(seq)), key=<span class="keyword">lambda</span> x: len(seq[x]))</span><br><span class="line">      <span class="comment">#sorted()排序,key参数可以自定义规则，按seq[x]的长度排序，seq[0]为第一句话长度</span></span><br><span class="line">       </span><br><span class="line">    <span class="comment"># 把中文和英文按照同样的顺序排序</span></span><br><span class="line">    <span class="keyword">if</span> sort_by_len:</span><br><span class="line">        sorted_index = len_argsort(out_en_sentences)</span><br><span class="line">    <span class="comment">#print(sorted_index)</span></span><br><span class="line">    <span class="comment">#sorted_index=[63, 1544, 1917, 2650, 3998, 6240, 6294, 6703, ....</span></span><br><span class="line">     <span class="comment">#前面的索引都是最短句子的索引</span></span><br><span class="line">      </span><br><span class="line">        out_en_sentences = [out_en_sentences[i] <span class="keyword">for</span> i <span class="keyword">in</span> sorted_index]</span><br><span class="line">     <span class="comment">#print(out_en_sentences)</span></span><br><span class="line">     <span class="comment">#out_en_sentences=[[2, 475, 4, 3], [2, 1318, 126, 3], [2, 1707, 126, 3], ......</span></span><br><span class="line">     </span><br><span class="line">        out_cn_sentences = [out_cn_sentences[i] <span class="keyword">for</span> i <span class="keyword">in</span> sorted_index]</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> out_en_sentences, out_cn_sentences</span><br><span class="line"></span><br><span class="line">train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)</span><br><span class="line">dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)</span><br></pre></td></tr></table></figure><p>In [6]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">k=10000</span><br><span class="line">print(&quot; &quot;.join([inv_cn_dict[i] for i in train_cn[k]])) #通过inv字典获取单词</span><br><span class="line">print(&quot; &quot;.join([inv_en_dict[i] for i in train_en[k]]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BOS 他 来 这 里 的 目 的 是 什 么 ？ EOS</span><br><span class="line">BOS for what purpose did he come here ? EOS</span><br></pre></td></tr></table></figure><p>把全部句子分成batch</p><p>In [0]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(np.arange(0, 100, 15))</span><br><span class="line">print(np.arange(0, 15))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ 0 15 30 45 60 75 90]</span><br><span class="line">[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]</span><br></pre></td></tr></table></figure><p>In [141]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_minibatches</span><span class="params">(n, minibatch_size, shuffle=True)</span>:</span></span><br><span class="line">    idx_list = np.arange(<span class="number">0</span>, n, minibatch_size) <span class="comment"># [0, 1, ..., n-1]</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        np.random.shuffle(idx_list) <span class="comment">#打乱数据</span></span><br><span class="line">    minibatches = []</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> idx_list:</span><br><span class="line">        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))</span><br><span class="line">        <span class="comment">#所有batch放在一个大列表里</span></span><br><span class="line">    <span class="keyword">return</span> minibatches</span><br></pre></td></tr></table></figure><p>In [10]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_minibatches(<span class="number">100</span>,<span class="number">15</span>) <span class="comment">#随机打乱的</span></span><br></pre></td></tr></table></figure><p>Out[10]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[array([75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]),</span><br><span class="line"> array([45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]),</span><br><span class="line"> array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]),</span><br><span class="line"> array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),</span><br><span class="line"> array([15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),</span><br><span class="line"> array([60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]),</span><br><span class="line"> array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99])]</span><br></pre></td></tr></table></figure><p>In [142]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_data</span><span class="params">(seqs)</span>:</span></span><br><span class="line"><span class="comment">#seqs=[[2, 12, 167, 23, 114, 5, 27, 1755, 4, 3], ........</span></span><br><span class="line">    lengths = [len(seq) <span class="keyword">for</span> seq <span class="keyword">in</span> seqs]<span class="comment">#每个batch里语句的长度统计出来</span></span><br><span class="line">    n_samples = len(seqs) <span class="comment">#一个batch有多少语句</span></span><br><span class="line">    max_len = np.max(lengths) <span class="comment">#取出最长的的语句长度，后面用这个做padding基准</span></span><br><span class="line">    x = np.zeros((n_samples, max_len)).astype(<span class="string">'int32'</span>)</span><br><span class="line">    <span class="comment">#先初始化全零矩阵，后面依次赋值</span></span><br><span class="line">    <span class="comment">#print(x.shape) #64*最大句子长度</span></span><br><span class="line">    </span><br><span class="line">    x_lengths = np.array(lengths).astype(<span class="string">"int32"</span>)</span><br><span class="line">    <span class="comment">#print(x_lengths) </span></span><br><span class="line"><span class="comment">#这里看下面的输入语句发现英文句子长度都一样，中文句子长短不一。</span></span><br><span class="line"><span class="comment">#说明英文句子是特征，中文句子是标签。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, seq <span class="keyword">in</span> enumerate(seqs):</span><br><span class="line">      <span class="comment">#取出一个batch的每条语句和对应的索引</span></span><br><span class="line">        x[idx, :lengths[idx]] = seq</span><br><span class="line">        <span class="comment">#每条语句按行赋值给x，x会有一些零值没有被赋值。</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> x, x_lengths <span class="comment">#x_mask</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_examples</span><span class="params">(en_sentences, cn_sentences, batch_size)</span>:</span></span><br><span class="line">    minibatches = get_minibatches(len(en_sentences), batch_size)</span><br><span class="line">    all_ex = []</span><br><span class="line">    <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line">        mb_en_sentences = [en_sentences[t] <span class="keyword">for</span> t <span class="keyword">in</span> minibatch]</span><br><span class="line"><span class="comment">#按打乱的batch序号分数据，打乱只是batch打乱，一个batach里面的语句还是顺序的。</span></span><br><span class="line">        <span class="comment">#print(mb_en_sentences)</span></span><br><span class="line">        </span><br><span class="line">        mb_cn_sentences = [cn_sentences[t] <span class="keyword">for</span> t <span class="keyword">in</span> minibatch]</span><br><span class="line">        mb_x, mb_x_len = prepare_data(mb_en_sentences)</span><br><span class="line">        <span class="comment">#返回的维度为：mb_x=(64 * 最大句子长度）,mb_x_len=最大句子长度</span></span><br><span class="line">        mb_y, mb_y_len = prepare_data(mb_cn_sentences)</span><br><span class="line">        </span><br><span class="line">        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))</span><br><span class="line">  <span class="comment">#这里把所有batch数据集合到一起。</span></span><br><span class="line">  <span class="comment">#依次为英文句子，英文长度，中文句子翻译，中文句子长度，这四个放在一个列表中</span></span><br><span class="line">  <span class="comment">#一个列表为一个batch的数据，所有batch组成一个大列表数据</span></span><br><span class="line">  </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> all_ex</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_data = gen_examples(train_en, train_cn, batch_size)</span><br><span class="line">random.shuffle(train_data)</span><br><span class="line">dev_data = gen_examples(dev_en, dev_cn, batch_size)</span><br></pre></td></tr></table></figure><p>In [28]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[0]</span><br></pre></td></tr></table></figure><p>Out[28]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">(array([[   2,   12,  707,   23,    7,  295,    4,    3],</span><br><span class="line">        [   2,   12,  120, 1207,  517,  604,    4,    3],</span><br><span class="line">        [   2,    8,   90,  433,   64, 1470,  126,    3],</span><br><span class="line">        [   2,   12,  144,   46,    9,   94,    4,    3],</span><br><span class="line">        [   2,   25,   10,    9,  535,  639,    4,    3],</span><br><span class="line">        [   2,   25,   10,   64,  377, 2512,    4,    3],</span><br><span class="line">        [   2,   12,   43,  309,    9,   96,    4,    3],</span><br><span class="line">        [   2,   43,  328, 1475,   25,  469,   11,    3],</span><br><span class="line">        [   2,   82, 1043,   34, 1991, 2514,    4,    3],</span><br><span class="line">        [   2,    5,   54,    7,  181, 1694,    4,    3],</span><br><span class="line">        [   2,   30,   51,  472,    6,  294,   11,    3],</span><br><span class="line">        [   2,    5,  241,   16,   65,  551,    4,    3],</span><br><span class="line">        [   2,   14,    8,   36, 2516,  680,   11,    3],</span><br><span class="line">        [   2,    8,   30,    9,   66,  333,    4,    3],</span><br><span class="line">        [   2,   12,   10,   34,   40,  777,    4,    3],</span><br><span class="line">        [   2,   29,   54,    9,  138, 1633,    4,    3],</span><br><span class="line">        [   2,   43,    8,  309,    9,   96,   11,    3],</span><br><span class="line">        [   2,   47,   12,   39,   59,  190,   11,    3],</span><br><span class="line">        [   2,   29,   85,   14,  150,  221,    4,    3],</span><br><span class="line">        [   2,   12,   70,   37,   36,  242,    4,    3],</span><br><span class="line">        [   2,    5,  239,   64, 2521, 1696,    4,    3],</span><br><span class="line">        [   2,    5,   14,   13,   36,  314,    4,    3],</span><br><span class="line">        [   2,    5,  234,    7,   45,   44,    4,    3],</span><br><span class="line">        [   2,    5,   76,  226,   17,  621,    4,    3],</span><br><span class="line">        [   2,   29,  180,    9,  269,  266,    4,    3],</span><br><span class="line">        [   2,   85,    5,   22,    6,  708,   11,    3],</span><br><span class="line">        [   2,    6,  788,   48,   37,  889,    4,    3],</span><br><span class="line">        [   2,    8,   63,  124,   45,   95,    4,    3],</span><br><span class="line">        [   2,  921,   10,   21,  640,  350,    4,    3],</span><br><span class="line">        [   2,   52,   10,    6,  296,   44,   11,    3],</span><br><span class="line">        [   2,  681,   10,  190,   24,  146,   11,    3],</span><br><span class="line">        [   2,   19, 1480,  838,    7,  596,    4,    3],</span><br><span class="line">        [   2,   29,   90,  472, 2036,  132,    4,    3],</span><br><span class="line">        [   2,    8,   90,    9,   66,  645,    4,    3],</span><br><span class="line">        [   2,    5,  192,  257,    7,  684,    4,    3],</span><br><span class="line">        [   2,    5,   68,   36,  384, 1686,    4,    3],</span><br><span class="line">        [   2,   12,   10,  120,   38,   23,    4,    3],</span><br><span class="line">        [   2,   18,   47,  965,  106,  112,    4,    3],</span><br><span class="line">        [   2,    8,   30,   37,    9,  250,    4,    3],</span><br><span class="line">        [   2,   31,   20,  129,   20,  900,   11,    3],</span><br><span class="line">        [   2,   29,  519,  118, 2044, 1313,    4,    3],</span><br><span class="line">        [   2,   29,   22,    6,  294,  229,    4,    3],</span><br><span class="line">        [   2,   25,  189, 1056,  335,  151,    4,    3],</span><br><span class="line">        [   2,    8,   67,   89,   57,  887,    4,    3],</span><br><span class="line">        [   2,   41,    8,   72,   59,  362,   11,    3],</span><br><span class="line">        [   2,   51,  923, 2534,   26,  364,    4,    3],</span><br><span class="line">        [   2,   22,    8, 1209,  914,  834,   11,    3],</span><br><span class="line">        [   2,   19,   48,    9, 1127,  847,    4,    3],</span><br><span class="line">        [   2,   25,  224,   70,   13,  425,    4,    3],</span><br><span class="line">        [   2,   19,  949,   62, 1112,  657,    4,    3],</span><br><span class="line">        [   2,   87,   10,    6,  751,  443,   11,    3],</span><br><span class="line">        [   2,   19,  144,   99,    9,  539,    4,    3],</span><br><span class="line">        [   2,   19,  599,  242,  117,  103,    4,    3],</span><br><span class="line">        [   2,   14,    8,   22,    9,  386,   11,    3],</span><br><span class="line">        [   2,   16,   20,   60,    7,   45,    4,    3],</span><br><span class="line">        [   2,   25,  145,  133,   10, 1974,    4,    3],</span><br><span class="line">        [   2,   25,   10,  426,   17,  343,    4,    3],</span><br><span class="line">        [   2,    5,   22,  239,    6,  461,    4,    3],</span><br><span class="line">        [   2,   14,   13,    8,  162,  242,   11,    3],</span><br><span class="line">        [   2,    8,   67,   13,  159,   59,    4,    3],</span><br><span class="line">        [   2,  140, 3452, 1220,   33,  601,    4,    3],</span><br><span class="line">        [   2,    5,   79, 1937,   35,  232,    4,    3],</span><br><span class="line">        [   2,   18, 1612,   35,  779,  926,    4,    3],</span><br><span class="line">        [   2,   12,  197,  599,    6,  632,    4,    3]], dtype=int32),</span><br><span class="line"> array([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,</span><br><span class="line">        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,</span><br><span class="line">        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8],</span><br><span class="line">       dtype=int32),</span><br><span class="line"> array([[  2,   9, 793, ...,   0,   0,   0],</span><br><span class="line">        [  2,   9, 504, ...,   0,   0,   0],</span><br><span class="line">        [  2,   8, 114, ...,   0,   0,   0],</span><br><span class="line">        ...,</span><br><span class="line">        [  2,   5, 154, ...,   0,   0,   0],</span><br><span class="line">        [  2, 214, 171, ..., 838,   4,   3],</span><br><span class="line">        [  2,   9,  74, ...,   0,   0,   0]], dtype=int32),</span><br><span class="line"> array([10, 12,  9, 10,  8, 10,  7, 13, 17,  8, 11, 10, 11,  9,  9, 12,  8,</span><br><span class="line">        12, 10,  9, 14,  9,  9,  6,  9, 10,  9, 10, 13, 11, 14, 13, 14,  8,</span><br><span class="line">         8, 10, 10,  9,  8,  7, 14, 12, 13, 13, 13, 12, 13,  8, 11, 11, 10,</span><br><span class="line">        12, 10,  9,  6, 10,  8, 11,  9, 11, 10, 12, 21,  9], dtype=int32))</span><br></pre></td></tr></table></figure><h3 id="没有Attention的版本"><a href="#没有Attention的版本" class="headerlink" title="没有Attention的版本"></a>没有Attention的版本</h3><p>下面是一个更简单的没有Attention的encoder decoder模型</p><p>In [143]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PlainEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, hidden_size, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        <span class="comment">#以英文为例，vocab_size=5493, hidden_size=100, dropout=0.2</span></span><br><span class="line">        super(PlainEncoder, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, hidden_size)</span><br><span class="line">        <span class="comment">#这里的hidden_size为embedding_dim：一个单词的维度 </span></span><br><span class="line">        <span class="comment">#torch.nn.Embedding(num_embeddings, embedding_dim, .....)</span></span><br><span class="line">        <span class="comment">#这里的hidden_size = 100</span></span><br><span class="line">        </span><br><span class="line">        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=<span class="literal">True</span>)      </span><br><span class="line">        <span class="comment">#第一个参数为input_size ：输入特征数量</span></span><br><span class="line">        <span class="comment">#第二个参数为hidden_size ：隐藏层特征数量</span></span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, lengths)</span>:</span> </span><br><span class="line">        <span class="comment">#x是输入的batch的所有单词，lengths：batch里每个句子的长度</span></span><br><span class="line">        <span class="comment">#因为需要把最后一个hidden state取出来，需要知道长度，因为句子长度不一样</span></span><br><span class="line">        <span class="comment">##print(x.shape,lengths),x.sahpe = torch.Size([64, 10])</span></span><br><span class="line">        <span class="comment"># lengths= =tensor([10, 10, 10, ..... 10, 10, 10])</span></span><br><span class="line">        </span><br><span class="line">        sorted_len, sorted_idx = lengths.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">#按照长度排序，descending=True长的在前。</span></span><br><span class="line">        <span class="comment">#返回两个参数，句子长度和未排序前的索引</span></span><br><span class="line">        <span class="comment"># sorted_idx=tensor([41, 40, 46, 45,...... 19, 18, 63])</span></span><br><span class="line">        <span class="comment"># sorted_len=tensor([10, 10, 10, ..... 10, 10, 10])</span></span><br><span class="line">        </span><br><span class="line">        x_sorted = x[sorted_idx.long()] <span class="comment">#句子用新的idx，按长度排好序了</span></span><br><span class="line">        </span><br><span class="line">        embedded = self.dropout(self.embed(x_sorted))</span><br><span class="line">        <span class="comment">#print(embedded.shape)=torch.Size([64, 10, 100])</span></span><br><span class="line">        <span class="comment">#tensor([[[-0.6312, -0.9863, -0.3123,  ..., -0.7384,  0.9230, -0.4311],....</span></span><br><span class="line"></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">#这个函数就是用来处理不同长度的句子的，https: // www.cnblogs.com / sbj123456789 / p / 9834018. html</span></span><br><span class="line"></span><br><span class="line">        packed_out, hid = self.rnn(packed_embedded)</span><br><span class="line">        <span class="comment">#hid.shape = torch.Size([1, 64, 100])</span></span><br><span class="line">        </span><br><span class="line">        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">#out.shape = torch.Size([64, 10, 100]),</span></span><br><span class="line"></span><br><span class="line">        _, original_idx = sorted_idx.sort(<span class="number">0</span>, descending=<span class="literal">False</span>)</span><br><span class="line">        out = out[original_idx.long()].contiguous()</span><br><span class="line">        hid = hid[:, original_idx.long()].contiguous()</span><br><span class="line">        <span class="comment">#out.shape = torch.Size([64, 10, 100])</span></span><br><span class="line">        <span class="comment">#hid.shape = torch.Size([1, 64, 100])</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> out, hid[[<span class="number">-1</span>]] <span class="comment">#有时候num_layers层数多，需要取出最后一层</span></span><br></pre></td></tr></table></figure><p>In [124]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PlainDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, hidden_size, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        super(PlainDecoder, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, hidden_size)</span><br><span class="line">        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.out = nn.Linear(hidden_size, vocab_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, y, y_lengths, hid)</span>:</span></span><br><span class="line">        <span class="comment">#print(y.shape)=torch.Size([64, 12])</span></span><br><span class="line">        <span class="comment">#print(hid.shape)=torch.Size([1, 64, 100])</span></span><br><span class="line">        <span class="comment">#中文的y和y_lengths</span></span><br><span class="line">        sorted_len, sorted_idx = y_lengths.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">        y_sorted = y[sorted_idx.long()]</span><br><span class="line">        hid = hid[:, sorted_idx.long()] <span class="comment">#隐藏层也要排序</span></span><br><span class="line"></span><br><span class="line">        y_sorted = self.dropout(self.embed(y_sorted)) </span><br><span class="line">        <span class="comment"># batch_size, output_length, embed_size</span></span><br><span class="line"></span><br><span class="line">        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=<span class="literal">True</span>)</span><br><span class="line">        out, hid = self.rnn(packed_seq, hid) <span class="comment">#加上隐藏层</span></span><br><span class="line">        <span class="comment">#print(hid.shape)=torch.Size([1, 64, 100])</span></span><br><span class="line">        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=<span class="literal">True</span>)</span><br><span class="line">        _, original_idx = sorted_idx.sort(<span class="number">0</span>, descending=<span class="literal">False</span>)</span><br><span class="line">        output_seq = unpacked[original_idx.long()].contiguous()</span><br><span class="line">        <span class="comment">#print(output_seq.shape)=torch.Size([64, 12, 100])</span></span><br><span class="line">        hid = hid[:, original_idx.long()].contiguous()</span><br><span class="line">        <span class="comment">#print(hid.shape)=torch.Size([1, 64, 100])</span></span><br><span class="line">        output = F.log_softmax(self.out(output_seq), <span class="number">-1</span>)</span><br><span class="line">        <span class="comment">#print(output.shape)=torch.Size([64, 12, 3195])</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, hid</span><br></pre></td></tr></table></figure><p>In [144]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PlainSeq2Seq</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder)</span>:</span></span><br><span class="line">        <span class="comment">#encoder是上面PlainEncoder的实例</span></span><br><span class="line">        <span class="comment">#decoder是上面PlainDecoder的实例</span></span><br><span class="line">        super(PlainSeq2Seq, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">       </span><br><span class="line">    <span class="comment">#把两个模型串起来 </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, x_lengths, y, y_lengths)</span>:</span></span><br><span class="line">        encoder_out, hid = self.encoder(x, x_lengths)</span><br><span class="line">        <span class="comment">#self.encoder(x, x_lengths)调用PlainEncoder里面forward的方法</span></span><br><span class="line">        <span class="comment">#返回forward的out和hid</span></span><br><span class="line">        </span><br><span class="line">        output, hid = self.decoder(y=y,y_lengths=y_lengths,hid=hid)</span><br><span class="line">        <span class="comment">#self.dencoder()调用PlainDecoder里面forward的方法</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">translate</span><span class="params">(self, x, x_lengths, y, max_length=<span class="number">10</span>)</span>:</span></span><br><span class="line">        <span class="comment">#x是一个句子，用数值表示</span></span><br><span class="line">        <span class="comment">#y是句子的长度</span></span><br><span class="line">        <span class="comment">#y是“bos”的数值索引=2</span></span><br><span class="line">        </span><br><span class="line">        encoder_out, hid = self.encoder(x, x_lengths)</span><br><span class="line">        preds = []</span><br><span class="line">        batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">        attns = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(max_length):</span><br><span class="line">            output, hid = self.decoder(y=y,</span><br><span class="line">                    y_lengths=torch.ones(batch_size).long().to(y.device),</span><br><span class="line">                    hid=hid) </span><br><span class="line">            </span><br><span class="line"><span class="comment">#刚开始循环bos作为模型的首个输入单词，后续更新y，下个预测单词的输入是上个输出单词</span></span><br><span class="line">            y = output.max(<span class="number">2</span>)[<span class="number">1</span>].view(batch_size, <span class="number">1</span>)</span><br><span class="line">            preds.append(y)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> torch.cat(preds, <span class="number">1</span>), <span class="literal">None</span></span><br></pre></td></tr></table></figure><p>In [145]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">hidden_size = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#传入中文和英文参数</span></span><br><span class="line">encoder = PlainEncoder(vocab_size=en_total_words,</span><br><span class="line">                      hidden_size=hidden_size,</span><br><span class="line">                      dropout=dropout)</span><br><span class="line">decoder = PlainDecoder(vocab_size=cn_total_words,</span><br><span class="line">                      hidden_size=hidden_size,</span><br><span class="line">                      dropout=dropout)</span><br><span class="line">model = PlainSeq2Seq(encoder, decoder)</span><br></pre></td></tr></table></figure><p>In [146]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># masked cross entropy loss</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LanguageModelCriterion</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LanguageModelCriterion, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, target, mask)</span>:</span></span><br><span class="line">        <span class="comment">#target=tensor([[5,108,8,4,3,0,0,0,0,0,0,0],....</span></span><br><span class="line">        <span class="comment">#  mask=tensor([[1,1 ,1,1,1,0,0,0,0,0,0,0],.....</span></span><br><span class="line">        <span class="comment">#print(input.shape,target.shape,mask.shape)</span></span><br><span class="line">        <span class="comment">#torch.Size([64, 12, 3195]) torch.Size([64, 12]) torch.Size([64, 12])</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># input: (batch_size * seq_len) * vocab_size</span></span><br><span class="line">        input = input.contiguous().view(<span class="number">-1</span>, input.size(<span class="number">2</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># target: batch_size * 1=768*1</span></span><br><span class="line">        target = target.contiguous().view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        mask = mask.contiguous().view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment">#print(-input.gather(1, target))</span></span><br><span class="line">        output = -input.gather(<span class="number">1</span>, target) * mask</span><br><span class="line"><span class="comment">#这里算得就是交叉熵损失，前面已经算了F.log_softmax</span></span><br><span class="line"><span class="comment">#.gather的作用https://blog.csdn.net/edogawachia/article/details/80515038</span></span><br><span class="line"><span class="comment">#output.shape=torch.Size([768, 1])</span></span><br><span class="line"><span class="comment">#mask作用是把padding为0的地方重置为零，因为input.gather时，为0的地方不是零了</span></span><br><span class="line">        </span><br><span class="line">        output = torch.sum(output) / torch.sum(mask)</span><br><span class="line">        <span class="comment">#均值损失</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>In [147]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = model.to(device)</span><br><span class="line">loss_fn = LanguageModelCriterion().to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters())</span><br></pre></td></tr></table></figure><p>pythonIn [151]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, data, num_epochs=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        total_num_words = total_loss = <span class="number">0.</span></span><br><span class="line">        <span class="keyword">for</span> it, (mb_x, mb_x_len, mb_y, mb_y_len) <span class="keyword">in</span> enumerate(data):</span><br><span class="line">            <span class="comment">#（英文batch，英文长度，中文batch，中文长度）</span></span><br><span class="line">            </span><br><span class="line">            mb_x = torch.from_numpy(mb_x).to(device).long()</span><br><span class="line">            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#前n-1个单词作为输入，后n-1个单词作为输出，因为输入的前一个单词要预测后一个单词</span></span><br><span class="line">            mb_input = torch.from_numpy(mb_y[:, :<span class="number">-1</span>]).to(device).long()</span><br><span class="line">            mb_output = torch.from_numpy(mb_y[:, <span class="number">1</span>:]).to(device).long()</span><br><span class="line">            <span class="comment">#</span></span><br><span class="line">            mb_y_len = torch.from_numpy(mb_y_len<span class="number">-1</span>).to(device).long()</span><br><span class="line">            <span class="comment">#输入输出的长度都减一。</span></span><br><span class="line">            </span><br><span class="line">            mb_y_len[mb_y_len&lt;=<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)</span><br><span class="line">            <span class="comment">#返回的是类PlainSeq2Seq里forward函数的两个返回值</span></span><br><span class="line">            </span><br><span class="line">            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[<span class="literal">None</span>, :] &lt; mb_y_len[:, <span class="literal">None</span>]</span><br><span class="line"><span class="comment">#mb_out_mask=tensor([[1, 1, 1,  ..., 0, 0, 0],[1, 1, 1,  ..., 0, 0, 0],</span></span><br><span class="line"><span class="comment">#mb_out_mask.shape= (64*19),这句代码咱不懂，这个mask就是padding的位置设置为0，其他设置为1</span></span><br><span class="line"><span class="comment">#mb_out_mask就是LanguageModelCriterion的传入参数mask。</span></span><br><span class="line"></span><br><span class="line">            mb_out_mask = mb_out_mask.float()</span><br><span class="line">            </span><br><span class="line">            loss = loss_fn(mb_pred, mb_output, mb_out_mask)</span><br><span class="line">            </span><br><span class="line">            num_words = torch.sum(mb_y_len).item()</span><br><span class="line">            <span class="comment">#一个batch里多少个单词</span></span><br><span class="line">            </span><br><span class="line">            total_loss += loss.item() * num_words</span><br><span class="line">            <span class="comment">#总损失，loss计算的是均值损失，每个单词都是都有损失，所以乘以单词数</span></span><br><span class="line">            </span><br><span class="line">            total_num_words += num_words</span><br><span class="line">            <span class="comment">#总单词数</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 更新模型</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">5.</span>)</span><br><span class="line">            <span class="comment">#为了防止梯度过大，设置梯度的阈值</span></span><br><span class="line">            </span><br><span class="line">            optimizer.step()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> it % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"Epoch"</span>, epoch, <span class="string">"iteration"</span>, it, <span class="string">"loss"</span>, loss.item())</span><br><span class="line"></span><br><span class="line">                </span><br><span class="line">        print(<span class="string">"Epoch"</span>, epoch, <span class="string">"Training loss"</span>, total_loss/total_num_words)</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            evaluate(model, dev_data) <span class="comment">#评估模型</span></span><br><span class="line">train(model, train_data, num_epochs=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0 iteration 0 loss 4.277793884277344</span><br><span class="line">Epoch 0 iteration 100 loss 3.5520756244659424</span><br><span class="line">Epoch 0 iteration 200 loss 3.483494997024536</span><br><span class="line">Epoch 0 Training loss 3.6435126089915557</span><br><span class="line">Evaluation loss 3.698509503997669</span><br><span class="line">Epoch 1 iteration 0 loss 4.158623218536377</span><br><span class="line">Epoch 1 iteration 100 loss 3.412541389465332</span><br><span class="line">Epoch 1 iteration 200 loss 3.3976175785064697</span><br><span class="line">Epoch 1 Training loss 3.5087569079050698</span><br></pre></td></tr></table></figure><p>In [135]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, data)</span>:</span></span><br><span class="line">    model.eval()</span><br><span class="line">    total_num_words = total_loss = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():<span class="comment">#不需要更新模型，不需要梯度</span></span><br><span class="line">        <span class="keyword">for</span> it, (mb_x, mb_x_len, mb_y, mb_y_len) <span class="keyword">in</span> enumerate(data):</span><br><span class="line">            mb_x = torch.from_numpy(mb_x).to(device).long()</span><br><span class="line">            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()</span><br><span class="line">            mb_input = torch.from_numpy(mb_y[:, :<span class="number">-1</span>]).to(device).long()</span><br><span class="line">            mb_output = torch.from_numpy(mb_y[:, <span class="number">1</span>:]).to(device).long()</span><br><span class="line">            mb_y_len = torch.from_numpy(mb_y_len<span class="number">-1</span>).to(device).long()</span><br><span class="line">            mb_y_len[mb_y_len&lt;=<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)</span><br><span class="line"></span><br><span class="line">            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[<span class="literal">None</span>, :] &lt; mb_y_len[:, <span class="literal">None</span>]</span><br><span class="line">            mb_out_mask = mb_out_mask.float()</span><br><span class="line"></span><br><span class="line">            loss = loss_fn(mb_pred, mb_output, mb_out_mask)</span><br><span class="line"></span><br><span class="line">            num_words = torch.sum(mb_y_len).item()</span><br><span class="line">            total_loss += loss.item() * num_words</span><br><span class="line">            total_num_words += num_words</span><br><span class="line">    print(<span class="string">"Evaluation loss"</span>, total_loss/total_num_words)</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#翻译个句子看看结果咋样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">translate_dev</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="comment">#随便取出句子</span></span><br><span class="line">    en_sent = <span class="string">" "</span>.join([inv_en_dict[w] <span class="keyword">for</span> w <span class="keyword">in</span> dev_en[i]])</span><br><span class="line">    print(en_sent)</span><br><span class="line">    cn_sent = <span class="string">" "</span>.join([inv_cn_dict[w] <span class="keyword">for</span> w <span class="keyword">in</span> dev_cn[i]])</span><br><span class="line">    print(<span class="string">""</span>.join(cn_sent))</span><br><span class="line"></span><br><span class="line">    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(<span class="number">1</span>, <span class="number">-1</span>)).long().to(device)</span><br><span class="line">    <span class="comment">#把句子升维，并转换成tensor</span></span><br><span class="line">    </span><br><span class="line">    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)</span><br><span class="line">    <span class="comment">#取出句子长度，并转换成tensor</span></span><br><span class="line">    </span><br><span class="line">    bos = torch.Tensor([[cn_dict[<span class="string">"BOS"</span>]]]).long().to(device)</span><br><span class="line">    <span class="comment">#bos=tensor([[2]])</span></span><br><span class="line"></span><br><span class="line">    translation, attn = model.translate(mb_x, mb_x_len, bos)</span><br><span class="line">    <span class="comment">#这里传入bos作为首个单词的输入</span></span><br><span class="line">    <span class="comment">#translation=tensor([[ 8,  6, 11, 25, 22, 57, 10,  5,  6,  4]])</span></span><br><span class="line">    </span><br><span class="line">    translation = [inv_cn_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> translation.data.cpu().numpy().reshape(<span class="number">-1</span>)]</span><br><span class="line">    trans = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> translation:</span><br><span class="line">        <span class="keyword">if</span> word != <span class="string">"EOS"</span>: <span class="comment"># 把数值变成单词形式</span></span><br><span class="line">            trans.append(word) <span class="comment">#</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    print(<span class="string">""</span>.join(trans))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>,<span class="number">120</span>):</span><br><span class="line">    translate_dev(i)</span><br><span class="line">    print()</span><br></pre></td></tr></table></figure><p>数据全部处理完成，现在我们开始构建seq2seq模型</p><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><ul><li>Encoder模型的任务是把输入文字传入embedding层和GRU层，转换成一些hidden states作为后续的context vectors</li></ul><h2 id="下面的注释我先把原理捋清楚吧"><a href="#下面的注释我先把原理捋清楚吧" class="headerlink" title="下面的注释我先把原理捋清楚吧"></a>下面的注释我先把原理捋清楚吧</h2><p>In [0]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        </span><br><span class="line">        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=<span class="literal">True</span>, bidirectional=<span class="literal">True</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.fc = nn.Linear(enc_hidden_size * <span class="number">2</span>, dec_hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, lengths)</span>:</span></span><br><span class="line">        sorted_len, sorted_idx = lengths.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">        x_sorted = x[sorted_idx.long()]</span><br><span class="line">        embedded = self.dropout(self.embed(x_sorted))</span><br><span class="line">        </span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=<span class="literal">True</span>)</span><br><span class="line">        packed_out, hid = self.rnn(packed_embedded)</span><br><span class="line">        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=<span class="literal">True</span>)</span><br><span class="line">        _, original_idx = sorted_idx.sort(<span class="number">0</span>, descending=<span class="literal">False</span>)</span><br><span class="line">        out = out[original_idx.long()].contiguous()</span><br><span class="line">        hid = hid[:, original_idx.long()].contiguous()</span><br><span class="line">        </span><br><span class="line">        hid = torch.cat([hid[<span class="number">-2</span>], hid[<span class="number">-1</span>]], dim=<span class="number">1</span>)</span><br><span class="line">        hid = torch.tanh(self.fc(hid)).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out, hid</span><br></pre></td></tr></table></figure><h4 id="Luong-Attention"><a href="#Luong-Attention" class="headerlink" title="Luong Attention"></a>Luong Attention</h4><ul><li>根据context vectors和当前的输出hidden states，计算输出</li></ul><p>In [0]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, enc_hidden_size, dec_hidden_size)</span>:</span></span><br><span class="line">        super(Attention, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.enc_hidden_size = enc_hidden_size</span><br><span class="line">        self.dec_hidden_size = dec_hidden_size</span><br><span class="line"></span><br><span class="line">        self.linear_in = nn.Linear(enc_hidden_size*<span class="number">2</span>, dec_hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.linear_out = nn.Linear(enc_hidden_size*<span class="number">2</span> + dec_hidden_size, dec_hidden_size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, output, context, mask)</span>:</span></span><br><span class="line">        <span class="comment"># output: batch_size, output_len, dec_hidden_size</span></span><br><span class="line">        <span class="comment"># context: batch_size, context_len, 2*enc_hidden_size</span></span><br><span class="line">    </span><br><span class="line">        batch_size = output.size(<span class="number">0</span>)</span><br><span class="line">        output_len = output.size(<span class="number">1</span>)</span><br><span class="line">        input_len = context.size(<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        context_in = self.linear_in(context.view(batch_size*input_len, <span class="number">-1</span>)).view(                </span><br><span class="line">            batch_size, input_len, <span class="number">-1</span>) <span class="comment"># batch_size, context_len, dec_hidden_size</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># context_in.transpose(1,2): batch_size, dec_hidden_size, context_len </span></span><br><span class="line">        <span class="comment"># output: batch_size, output_len, dec_hidden_size</span></span><br><span class="line">        attn = torch.bmm(output, context_in.transpose(<span class="number">1</span>,<span class="number">2</span>)) </span><br><span class="line">        <span class="comment"># batch_size, output_len, context_len</span></span><br><span class="line"></span><br><span class="line">        attn.data.masked_fill(mask, <span class="number">-1e6</span>)</span><br><span class="line"></span><br><span class="line">        attn = F.softmax(attn, dim=<span class="number">2</span>) </span><br><span class="line">        <span class="comment"># batch_size, output_len, context_len</span></span><br><span class="line"></span><br><span class="line">        context = torch.bmm(attn, context) </span><br><span class="line">        <span class="comment"># batch_size, output_len, enc_hidden_size</span></span><br><span class="line">        </span><br><span class="line">        output = torch.cat((context, output), dim=<span class="number">2</span>) <span class="comment"># batch_size, output_len, hidden_size*2</span></span><br><span class="line"></span><br><span class="line">        output = output.view(batch_size*output_len, <span class="number">-1</span>)</span><br><span class="line">        output = torch.tanh(self.linear_out(output))</span><br><span class="line">        output = output.view(batch_size, output_len, <span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> output, attn</span><br></pre></td></tr></table></figure><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><ul><li>decoder会根据已经翻译的句子内容，和context vectors，来决定下一个输出的单词</li></ul><p>In [0]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.attention = Attention(enc_hidden_size, dec_hidden_size)</span><br><span class="line">        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.out = nn.Linear(dec_hidden_size, vocab_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_mask</span><span class="params">(self, x_len, y_len)</span>:</span></span><br><span class="line">        <span class="comment"># a mask of shape x_len * y_len</span></span><br><span class="line">        device = x_len.device</span><br><span class="line">        max_x_len = x_len.max()</span><br><span class="line">        max_y_len = y_len.max()</span><br><span class="line">        x_mask = torch.arange(max_x_len, device=x_len.device)[<span class="literal">None</span>, :] &lt; x_len[:, <span class="literal">None</span>]</span><br><span class="line">        y_mask = torch.arange(max_y_len, device=x_len.device)[<span class="literal">None</span>, :] &lt; y_len[:, <span class="literal">None</span>]</span><br><span class="line">        mask = (<span class="number">1</span> - x_mask[:, :, <span class="literal">None</span>] * y_mask[:, <span class="literal">None</span>, :]).byte()</span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, ctx, ctx_lengths, y, y_lengths, hid)</span>:</span></span><br><span class="line">        sorted_len, sorted_idx = y_lengths.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">        y_sorted = y[sorted_idx.long()]</span><br><span class="line">        hid = hid[:, sorted_idx.long()]</span><br><span class="line">        </span><br><span class="line">        y_sorted = self.dropout(self.embed(y_sorted)) <span class="comment"># batch_size, output_length, embed_size</span></span><br><span class="line"></span><br><span class="line">        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=<span class="literal">True</span>)</span><br><span class="line">        out, hid = self.rnn(packed_seq, hid)</span><br><span class="line">        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=<span class="literal">True</span>)</span><br><span class="line">        _, original_idx = sorted_idx.sort(<span class="number">0</span>, descending=<span class="literal">False</span>)</span><br><span class="line">        output_seq = unpacked[original_idx.long()].contiguous()</span><br><span class="line">        hid = hid[:, original_idx.long()].contiguous()</span><br><span class="line"></span><br><span class="line">        mask = self.create_mask(y_lengths, ctx_lengths)</span><br><span class="line"></span><br><span class="line">        output, attn = self.attention(output_seq, ctx, mask)</span><br><span class="line">        output = F.log_softmax(self.out(output), <span class="number">-1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, hid, attn</span><br></pre></td></tr></table></figure><h4 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h4><ul><li>最后我们构建Seq2Seq模型把encoder, attention, decoder串到一起</li></ul><p>In [0]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2Seq</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder)</span>:</span></span><br><span class="line">        super(Seq2Seq, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, x_lengths, y, y_lengths)</span>:</span></span><br><span class="line">        encoder_out, hid = self.encoder(x, x_lengths)</span><br><span class="line">        output, hid, attn = self.decoder(ctx=encoder_out, </span><br><span class="line">                    ctx_lengths=x_lengths,</span><br><span class="line">                    y=y,</span><br><span class="line">                    y_lengths=y_lengths,</span><br><span class="line">                    hid=hid)</span><br><span class="line">        <span class="keyword">return</span> output, attn</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">translate</span><span class="params">(self, x, x_lengths, y, max_length=<span class="number">100</span>)</span>:</span></span><br><span class="line">        encoder_out, hid = self.encoder(x, x_lengths)</span><br><span class="line">        preds = []</span><br><span class="line">        batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">        attns = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(max_length):</span><br><span class="line">            output, hid, attn = self.decoder(ctx=encoder_out, </span><br><span class="line">                    ctx_lengths=x_lengths,</span><br><span class="line">                    y=y,</span><br><span class="line">                    y_lengths=torch.ones(batch_size).long().to(y.device),</span><br><span class="line">                    hid=hid)</span><br><span class="line">            y = output.max(<span class="number">2</span>)[<span class="number">1</span>].view(batch_size, <span class="number">1</span>)</span><br><span class="line">            preds.append(y)</span><br><span class="line">            attns.append(attn)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(preds, <span class="number">1</span>), torch.cat(attns, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>训练</p><p>In [0]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">embed_size = hidden_size = <span class="number">100</span></span><br><span class="line">encoder = Encoder(vocab_size=en_total_words,</span><br><span class="line">                       embed_size=embed_size,</span><br><span class="line">                      enc_hidden_size=hidden_size,</span><br><span class="line">                       dec_hidden_size=hidden_size,</span><br><span class="line">                      dropout=dropout)</span><br><span class="line">decoder = Decoder(vocab_size=cn_total_words,</span><br><span class="line">                      embed_size=embed_size,</span><br><span class="line">                      enc_hidden_size=hidden_size,</span><br><span class="line">                       dec_hidden_size=hidden_size,</span><br><span class="line">                      dropout=dropout)</span><br><span class="line">model = Seq2Seq(encoder, decoder)</span><br><span class="line">model = model.to(device)</span><br><span class="line">loss_fn = LanguageModelCriterion().to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters())</span><br></pre></td></tr></table></figure><p>In [2]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(model, train_data, num_epochs=30)</span><br></pre></td></tr></table></figure><p>In [0]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for i in range(100,120):</span><br><span class="line">    translate_dev(i)</span><br><span class="line">    print()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">BOS you have nice skin . EOS</span><br><span class="line">BOS 你 的 皮 膚 真 好 。 EOS</span><br><span class="line">你好害怕。</span><br><span class="line"></span><br><span class="line">BOS you &apos;re UNK correct . EOS</span><br><span class="line">BOS 你 部 分 正 确 。 EOS</span><br><span class="line">你是全子的声音。</span><br><span class="line"></span><br><span class="line">BOS everyone admired his courage . EOS</span><br><span class="line">BOS 每 個 人 都 佩 服 他 的 勇 氣 。 EOS</span><br><span class="line">他的袋子是他的勇氣。</span><br><span class="line"></span><br><span class="line">BOS what time is it ? EOS</span><br><span class="line">BOS 几 点 了 ？ EOS</span><br><span class="line">多少时间是什么？</span><br><span class="line"></span><br><span class="line">BOS i &apos;m free tonight . EOS</span><br><span class="line">BOS 我 今 晚 有 空 。 EOS</span><br><span class="line">我今晚有空。</span><br><span class="line"></span><br><span class="line">BOS here is your book . EOS</span><br><span class="line">BOS 這 是 你 的 書 。 EOS</span><br><span class="line">这儿是你的书。</span><br><span class="line"></span><br><span class="line">BOS they are at lunch . EOS</span><br><span class="line">BOS 他 们 在 吃 午 饭 。 EOS</span><br><span class="line">他们在午餐。</span><br><span class="line"></span><br><span class="line">BOS this chair is UNK . EOS</span><br><span class="line">BOS 這 把 椅 子 很 UNK 。 EOS</span><br><span class="line">這些花一下是正在的。</span><br><span class="line"></span><br><span class="line">BOS it &apos;s pretty heavy . EOS</span><br><span class="line">BOS 它 真 重 。 EOS</span><br><span class="line">它很美的脚。</span><br><span class="line"></span><br><span class="line">BOS many attended his funeral . EOS</span><br><span class="line">BOS 很 多 人 都 参 加 了 他 的 葬 礼 。 EOS</span><br><span class="line">多多衛年轻地了他。</span><br><span class="line"></span><br><span class="line">BOS training will be provided . EOS</span><br><span class="line">BOS 会 有 训 练 。 EOS</span><br><span class="line">别将被付錢。</span><br><span class="line"></span><br><span class="line">BOS someone is watching you . EOS</span><br><span class="line">BOS 有 人 在 看 著 你 。 EOS</span><br><span class="line">有人看你。</span><br><span class="line"></span><br><span class="line">BOS i slapped his face . EOS</span><br><span class="line">BOS 我 摑 了 他 的 臉 。 EOS</span><br><span class="line">我把他的臉抱歉。</span><br><span class="line"></span><br><span class="line">BOS i like UNK music . EOS</span><br><span class="line">BOS 我 喜 歡 流 行 音 樂 。 EOS</span><br><span class="line">我喜歡音樂。</span><br><span class="line"></span><br><span class="line">BOS tom had no children . EOS</span><br><span class="line">BOS T o m 沒 有 孩 子 。 EOS</span><br><span class="line">汤姆没有照顧孩子。</span><br><span class="line"></span><br><span class="line">BOS please lock the door . EOS</span><br><span class="line">BOS 請 把 門 鎖 上 。 EOS</span><br><span class="line">请把門開門。</span><br><span class="line"></span><br><span class="line">BOS tom has calmed down . EOS</span><br><span class="line">BOS 汤 姆 冷 静 下 来 了 。 EOS</span><br><span class="line">汤姆在做了。</span><br><span class="line"></span><br><span class="line">BOS please speak more loudly . EOS</span><br><span class="line">BOS 請 說 大 聲 一 點 兒 。 EOS</span><br><span class="line">請說更多。</span><br><span class="line"></span><br><span class="line">BOS keep next sunday free . EOS</span><br><span class="line">BOS 把 下 周 日 空 出 来 。 EOS</span><br><span class="line">繼續下週一下一步。</span><br><span class="line"></span><br><span class="line">BOS i made a mistake . EOS</span><br><span class="line">BOS 我 犯 了 一 個 錯 。 EOS</span><br><span class="line">我做了一件事。</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Seq2Seq </tag>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器翻译与文本摘要</title>
      <link href="/2020/04/09/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/"/>
      <url>/2020/04/09/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/</url>
      
        <content type="html"><![CDATA[<h1 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h1><p><img src="https://uploader.shimo.im/f/brdWUlzu4FsL8Owh.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/L7hZdGTE2Rw7LaK7.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/khdZXF2WzdUjtnfh.png!thumbnail" alt="img"></p><h1 id><a href="#" class="headerlink" title></a><img src="https://uploader.shimo.im/f/OA9BcNUkI4USlE4s.png!thumbnail" alt="img"></h1><p>现在的<strong>机器翻译模型都是由数据驱动</strong>的。什么数据？</p><ul><li><p>新闻</p></li><li><p>公司网页</p></li><li><p>法律/专利文件，联合国documents</p></li><li><p>电影/电视字幕</p></li></ul><p>IBM fire a linguist, their machine translation system improves by 1%</p><p>Parallel Data</p><ul><li><p>我们希望使用双语的，有对应关系的数据</p></li><li><p>大部分数据都是由文档级别的</p></li></ul><p>如何<strong>评估</strong>翻译模型？</p><ul><li><p><strong>人工评估</strong>最好，但是非常<strong>费时费力</strong></p></li><li><p>还有哪些问题需要人类评估？</p></li><li><p>需要一些自动评估的手段</p></li><li><p><strong>BLUE</strong> (Bilingual Evaluation Understudy), Papineni et al. (2002)</p></li><li><p>计算系统生成翻译与人类参考翻译之间的n-gram overlap</p></li><li><p>BLEU score与<strong>人类评测的相关度非常高</strong></p></li><li><p><a href="https://www.aclweb.org/anthology/P02-1040.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/P02-1040.pdf</a></p></li><li><p>precision based metric</p></li><li><p>自动评估依然是一个<strong>有价值的研究问题</strong></p></li></ul><p>precision: 在我翻译的单词当中，有哪些单词是正确的。</p><p>unigram, bigram, trigram, 4-gram precision </p><p><strong>BLEU-4</strong>: average of the 4 kinds of grams</p><p><strong>BLEU-3</strong></p><p>统计学翻译模型</p><p><img src="https://uploader.shimo.im/f/phryZdcQGH8Z5i5V.png!thumbnail" alt="img"></p><p>Encoder-decoder 模型</p><p>x：英文</p><p><strong>y：中文</strong></p><p>P(y|x) x: noisy input</p><p><img src="https://uploader.shimo.im/f/1zBNjrukMK8ennr8.png!thumbnail" alt="img"></p><p>P(y|x) = P(x, y) / P(x) = P(x|y)P(y) / P(x)</p><p>argmax_y P(y|x) = <strong>argmax_y P(x|y)P(y)</strong></p><p><strong>P(x|y)</strong> </p><p><strong>P(y)</strong></p><h2 id="Encoder-Decoder-Model"><a href="#Encoder-Decoder-Model" class="headerlink" title="Encoder-Decoder Model"></a>Encoder-Decoder Model</h2><p><img src="https://uploader.shimo.im/f/fSgtSMHGwlsMwqR8.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/NJXXeu6kA4QJzzj3.png!thumbnail" alt="img"></p><p>RNN(x) –&gt; c (<strong>c能够完全包含整个句子的信息?</strong>）</p><p>RNN(c) –&gt; y (c作为输入进入每一个decoding step)</p><p>训练方式是什么？损失函数是什么？</p><ul><li><p>cross entropy loss， 作业一中的context模型</p></li><li><p>SGD, Adam</p></li></ul><p>GRU</p><p><a href="https://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1406.1078.pdf</a></p><p><img src="https://uploader.shimo.im/f/UBhRdKWsAvEKpbz0.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/eF9pRfBLFSQi5NHd.png!thumbnail" alt="img"></p><h2 id="Attention机制"><a href="#Attention机制" class="headerlink" title="Attention机制"></a>Attention机制</h2><p><img src="https://uploader.shimo.im/f/CkL5KNLrUQE2tzH4.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/JQtrWTdEgiURNKTX.png!thumbnail" alt="img"></p><p>图片来自 Bahdanau et al., Neural Machine Translation by Jointly Learning to Align and Translate <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1409.0473.pdf</a></p><h2 id="-1"><a href="#-1" class="headerlink" title></a><img src="https://uploader.shimo.im/f/dWsWHO9MF20QJ2kI.png!thumbnail" alt="img"></h2><p><img src="https://uploader.shimo.im/f/KRbuH9pTLpoNLHN7.png!thumbnail" alt="img"></p><p>图片来自Luong et al., Effective Approaches to Attention-based Neural Machine Translation</p><p><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1508.04025.pdf</a></p><p>Google Neural Machine Translation</p><p><a href="https://arxiv.org/pdf/1609.08144.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1609.08144.pdf</a></p><p><img src="https://uploader.shimo.im/f/en9dH9PnTeoPDMMv.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/W8BSXh4U2Kc8ZKjL.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/vpGOGoKHN5AQvKiB.png!thumbnail" alt="img"></p><h2 id="Zero-shot-NMT"><a href="#Zero-shot-NMT" class="headerlink" title="Zero-shot NMT"></a>Zero-shot NMT</h2><p><img src="https://uploader.shimo.im/f/I5SzyIfYl6sfFUoA.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/v9nmM5q6jDoyq7ZR.png!thumbnail" alt="img"></p><h2 id="Transformer模型"><a href="#Transformer模型" class="headerlink" title="Transformer模型"></a>Transformer模型</h2><p><a href="https://shimo.im/docs/gPwkqCXrkJyRW89V" target="_blank" rel="noopener">https://shimo.im/docs/gPwkqCXrkJyRW89V</a></p><p>这个模型非常重要</p><p>模型 x –&gt; encoder decoder model –&gt; \hat{y}</p><p>cross entropy loss (\hat{y}, y)</p><p>训练 P(y_i | x, <strong>y_1, …, y_{i-1}</strong>) 训练的时候，我们知道y_1 … y_{i-1}</p><p>在预测的时候，我们不知道y_1 … y_{i-1}</p><p>怎么样统一训练和测试</p><h2 id="Model-Inference"><a href="#Model-Inference" class="headerlink" title="Model Inference"></a>Model Inference</h2><p>在各类文本生成任务中，其实文本的生成与训练是两种不同的情形。在训练的过程中，我们假设模型在生成下一个单词的时候知道所有之前的单词（groud truth）。然而在真正使用模型生成文本的时候，每一步生成的文本都来自于模型本身。这其中训练和预测的不同导致了模型的效果可能会很差。为了解决这一问题，人们发明了各种提升模型预测水平的方法，例如Beam Search。</p><p><strong>Beam Search</strong></p><p>Kyunghyun Cho Lecture Notes Page 94-96 <a href="https://arxiv.org/pdf/1511.07916.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1511.07916.pdf</a></p><p>Encoder(我喜欢自然语言处理) –&gt; c</p><p>Decoder(c) –&gt; y_1</p><p>Decoder(c, y_1) –&gt; y_2</p><p>Decoder(c, y_1, y_2) –&gt; y_3</p><p>…..</p><p>EOS</p><p>argmax_y P(y|x) </p><p>greedy search</p><p>argmax y_1</p><p>Beam 横梁</p><p>————————————————</p><p>一种固定宽度的装置</p><p>————————————————</p><p>在后续的课程中我们还会介绍一些别的方法用于生成文本。</p><p>美国总统和中国主席打电话</p><p>–&gt; K = 无穷大 |V|^seq_len</p><p>American, U.S. , United</p><p>….</p><p>decoding step: K</p><p>K x |V| –&gt; K</p><p>K x |V| –&gt; K</p><h2 id="开源项目"><a href="#开源项目" class="headerlink" title="开源项目"></a>开源项目</h2><p>FairSeq <a href="https://github.com/pytorch/fairseq" target="_blank" rel="noopener">https://github.com/pytorch/fairseq</a></p><p>Tensor2Tensor <a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener">https://github.com/tensorflow/tensor2tensor</a></p><p>Trax <a href="https://github.com/google/trax" target="_blank" rel="noopener">https://github.com/google/trax</a></p><h1 id="文本摘要"><a href="#文本摘要" class="headerlink" title="文本摘要"></a>文本摘要</h1><p>文本摘要这个任务定义非常简单，给定一段长文章，我们希望生成一段比较精简的文本摘要，可以覆盖整篇文章的信息。</p><p>文本摘要按照任务的定义大致可以分为两类。</p><ul><li><p>抽取式：给定一个包含多个句子的长文本，选择其中的一些句子作为短文本。这本质上是个分类问题，也就是判断哪些句子需要保留，哪些句子需要丢弃。<strong>二分类任务</strong></p></li><li><p>生成式：与抽取式文本摘要不同，这里我们不仅仅是希望选出一些句子，而是希望能够总结归纳文本的信息，用自己的话复述一遍。<strong>直接上transformer模型</strong></p></li></ul><p>gold standard</p><p>评估手段: <strong>ROUGE</strong></p><p>ROUGE评估的是系统生成文本和参考文本之间 n-gram overlap 的 recall。</p><p><strong>Candidate</strong> Summary</p><p>the cat was found under the bed</p><p><strong>Reference</strong> Summary</p><p>the cat was under the bed</p><p>针对这一个例子，ROUGE-1分数为1， ROUGE-2为4/5。</p><p>s: the cat was found under the bed</p><p>p: <strong>the cat was under the bed</strong></p><p>ROUGE-L，基于 longest common subsequence的F1 score</p><p>例如上面这个案例 LCS  = 6</p><p>P = 6/7 </p><p>R = 6/6</p><p>F1 = 2 / (6/6 + 7/6 )  = 12/13</p><p>harmoic mean</p><p><img src="https://uploader.shimo.im/f/oNU6qvIsXX8cK7Ta.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/65Raa96bt7kl0ldv.png!thumbnail" alt="img"></p><p><a href="https://arxiv.org/pdf/1908.08345.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1908.08345.pdf</a></p><p><img src="https://uploader.shimo.im/f/8YQylm0VFkgRXqzU.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/vC7ZiGFfsBInlMtQ.png!thumbnail" alt="img"></p><p>上期学员的博客</p><p><a href="https://blog.csdn.net/Chen_Meng_/article/details/103756716" target="_blank" rel="noopener">https://blog.csdn.net/Chen_Meng_/article/details/103756716</a></p><p>CopyNet</p><p><a href="https://arxiv.org/pdf/1603.06393.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.06393.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GRU </tag>
            
            <tag> BLUE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sentiment情感分析代码注释</title>
      <link href="/2020/03/28/sentiment%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E4%BB%A3%E7%A0%81%E6%B3%A8%E9%87%8A/"/>
      <url>/2020/03/28/sentiment%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E4%BB%A3%E7%A0%81%E6%B3%A8%E9%87%8A/</url>
      
        <content type="html"><![CDATA[<h2 id="情感分析"><a href="#情感分析" class="headerlink" title="情感分析"></a>情感分析</h2><h3 id="第一步：导入豆瓣电影数据集，只有训练集和测试集"><a href="#第一步：导入豆瓣电影数据集，只有训练集和测试集" class="headerlink" title="第一步：导入豆瓣电影数据集，只有训练集和测试集"></a>第一步：导入豆瓣电影数据集，只有训练集和测试集</h3><ul><li><p>TorchText中的一个重要概念是<code>Field</code>。<code>Field</code>决定了你的数据会被怎样处理。在我们的情感分类任务中，我们所需要接触到的数据有文本字符串和两种情感，”pos”或者”neg”。</p></li><li><p><code>Field</code>的参数制定了数据会被怎样处理。</p></li><li><p>我们使用<code>TEXT</code> field来定义如何处理电影评论，使用<code>LABEL</code> field来处理两个情感类别。</p></li><li><p>我们的<code>TEXT</code> field带有<code>tokenize=&#39;spacy&#39;</code>，这表示我们会用<a href="https://spacy.io/" target="_blank" rel="noopener">spaCy</a> tokenizer来tokenize英文句子。如果我们不特别声明<code>tokenize</code>这个参数，那么默认的分词方法是使用空格。</p></li><li><p>安装spaCy</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install -U spacy</span><br><span class="line">python -m spacy download en</span><br></pre></td></tr></table></figure></li><li><p><code>LABEL</code>由<code>LabelField</code>定义。这是一种特别的用来处理label的<code>Field</code>。我们后面会解释dtype。</p></li><li><p>更多关于<code>Fields</code>，参见<a href="https://github.com/pytorch/text/blob/master/torchtext/data/field.py" target="_blank" rel="noopener">https://github.com/pytorch/text/blob/master/torchtext/data/field.py</a></p></li><li><p>和之前一样，我们会设定random seeds使实验可以复现。</p></li><li><p>TorchText支持很多常见的自然语言处理数据集。</p></li><li><p>下面的代码会自动下载IMDb数据集，然后分成train/test两个<code>torchtext.datasets</code>类别。数据被前面的<code>Fields</code>处理。IMDb数据集一共有50000电影评论，每个评论都被标注为正面的或负面的。</p></li></ul><p><strong>先了解下Spacy库：<a href="https://juejin.im/post/5971a4b9f265da6c42353332?utm_source=gold_browser_extension%5D" target="_blank" rel="noopener">spaCy介绍和使用教程</a></strong><br><strong>再了解下torchtext库：<a href="https://blog.csdn.net/u012436149/article/details/79310176" target="_blank" rel="noopener">torchtext介绍和使用教程</a>：这个新手必看，不看下面代码听不懂</strong></p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!ls</span><br></pre></td></tr></table></figure><p>In [4]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1234</span></span><br><span class="line"></span><br><span class="line">torch.manual_seed(SEED) <span class="comment">#为CPU设置随机种子</span></span><br><span class="line">torch.cuda.manual_seed(SEED)<span class="comment">#为GPU设置随机种子</span></span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span>  <span class="comment">#在程序刚开始加这条语句可以提升一点训练速度，没什么额外开销。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#首先，我们要创建两个Field 对象：这两个对象包含了我们打算如何预处理文本数据的信息。</span></span><br><span class="line">TEXT = data.Field(tokenize=<span class="string">'spacy'</span>)</span><br><span class="line"><span class="comment">#torchtext.data.Field : 用来定义字段的处理方法（文本字段，标签字段）</span></span><br><span class="line"><span class="comment"># spaCy:英语分词器,类似于NLTK库，如果没有传递tokenize参数，则默认只是在空格上拆分字符串。</span></span><br><span class="line">LABEL = data.LabelField(dtype=torch.float)</span><br><span class="line"><span class="comment">#LabelField是Field类的一个特殊子集，专门用于处理标签。</span></span><br></pre></td></tr></table></figure><p>In [2]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> datasets</span><br><span class="line">train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)</span><br><span class="line"><span class="comment"># 加载豆瓣电影评论数据集</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">downloading aclImdb_v1.tar.gz</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aclImdb_v1.tar.gz: <span class="number">100</span>%|██████████| <span class="number">84.1</span>M/<span class="number">84.1</span>M [<span class="number">00</span>:<span class="number">03</span>&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">22.8</span>MB/s]</span><br></pre></td></tr></table></figure><p>In [3]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(vars(train_data.examples[<span class="number">0</span>])) <span class="comment">#可以查看数据集长啥样子</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&apos;text&apos;: [&apos;This&apos;, &apos;movie&apos;, &apos;is&apos;, &apos;visually&apos;, &apos;stunning&apos;, &apos;.&apos;, &apos;Who&apos;, &apos;cares&apos;, &apos;if&apos;, &apos;she&apos;, &apos;can&apos;, &apos;act&apos;, &apos;or&apos;, &apos;not&apos;, &apos;.&apos;, &apos;Each&apos;, &apos;scene&apos;, &apos;is&apos;, &apos;a&apos;, &apos;work&apos;, &apos;of&apos;, &apos;art&apos;, &apos;composed&apos;, &apos;and&apos;, &apos;captured&apos;, &apos;by&apos;, &apos;John&apos;, &apos;Derek&apos;, &apos;.&apos;, &apos;The&apos;, &apos;locations&apos;, &apos;,&apos;, &apos;set&apos;, &apos;designs&apos;, &apos;,&apos;, &apos;and&apos;, &apos;costumes&apos;, &apos;function&apos;, &apos;perfectly&apos;, &apos;to&apos;, &apos;convey&apos;, &apos;what&apos;, &apos;is&apos;, &apos;found&apos;, &apos;in&apos;, &apos;a&apos;, &apos;love&apos;, &apos;story&apos;, &apos;comprised&apos;, &apos;of&apos;, &apos;beauty&apos;, &apos;,&apos;, &apos;youth&apos;, &apos;and&apos;, &apos;wealth&apos;, &apos;.&apos;, &apos;In&apos;, &apos;some&apos;, &apos;ways&apos;, &apos;I&apos;, &apos;would&apos;, &apos;like&apos;, &apos;to&apos;, &apos;see&apos;, &apos;this&apos;, &apos;movie&apos;, &apos;as&apos;, &apos;a&apos;, &apos;tribute&apos;, &apos;to&apos;, &apos;John&apos;, &apos;and&apos;, &apos;Bo&apos;, &apos;Derek&apos;, &quot;&apos;s&quot;, &apos;story&apos;, &apos;.&apos;, &apos;And&apos;, &apos;...&apos;, &apos;this&apos;, &apos;commentary&apos;, &apos;would&apos;, &apos;not&apos;, &apos;be&apos;, &apos;complete&apos;, &apos;without&apos;, &apos;mentioning&apos;, &apos;Anthony&apos;, &apos;Quinn&apos;, &quot;&apos;s&quot;, &apos;role&apos;, &apos;as&apos;, &apos;father&apos;, &apos;,&apos;, &apos;mentor&apos;, &apos;,&apos;, &apos;lover&apos;, &apos;,&apos;, &apos;and&apos;, &apos;his&apos;, &apos;portrayal&apos;, &apos;of&apos;, &apos;a&apos;, &apos;man&apos;, &apos;,&apos;, &apos;of&apos;, &apos;men&apos;, &apos;,&apos;, &apos;lost&apos;, &apos;to&apos;, &apos;a&apos;, &apos;bygone&apos;, &apos;era&apos;, &apos;when&apos;, &apos;men&apos;, &apos;were&apos;, &apos;men&apos;, &apos;.&apos;, &apos;There&apos;, &apos;are&apos;, &apos;some&apos;, &apos;of&apos;, &apos;us&apos;, &apos;who&apos;, &apos;find&apos;, &apos;value&apos;, &apos;in&apos;, &apos;strength&apos;, &apos;and&apos;, &apos;direction&apos;, &apos;wrapped&apos;, &apos;in&apos;, &apos;a&apos;, &apos;confidence&apos;, &apos;that&apos;, &apos;contributes&apos;, &apos;to&apos;, &apos;a&apos;, &apos;sense&apos;, &apos;of&apos;, &apos;confidence&apos;, &apos;,&apos;, &apos;containment&apos;, &apos;,&apos;, &apos;and&apos;, &apos;security&apos;, &apos;.&apos;, &apos;Yes&apos;, &apos;,&apos;, &apos;they&apos;, &apos;do&apos;, &apos;not&apos;, &apos;make&apos;, &apos;men&apos;, &apos;like&apos;, &apos;that&apos;, &apos;anymore&apos;, &apos;!&apos;, &apos;But&apos;, &apos;,&apos;, &apos;then&apos;, &apos;how&apos;, &apos;often&apos;, &apos;do&apos;, &apos;you&apos;, &apos;find&apos;, &apos;women&apos;, &apos;who&apos;, &apos;are&apos;, &apos;made&apos;, &apos;like&apos;, &apos;Bo&apos;, &apos;Derek&apos;, &apos;.&apos;], &apos;label&apos;: &apos;pos&apos;&#125;</span><br></pre></td></tr></table></figure><h2 id="第二步：训练集划分为训练集和验证集"><a href="#第二步：训练集划分为训练集和验证集" class="headerlink" title="第二步：训练集划分为训练集和验证集"></a>第二步：训练集划分为训练集和验证集</h2><ul><li>由于我们现在只有train/test这两个分类，所以我们需要创建一个新的validation set。我们可以使用<code>.split()</code>创建新的分类。</li><li>默认的数据分割是 70、30，如果我们声明<code>split_ratio</code>，可以改变split之间的比例，<code>split_ratio=0.8</code>表示80%的数据是训练集，20%是验证集。</li><li>我们还声明<code>random_state</code>这个参数，确保我们每次分割的数据集都是一样的。</li></ul><p>In [4]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">train_data, valid_data = train_data.split(random_state=random.seed(SEED)) <span class="comment">#默认split_ratio=0.7</span></span><br></pre></td></tr></table></figure><p>In [5]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f'Number of training examples: <span class="subst">&#123;len(train_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Number of validation examples: <span class="subst">&#123;len(valid_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Number of testing examples: <span class="subst">&#123;len(test_data)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Number of training examples: <span class="number">17500</span></span><br><span class="line">Number of validation examples: <span class="number">7500</span></span><br><span class="line">Number of testing examples: <span class="number">25000</span></span><br></pre></td></tr></table></figure><h2 id="第三步：用训练集建立vocabulary，就是把每个单词一一映射到一个数字。"><a href="#第三步：用训练集建立vocabulary，就是把每个单词一一映射到一个数字。" class="headerlink" title="第三步：用训练集建立vocabulary，就是把每个单词一一映射到一个数字。"></a>第三步：用训练集建立vocabulary，就是把每个单词一一映射到一个数字。</h2><ul><li>下一步我们需要创建 <em>vocabulary</em> 。<em>vocabulary</em> 就是把每个单词一一映射到一个数字。<img src="file:///Users/mmy/Downloads/assets/sentiment5.png" alt="img"></li><li>我们使用最常见的25k个单词来构建我们的单词表，用<code>max_size</code>这个参数可以做到这一点。</li><li>所有其他的单词都用<code>&lt;unk&gt;</code>来表示。</li></ul><p>In [6]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TEXT.build_vocab(train_data, max_size=25000)</span></span><br><span class="line"><span class="comment"># LABEL.build_vocab(train_data)</span></span><br><span class="line">TEXT.build_vocab(train_data, max_size=<span class="number">25000</span>, vectors=<span class="string">"glove.6B.100d"</span>, unk_init=torch.Tensor.normal_)</span><br><span class="line"><span class="comment">#从预训练的词向量（vectors） 中，将当前(corpus语料库)词汇表的词向量抽取出来，构成当前 corpus 的 Vocab（词汇表）。</span></span><br><span class="line"><span class="comment">#预训练的 vectors 来自glove模型，每个单词有100维。glove模型训练的词向量参数来自很大的语料库，</span></span><br><span class="line"><span class="comment">#而我们的电影评论的语料库小一点，所以词向量需要更新，glove的词向量适合用做初始化参数。</span></span><br><span class="line">LABEL.build_vocab(train_data)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.vector_cache/glove<span class="number">.6</span>B.zip: <span class="number">862</span>MB [<span class="number">00</span>:<span class="number">23</span>, <span class="number">36.0</span>MB/s]                               </span><br><span class="line"><span class="number">100</span>%|█████████▉| <span class="number">399597</span>/<span class="number">400000</span> [<span class="number">00</span>:<span class="number">25</span>&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">16569.01</span>it/s]</span><br></pre></td></tr></table></figure><p>In [7]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f"Unique tokens in TEXT vocabulary: <span class="subst">&#123;len(TEXT.vocab)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Unique tokens in LABEL vocabulary: <span class="subst">&#123;len(LABEL.vocab)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Unique tokens in TEXT vocabulary: 25002</span><br><span class="line">Unique tokens in LABEL vocabulary: 2</span><br></pre></td></tr></table></figure><p>In [8]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(list(LABEL.vocab.stoi.items())) <span class="comment"># 只有两个类别值</span></span><br><span class="line">print(list(TEXT.vocab.stoi.items())[:<span class="number">20</span>])</span><br><span class="line"><span class="comment">#语料库单词频率越高，索引越靠前。前两个默认为unk和pad。</span></span><br><span class="line">print(<span class="string">"------"</span>*<span class="number">10</span>)</span><br><span class="line">print(TEXT.vocab.freqs.most_common(<span class="number">20</span>))</span><br><span class="line"><span class="comment"># 这里可以看到unk和pad没有计数</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;neg&apos;, 0), (&apos;pos&apos;, 1)]</span><br><span class="line">[(&apos;&lt;unk&gt;&apos;, 0), (&apos;&lt;pad&gt;&apos;, 1), (&apos;the&apos;, 2), (&apos;,&apos;, 3), (&apos;.&apos;, 4), (&apos;a&apos;, 5), (&apos;and&apos;, 6), (&apos;of&apos;, 7), (&apos;to&apos;, 8), (&apos;is&apos;, 9), (&apos;in&apos;, 10), (&apos;I&apos;, 11), (&apos;it&apos;, 12), (&apos;that&apos;, 13), (&apos;&quot;&apos;, 14), (&quot;&apos;s&quot;, 15), (&apos;this&apos;, 16), (&apos;-&apos;, 17), (&apos;/&gt;&lt;br&apos;, 18), (&apos;was&apos;, 19)]</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">[(&apos;the&apos;, 201815), (&apos;,&apos;, 192511), (&apos;.&apos;, 165127), (&apos;a&apos;, 109096), (&apos;and&apos;, 108875), (&apos;of&apos;, 100402), (&apos;to&apos;, 93905), (&apos;is&apos;, 76001), (&apos;in&apos;, 61097), (&apos;I&apos;, 54439), (&apos;it&apos;, 53649), (&apos;that&apos;, 49325), (&apos;&quot;&apos;, 44431), (&quot;&apos;s&quot;, 43359), (&apos;this&apos;, 42423), (&apos;-&apos;, 37142), (&apos;/&gt;&lt;br&apos;, 35613), (&apos;was&apos;, 34947), (&apos;as&apos;, 30412), (&apos;movie&apos;, 29873)]</span><br></pre></td></tr></table></figure><p>In [9]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(TEXT.vocab.itos[:10]) #查看TEXT单词表</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&apos;&lt;unk&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;the&apos;, &apos;,&apos;, &apos;.&apos;, &apos;a&apos;, &apos;and&apos;, &apos;of&apos;, &apos;to&apos;, &apos;is&apos;]</span><br></pre></td></tr></table></figure><h2 id="第四步：创建iterators，每个itartion都会返回一个batch的样本。"><a href="#第四步：创建iterators，每个itartion都会返回一个batch的样本。" class="headerlink" title="第四步：创建iterators，每个itartion都会返回一个batch的样本。"></a>第四步：创建iterators，每个itartion都会返回一个batch的样本。</h2><ul><li>最后一步数据的准备是创建iterators。每个itartion都会返回一个batch的examples。</li><li>我们会使用<code>BucketIterator</code>。<code>BucketIterator</code>会把长度差不多的句子放到同一个batch中，确保每个batch中不出现太多的padding。</li><li>严格来说，我们这份notebook中的模型代码都有一个问题，也就是我们把<code>&lt;pad&gt;</code>也当做了模型的输入进行训练。更好的做法是在模型中把由<code>&lt;pad&gt;</code>产生的输出给消除掉。在这节课中我们简单处理，直接把<code>&lt;pad&gt;</code>也用作模型输入了。由于<code>&lt;pad&gt;</code>数量不多，模型的效果也不差。</li><li>如果我们有GPU，还可以指定每个iteration返回的tensor都在GPU上。</li></ul><p>In [11]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#相当于把样本划分batch，把相等长度的单词尽可能的划分到一个batch，不够长的就用padding。</span></span><br><span class="line">train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(</span><br><span class="line">    (train_data, valid_data, test_data), </span><br><span class="line">    batch_size=BATCH_SIZE,</span><br><span class="line">    device=device)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Iterator：标准迭代器</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">BucketIerator：相比于标准迭代器，会将类似长度的样本当做一批来处理，</span></span><br><span class="line"><span class="string">因为在文本处理中经常会需要将每一批样本长度补齐为当前批中最长序列的长度，</span></span><br><span class="line"><span class="string">因此当样本长度差别较大时，使用BucketIerator可以带来填充效率的提高。</span></span><br><span class="line"><span class="string">除此之外，我们还可以在Field中通过fix_length参数来对样本进行截断补齐操作。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">BPTTIterator: 基于BPTT(基于时间的反向传播算法)的迭代器，一般用于语言模型中。</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><p>Out[11]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;\nIterator：标准迭代器\n\nBucketIerator：相比于标准迭代器，会将类似长度的样本当做一批来处理，\n因为在文本处理中经常会需要将每一批样本长度补齐为当前批中最长序列的长度，\n因此当样本长度差别较大时，使用BucketIerator可以带来填充效率的提高。\n除此之外，我们还可以在Field中通过fix_length参数来对样本进行截断补齐操作。\n\nBPTTIterator: 基于BPTT(基于时间的反向传播算法)的迭代器，一般用于语言模型中。\n&apos;</span><br></pre></td></tr></table></figure><p>In [12]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(next(iter(train_iterator)).label.shape)</span><br><span class="line">print(next(iter(train_iterator)).text.shape)<span class="comment"># </span></span><br><span class="line"><span class="comment"># 多运行一次可以发现一条评论的单词长度会变</span></span><br><span class="line"><span class="comment"># 下面text的维度983*64，983为一条评论的单词长度</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">64</span>])</span><br><span class="line">torch.Size([<span class="number">983</span>, <span class="number">64</span>])</span><br></pre></td></tr></table></figure><p>In [13]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取出一句评论</span></span><br><span class="line">batch = next(iter(train_iterator))</span><br><span class="line">print(batch.text.shape) </span><br><span class="line">print([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.text[:,<span class="number">0</span>]])</span><br><span class="line"><span class="comment"># 可以看到这句话的长度是1077，最后面有很多pad</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([1077, 64])</span><br><span class="line">[&apos;It&apos;, &apos;was&apos;, &apos;interesting&apos;, &apos;to&apos;, &apos;see&apos;, &apos;how&apos;, &apos;accurate&apos;, &apos;the&apos;, &apos;writing&apos;, &apos;was&apos;, &apos;on&apos;, &apos;the&apos;, &apos;geek&apos;, &apos;buzz&apos;, &apos;words&apos;, &apos;,&apos;, &apos;yet&apos;, &apos;very&apos;, &apos;naive&apos;, &apos;on&apos;, &apos;the&apos;, &apos;corporate&apos;, &apos;world&apos;, &apos;.&apos;, &apos;The&apos;, &apos;Justice&apos;, &apos;Department&apos;, &apos;would&apos;, &apos;catch&apos;, &apos;more&apos;, &apos;of&apos;, &apos;the&apos;, &apos;big&apos;, &apos;&lt;unk&gt;&apos;, &apos;giants&apos;, &apos;if&apos;, &apos;they&apos;, &apos;did&apos;, &apos;such&apos;, &apos;naive&apos;, &apos;things&apos;, &apos;to&apos;, &apos;win&apos;, &apos;.&apos;, &apos;The&apos;, &apos;real&apos;, &apos;corporate&apos;, &apos;world&apos;, &apos;is&apos;, &apos;much&apos;, &apos;more&apos;, &apos;subtle&apos;, &apos;and&apos;, &apos;interesting&apos;, &apos;,&apos;, &apos;yet&apos;, &apos;every&apos;, &apos;bit&apos;, &apos;as&apos;, &apos;sinister&apos;, &apos;.&apos;, &apos;I&apos;, &apos;seriously&apos;, &apos;doubt&apos;, &apos;ANY&apos;, &apos;&lt;unk&gt;&apos;, &apos;would&apos;, &apos;actually&apos;, &apos;kill&apos;, &apos;someone&apos;, &apos;directly&apos;, &apos;;&apos;, &apos;even&apos;, &apos;the&apos;, &apos;&lt;unk&gt;&apos;, &apos;is&apos;, &apos;more&apos;, &apos;&lt;unk&gt;&apos;, &apos;these&apos;, &apos;days&apos;, &apos;.&apos;, &apos;In&apos;, &apos;the&apos;, &apos;real&apos;, &apos;world&apos;, &apos;,&apos;, &apos;they&apos;, &apos;do&apos;, &apos;kill&apos;, &apos;people&apos;, &apos;with&apos;, &apos;&lt;unk&gt;&apos;, &apos;,&apos;, &apos;pollution&apos;, &apos;,&apos;, &apos;&lt;unk&gt;&apos;, &apos;,&apos;, &apos;&lt;unk&gt;&apos;, &apos;,&apos;, &apos;etc&apos;, &apos;.&apos;, &apos;This&apos;, &apos;movie&apos;, &apos;must&apos;, &apos;have&apos;, &apos;been&apos;, &apos;developed&apos;, &apos;by&apos;, &apos;some&apos;, &apos;garage&apos;, &apos;geeks&apos;, &apos;,&apos;, &apos;I&apos;, &apos;think&apos;, &apos;,&apos;, &apos;and&apos;, &apos;the&apos;, &apos;studios&apos;, &apos;did&apos;, &quot;n&apos;t&quot;, &apos;know&apos;, &apos;the&apos;, &apos;difference&apos;, &apos;.&apos;, &apos;They&apos;, &apos;just&apos;, &apos;wanted&apos;, &apos;something&apos;, &apos;to&apos;, &apos;capitalize&apos;, &apos;on&apos;, &apos;the&apos;, &apos;Microsoft&apos;, &apos;&lt;unk&gt;&apos;, &apos;case&apos;, &apos;in&apos;, &apos;the&apos;, &apos;news&apos;, &apos;.&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;]</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="第五步：创建Word-Averaging模型"><a href="#第五步：创建Word-Averaging模型" class="headerlink" title="第五步：创建Word Averaging模型"></a>第五步：创建Word Averaging模型</h2><h3 id="Word-Averaging模型"><a href="#Word-Averaging模型" class="headerlink" title="Word Averaging模型"></a>Word Averaging模型</h3><ul><li>我们首先介绍一个简单的Word Averaging模型。这个模型非常简单，我们把每个单词都通过<code>Embedding</code>层投射成word embedding vector，然后把一句话中的所有word vector做个平均，就是整个句子的vector表示了。接下来把这个sentence vector传入一个<code>Linear</code>层，做分类即可。</li></ul><p><img src="file:///Users/mmy/Downloads/assets/sentiment8.png" alt="img"></p><ul><li>我们使用<a href="https://pytorch.org/docs/stable/nn.html?highlight=avg_pool2d#torch.nn.functional.avg_pool2d" target="_blank" rel="noopener"><code>avg_pool2d</code></a>来做average pooling。我们的目标是把sentence length那个维度平均成1，然后保留embedding这个维度。</li></ul><p><img src="file:///Users/mmy/Downloads/assets/sentiment9.png" alt="img"></p><ul><li><code>avg_pool2d</code>的kernel size是 (<code>embedded.shape[1]</code>, 1)，所以句子长度的那个维度会被压扁。</li></ul><p><img src="file:///Users/mmy/Downloads/assets/sentiment10.png" alt="img"></p><p><img src="file:///Users/mmy/Downloads/assets/sentiment11.png" alt="img"></p><p>In [5]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordAVGModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, output_dim, pad_idx)</span>:</span></span><br><span class="line">        <span class="comment">#初始化参数，</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">        <span class="comment">#vocab_size=词汇表长度=25002，embedding_dim=每个单词的维度=100</span></span><br><span class="line">        <span class="comment">#padding_idx：如果提供的话，这里如果遇到padding的单词就用0填充。</span></span><br><span class="line">        </span><br><span class="line">        self.fc = nn.Linear(embedding_dim, output_dim)</span><br><span class="line">        <span class="comment">#output_dim输出的维度，一个数就可以了，=1</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="comment"># text.shape = (seq_len,batch_size)</span></span><br><span class="line">        <span class="comment"># text下面会指定，为一个batch的数据，seq_len为一条评论的单词长度</span></span><br><span class="line">        embedded = self.embedding(text) </span><br><span class="line">        <span class="comment"># embedded = [seq_len, batch_size, embedding_dim] </span></span><br><span class="line">        embedded = embedded.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>) </span><br><span class="line">        <span class="comment"># [batch_size, seq_len, embedding_dim]更换顺序</span></span><br><span class="line">        </span><br><span class="line">        pooled = F.avg_pool2d(embedded, (embedded.shape[<span class="number">1</span>], <span class="number">1</span>)).squeeze(<span class="number">1</span>) </span><br><span class="line">        <span class="comment"># [batch size, embedding_dim] 把单词长度的维度压扁为1，并降维</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.fc(pooled)  </span><br><span class="line">        <span class="comment">#（batch size, embedding_dim）*（embedding_dim, output_dim）=（batch size,output_dim）</span></span><br></pre></td></tr></table></figure><p>In [6]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = len(TEXT.vocab) <span class="comment">#25002</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">OUTPUT_DIM = <span class="number">1</span> <span class="comment"># 大于某个值是正，小于是负</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] </span><br><span class="line"><span class="comment"># TEXT.pad_token = pad</span></span><br><span class="line"><span class="comment"># PAD_IDX = 1 为pad的索引</span></span><br><span class="line"></span><br><span class="line">model = WordAVGModel(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">---------------------------------------------------------------------------</span><br><span class="line">AttributeError                            Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-6</span>-d9889c88c56d&gt; <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">----&gt; 1 INPUT_DIM = len(TEXT.vocab) #25002</span><br><span class="line">      <span class="number">2</span> EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">      <span class="number">3</span> OUTPUT_DIM = <span class="number">1</span> <span class="comment"># 大于某个值是正，小于是负</span></span><br><span class="line">      <span class="number">4</span> PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</span><br><span class="line">      <span class="number">5</span> <span class="comment"># TEXT.pad_token = pad</span></span><br><span class="line"></span><br><span class="line">AttributeError: <span class="string">'Field'</span> object has no attribute <span class="string">'vocab'</span></span><br></pre></td></tr></table></figure><p>In [16]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TEXT.pad_token</span><br></pre></td></tr></table></figure><p>Out[16]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;&lt;pad&gt;&apos;</span><br></pre></td></tr></table></figure><p>In [17]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_parameters</span><span class="params">(model)</span>:</span> <span class="comment">#统计参数，可以不用管</span></span><br><span class="line">    <span class="keyword">return</span> sum(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br><span class="line"><span class="comment"># &#123;&#125;大括号里调用了函数</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The model has 2,500,301 trainable parameters</span><br></pre></td></tr></table></figure><h2 id="第六步：初始化参数"><a href="#第六步：初始化参数" class="headerlink" title="第六步：初始化参数"></a>第六步：初始化参数</h2><p>In [18]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把模型参数初始化成glove的向量参数</span></span><br><span class="line">pretrained_embeddings = TEXT.vocab.vectors  <span class="comment"># 取出glove embedding词向量的参数</span></span><br><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings) <span class="comment">#遇到_的语句直接替换，不需要另外赋值=</span></span><br><span class="line"><span class="comment">#把上面vectors="glove.6B.100d"取出的词向量作为初始化参数，数量为25000*100个参数</span></span><br></pre></td></tr></table></figure><p>Out[18]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],</span><br><span class="line">        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],</span><br><span class="line">        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],</span><br><span class="line">        ...,</span><br><span class="line">        [-0.1419,  0.0282,  0.2185,  ..., -0.1100, -0.1250,  0.0282],</span><br><span class="line">        [-0.3326, -0.9215,  0.9239,  ...,  0.5057, -1.2898,  0.1782],</span><br><span class="line">        [-0.8304,  0.3732,  0.0726,  ..., -0.0122,  0.2313, -0.2783]])</span><br></pre></td></tr></table></figure><p>In [19]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token] <span class="comment"># UNK_IDX=0</span></span><br><span class="line"></span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM) <span class="comment">#</span></span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line"><span class="comment">#词汇表25002个单词，前两个unk和pad也需要初始化成EMBEDDING_DIM维的向量</span></span><br></pre></td></tr></table></figure><h2 id="第七步：训练模型"><a href="#第七步：训练模型" class="headerlink" title="第七步：训练模型"></a>第七步：训练模型</h2><p>In [20]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.parameters()) <span class="comment">#定义优化器</span></span><br><span class="line">criterion = nn.BCEWithLogitsLoss()  <span class="comment">#定义损失函数，这个BCEWithLogitsLoss特殊情况，二分类损失函数</span></span><br><span class="line"><span class="comment"># nn.BCEWithLogitsLoss()看这个：https://blog.csdn.net/qq_22210253/article/details/85222093</span></span><br><span class="line">model = model.to(device) <span class="comment">#送到gpu上去</span></span><br><span class="line">criterion = criterion.to(device) <span class="comment">#送到gpu上去</span></span><br></pre></td></tr></table></figure><p>计算预测的准确率</p><p>In [21]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_accuracy</span><span class="params">(preds, y)</span>:</span> <span class="comment">#计算准确率</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#round predictions to the closest integer</span></span><br><span class="line">    rounded_preds = torch.round(torch.sigmoid(preds))</span><br><span class="line">    <span class="comment">#.round函数：四舍五入</span></span><br><span class="line">    </span><br><span class="line">    correct = (rounded_preds == y).float() <span class="comment">#convert into float for division </span></span><br><span class="line">    acc = correct.sum()/len(correct)</span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure><p>In [22]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, iterator, optimizer, criterion)</span>:</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line">    total_len = <span class="number">0</span></span><br><span class="line">    model.train() <span class="comment">#model.train()代表了训练模式</span></span><br><span class="line">    <span class="comment">#这步一定要加，是为了区分model训练和测试的模式的。</span></span><br><span class="line">    <span class="comment">#有时候训练时会用到dropout、归一化等方法，但是测试的时候不能用dropout等方法。</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> iterator: <span class="comment">#iterator为train_iterator</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment">#加这步防止梯度叠加</span></span><br><span class="line">        </span><br><span class="line">        predictions = model(batch.text).squeeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#batch.text 就是上面forward函数的参数text</span></span><br><span class="line">        <span class="comment"># squeeze(1)压缩维度，不然跟batch.label维度对不上</span></span><br><span class="line">        </span><br><span class="line">        loss = criterion(predictions, batch.label)</span><br><span class="line">        acc = binary_accuracy(predictions, batch.label)</span><br><span class="line">        <span class="comment"># 每次迭代都计算一边准确率</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        loss.backward() <span class="comment">#反向传播</span></span><br><span class="line">        optimizer.step() <span class="comment">#梯度下降</span></span><br><span class="line">        </span><br><span class="line">        epoch_loss += loss.item() * len(batch.label)</span><br><span class="line">        <span class="comment">#二分类损失函数loss因为已经平均化了，这里需要乘以len(batch.label)，</span></span><br><span class="line">        <span class="comment">#得到一个batch的损失，累加得到所有样本损失。</span></span><br><span class="line">        </span><br><span class="line">        epoch_acc += acc.item() * len(batch.label)</span><br><span class="line">        <span class="comment">#（acc.item()：一个batch的正确率） *batch数 = 正确数</span></span><br><span class="line">        <span class="comment"># 累加得到所有训练样本正确数。</span></span><br><span class="line">        </span><br><span class="line">        total_len += len(batch.label)</span><br><span class="line">        <span class="comment">#计算train_iterator所有样本的数量，不出意外应该是17500</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> epoch_loss / total_len, epoch_acc / total_len</span><br><span class="line">    <span class="comment">#epoch_loss / total_len ：train_iterator所有batch的平均损失</span></span><br><span class="line">    <span class="comment">#epoch_acc / total_len ：train_iterator所有batch的平均正确率</span></span><br></pre></td></tr></table></figure><p>In [23]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, iterator, criterion)</span>:</span></span><br><span class="line">     </span><br><span class="line">    </span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line">    total_len = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    model.eval()</span><br><span class="line">    <span class="comment">#转换成测试模式，冻结dropout层或其他层。</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> iterator: </span><br><span class="line">            <span class="comment">#iterator为valid_iterator</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">#没有反向传播和梯度下降</span></span><br><span class="line">            predictions = model(batch.text).squeeze(<span class="number">1</span>)</span><br><span class="line">            loss = criterion(predictions, batch.label)</span><br><span class="line">            acc = binary_accuracy(predictions, batch.label)</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            epoch_loss += loss.item() * len(batch.label)</span><br><span class="line">            epoch_acc += acc.item() * len(batch.label)</span><br><span class="line">            total_len += len(batch.label)</span><br><span class="line">    model.train() <span class="comment">#调回训练模式   </span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> epoch_loss / total_len, epoch_acc / total_len</span><br></pre></td></tr></table></figure><p>In [24]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">epoch_time</span><span class="params">(start_time, end_time)</span>:</span>  <span class="comment">#查看每个epoch的时间</span></span><br><span class="line">    elapsed_time = end_time - start_time</span><br><span class="line">    elapsed_mins = int(elapsed_time / <span class="number">60</span>)</span><br><span class="line">    elapsed_secs = int(elapsed_time - (elapsed_mins * <span class="number">60</span>))</span><br><span class="line">    <span class="keyword">return</span> elapsed_mins, elapsed_secs</span><br></pre></td></tr></table></figure><h2 id="第八步：查看模型运行结果"><a href="#第八步：查看模型运行结果" class="headerlink" title="第八步：查看模型运行结果"></a>第八步：查看模型运行结果</h2><p>In [25]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同上，这里用的kaggleGPU跑的，花了2分钟。</span></span><br><span class="line">N_EPOCHS = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>) <span class="comment">#无穷大</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    </span><br><span class="line">    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">    <span class="comment"># 得到训练集每个epoch的平均损失和准确率</span></span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line">    <span class="comment"># 得到验证集每个epoch的平均损失和准确率，这个model里传入的参数是训练完的参数</span></span><br><span class="line">    </span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss: <span class="comment">#只要模型效果变好，就存模型</span></span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'wordavg-model.pt'</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 01 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.684 | Train Acc: 58.78%</span><br><span class="line"> Val. Loss: 0.617 |  Val. Acc: 72.51%</span><br><span class="line">Epoch: 02 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.642 | Train Acc: 72.62%</span><br><span class="line"> Val. Loss: 0.504 |  Val. Acc: 76.65%</span><br><span class="line">Epoch: 03 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.569 | Train Acc: 78.81%</span><br><span class="line"> Val. Loss: 0.439 |  Val. Acc: 81.07%</span><br><span class="line">Epoch: 04 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.497 | Train Acc: 82.97%</span><br><span class="line"> Val. Loss: 0.404 |  Val. Acc: 84.03%</span><br><span class="line">Epoch: 05 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.435 | Train Acc: 85.95%</span><br><span class="line"> Val. Loss: 0.400 |  Val. Acc: 85.69%</span><br><span class="line">Epoch: 06 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.388 | Train Acc: 87.73%</span><br><span class="line"> Val. Loss: 0.412 |  Val. Acc: 86.80%</span><br><span class="line">Epoch: 07 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.349 | Train Acc: 88.83%</span><br><span class="line"> Val. Loss: 0.425 |  Val. Acc: 87.64%</span><br><span class="line">Epoch: 08 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.319 | Train Acc: 89.84%</span><br><span class="line"> Val. Loss: 0.446 |  Val. Acc: 87.83%</span><br><span class="line">Epoch: 09 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.293 | Train Acc: 90.54%</span><br><span class="line"> Val. Loss: 0.464 |  Val. Acc: 88.25%</span><br><span class="line">Epoch: 10 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.272 | Train Acc: 91.19%</span><br><span class="line"> Val. Loss: 0.480 |  Val. Acc: 88.68%</span><br><span class="line">Epoch: 11 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.254 | Train Acc: 91.82%</span><br><span class="line"> Val. Loss: 0.498 |  Val. Acc: 88.87%</span><br><span class="line">Epoch: 12 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.238 | Train Acc: 92.53%</span><br><span class="line"> Val. Loss: 0.517 |  Val. Acc: 89.01%</span><br><span class="line">Epoch: 13 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.222 | Train Acc: 93.03%</span><br><span class="line"> Val. Loss: 0.532 |  Val. Acc: 89.25%</span><br><span class="line">Epoch: 14 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.210 | Train Acc: 93.47%</span><br><span class="line"> Val. Loss: 0.547 |  Val. Acc: 89.44%</span><br><span class="line">Epoch: 15 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.198 | Train Acc: 93.95%</span><br><span class="line"> Val. Loss: 0.564 |  Val. Acc: 89.49%</span><br><span class="line">Epoch: 16 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.186 | Train Acc: 94.31%</span><br><span class="line"> Val. Loss: 0.582 |  Val. Acc: 89.68%</span><br><span class="line">Epoch: 17 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.175 | Train Acc: 94.74%</span><br><span class="line"> Val. Loss: 0.596 |  Val. Acc: 89.69%</span><br><span class="line">Epoch: 18 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.166 | Train Acc: 95.09%</span><br><span class="line"> Val. Loss: 0.615 |  Val. Acc: 89.95%</span><br><span class="line">Epoch: 19 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.156 | Train Acc: 95.36%</span><br><span class="line"> Val. Loss: 0.631 |  Val. Acc: 89.91%</span><br><span class="line">Epoch: 20 | Epoch Time: 0m 5s</span><br><span class="line">Train Loss: 0.147 | Train Acc: 95.75%</span><br><span class="line"> Val. Loss: 0.647 |  Val. Acc: 90.07%</span><br></pre></td></tr></table></figure><h2 id="第九步：预测结果"><a href="#第九步：预测结果" class="headerlink" title="第九步：预测结果"></a>第九步：预测结果</h2><p>In [26]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!ls</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__notebook_source__.ipynb  wordavg-model.pt</span><br></pre></td></tr></table></figure><p>In [55]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kaggle上下载模型文件到本地，运行下面代码，点击输出的链接就行</span></span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> HTML</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_download_link</span><span class="params">(title = <span class="string">"Download model file"</span>, filename = <span class="string">"CNN-model.pt"</span>)</span>:</span>  </span><br><span class="line">    html = <span class="string">'&lt;a href=&#123;filename&#125;&gt;&#123;title&#125;&lt;/a&gt;'</span></span><br><span class="line">    html = html.format(title=title,filename=filename)</span><br><span class="line">    <span class="keyword">return</span> HTML(html)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a link to download the dataframe which was saved with .to_csv method</span></span><br><span class="line">create_download_link(filename=<span class="string">'wordavg-model.pt'</span>)</span><br></pre></td></tr></table></figure><p>Out[55]:</p><p><a href="file:///Users/mmy/Downloads/wordavg-model.pt" target="_blank" rel="noopener">Download model file</a></p><p>In [1]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(&quot;wordavg-model.pt&quot;))</span><br><span class="line">#用保存的模型参数预测数据</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">---------------------------------------------------------------------------</span><br><span class="line">NameError                                 Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input-1-f795a3e78d6a&gt; in &lt;module&gt;</span><br><span class="line">----&gt; 1 model.load_state_dict(torch.load(&quot;wordavg-model.pt&quot;))</span><br><span class="line">      2 #用保存的模型参数预测数据</span><br><span class="line"></span><br><span class="line">NameError: name &apos;model&apos; is not defined</span><br></pre></td></tr></table></figure><p>In [28]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy  <span class="comment">#分词工具，跟NLTK类似</span></span><br><span class="line">nlp = spacy.load(<span class="string">'en'</span>) </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_sentiment</span><span class="params">(sentence)</span>:</span> <span class="comment"># 传入预测的句子I love This film bad </span></span><br><span class="line">    tokenized = [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> nlp.tokenizer(sentence)] <span class="comment">#分词</span></span><br><span class="line">    <span class="comment"># print(tokenized) = ['I', 'love', 'This', 'film', 'bad']</span></span><br><span class="line">    indexed = [TEXT.vocab.stoi[t] <span class="keyword">for</span> t <span class="keyword">in</span> tokenized] </span><br><span class="line">    <span class="comment">#sentence的在25002中的索引</span></span><br><span class="line">    </span><br><span class="line">    tensor = torch.LongTensor(indexed).to(device) <span class="comment">#seq_len</span></span><br><span class="line">    <span class="comment"># 所有词向量都应该变成LongTensor</span></span><br><span class="line">    </span><br><span class="line">    tensor = tensor.unsqueeze(<span class="number">1</span>) </span><br><span class="line">    <span class="comment">#模型的输入是默认有batch_size的,需要升维，seq_len * batch_size（1）</span></span><br><span class="line">    </span><br><span class="line">    prediction = torch.sigmoid(model(tensor))</span><br><span class="line">    <span class="comment"># 预测准确率，在0，1之间，需要sigmoid下</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> prediction.item()</span><br></pre></td></tr></table></figure><p>In [29]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(&quot;I love This film bad&quot;)</span><br></pre></td></tr></table></figure><p>Out[29]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9373546242713928</span><br></pre></td></tr></table></figure><p>In [30]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(&quot;This film is great&quot;)</span><br></pre></td></tr></table></figure><p>Out[30]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.0</span><br></pre></td></tr></table></figure><h2 id="RNN模型"><a href="#RNN模型" class="headerlink" title="RNN模型"></a>RNN模型</h2><ul><li><p>下面我们尝试把模型换成一个</p><p>recurrent neural network</p></li></ul><p>  (RNN)。RNN经常会被用来encode一个sequence</p><p>  ℎ𝑡=RNN(𝑥𝑡,ℎ𝑡−1)ht=RNN(xt,ht−1)</p><ul><li><p>我们使用最后一个hidden state ℎ𝑇hT来表示整个句子。</p></li><li><p>然后我们把ℎ𝑇hT通过一个线性变换𝑓f，然后用来预测句子的情感。</p></li></ul><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In [32]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, hidden_dim, output_dim, </span></span></span><br><span class="line"><span class="function"><span class="params">                 n_layers, bidirectional, dropout, pad_idx)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, </span><br><span class="line">                           bidirectional=bidirectional, dropout=dropout)</span><br><span class="line">        <span class="comment">#embedding_dim：每个单词维度</span></span><br><span class="line">        <span class="comment">#hidden_dim：隐藏层维度</span></span><br><span class="line">        <span class="comment">#num_layers：神经网络深度，纵向深度</span></span><br><span class="line">        <span class="comment">#bidirectional：是否双向循环RNN</span></span><br><span class="line">        <span class="comment">#这个自己先得理解LSTM各个维度，不然容易晕，双向RNN网络图示看上面，可以借鉴下</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        self.fc = nn.Linear(hidden_dim*<span class="number">2</span>, output_dim)</span><br><span class="line">        <span class="comment"># 这里hidden_dim乘以2是因为是双向，需要拼接两个方向，跟n_layers的层数无关。</span></span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="comment"># text.shape=[seq_len, batch_size]</span></span><br><span class="line">        embedded = self.dropout(self.embedding(text)) <span class="comment">#[seq_len, batch_size, emb_dim]</span></span><br><span class="line">        output, (hidden, cell) = self.rnn(embedded)</span><br><span class="line">        <span class="comment"># output = [seq_len, batch size, hid_dim * num directions]</span></span><br><span class="line">        <span class="comment"># hidden = [num layers * num directions, batch_size, hid_dim]</span></span><br><span class="line">        <span class="comment"># cell = [num layers * num directions, batch_size, hid_dim]</span></span><br><span class="line">        <span class="comment"># 这里的num layers * num directions可以看上面图，上面图除掉输入输出层只有两层双向网络。</span></span><br><span class="line">        <span class="comment"># num layers = 2表示需要纵向上在加两层双向，总共有4层神经元。</span></span><br><span class="line">        <span class="comment"># 对于LSTM模型的任意一个时间序列t，h层的输出维度</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment">#concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers</span></span><br><span class="line">        <span class="comment">#and apply dropout</span></span><br><span class="line">        hidden = self.dropout(torch.cat((hidden[<span class="number">-2</span>,:,:], hidden[<span class="number">-1</span>,:,:]), dim=<span class="number">1</span>)) </span><br><span class="line">        <span class="comment"># hidden = [batch size, hid dim * num directions]，</span></span><br><span class="line">        <span class="comment"># 看下上面图示，最后前向和后向输出的隐藏层会concat到输出层，4层神经元最后两层作为最终的输出。</span></span><br><span class="line">        <span class="comment"># 这里因为我们只需要得到最后一个时间序列的输出，所以最终输出的hidden跟seq_len无关。</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.fc(hidden.squeeze(<span class="number">0</span>)) <span class="comment"># 在接一个全连接层，最终输出[batch size, output_dim]</span></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In [36]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = len(TEXT.vocab)</span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">HIDDEN_DIM = <span class="number">256</span></span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line">N_LAYERS = <span class="number">2</span></span><br><span class="line">BIDIRECTIONAL = <span class="literal">True</span></span><br><span class="line">DROPOUT = <span class="number">0.5</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</span><br><span class="line"></span><br><span class="line">model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, </span><br><span class="line">            N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)</span><br><span class="line">model</span><br></pre></td></tr></table></figure><p>Out[36]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">RNN(</span><br><span class="line">  (embedding): Embedding(<span class="number">25002</span>, <span class="number">100</span>, padding_idx=<span class="number">1</span>)</span><br><span class="line">  (rnn): LSTM(<span class="number">100</span>, <span class="number">256</span>, num_layers=<span class="number">2</span>, dropout=<span class="number">0.5</span>, bidirectional=<span class="literal">True</span>)</span><br><span class="line">  (fc): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (dropout): Dropout(p=<span class="number">0.5</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>In [34]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br><span class="line"><span class="comment"># 比averge model模型多了一倍的参数</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The model has 4,810,857 trainable parameters</span><br></pre></td></tr></table></figure><p>In [37]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同上初始化</span></span><br><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings)</span><br><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]</span><br><span class="line"></span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line"></span><br><span class="line">print(model.embedding.weight.data)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],</span><br><span class="line">        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],</span><br><span class="line">        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],</span><br><span class="line">        ...,</span><br><span class="line">        [-0.1419,  0.0282,  0.2185,  ..., -0.1100, -0.1250,  0.0282],</span><br><span class="line">        [-0.3326, -0.9215,  0.9239,  ...,  0.5057, -1.2898,  0.1782],</span><br><span class="line">        [-0.8304,  0.3732,  0.0726,  ..., -0.0122,  0.2313, -0.2783]])</span><br></pre></td></tr></table></figure><h2 id="训练RNN模型"><a href="#训练RNN模型" class="headerlink" title="训练RNN模型"></a>训练RNN模型</h2><p>In [38]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure><p>In [39]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同上，这里用的kaggleGPU跑的，花了40分钟。</span></span><br><span class="line">N_EPOCHS = <span class="number">20</span></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line">    </span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'lstm-model.pt'</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 01 | Epoch Time: 2m 1s</span><br><span class="line">Train Loss: 0.667 | Train Acc: 59.09%</span><br><span class="line"> Val. Loss: 0.633 |  Val. Acc: 64.67%</span><br><span class="line">Epoch: 02 | Epoch Time: 2m 1s</span><br><span class="line">Train Loss: 0.663 | Train Acc: 60.33%</span><br><span class="line"> Val. Loss: 0.669 |  Val. Acc: 69.21%</span><br><span class="line">Epoch: 03 | Epoch Time: 2m 2s</span><br><span class="line">Train Loss: 0.650 | Train Acc: 61.06%</span><br><span class="line"> Val. Loss: 0.579 |  Val. Acc: 70.55%</span><br><span class="line">Epoch: 04 | Epoch Time: 2m 2s</span><br><span class="line">Train Loss: 0.493 | Train Acc: 77.43%</span><br><span class="line"> Val. Loss: 0.382 |  Val. Acc: 83.43%</span><br><span class="line">Epoch: 05 | Epoch Time: 2m 2s</span><br><span class="line">Train Loss: 0.394 | Train Acc: 83.71%</span><br><span class="line"> Val. Loss: 0.338 |  Val. Acc: 85.97%</span><br><span class="line">Epoch: 06 | Epoch Time: 2m 3s</span><br><span class="line">Train Loss: 0.338 | Train Acc: 86.26%</span><br><span class="line"> Val. Loss: 0.309 |  Val. Acc: 87.21%</span><br><span class="line">Epoch: 07 | Epoch Time: 2m 2s</span><br><span class="line">Train Loss: 0.292 | Train Acc: 88.37%</span><br><span class="line"> Val. Loss: 0.295 |  Val. Acc: 88.73%</span><br><span class="line">Epoch: 08 | Epoch Time: 2m 3s</span><br><span class="line">Train Loss: 0.252 | Train Acc: 90.26%</span><br><span class="line"> Val. Loss: 0.300 |  Val. Acc: 89.31%</span><br><span class="line">Epoch: 09 | Epoch Time: 2m 2s</span><br><span class="line">Train Loss: 0.246 | Train Acc: 90.51%</span><br><span class="line"> Val. Loss: 0.282 |  Val. Acc: 88.76%</span><br><span class="line">Epoch: 10 | Epoch Time: 2m 3s</span><br><span class="line">Train Loss: 0.205 | Train Acc: 92.37%</span><br><span class="line"> Val. Loss: 0.295 |  Val. Acc: 88.31%</span><br><span class="line">Epoch: 11 | Epoch Time: 2m 1s</span><br><span class="line">Train Loss: 0.203 | Train Acc: 92.46%</span><br><span class="line"> Val. Loss: 0.289 |  Val. Acc: 89.25%</span><br><span class="line">Epoch: 12 | Epoch Time: 2m 3s</span><br><span class="line">Train Loss: 0.178 | Train Acc: 93.58%</span><br><span class="line"> Val. Loss: 0.301 |  Val. Acc: 89.41%</span><br><span class="line">Epoch: 13 | Epoch Time: 2m 3s</span><br><span class="line">Train Loss: 0.158 | Train Acc: 94.43%</span><br><span class="line"> Val. Loss: 0.301 |  Val. Acc: 89.51%</span><br><span class="line">Epoch: 14 | Epoch Time: 2m 2s</span><br><span class="line">Train Loss: 0.158 | Train Acc: 94.63%</span><br><span class="line"> Val. Loss: 0.289 |  Val. Acc: 89.95%</span><br><span class="line">Epoch: 15 | Epoch Time: 2m 2s</span><br><span class="line">Train Loss: 0.142 | Train Acc: 95.00%</span><br><span class="line"> Val. Loss: 0.314 |  Val. Acc: 89.59%</span><br><span class="line">Epoch: 16 | Epoch Time: 2m 2s</span><br><span class="line">Train Loss: 0.123 | Train Acc: 95.62%</span><br><span class="line"> Val. Loss: 0.329 |  Val. Acc: 89.99%</span><br><span class="line">Epoch: 17 | Epoch Time: 2m 4s</span><br><span class="line">Train Loss: 0.107 | Train Acc: 96.16%</span><br><span class="line"> Val. Loss: 0.325 |  Val. Acc: 89.75%</span><br><span class="line">Epoch: 18 | Epoch Time: 2m 4s</span><br><span class="line">Train Loss: 0.100 | Train Acc: 96.66%</span><br><span class="line"> Val. Loss: 0.341 |  Val. Acc: 89.49%</span><br><span class="line">Epoch: 19 | Epoch Time: 2m 3s</span><br><span class="line">Train Loss: 0.096 | Train Acc: 96.63%</span><br><span class="line"> Val. Loss: 0.340 |  Val. Acc: 89.79%</span><br><span class="line">Epoch: 20 | Epoch Time: 2m 3s</span><br><span class="line">Train Loss: 0.080 | Train Acc: 97.31%</span><br><span class="line"> Val. Loss: 0.380 |  Val. Acc: 89.83%</span><br></pre></td></tr></table></figure><p>You may have noticed the loss is not really decreasing and the accuracy is poor. This is due to several issues with the model which we’ll improve in the next notebook.</p><p>Finally, the metric we actually care about, the test loss and accuracy, which we get from our parameters that gave us the best validation loss.</p><p>In [40]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载文件到本地</span></span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> HTML</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_download_link</span><span class="params">(title = <span class="string">"Download model file"</span>, filename = <span class="string">"wordavg-model.pt"</span>)</span>:</span>  </span><br><span class="line">    html = <span class="string">'&lt;a href=&#123;filename&#125;&gt;&#123;title&#125;&lt;/a&gt;'</span></span><br><span class="line">    html = html.format(title=title,filename=filename)</span><br><span class="line">    <span class="keyword">return</span> HTML(html)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a link to download the dataframe which was saved with .to_csv method</span></span><br><span class="line">create_download_link(filename=<span class="string">'lstm-model.pt'</span>)</span><br></pre></td></tr></table></figure><p>Out[40]:</p><p><a href="file:///Users/mmy/Downloads/lstm-model.pt" target="_blank" rel="noopener">Download model file</a></p><p>In [41]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(<span class="string">'lstm-model.pt'</span>))</span><br><span class="line">test_loss, test_acc = evaluate(model, test_iterator, criterion)</span><br><span class="line">print(<span class="string">f'Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test Acc: <span class="subst">&#123;test_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Test Loss: 0.304 | Test Acc: 88.11%</span><br></pre></td></tr></table></figure><p>In [44]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(&quot;I feel This film bad&quot;)</span><br></pre></td></tr></table></figure><p>Out[44]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.3637591600418091</span><br></pre></td></tr></table></figure><p>In [43]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(&quot;This film is great&quot;)</span><br></pre></td></tr></table></figure><p>Out[43]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9947803020477295</span><br></pre></td></tr></table></figure><h2 id="CNN模型"><a href="#CNN模型" class="headerlink" title="CNN模型"></a>CNN模型</h2><p>In [45]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, n_filters, </span></span></span><br><span class="line"><span class="function"><span class="params">                 filter_sizes, output_dim, dropout, pad_idx)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        </span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">        self.convs = nn.ModuleList([</span><br><span class="line">                                    nn.Conv2d(in_channels = <span class="number">1</span>, out_channels = n_filters, </span><br><span class="line">                                              kernel_size = (fs, embedding_dim)) </span><br><span class="line">                                    <span class="keyword">for</span> fs <span class="keyword">in</span> filter_sizes</span><br><span class="line">                                    ])</span><br><span class="line">        <span class="comment"># in_channels：输入的channel，文字都是1</span></span><br><span class="line">        <span class="comment"># out_channels：输出的channel维度</span></span><br><span class="line">        <span class="comment"># fs：每次滑动窗口计算用到几个单词</span></span><br><span class="line">        <span class="comment"># for fs in filter_sizes打算用好几个卷积模型最后concate起来看效果。</span></span><br><span class="line">        </span><br><span class="line">        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        text = text.permute(<span class="number">1</span>, <span class="number">0</span>) <span class="comment"># [batch size, sent len]</span></span><br><span class="line">        embedded = self.embedding(text) <span class="comment"># [batch size, sent len, emb dim]</span></span><br><span class="line">        embedded = embedded.unsqueeze(<span class="number">1</span>) <span class="comment"># [batch size, 1, sent len, emb dim]</span></span><br><span class="line">        <span class="comment"># 升维是为了和nn.Conv2d的输入维度吻合，把channel列升维。</span></span><br><span class="line">        conved = [F.relu(conv(embedded)).squeeze(<span class="number">3</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs]</span><br><span class="line">        <span class="comment"># conved = [batch size, n_filters, sent len - filter_sizes+1]</span></span><br><span class="line">        <span class="comment"># 有几个filter_sizes就有几个conved</span></span><br><span class="line">    </span><br><span class="line">        </span><br><span class="line">        pooled = [F.max_pool1d(conv, conv.shape[<span class="number">2</span>]).squeeze(<span class="number">2</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> conved]</span><br><span class="line">        <span class="comment"># 把conv的第三个维度最大池化了</span></span><br><span class="line">        <span class="comment">#pooled_n = [batch size, n_filters]</span></span><br><span class="line">        </span><br><span class="line">        cat = self.dropout(torch.cat(pooled, dim=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># cat = [batch size, n_filters * len(filter_sizes)]</span></span><br><span class="line">        <span class="comment"># 把 len(filter_sizes)个卷积模型concate起来传到全连接层。</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> self.fc(cat)</span><br></pre></td></tr></table></figure><p>In [47]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同上</span></span><br><span class="line">INPUT_DIM = len(TEXT.vocab)</span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">N_FILTERS = <span class="number">100</span></span><br><span class="line">FILTER_SIZES = [<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line">DROPOUT = <span class="number">0.5</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)</span><br><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings)</span><br><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]</span><br><span class="line"></span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model = model.to(device)</span><br><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br><span class="line"><span class="comment"># 比averge model模型参数差不多</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The model has 2,620,801 trainable parameters</span><br></pre></td></tr></table></figure><p>In [48]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同上，需要花8分钟左右</span></span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">criterion = criterion.to(device)</span><br><span class="line"></span><br><span class="line">N_EPOCHS = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    </span><br><span class="line">    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line">    </span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'CNN-model.pt'</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 01 | Epoch Time: 0m 19s</span><br><span class="line">Train Loss: 0.652 | Train Acc: 61.81%</span><br><span class="line"> Val. Loss: 0.527 |  Val. Acc: 76.20%</span><br><span class="line">Epoch: 02 | Epoch Time: 0m 19s</span><br><span class="line">Train Loss: 0.427 | Train Acc: 80.66%</span><br><span class="line"> Val. Loss: 0.358 |  Val. Acc: 84.36%</span><br><span class="line">Epoch: 03 | Epoch Time: 0m 19s</span><br><span class="line">Train Loss: 0.304 | Train Acc: 87.14%</span><br><span class="line"> Val. Loss: 0.318 |  Val. Acc: 86.45%</span><br><span class="line">Epoch: 04 | Epoch Time: 0m 19s</span><br><span class="line">Train Loss: 0.215 | Train Acc: 91.42%</span><br><span class="line"> Val. Loss: 0.313 |  Val. Acc: 86.92%</span><br><span class="line">Epoch: 05 | Epoch Time: 0m 19s</span><br><span class="line">Train Loss: 0.156 | Train Acc: 94.18%</span><br><span class="line"> Val. Loss: 0.326 |  Val. Acc: 87.01%</span><br><span class="line">Epoch: 06 | Epoch Time: 0m 19s</span><br><span class="line">Train Loss: 0.105 | Train Acc: 96.33%</span><br><span class="line"> Val. Loss: 0.344 |  Val. Acc: 87.16%</span><br><span class="line">Epoch: 07 | Epoch Time: 0m 19s</span><br><span class="line">Train Loss: 0.075 | Train Acc: 97.61%</span><br><span class="line"> Val. Loss: 0.372 |  Val. Acc: 87.28%</span><br><span class="line">Epoch: 08 | Epoch Time: 0m 19s</span><br><span class="line">Train Loss: 0.052 | Train Acc: 98.39%</span><br><span class="line"> Val. Loss: 0.403 |  Val. Acc: 87.21%</span><br><span class="line">Epoch: 09 | Epoch Time: 0m 19s</span><br><span class="line">Train Loss: 0.041 | Train Acc: 98.64%</span><br><span class="line"> Val. Loss: 0.433 |  Val. Acc: 87.09%</span><br><span class="line">Epoch: 10 | Epoch Time: 0m 19s</span><br><span class="line">Train Loss: 0.031 | Train Acc: 99.10%</span><br><span class="line"> Val. Loss: 0.462 |  Val. Acc: 87.01%</span><br><span class="line">Epoch: 11 | Epoch Time: 0m 19s</span><br><span class="line">Train Loss: 0.023 | Train Acc: 99.29%</span><br><span class="line"> Val. Loss: 0.495 |  Val. Acc: 86.93%</span><br><span class="line">Epoch: 12 | Epoch Time: 0m 19s</span><br><span class="line">Train Loss: 0.021 | Train Acc: 99.34%</span><br><span class="line"> Val. Loss: 0.530 |  Val. Acc: 86.84%</span><br><span class="line">Epoch: 13 | Epoch Time: 0m 19s</span><br><span class="line">Train Loss: 0.015 | Train Acc: 99.60%</span><br><span class="line"> Val. Loss: 0.559 |  Val. Acc: 86.73%</span><br><span class="line">Epoch: 14 | Epoch Time: 0m 19s</span><br><span class="line">Train Loss: 0.013 | Train Acc: 99.69%</span><br><span class="line"> Val. Loss: 0.597 |  Val. Acc: 86.48%</span><br><span class="line">Epoch: 15 | Epoch Time: 0m 19s</span><br><span class="line">Train Loss: 0.012 | Train Acc: 99.70%</span><br><span class="line"> Val. Loss: 0.608 |  Val. Acc: 86.63%</span><br><span class="line">Epoch: 16 | Epoch Time: 0m 19s</span><br><span class="line">Train Loss: 0.009 | Train Acc: 99.76%</span><br><span class="line"> Val. Loss: 0.640 |  Val. Acc: 86.77%</span><br><span class="line">Epoch: 17 | Epoch Time: 0m 19s</span><br><span class="line">Train Loss: 0.010 | Train Acc: 99.73%</span><br><span class="line"> Val. Loss: 0.674 |  Val. Acc: 86.51%</span><br><span class="line">Epoch: 18 | Epoch Time: 0m 19s</span><br><span class="line">Train Loss: 0.012 | Train Acc: 99.63%</span><br><span class="line"> Val. Loss: 0.704 |  Val. Acc: 86.71%</span><br><span class="line">Epoch: 19 | Epoch Time: 0m 19s</span><br><span class="line">Train Loss: 0.010 | Train Acc: 99.65%</span><br><span class="line"> Val. Loss: 0.757 |  Val. Acc: 86.44%</span><br><span class="line">Epoch: 20 | Epoch Time: 0m 20s</span><br><span class="line">Train Loss: 0.006 | Train Acc: 99.80%</span><br><span class="line"> Val. Loss: 0.756 |  Val. Acc: 86.55%</span><br></pre></td></tr></table></figure><p>In [49]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 发现上面结果过拟合了，同学们可以自行调参</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">'CNN-model.pt'</span>))</span><br><span class="line">test_loss, test_acc = evaluate(model, test_iterator, criterion)</span><br><span class="line">print(<span class="string">f'Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test Acc: <span class="subst">&#123;test_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Test Loss: 0.339 | Test Acc: 85.68%</span><br></pre></td></tr></table></figure><p>In [50]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(&quot;I feel This film bad&quot;)</span><br></pre></td></tr></table></figure><p>Out[50]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.6535547375679016</span><br></pre></td></tr></table></figure><p>In [52]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(&quot;This film is great well&quot;) </span><br><span class="line"># 我后面加了个well，不加会报错，因为我们的FILTER_SIZES = [3,4,5]有设置为5，所以输出的句子长度不能小于5</span><br></pre></td></tr></table></figure><p>Out[52]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9950380921363831</span><br></pre></td></tr></table></figure><p>In [54]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kaggle上下载模型文件到本地</span></span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> HTML</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_download_link</span><span class="params">(title = <span class="string">"Download model file"</span>, filename = <span class="string">"CNN-model.pt"</span>)</span>:</span>  </span><br><span class="line">    html = <span class="string">'&lt;a href=&#123;filename&#125;&gt;&#123;title&#125;&lt;/a&gt;'</span></span><br><span class="line">    html = html.format(title=title,filename=filename)</span><br><span class="line">    <span class="keyword">return</span> HTML(html)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a link to download the dataframe which was saved with .to_csv method</span></span><br><span class="line">create_download_link(filename=<span class="string">'CNN-model.pt'</span>)</span><br></pre></td></tr></table></figure><p>Out[54]:</p><p><a href="file:///Users/mmy/Downloads/CNN-model.pt" target="_blank" rel="noopener">Download model file</a></p><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TorchText </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>聊天机器人二</title>
      <link href="/2020/03/11/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%BA%8C/"/>
      <url>/2020/03/11/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%BA%8C/</url>
      
        <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>比较著名的聊天系统</p><p><img src="https://uploader.shimo.im/f/e5gmAxm4zzEpvbgO.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/MEdcyUq5bw0bd0aG.png!thumbnail" alt="img"></p><p>聊天机器人的历史</p><ul><li><p>1950: Turing Test</p></li><li><p>1966: ELIZA, MIT chatbot</p></li><li><p>1995: ALICE, pattern recognition chatbot</p></li><li><p>2011-2012: Siri, Watson, Google Assistant</p></li><li><p>2015: Amazon Alexa, Microsoft Cortana</p></li><li><p>2016: Bot 元年： Microsoft Bot Framework, FB Messenger, Google Assistant …</p></li><li><p>2017: 百度度秘，腾讯云小微，阿里小蜜 …</p></li></ul><p>ASR: Acoustic speech recognition: Speech –&gt; Text</p><p>SLU: Spoken Language Understanding</p><p><img src="https://uploader.shimo.im/f/5jqPVZomqwIRiaKY.png!thumbnail" alt="img"></p><p>聊天机器人按照其功能主要可以分为两类</p><ul><li><p>任务导向的聊天机器人(Task-Oriented Chatbot)</p></li><li><p><strong>有具体的聊天任务，例如酒店、机票、饭店预订，电话目录</strong></p></li><li><p>也可以做一些更复杂的工作，例如假期日程安排，<strong>讨价还价</strong></p></li><li><p>goal-oriented chatbot</p></li><li><p>非任务导向的聊天机器人(Non-Task-Oriented Chatbot)</p></li><li><p>没有具体的聊天目标，主要目的是闲聊，能够和用户有更多的交互</p></li><li><p>也有上述任务导向与非任务导向的混合聊天机器人，同时具备两种功能</p></li></ul><p>按照聊天机器人的聊天领域 (Domain)</p><ul><li><p>对于任务导向的机器人，聊天的领域就是任务的领域</p></li><li><p>关于某个特定任务的数据库，例如机票信息，酒店信息</p></li><li><p>也可以同时包含多个领域的信息</p></li><li><p>对于非任务导向的机器人</p></li></ul><p>按照聊天的发起方</p><ul><li><p>系统主导（System Initiative）的聊天机器人</p></li><li><p>适用于一些简单的任务：例如很多大公司的自动电话语音系统</p></li><li><p>用户主导（User Initiative）的聊天机器人</p></li><li><p>用户主导聊天的主题</p></li><li><p>系统需要去尽量迎合用户的喜好，陪他们聊天</p></li><li><p>系统/用户混合主导（Mixed Initiative）</p></li></ul><p>Alexa Prize Challenge 亚马逊举办的聊天机器人大赛</p><p><a href="https://developer.amazon.com/alexaprize/challenges/past-challenges/2018/" target="_blank" rel="noopener">https://developer.amazon.com/alexaprize/challenges/past-challenges/2018/</a></p><p>DSTC 2 &amp; 3</p><p>关于构建聊天机器人的建议</p><ul><li><p>迅速创建一个baseline，在这个Baseline的基础上去不断提高</p></li><li><p><strong>多测试自己的聊天机器人</strong></p></li></ul><p><strong>聊天机器人的评估方法</strong></p><ul><li><p>对于任务导向性聊天机器人，我们可以使用任务是否完成来评估聊天机器人是否成功</p></li><li><p><strong>Efficiency</strong></p></li><li><p><strong>Effectiveness</strong></p></li><li><p>Usability</p></li><li><p>针对非任务聊天机器人</p></li><li><p><strong>自动评估</strong></p></li><li><p><strong>对话进行的长度/轮数</strong></p></li><li><p>User sentiment analysis</p></li><li><p>Positive user responses / total user responses</p></li><li><p><strong>人类评估</strong> </p></li><li><p>Coherence</p></li><li><p>Appropriateness</p></li><li><p>Rating</p></li><li><p>基于参考答案的评估指标，<strong>BLEU, ROUGE, METEOR</strong> (有可能不太准确)</p></li></ul><p>也有文章指出，这种基于参考答案的评价指标不能够很好地反映聊天机器人的好坏</p><ul><li><p>How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation</p></li><li><p><a href="https://www.aclweb.org/anthology/D16-1230" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D16-1230</a></p></li></ul><p>所以很多时候，我们对聊天机器人，尤其是闲聊机器人的评估依然依赖于人类评估。人们也开始尝试一些<strong>深度学习的模型</strong>来预测人类给模型的打分。</p><p>构建聊天机器人的三大模块</p><p><img src="https://uploader.shimo.im/f/xAwqqhmELsc4JfFY.png!thumbnail" alt="img"></p><ul><li><p>Coherence</p></li><li><p>User experience</p></li><li><p>Engagement Management</p></li></ul><h3 id="自然语言理解（Natural-Language-Understanding）"><a href="#自然语言理解（Natural-Language-Understanding）" class="headerlink" title="自然语言理解（Natural Language Understanding）"></a>自然语言理解（Natural Language Understanding）</h3><p>NLU的主要任务是从文本中提取信息，包括对话<strong>文本的意图</strong>，文本中<strong>关键的信息</strong>，例如<strong>命名实体</strong>，并且将这些信息转成比较<strong>标准化</strong>的表示方式以供后续聊天机器人模块使用。</p><p><strong>面临的挑战</strong></p><ul><li><p><strong>去除口语化的表达</strong></p></li><li><p><strong>话语重复</strong></p></li><li><p><strong>讲话中自我修正</strong></p></li><li><p><strong>口语化的表述和停顿</strong></p></li></ul><h3 id="Frame-based-SLU-Spoken-Language-Understanding"><a href="#Frame-based-SLU-Spoken-Language-Understanding" class="headerlink" title="Frame-based SLU (Spoken Language Understanding)"></a>Frame-based SLU (Spoken Language Understanding)</h3><p>Meaning Representation Language</p><p>把自然语言转变成一种固定的结构化数据表达形式</p><ul><li><p>有固定的语法结构</p></li><li><p>计算机可执行的语言</p></li></ul><p>Do you have any flights from <strong>Seattle</strong> to <strong>Boston</strong> on <strong>December 24th</strong>? </p><p>O O O O O O BDCity O BACity O BDDate IDDate</p><p>BiLSTM –&gt; Classification (CRF)</p><p><img src="https://uploader.shimo.im/f/m78yw5M7YBkbMIb8.png!thumbnail" alt="img"></p><p>Intent classification –&gt; ShowFlight</p><p>POS </p><p>这种信息抽取的任务往往可以通过<strong>HMM, CRF</strong>等模型实现</p><p>other</p><p>B-Departure</p><p>I-Departure</p><p>B-Arrival</p><p>I-Arrival</p><p>B-Date</p><p>I-Date</p><p>conll-2003</p><h3 id="意图识别-Intent-Classification"><a href="#意图识别-Intent-Classification" class="headerlink" title="意图识别 (Intent Classification)"></a>意图识别 (Intent Classification)</h3><p>Coarse-grained</p><ul><li><p>把用户的当前讲话的意图归类到我们提前指定的一些类别中去</p></li><li><p>给定一句话 (utterance X)，给定一系列 intent classes: C_1, …, C_M。预测当前 utterance 属于哪一个intent类别。</p></li><li><p>同样的intent可能有很多不同的表达方式</p></li><li><p>我想要预订后天北京到上海的机票</p></li><li><p>能帮我订一张后天从首都机场到浦东机场的机票吗？</p></li><li><p>后天从北京到虹桥的机票有吗？</p></li></ul><p><img src="https://uploader.shimo.im/f/ERAAvjjCqQ80MNQ1.png!thumbnail" alt="img"></p><h3 id="命名实体识别"><a href="#命名实体识别" class="headerlink" title="命名实体识别"></a>命名实体识别</h3><p>从一段文本中抽取<strong>命名实体</strong></p><p>命名实体的类别</p><ul><li>机构、任务、地点、时间、日期、金钱、百分比</li></ul><p>主要的方法</p><ul><li>基于HMM, CRF, RNN (LSTM) 等模型的 token 标注方法</li></ul><h3 id="关于-semantic-parsing-和-slot-filling"><a href="#关于-semantic-parsing-和-slot-filling" class="headerlink" title="关于 semantic parsing 和 slot filling"></a>关于 semantic parsing 和 slot filling</h3><p>semantic parsing: 非结构化的语言转换成结构化的指令</p><p><a href="https://nlpprogress.com/english/semantic_parsing.html" target="_blank" rel="noopener">https://nlpprogress.com/english/semantic_parsing.html</a></p><p><img src="https://uploader.shimo.im/f/qbDxmHsMGfcnMNQa.png!thumbnail" alt="img"></p><p>Find_Flight(Boston, New York)</p><p>semantic parsing</p><ul><li><p>intent classification </p></li><li><p>arguments parsing</p></li></ul><p>Open(“baidumap”, city=”Beijing”);</p><p>Open(“Google map”, city=”Beijing”);</p><p>Close(“Google map”, city=”Beijing”);</p><p>Install(“”)</p><p>BERT for Joint Intent Classification and Slot Filling</p><p><a href="https://arxiv.org/pdf/1902.10909.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1902.10909.pdf</a></p><h3 id="Dialogue-State-Tracking"><a href="#Dialogue-State-Tracking" class="headerlink" title="Dialogue State Tracking"></a>Dialogue State Tracking</h3><p>DSTC 比赛和数据集</p><p><a href="http://camdial.org/~mh521/dstc/" target="_blank" rel="noopener">http://camdial.org/~mh521/dstc/</a></p><h1 id="对话管理"><a href="#对话管理" class="headerlink" title="对话管理"></a><strong>对话管理</strong></h1><p>什么是对话管理？</p><ul><li><p>一般是聊天机器人的中心模块，控制整个聊天的过程。上面承接NLU，后面连着NLG，负责<strong>储存聊天的信息和状态(slot filling)</strong>，根据已有的信息 (当前聊天记录和外部信息)，决定下一步做什么。</p></li><li><p>负责与一些外部的<strong>知识库</strong>做交互，例如Knowledge base，各种数据库</p></li></ul><p>Dialogue <strong>Context</strong> Modeling</p><ul><li><p>聊天的过程往往是高度基于对话记录的，很多时候我们会用代词指代之前的名词或实体</p></li><li><p>你想聊一些关于科技的还是民生的话题？</p></li><li><p>聊第二个吧 –&gt; 民生</p></li><li><p><strong>共指消解</strong> (Coreference Resolution)</p></li><li><p>有时候我们会<strong>省略</strong>一些显而易见的信息</p></li><li><p>你打算什么时候吃饭？</p></li><li><p>晚上7点（<strong>吃饭</strong>）</p></li><li><p>有时候我们会省略一些信息（当我们和一些聊天机器人聊天的时候）</p></li><li><p>放点<strong>好听</strong>的音乐（放什么音乐？根据历史记录放？选用户最喜欢的音乐？）</p></li><li><p><strong>开灯</strong> （开什么灯？床头灯？顶灯？客厅的灯？卧室的灯？）</p></li></ul><p>聊天<strong>context</strong>的来源</p><ul><li><p>聊天历史，例如<strong>聊天历史</strong>的纯文本内容，聊天记录中提到过的<strong>命名实体</strong>、<strong>主题</strong>等等</p></li><li><p>任务记录</p></li><li><p>对于一个任务型聊天机器人来说，聊天机器人往往会保存一些结构化的聊天记录，这些记录有时候被称为 <strong>form</strong>, frame, template, status graph。例如对于一个订机票聊天机器人来说，它需要收集用户的<strong>姓名，身份证号码，出发机场，到达机场，航班时间要求，价格要求</strong>，等等。然后才可以帮助用户做决策。</p></li><li><p>我们需要知道哪些信息已经被收集到了，哪些信息还没有被收集到。然后根据需要收集的信息确定下一步机器人需要跟用户说什么。</p></li></ul><p>Knowledge Base</p><p>根据<strong>聊天任务的不同</strong>，聊天机器人需要不同的Knowledge Base</p><ul><li><p>对于航班订票机器人来说，需要航班信息的数据库</p></li><li><p>对于酒店预订机器人来说，需要酒店数据库</p></li><li><p>对于闲聊机器人来说，各种闲聊中需要用到的信息，新闻、财经、电影娱乐等等</p></li></ul><p>Dialogue Control (Dialogue Act)   </p><p>根据Context, 聊天历史，knowledge base –&gt; action 可能是<strong>rule based 决策</strong>，也可能是基于机器学习模型的决策。</p><ul><li><p>根据当前（从用户和其他数据来源）获取的信息，决定下一步需要采取怎样的行动</p></li><li><p>可以做的决策有：</p></li><li><p>从用户处<strong>收集更多的信息</strong></p></li><li><p>与用户<strong>确认</strong>之前的信息</p></li><li><p><strong>向用户输出一些信息</strong></p></li><li><p>一些设计要素</p></li><li><p>由用户还是系统来主导对话</p></li><li><p>是否要向用户解释自己的动作</p></li></ul><p>对话的主导方</p><ul><li><p>用户主导</p></li><li><p>用户来控制对话的进程</p></li><li><p>系统不会自由发挥，而是跟随用户的思路</p></li><li><p>在一些<strong>QA系统和基于搜索的系统</strong>中较为常见</p></li><li><p><strong>系统主导</strong></p></li><li><p>系统控制对话的进程</p></li><li><p>系统决定了用户能说什么，不能说什么</p></li><li><p>对于系统听不懂的话，系统可以忽略或者告诉用户自己无法解决</p></li><li><p>在一些简单任务的机器人中很常见</p></li><li><p>混合主导</p></li><li><p>以上两种系统的混合版，可以是简单的组合，也可以设计地更复杂</p></li></ul><p>Dialogue Control 的一些方法</p><ul><li><p><strong>基于Finite state machine</strong> </p></li><li><p><strong>基于Frame</strong></p></li><li><p><strong>基于统计学模型(机器学习）</strong></p></li><li><p>AI planning</p></li></ul><h3 id="基于-Finite-State-的聊天控制"><a href="#基于-Finite-State-的聊天控制" class="headerlink" title="基于 Finite-State 的聊天控制"></a>基于 Finite-State 的聊天控制</h3><p>Finite State Automata 有限状态机</p><ul><li><p>聊天机器人跟从一个类似 finite state automata 的系统</p></li><li><p>聊天机器人在每个节点上可以做的事情是确定的</p></li><li><p>2018年 Alexa Prize 的 Tartan 模型 </p></li><li><p><a href="https://m.media-amazon.com/images/G/01/mobile-apps/dex/alexa/alexaprize/assets/pdf/2018/Tartan.pdf" target="_blank" rel="noopener">https://m.media-amazon.com/images/G/01/mobile-apps/dex/alexa/alexaprize/assets/pdf/2018/Tartan.pdf</a></p></li></ul><p><img src="https://uploader.shimo.im/f/J3a3dBGf89MpaIdI.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/QThWtsbtshQkEqNS.png!thumbnail" alt="img"></p><ul><li><p>系统完全控制了整场对话</p></li><li><p>系统会一直向用户提问</p></li><li><p>系统会忽略任何它不需要的信息</p></li></ul><p>这种控制系统的好处是</p><ul><li><p>容易设计和实现，完全使用<strong>if-else语句</strong></p></li><li><p>功能非常确定，可控制</p></li><li><p>其中<strong>State的transition可以基于非常复杂的条件和对话状态</strong></p></li></ul><p>坏处是</p><ul><li><p>没有什么灵活性，真的是一个机器人</p></li><li><p>只能支持系统主导的对话</p></li></ul><h3 id="Frame-Based-Dialogue-Control"><a href="#Frame-Based-Dialogue-Control" class="headerlink" title="Frame-Based Dialogue Control"></a>Frame-Based Dialogue Control</h3><ul><li>预先指定了一张<strong>表格 (Frame)</strong>，聊天机器人的目标就是把这张表格填满</li></ul><p><img src="https://uploader.shimo.im/f/uQcxBjpZ5LEGAZfQ.png!thumbnail" alt="img"></p><p>我们可以预先指定一些问题，用来从用户处得到我们想要的信息</p><table><thead><tr><th><strong>Slot</strong></th><th><strong>Question</strong></th></tr></thead><tbody><tr><td>出发地</td><td>你从哪个城市或机场出发？</td></tr><tr><td>目的地</td><td>你要去哪个城市？</td></tr><tr><td>起飞日期</td><td>你的起飞日期是？</td></tr><tr><td>起飞时间</td><td>你想几点钟起飞？</td></tr><tr><td>航空公司</td><td>你有偏好的航空公司吗？</td></tr><tr><td>姓名</td><td>你的名字叫什么？</td></tr><tr><td>证件号码</td><td>你的身份证号码是多少？ –&gt; 护照</td></tr><tr><td></td><td></td></tr></tbody></table><p>提问的先后不需要确定。用户也可以同时回答多个问题。</p><p>根据当前未知信息的组合，系统可以提出不同的问题</p><ul><li><p>未知信息（出发地、目的地）：你的旅行路线是？</p></li><li><p>未知信息（出发地）：你的出发城市是哪里？</p></li><li><p>未知信息（目的地）：你的目的地是哪里？</p></li><li><p>出发地+起飞日期：你打算哪天从哪个机场出发？</p></li></ul><p>只要这张mapping的表格足够全面，我们就可以处理各种情况。</p><p><strong>Frame-Based的方法比Finite-State要</strong>灵活一些，但本质上还是一个非常固定的方法，在有限的空间里完成一项特定的任务。</p><p>这两种方法的主要缺陷在于：</p><ul><li><strong>需要花很多时间考虑各种情况，人为设计对话路线，有可能会出现一些情况没有被设计到。</strong></li></ul><p>intent (entities), state –&gt; <strong>action</strong> , reward –&gt; intent (entities), state –&gt; <strong>action</strong> , reward –&gt; intent (entities), state –&gt; ? </p><p>optional below</p><h3 id="基于统计模型的Dialogue-Control"><a href="#基于统计模型的Dialogue-Control" class="headerlink" title="基于统计模型的Dialogue Control"></a>基于统计模型的Dialogue Control</h3><p>基于统计学和数据的聊天机器人模型</p><ul><li><p>一套固定的states S 聊天历史</p></li><li><p>一套固定的actions A 下一句要讲的话</p></li><li><p>一套系统performance的评价指标 <strong>reward 自己设计</strong></p></li><li><p>一个<strong>policy</strong> \pi，决定了在一个state下可以采取怎样的action （可能是一个神经网络）根据当前的聊天历史，决定下一句话讲什么</p></li></ul><p>训练方法</p><ul><li><p>监督学习，需要很多的训练数据</p></li><li><p><strong>强化学习 (Reinforcement Learning)，需要优化模型的最终回报 (return)</strong></p></li><li><p><strong>除了上面的信息之外，还要加入一个回报函数</strong></p></li></ul><p>关于如何做<strong>强化学习</strong>？我们这里不再详细展开，感兴趣的同学可以阅读</p><ul><li><p><a href="https://hao-cheng.github.io/ee596_spr2019/slides/lecture_5-dialog_management.pdf" target="_blank" rel="noopener">https://hao-cheng.github.io/ee596_spr2019/slides/lecture_5-dialog_management.pdf</a></p></li><li><p><strong>David Silver</strong>的RL课程 <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="noopener">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html</a></p></li><li><p>Richard Sutton</p></li><li><p><strong>强化学习知识大讲堂</strong>  <a href="https://zhuanlan.zhihu.com/p/25498081" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25498081</a></p></li></ul><p>这篇来自Jiwei Li的文章引用量很高</p><p>Deep Reinforcement Learning for Dialogue Generation</p><p><a href="https://aclweb.org/anthology/D16-1127" target="_blank" rel="noopener">https://aclweb.org/anthology/D16-1127</a></p><p>在闲聊机器人中的Dialogue Control</p><ul><li><p>可能涉及到的话题空间较大，聊天的控制比较复杂</p></li><li><p>没有明确的任务目标，很难定义reward函数，可能唯一的目标就是让聊天时间变长</p></li></ul><p>常见的做法</p><ul><li><p><strong>把聊天机器人分成几个不同的模块</strong>，每个<strong>模块可以负责一些聊天的子话题</strong>，或者由一些不同的模型实现</p></li><li><p>有一个master模块负责分配聊天任务给不同的模块</p></li></ul><p>阶梯化的模块</p><ul><li><p>聊天历史记录模块 (Dialogue State/Context Tracking)</p></li><li><p><strong>Master Dialogue Manage</strong>r</p></li><li><p><strong>Miniskill Dialogue Manager</strong></p></li></ul><p>很多Miniskill Dialogue Manager是由finite-state-machine来实现的，可以通过引入一些non-deterministic finite automata来增加聊天的丰富度和多样性</p><p><strong>action, slot values (key value pairs)</strong></p><p>dialogue history</p><h1 id="自然语言生成-Natural-Language-Generation-NLG"><a href="#自然语言生成-Natural-Language-Generation-NLG" class="headerlink" title="自然语言生成 Natural Language Generation (NLG)"></a>自然语言生成 Natural Language Generation (NLG)</h1><h2 id="Template-based"><a href="#Template-based" class="headerlink" title="Template based"></a>Template based</h2><p>display_flight_info </p><p>action: ask_departure_city</p><ul><li><p>你要从哪个机场离开？</p></li><li><p>你从哪里起飞？</p></li></ul><p>ask_time</p><p>使用模板来生成句子</p><ul><li><p>[DEPARTURE-CITY]:  你打算几点钟离开 [DEPARTURE-CITY]?</p></li><li><p>[TOPIC]: 不如聊聊 [TOPIC]?</p></li></ul><h2 id="Retrieval-Based"><a href="#Retrieval-Based" class="headerlink" title="Retrieval Based"></a>Retrieval Based</h2><p>Response <strong>R**</strong>e<strong><strong>tr</strong></strong>i<strong>**eval</strong></p><ul><li><p>根据当前的对话场景/历史决定提取怎样的回复</p></li><li><p>基于retrieval而不是generation的方法</p></li><li><p>可以使用机器学习的模型来训练抽取模型</p></li><li><p>可以根据<strong>similarity</strong> <strong>mat**</strong>c<strong><strong>hi</strong></strong>n<strong>**g</strong>的方法：ELMo, average, cosine similarity. Google universal sentence encoder. 自己训练一个模型？</p></li><li><p>可以利用一些别的基于搜索的方法</p></li></ul><p><img src="https://uploader.shimo.im/f/2QSr9OIvUKEFJ9B0.png!thumbnail" alt="img"></p><p>思考一下你会怎么构建这个模型？</p><p>可以用它来制作一个问答机器人，回答常见的问题。</p><h2 id="基于深度学习的聊天机器人（偏向实验性质）"><a href="#基于深度学习的聊天机器人（偏向实验性质）" class="headerlink" title="基于深度学习的聊天机器人（偏向实验性质）"></a>基于深度学习的聊天机器人（偏向实验性质）</h2><p>用Seq2Seq模型来做生成</p><p><img src="https://uploader.shimo.im/f/WV5HzLQb0DsDD4Cj.png!thumbnail" alt="img"></p><ul><li><p>多样性很差</p></li><li><p>很难控制</p></li></ul><p>Hierarchical LSTM, Hierachical BERT? </p><p>在聊天机器人中使用生成模型有一个很大的问题，就是你无法完全掌控生成句子的各种属性，我们无法知道模型会生成什么样的句子。这也导致了基于神经网络的模型，例如Seq2Seq，在有任务的聊天机器人中并没有得到非常多的使用，而是更多地出现在一些娱乐性的项目之中。例如如果我们想要训练一只“<strong>小黄鸡</strong>”，那么你可以大胆地使用Seq2Seq等神经网络模型。可是如果你想要</p><p>Jiwei Li的一系列基于深度学习的Dialogue Generation</p><p>Jiwei 在 斯坦福的slides <a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">h</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">t</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">tps://w</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">e</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">b.</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">sta</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">nford.edu/class/cs224s/lec</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">t</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">ur</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">e</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">s/224s.17.lec12.pdf</a></p><h3 id="Deep-Reinforcement-Learning-for-Dialogue-Generation"><a href="#Deep-Reinforcement-Learning-for-Dialogue-Generation" class="headerlink" title="Deep Reinforcement Learning for Dialogue Generation"></a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">Deep Reinfo</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">r</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">ce</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">me</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">nt</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener"> </a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">Le</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">a</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">r</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">ning </a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">for D</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">ia</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">logu</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">e </a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">Ge</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">ne</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">r</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">at</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">i</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">o</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">n</a></h3><p>使用深度增强学习来训练聊天机器人。</p><p>增强学习 Framework</p><p>action: 下一句要生成的话</p><p>state: 当前的聊天历史，在本文中使用最近的两句对话</p><p>policy: 一个基于LSTM的 Seq2Seq 模型</p><p>reward: 对当前生成对话的评价指标，本文中采用了三项指标。</p><ul><li><p>Ease of answering: 这句对话是不是很容易回答，不要“把天聊死”。</p></li><li><p>Information Flow: 当前生成的对话应该和之前的聊天记录有所变化。</p></li><li><p>Semantic Coherence：上下文是否连贯。</p></li></ul><h3 id="Adversarial-Learning-for-Neural-Dialogue-Generation"><a href="#Adversarial-Learning-for-Neural-Dialogue-Generation" class="headerlink" title="Adversarial Learning for Neural Dialogue Generation"></a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">Adve</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">r</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">s</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">a</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">rial L</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">ea</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">rning for N</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">e</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">ural</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener"> Dialogue </a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">Ge</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">n</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">era</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">t</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">i</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">o</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">n</a></h3><p>把GAN的想法用于聊天机器人的评估中。与上文相同，生成器是一个聊天机器人，可以生成对话。判别器是一个打分系统，可以对聊天机器人生成的对话进行打分。然后使用增强学习(REINFORCE, Policy Gradient算法)来训练生成器。</p><p>关于如何实现policy gradient </p><p><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">http</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">s</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">://</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">di</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">scuss.py</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">t</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">orch.org/t/whats-th</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">e-</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">righ</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">t</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">-w</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">a</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">y-of-implemen</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">t</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">ing-policy-gradi</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">e</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">nt/4003/2</a></p><p>参考该项目 <a href="https://github.com/suragnair/seqGAN" target="_blank" rel="noopener">https://github.com/suragnair/seqGAN</a></p><h1 id="Alexa-Prize-Challenge"><a href="#Alexa-Prize-Challenge" class="headerlink" title="Alexa Prize Challenge"></a>Alexa Prize Challenge</h1><p>2018年冠军 <strong>Gunrock: Building A Human-Like Social Bot By Leveraging Large Scale Real User Data</strong></p><p><a href="https://m.media-amazon.com/images/G/01/mobile-apps/dex/alexa/alexaprize/assets/pdf/2018/Gunrock.pdf" target="_blank" rel="noopener">https://m.media-amazon.com/images/G/01/mobile-apps/dex/alexa/alexaprize/assets/pdf/2018/Gunrock.pdf</a></p><p><img src="https://uploader.shimo.im/f/APzsIXJhTrsQRYOw.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/JZM2qSNuXLorcVr3.png!thumbnail" alt="img"></p><p>2017年冠军 Sounding Board – University of Washington’s Alexa Prize Submission</p><p><a href="https://m.media-amazon.com/images/G/01/mobile-apps/dex/alexa/alexaprize/assets/pdf/2017/Soundingboard.pdf" target="_blank" rel="noopener">https://m.media-amazon.com/images/G/01/mobile-apps/dex/alexa/alexaprize/assets/pdf/2017/Soundingboard.pdf</a></p><p><img src="https://uploader.shimo.im/f/xAwqqhmELsc4JfFY.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/E7m8GJsDFTcXJ9xY.png!thumbnail" alt="img"></p><p>参考课程资料</p><ul><li><p><a href="https://hao-cheng.github.io/ee596_spr2019/" target="_blank" rel="noopener">https://hao-cheng.github.io/ee596_spr2019/</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/30533380" target="_blank" rel="noopener">Neural Response Generation – 关于回复生成工作的一些总结</a></p></li><li><p>斯坦福的课程资料 <a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec10.pdf" target="_blank" rel="noopener">https://web.stanford.edu/class/cs224s/lectures/224s.17.lec10.pdf</a></p></li><li><p>EMNLP 2018 tutorial <a href="http://www.ruiyan.me/pubs/tutorial-emnlp18.pdf" target="_blank" rel="noopener">http://www.ruiyan.me/pubs/tutorial-emnlp18.pdf</a></p></li></ul><p>中文聊天机器人的资料</p><ul><li><p><a href="https://blog.csdn.net/hfutdog/article/details/78155467" target="_blank" rel="noopener">https://blog.csdn.net/hfutdog/article/details/78155467</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/23356655" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/23356655</a></p></li></ul><p>一些公司</p><ul><li><p>Gowild <a href="https://www.gowild.cn/home/ours/index.html" target="_blank" rel="noopener">https://www.gowild.cn/home/ours/index.html</a></p></li><li><p>Huggingface <a href="https://huggingface.co/" target="_blank" rel="noopener">https://huggingface.co/</a></p></li></ul><p>数据集</p><ul><li><p>movie dialogue dataset</p></li><li><p>ubuntu dialogue dataset</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 聊天机器人 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>聊天机器人一</title>
      <link href="/2020/03/10/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%80/"/>
      <url>/2020/03/10/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%80/</url>
      
        <content type="html"><![CDATA[<h2 id="聊天机器人"><a href="#聊天机器人" class="headerlink" title="聊天机器人"></a>聊天机器人</h2><h3 id="流行平台框架与实战"><a href="#流行平台框架与实战" class="headerlink" title="流行平台框架与实战"></a>流行平台框架与实战</h3><p>这堂课主要是带大家一起看看国内外一些主流的聊天机器人搭建平台。</p><h3 id="图灵机器人"><a href="#图灵机器人" class="headerlink" title="图灵机器人"></a>图灵机器人</h3><p>link: <a href="http://www.tuling123.com/" target="_blank" rel="noopener">http://www.tuling123.com/</a></p><p><img src="https://s1.ax1x.com/2020/06/09/t4iSHI.png" alt="title"></p><p>传说中在中文语境下智能度最高的机器人大脑。</p><h4 id="用法Demo"><a href="#用法Demo" class="headerlink" title="用法Demo"></a>用法Demo</h4><ol><li>注册</li><li>创建自己的机器人</li><li>连接微信公众号或QQ</li><li>社区</li></ol><h3 id="wxBot"><a href="#wxBot" class="headerlink" title="wxBot"></a>wxBot</h3><p>图灵机器人很棒，基本帮我们做到了傻瓜式接入主流中文社交软件。</p><p>但是，我们可以发现，这里依旧有限制，</p><p>官方提供的傻瓜式服务只能接入微信公众号或者QQ，</p><p>如果有更多的需求，那我们就得自己动手了。</p><p>比如，<strong>将个人微信变成聊天机器人</strong></p><p>这里我们介绍一个很棒的GitHub项目：WxBot（credit: @liuwons）</p><p>link: <a href="https://github.com/liuwons/wxBot" target="_blank" rel="noopener">https://github.com/liuwons/wxBot</a></p><p>这是一个是用Python包装Web微信协议实现的微信机器人框架。</p><p>换句话说，就是模拟了我们网页登录微信的状态，并通过网页微信协议传输对话。</p><p>好，接下来，我们来用wxbot实现一些有趣的功能：</p><h3 id="群聊天机器人"><a href="#群聊天机器人" class="headerlink" title="群聊天机器人"></a>群聊天机器人</h3><p>（转载自：<a href="http://blog.csdn.net/tobacco5648/article/details/50802922" target="_blank" rel="noopener">http://blog.csdn.net/tobacco5648/article/details/50802922</a> ）</p><p>实现效果</p><p><img src="https://s1.ax1x.com/2020/06/09/t4PzDA.png" alt="title"></p><p>有点像谷歌旗下的聊天工具 Allo</p><p><img src="https://s1.ax1x.com/2020/06/09/t4Pxud.jpg" alt="title"></p><p>简单说就是，在你聊天的时候，可以随时把小机器人给at出来，并且拉出来跟你聊天。</p><p>同时，如果调用图灵机器人的服务的话，你可以让它做很多复杂的工作，</p><p>比如，找附近的商店啊，查火车时刻表啊，等等。</p><h4 id="运行方法"><a href="#运行方法" class="headerlink" title="运行方法"></a>运行方法</h4><ol><li><p>下载wxBot， 安装python的依赖包。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install requests</span><br><span class="line">pip install pyqrcode</span><br><span class="line">pip install pypng</span><br><span class="line">pip install Pillow</span><br></pre></td></tr></table></figure></li><li><p>在图灵机器人官网注册账号，申请图灵API key</p></li><li><p>在bot.py文件所在目录下新建conf.ini文件，内容为(key字段内容为申请到的图灵key):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[main]</span><br><span class="line">key=1d2678900f734aa0a23734ace8aec5b1</span><br></pre></td></tr></table></figure></li><li><p>最后我们运行bot.py即可</p></li></ol><p>运行之后，你的terminal会跳出一个二维码。</p><p>你按照登录网页版微信的方式，扫一下 登录一下。</p><p>你的微信就被”托管“了 &gt;.&lt;</p><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># coding: utf-8</span><br><span class="line"></span><br><span class="line">from wxbot import *</span><br><span class="line">import ConfigParser</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">class TulingWXBot(WXBot):</span><br><span class="line">    # 拿到API Key</span><br><span class="line">    def __init__(self):</span><br><span class="line">        WXBot.__init__(self)</span><br><span class="line"></span><br><span class="line">        self.tuling_key = &quot;&quot;</span><br><span class="line">        self.robot_switch = True</span><br><span class="line"></span><br><span class="line">        try:</span><br><span class="line">            cf = ConfigParser.ConfigParser()</span><br><span class="line">            cf.read(&apos;conf.ini&apos;)</span><br><span class="line">            self.tuling_key = cf.get(&apos;main&apos;, &apos;key&apos;)</span><br><span class="line">        except Exception:</span><br><span class="line">            pass</span><br><span class="line">        print &apos;tuling_key:&apos;, self.tuling_key</span><br><span class="line">    </span><br><span class="line">    # 从图灵API返回自动回复</span><br><span class="line">    def tuling_auto_reply(self, uid, msg):</span><br><span class="line">        if self.tuling_key:</span><br><span class="line">            # API url</span><br><span class="line">            url = &quot;http://www.tuling123.com/openapi/api&quot;</span><br><span class="line">            user_id = uid.replace(&apos;@&apos;, &apos;&apos;)[:30]</span><br><span class="line">            # API request</span><br><span class="line">            body = &#123;&apos;key&apos;: self.tuling_key, &apos;info&apos;: msg.encode(&apos;utf8&apos;), &apos;userid&apos;: user_id&#125;</span><br><span class="line">            r = requests.post(url, data=body)</span><br><span class="line">            respond = json.loads(r.text)</span><br><span class="line">            result = &apos;&apos;</span><br><span class="line">            # 拿到回复，进行处理</span><br><span class="line">                # 按照API返回的code进行分类</span><br><span class="line">                # 是否是一个link</span><br><span class="line">                # 还是一句话</span><br><span class="line">                # 还是一组(list)回复</span><br><span class="line">            if respond[&apos;code&apos;] == 100000:</span><br><span class="line">                result = respond[&apos;text&apos;].replace(&apos;&lt;br&gt;&apos;, &apos;  &apos;)</span><br><span class="line">                result = result.replace(u&apos;\xa0&apos;, u&apos; &apos;)</span><br><span class="line">            elif respond[&apos;code&apos;] == 200000:</span><br><span class="line">                result = respond[&apos;url&apos;]</span><br><span class="line">            elif respond[&apos;code&apos;] == 302000:</span><br><span class="line">                for k in respond[&apos;list&apos;]:</span><br><span class="line">                    result = result + u&quot;【&quot; + k[&apos;source&apos;] + u&quot;】 &quot; +\</span><br><span class="line">                        k[&apos;article&apos;] + &quot;\t&quot; + k[&apos;detailurl&apos;] + &quot;\n&quot;</span><br><span class="line">            else:</span><br><span class="line">                result = respond[&apos;text&apos;].replace(&apos;&lt;br&gt;&apos;, &apos;  &apos;)</span><br><span class="line">                result = result.replace(u&apos;\xa0&apos;, u&apos; &apos;)</span><br><span class="line"></span><br><span class="line">            print &apos;    ROBOT:&apos;, result</span><br><span class="line">            return result</span><br><span class="line">        # 加了个exception，如果没有图灵API Key的话，</span><br><span class="line">        # 那就无脑回知道了。</span><br><span class="line">        else:</span><br><span class="line">            return u&quot;知道啦&quot;</span><br><span class="line">    </span><br><span class="line">    # 如果，用户不想机器人继续BB了，</span><br><span class="line">    # 或者，用户想言语调出机器人：</span><br><span class="line">    def auto_switch(self, msg):</span><br><span class="line">        msg_data = msg[&apos;content&apos;][&apos;data&apos;]</span><br><span class="line">        stop_cmd = [u&apos;退下&apos;, u&apos;走开&apos;, u&apos;关闭&apos;, u&apos;关掉&apos;, u&apos;休息&apos;, u&apos;滚开&apos;]</span><br><span class="line">        start_cmd = [u&apos;出来&apos;, u&apos;启动&apos;, u&apos;工作&apos;]</span><br><span class="line">        if self.robot_switch:</span><br><span class="line">            for i in stop_cmd:</span><br><span class="line">                if i == msg_data:</span><br><span class="line">                    self.robot_switch = False</span><br><span class="line">                    self.send_msg_by_uid(u&apos;[Robot]&apos; + u&apos;机器人已关闭！&apos;, msg[&apos;to_user_id&apos;])</span><br><span class="line">        else:</span><br><span class="line">            for i in start_cmd:</span><br><span class="line">                if i == msg_data:</span><br><span class="line">                    self.robot_switch = True</span><br><span class="line">                    self.send_msg_by_uid(u&apos;[Robot]&apos; + u&apos;机器人已开启！&apos;, msg[&apos;to_user_id&apos;])</span><br><span class="line">    </span><br><span class="line">    # 从微信回复</span><br><span class="line">    def handle_msg_all(self, msg):</span><br><span class="line">        if not self.robot_switch and msg[&apos;msg_type_id&apos;] != 1:</span><br><span class="line">            return</span><br><span class="line">        if msg[&apos;msg_type_id&apos;] == 1 and msg[&apos;content&apos;][&apos;type&apos;] == 0:  # reply to self</span><br><span class="line">            self.auto_switch(msg)</span><br><span class="line">        elif msg[&apos;msg_type_id&apos;] == 4 and msg[&apos;content&apos;][&apos;type&apos;] == 0:  # text message from contact</span><br><span class="line">            self.send_msg_by_uid(self.tuling_auto_reply(msg[&apos;user&apos;][&apos;id&apos;], msg[&apos;content&apos;][&apos;data&apos;]), msg[&apos;user&apos;][&apos;id&apos;])</span><br><span class="line">        elif msg[&apos;msg_type_id&apos;] == 3 and msg[&apos;content&apos;][&apos;type&apos;] == 0:  # group text message</span><br><span class="line">            if &apos;detail&apos; in msg[&apos;content&apos;]:</span><br><span class="line">                my_names = self.get_group_member_name(msg[&apos;user&apos;][&apos;id&apos;], self.my_account[&apos;UserName&apos;])</span><br><span class="line">                if my_names is None:</span><br><span class="line">                    my_names = &#123;&#125;</span><br><span class="line">                if &apos;NickName&apos; in self.my_account and self.my_account[&apos;NickName&apos;]:</span><br><span class="line">                    my_names[&apos;nickname2&apos;] = self.my_account[&apos;NickName&apos;]</span><br><span class="line">                if &apos;RemarkName&apos; in self.my_account and self.my_account[&apos;RemarkName&apos;]:</span><br><span class="line">                    my_names[&apos;remark_name2&apos;] = self.my_account[&apos;RemarkName&apos;]</span><br><span class="line"></span><br><span class="line">                is_at_me = False</span><br><span class="line">                for detail in msg[&apos;content&apos;][&apos;detail&apos;]:</span><br><span class="line">                    if detail[&apos;type&apos;] == &apos;at&apos;:</span><br><span class="line">                        for k in my_names:</span><br><span class="line">                            if my_names[k] and my_names[k] == detail[&apos;value&apos;]:</span><br><span class="line">                                is_at_me = True</span><br><span class="line">                                break</span><br><span class="line">                if is_at_me:</span><br><span class="line">                    src_name = msg[&apos;content&apos;][&apos;user&apos;][&apos;name&apos;]</span><br><span class="line">                    reply = &apos;to &apos; + src_name + &apos;: &apos;</span><br><span class="line">                    if msg[&apos;content&apos;][&apos;type&apos;] == 0:  # text message</span><br><span class="line">                        reply += self.tuling_auto_reply(msg[&apos;content&apos;][&apos;user&apos;][&apos;id&apos;], msg[&apos;content&apos;][&apos;desc&apos;])</span><br><span class="line">                    else:</span><br><span class="line">                        reply += u&quot;对不起，只认字，其他杂七杂八的我都不认识，,,Ծ‸Ծ,,&quot;</span><br><span class="line">                    self.send_msg_by_uid(reply, msg[&apos;user&apos;][&apos;id&apos;])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    bot = TulingWXBot()</span><br><span class="line">    bot.DEBUG = True</span><br><span class="line">    bot.conf[&apos;qr&apos;] = &apos;png&apos;</span><br><span class="line"></span><br><span class="line">    bot.run()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>GitHub上还有不少相似的项目，大家都可以关注一下：</p><p><a href="https://github.com/feit/Weixinbot" target="_blank" rel="noopener">feit/Weixinbot</a> Nodejs 封装网页版微信的接口，可编程控制微信消息</p><p><a href="https://github.com/littlecodersh/ItChat" target="_blank" rel="noopener">littlecodersh/ItChat</a> 微信个人号接口、微信机器人及命令行微信，Command line talks through Wechat</p><p><a href="https://github.com/Urinx/WeixinBot" target="_blank" rel="noopener">Urinx/WeixinBot</a> 网页版微信API，包含终端版微信及微信机器人</p><p><a href="https://github.com/zixia/wechaty" target="_blank" rel="noopener">zixia/wechaty</a> Wechaty is wechat for bot in Javascript(ES6). It’s a Personal Account Robot Framework/Library.</p><p><a href="https://coding.net/u/vivre/p/WxbotManage/git" target="_blank" rel="noopener">WxbotManage</a> 基于Wxbot的微信多开管理和Webapi系统</p><h3 id="自定义API接口"><a href="#自定义API接口" class="headerlink" title="自定义API接口"></a>自定义API接口</h3><p>刚刚我们讲的部分，还都是调用图灵机器人的API。</p><p>我们来看看，如何使用自己的ChatBot模型。</p><p>我们这里用之前讲过的Chatterbot库做个例子。</p><p>思路还是一样。我们用WxBot来处理微信端的工作，</p><p>然后，我们架设一个API，来把chatterbot的回复给传到微信去。</p><p>这里我们用一个简单粗暴的API框架：Hug。</p><p>大家也可以用其他各种框架，比如Flask。</p><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># coding: utf-8</span><br><span class="line"></span><br><span class="line"># 导入chatterbot自带的语料库</span><br><span class="line">from chatterbot import ChatBot</span><br><span class="line">from chatterbot.trainers import ChatterBotCorpusTrainer</span><br><span class="line">import hug</span><br><span class="line"></span><br><span class="line">deepThought = ChatBot(&quot;deepThought&quot;)</span><br><span class="line">deepThought.set_trainer(ChatterBotCorpusTrainer)</span><br><span class="line"># 使用中文语料库训练它</span><br><span class="line">deepThought.train(&quot;chatterbot.corpus.chinese&quot;)  # 语料库</span><br><span class="line"></span><br><span class="line"># API框架</span><br><span class="line">@hug.get()</span><br><span class="line">def get_response(user_input):</span><br><span class="line">    response = deepThought.get_response(user_input).text</span><br><span class="line">    return &#123;&quot;response&quot;:response&#125;</span><br></pre></td></tr></table></figure><p>hug -f bot_api.py</p><p>跑起来以后，你的terminal大概长这样：</p><p><img src="https://s1.ax1x.com/2020/06/09/t4PjjH.png" alt="title"></p><p>于是你可以在浏览器中尝试：</p><p><img src="https://s1.ax1x.com/2020/06/09/t4PXge.png" alt="title"></p><p>好，</p><p>接下来的部分，依旧是感谢@liuwons的wxbot：</p><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># coding: utf-8</span><br><span class="line"></span><br><span class="line">from wxbot import WXBot</span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line"># base url</span><br><span class="line">bot_api=&quot;http://127.0.0.1:8000/get_response&quot;</span><br><span class="line"></span><br><span class="line"># 处理回复</span><br><span class="line">class MyWXBot(WXBot):</span><br><span class="line">    def handle_msg_all(self, msg):</span><br><span class="line">        if msg[&apos;msg_type_id&apos;] == 4 and msg[&apos;content&apos;][&apos;type&apos;] == 0:</span><br><span class="line">            user_input = msg[&quot;content&quot;][&quot;data&quot;]</span><br><span class="line">            payload=&#123;&quot;user_input&quot;:user_input&#125;</span><br><span class="line">            response = requests.get(bot_api,params=payload).json()[&quot;response&quot;]</span><br><span class="line">            self.send_msg_by_uid(response, msg[&apos;user&apos;][&apos;id&apos;])</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    bot = MyWXBot()</span><br><span class="line">    bot.DEBUG = True</span><br><span class="line">    bot.conf[&apos;qr&apos;] = &apos;png&apos;</span><br><span class="line">    bot.run()</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>聊起来大概是这个节奏。</p><p><img src="https://s1.ax1x.com/2020/06/09/t4PL9O.jpg" alt="title"></p><p>好，如此，我们能做的显然不仅仅是用了个chatterbot</p><p>因为我们这样的一个结构，我们其实可以把任何聊天模型应用进来。</p><p>简单那来说，</p><p>Model &lt;–API–&gt; WxBot &lt;–web–&gt; WeChat</p><p>我们要做的就是封装一下我们的模型，让它看到一句话，给一个回复。</p><p>或者甚至是把VQA加到微信机器人中，发图片再发问题，求回复~</p><h3 id="一些有趣的微信机器人的idea"><a href="#一些有趣的微信机器人的idea" class="headerlink" title="一些有趣的微信机器人的idea"></a>一些有趣的微信机器人的idea</h3><ul><li>跨群转发。这是个非常实用的功能。对群来说，因为微信一个群最多500人， 跨群转发可以有效地把两个群拼到一起，实现更广泛的讨论。对个人来说，也可以用有选择的转发来把信息归档。比如看老板或者妹子在你加的几个群里每天都说了啥等等。 </li><li>聊天消息的主题归并，分析和搜索。微信聊天的基本单位是消息，但消息本身是非常碎片化的，很不适合搜索和分析。机器人可以把相关主题的消息归并起来，一方面可以大幅减小信息过载，一方面也可以从中得到更有价值的信息（类似视频分析里面把帧变成镜头）。这样分析以后可以做知识归档，用OneNote/印象笔记甚至公众号把讨论的成果沉淀下来。 </li><li>聊天脉络的梳理。群里的人一多，经常会出现几个话题并行出现的情况。这种情况对于理解和搜索都是非常不利的。机器人也需要把聊天的脉络进行梳理，在同一时间，把不同主题分别开。 </li><li>基本的统计数据。比如发言时间的分布，群的活跃度，成员的活跃度等等。做成漂亮的可视化，用户应该也会喜欢，给产品加分。 </li></ul><h3 id="国际上比较主流的几大聊天机器人框架"><a href="#国际上比较主流的几大聊天机器人框架" class="headerlink" title="国际上比较主流的几大聊天机器人框架"></a>国际上比较主流的几大聊天机器人框架</h3><ul><li>wit.ai</li><li>api.ai</li><li>microsoft bot framework</li></ul><p>虽然…</p><p>但是我们还是要稍微了解一下行业内的一些insights，</p><p>看看现在业内大火的各种炒聊天机器人概念的startups都是怎么玩的。</p><p>我们选点简单的开始玩起。</p><p>机器人端：api.ai，背后是谷歌</p><p>聊天端，我们选个Telegram，主打安全和快速的画风清奇的聊天软件~</p><h4 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h4><ol><li>注册api.ai</li><li>创建api.ai机器人</li><li>设置intents, entities等等</li><li>及时测试</li><li>走起telegram</li><li>跟机器人爸爸聊天, 设置</li><li><img src="https://s1.ax1x.com/2020/06/09/t4Pb4K.png" alt="avatar"></li><li>integrate</li><li>托管GitHub, Heroku</li><li>手机端试试玩儿</li><li><img src="https://s1.ax1x.com/2020/06/09/t4PIBR.png" alt="avatar"></li></ol><h3 id="直接调用api-ai进你自己的app-平台"><a href="#直接调用api-ai进你自己的app-平台" class="headerlink" title="直接调用api.ai进你自己的app/平台"></a>直接调用api.ai进你自己的app/平台</h3><p>其实这几个大平台都有自己的各个语言的官方支持库，让生活变得灰常简单：</p><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">!/usr/bin/env python</span><br><span class="line"> -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">import os.path</span><br><span class="line">import sys</span><br><span class="line">import json</span><br><span class="line">try:</span><br><span class="line">    import apiai</span><br><span class="line">except ImportError:</span><br><span class="line">    sys.path.append(</span><br><span class="line">        os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir)</span><br><span class="line">    )</span><br><span class="line">    import apiai</span><br><span class="line">    </span><br><span class="line"># api token</span><br><span class="line">CLIENT_ACCESS_TOKEN = &apos;your client access token&apos;</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    while(1):</span><br><span class="line">        ai = apiai.ApiAI(CLIENT_ACCESS_TOKEN)</span><br><span class="line">        request = ai.text_request()</span><br><span class="line">        request.lang = &apos;en&apos;  # 中文英文法语西语等等各种</span><br><span class="line"></span><br><span class="line">        # request.session_id = &quot;&lt;SESSION ID, UBIQUE FOR EACH USER&gt;&quot;</span><br><span class="line">        print(&quot;\n\nYour Input : &quot;,end=&quot; &quot;)</span><br><span class="line">        request.query = input()</span><br><span class="line"></span><br><span class="line">        print(&quot;\n\nBot\&apos;s response :&quot;,end=&quot; &quot;)</span><br><span class="line">        response = request.getresponse()</span><br><span class="line">        responsestr = response.read().decode(&apos;utf-8&apos;)</span><br><span class="line">        response_obj = json.loads(responsestr)</span><br><span class="line"></span><br><span class="line">        print(response_obj[&quot;result&quot;][&quot;fulfillment&quot;][&quot;speech&quot;])</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>跟之前的思路其实都差不多，</p><p>就是用框架的API调用来传送『对话』，解决问题</p><p>:P</p><p>就是这样！</p><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> wxBot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>结构化预测</title>
      <link href="/2020/03/09/%E7%BB%93%E6%9E%84%E5%8C%96%E9%A2%84%E6%B5%8B/"/>
      <url>/2020/03/09/%E7%BB%93%E6%9E%84%E5%8C%96%E9%A2%84%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<p>Beam search: greedy</p><p>P(y|x): x中文句子, y英文句子</p><p>|V|^n 50000^20</p><p>sampling: </p><p>dynamic programming: HMM</p><h3 id="什么是结构化预测？"><a href="#什么是结构化预测？" class="headerlink" title="什么是结构化预测？"></a>什么是结构化预测？</h3><p>分类器</p><ul><li><p>将输入x匹配到输出y</p></li><li><p>一种简单的分类器</p></li><li><p>对任意一个输入x，计算每个可能的y的分数 score (x, y, \theta)，其中\theta是模型参数</p></li><li><p>选择分数最高的label y作为预测的类别</p></li><li><p>一般来说，如果输出空间是指数(exponential)级别或者无限的，我们把这类问题成为结构化预测(structured prediction)</p></li></ul><p>y = argmax_{candidate}(score(x, candidate, \theta))</p><p>结构化预测案例</p><ul><li><p>POS Tagging</p></li><li><p>Unlabeled Segmentation</p></li><li><p>莎拉波娃现在居住在美国东南部的佛罗里达。 </p></li><li><p>莎拉波娃 现在 居住 在 美国 东南部 的 佛罗里达 。</p></li><li><p>Labeled Segmentation (Named Entity Recognition 命名实体识别)</p></li><li><p>Some questioned if <strong>Tim Cook</strong>’s first product would be a breakaway hit for <strong>Apple</strong>.</p></li></ul><p><img src="https://uploader.shimo.im/f/0b2FQCgcQDoaFgYO.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/2rIXUkEW7EABqhny.png!thumbnail" alt="img"></p><ul><li>Constituency Parsing</li></ul><p><img src="https://uploader.shimo.im/f/Y67R5D7B5xU9NIfx.png!thumbnail" alt="img"></p><ul><li>Coreference Resolution</li></ul><p><img src="https://uploader.shimo.im/f/JBZC4rYqXikNmCAF.png!thumbnail" alt="img"></p><ul><li><p>语言生成：有很多NLP任务涉及到生成一段话，一个短语，一篇文章等等</p></li><li><p>问答系统</p></li><li><p>文本翻译</p></li><li><p>文本摘要</p></li></ul><p>什么是结构化预测？</p><ul><li><p>比较官方的定义是，当parts functions无法被拆分成minimal parts的时候，这就是一个结构化预测的问题。但是我们这里不详细展开。</p></li><li><p>part是一个问题的一部分子问题</p></li><li><p>parts function：用来把输入/输出拆分成parts</p></li><li><p>parts可以重叠</p></li><li><p>minimal parts：该任务最小的parts</p></li><li><p>minimal parts是不重叠的</p></li><li><p>结构化score/loss函数是没有办法拆分成minimal parts的。当我们使用一个结构化的score或者损失函数的时候，我们就在做structured predcition。</p></li></ul><h1 id="结构化预测解决的问题"><a href="#结构化预测解决的问题" class="headerlink" title="结构化预测解决的问题"></a>结构化预测解决的问题</h1><h2 id="序列标签问题"><a href="#序列标签问题" class="headerlink" title="序列标签问题"></a>序列标签问题</h2><ul><li><p>输入长度为T</p></li><li><p>输出长度也是T</p></li><li><p>每个位置可能的候选是N个label中的一个</p></li><li><p>输出空间为 N ^ T</p></li></ul><p>序列标签的案例</p><ul><li><p>前向神经网络做POS tagging</p></li><li><p>输入是一个单词和它相邻的单词</p></li><li><p>输出是中心词的POS tag</p></li><li><p>训练loss: 每个位置中心词的log loss，每个位置的loss相加</p></li></ul><p><img src="https://uploader.shimo.im/f/MD4bUVFlbRIubgvg.png!thumbnail" alt="img"></p><ul><li>这不是一个”结构化预测“问题</li></ul><p>如果我们采用一个RNN模型来做词性标注，这就成为了一个结构化预测的问题。</p><p><img src="https://uploader.shimo.im/f/Zw4WT0bR9vckZebr.png!thumbnail" alt="img"></p><p>如果我们使用HMM模型，这也是一个结构化预测问题。</p><p><img src="https://uploader.shimo.im/f/LXDKAKfdUAk7t4HK.png!thumbnail" alt="img"></p><ul><li>不要把语言当做“bag of words”，单词之间有“结构”。</li></ul><p>训练完模型之后，如何很好地快速地做搜索？</p><p>Viterbi 算法 </p><p><a href="https://shimo.im/docs/TRvGRjwJP8TDrCjc" target="_blank" rel="noopener">https://shimo.im/docs/TRvGRjwJP8TDrCjc</a></p><p>图解Viterbi维特比算法</p><p><a href="https://zhuanlan.zhihu.com/p/63087935" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/63087935</a></p><p>Viterbi算法的时间复杂度比较高。（回忆一下Viterbi算法的时间复杂度是多少？）</p><p>所以人们发明了一些近似的算法，例如greedy decoding，以及Beam Seaech。</p><h1 id="CRF模型"><a href="#CRF模型" class="headerlink" title="CRF模型"></a>CRF模型</h1><p><img src="https://uploader.shimo.im/f/0u599eTQh9YH50Mk.png!thumbnail" alt="img"></p><p>CRF的条件概率公式</p><p><img src="https://uploader.shimo.im/f/HsY8kd4QK9sSyi9W.png!thumbnail" alt="img"></p><p>分母是很大的</p><p>y 有五种不同的可能性</p><p>20   5^20</p><p><img src="https://uploader.shimo.im/f/7JqSkdgEheIIZ6c9.png!thumbnail" alt="img"></p><p>参考资料</p><p><a href="http://www.robots.ox.ac.uk/~davidc/pubs/crfs_jan2015.pdf" target="_blank" rel="noopener">http://www.robots.ox.ac.uk/~davidc/pubs/crfs_jan2015.pdf</a></p><p><a href="http://www.cs.cmu.edu/~10715-f18/lectures/lecture2-crf.pdf" target="_blank" rel="noopener">http://www.cs.cmu.edu/~10715-f18/lectures/lecture2-crf.pdf</a></p><p><a href="http://www.davidsbatista.net/blog/2017/11/13/Conditional_Random_Fields/" target="_blank" rel="noopener">http://www.davidsbatista.net/blog/2017/11/13/Conditional_Random_Fields/</a></p><h2 id="Michael-Collins’-notes"><a href="#Michael-Collins’-notes" class="headerlink" title="Michael Collins’ notes"></a>Michael Collins’ notes</h2><p>Log Linear tagggers <a href="http://www.cs.columbia.edu/~mcollins/fall2014-loglineartaggers.pdf" target="_blank" rel="noopener">http://www.cs.columbia.edu/~mcollins/fall2014-loglineartaggers.pdf</a></p><p>CRF Model <a href="http://www.cs.columbia.edu/~mcollins/crf.pdf" target="_blank" rel="noopener">http://www.cs.columbia.edu/~mcollins/crf.pdf</a></p><p>Forward-Backward算法 <a href="http://www.cs.columbia.edu/~mcollins/fb.pdf" target="_blank" rel="noopener">http://www.cs.columbia.edu/~mcollins/fb.pdf</a></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><p><a href="https://taehwanptl.github.io/" target="_blank" rel="noopener">Advanced Topics in Machine Learning: Structured Prediction</a></p></li><li><p><a href="http://www.cs.cmu.edu/~nasmith/slides/sp4nlp.icml09.pdf" target="_blank" rel="noopener">Noah Smith的课件</a></p></li><li><p><a href="https://www.zhihu.com/question/35866596/answer/236886066?hb_wx_block=0" target="_blank" rel="noopener">条件随机场</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Beam search </tag>
            
            <tag> CRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SVM</title>
      <link href="/2020/02/20/SVM/"/>
      <url>/2020/02/20/SVM/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="https://zhuanlan.zhihu.com/p/36332083" target="_blank" rel="noopener">攀登传统机器学习的珠峰-SVM (上)</a></li><li><a href="https://zhuanlan.zhihu.com/p/36379394" target="_blank" rel="noopener">攀登传统机器学习的珠峰-SVM (中)</a></li><li><a href="https://zhuanlan.zhihu.com/p/36535299" target="_blank" rel="noopener">攀登传统机器学习的珠峰-SVM (下)</a></li></ul><h2 id="机器学习中的SVM"><a href="#机器学习中的SVM" class="headerlink" title="机器学习中的SVM"></a>机器学习中的SVM</h2><p>支持向量机是一种经典的二分类模型，基本模型定义为特征空间中最大间隔的线性分类器，其学习的优化目标便是间隔最大化，因此支持向量机本身可以转化为一个凸二次规划求解的问题。</p><h3 id="函数间隔与几何间隔"><a href="#函数间隔与几何间隔" class="headerlink" title="函数间隔与几何间隔"></a>函数间隔与几何间隔</h3><p>对于二分类学习，假设现在的数据是线性可分的，这时分类学习最基本的想法就是找到一个合适的超平面，该超平面能够将不同类别的样本分开，类似二维平面使用ax+by+c=0来表示，超平面实际上表示的就是高维的平面，如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f6a2ec8a.png" alt="1.png"></p><p>对数据点进行划分时，易知：当超平面距离与它最近的数据点的间隔越大，分类的鲁棒性越好，即当新的数据点加入时，超平面对这些点的适应性最强，出错的可能性最小。因此需要让所选择的超平面能够最大化这个间隔Gap（如下图所示）， 常用的间隔定义有两种，一种称之为函数间隔，一种为几何间隔，下面将分别介绍这两种间隔，并对SVM为什么会选用几何间隔做了一些阐述。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f6a06d5a.png" alt="2.png"></p><h4 id="函数间隔"><a href="#函数间隔" class="headerlink" title="函数间隔"></a>函数间隔</h4><p>在超平面w’x+b=0确定的情况下，|w’x<em>+b|能够代表点x*距离超平面的远近，易知：当w’x</em>+b&gt;0时，表示x<em>在超平面的一侧（正类，类标为1），而当w’x</em>+b&lt;0时，则表示x<em>在超平面的另外一侧（负类，类别为-1），因此（w’x</em>+b）y* 的正负性恰能表示数据点x<em>是否被分类正确。于是便引出了*</em>函数间隔**的定义（functional margin）:</p><p><img src="https://i.loli.net/2018/10/17/5bc72f690a14b.png" alt="3.png"></p><p>而超平面（w,b）关于所有样本点（Xi，Yi）的函数间隔最小值则为超平面在训练数据集T上的函数间隔：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f690ac26.png" alt="4.png"></p><p>可以看出：这样定义的函数间隔在处理SVM上会有问题，当超平面的两个参数w和b同比例改变时，函数间隔也会跟着改变，但是实际上超平面还是原来的超平面，并没有变化。例如：w1x1+w2x2+w3x3+b=0其实等价于2w1x1+2w2x2+2w3x3+2b=0，但计算的函数间隔却翻了一倍。从而引出了能真正度量点到超平面距离的概念–几何间隔（geometrical margin）。</p><h4 id="几何间隔"><a href="#几何间隔" class="headerlink" title="几何间隔"></a>几何间隔</h4><p><strong>几何间隔</strong>代表的则是数据点到超平面的真实距离，对于超平面w’x+b=0，w代表的是该超平面的法向量，设x<em>为超平面外一点x在法向量w方向上的投影点，x与超平面的距离为r，则有x</em>=x-r(w/||w||)，又x<em>在超平面上，即w’x</em>+b=0，代入即可得：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f697d499.png" alt="5.png"></p><p>为了得到r的绝对值，令r呈上其对应的类别y，即可得到几何间隔的定义：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f696fd10.png" alt="6.png"></p><p>从上述函数间隔与几何间隔的定义可以看出：实质上函数间隔就是|w’x+b|，而几何间隔就是点到超平面的距离。</p><h3 id="最大间隔与支持向量"><a href="#最大间隔与支持向量" class="headerlink" title="最大间隔与支持向量"></a>最大间隔与支持向量</h3><p>通过前面的分析可知：函数间隔不适合用来最大化间隔，因此这里我们要找的最大间隔指的是几何间隔，于是最大间隔分类器的目标函数定义为：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f69af163.png" alt="7.png"></p><p>一般地，我们令r^为1（这样做的目的是为了方便推导和目标函数的优化），从而上述目标函数转化为：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f697bb1d.png" alt="8.png"></p><p>对于y(w’x+b)=1的数据点，即下图中位于w’x+b=1或w’x+b=-1上的数据点，我们称之为<strong>支持向量</strong>（support vector），易知：对于所有的支持向量，它们恰好满足y<em>(w’x</em>+b)=1，而所有不是支持向量的点，有y<em>(w’x</em>+b)&gt;1。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f6a838c4.png" alt="9.png"></p><h3 id="从原始优化问题到对偶问题"><a href="#从原始优化问题到对偶问题" class="headerlink" title="从原始优化问题到对偶问题"></a>从原始优化问题到对偶问题</h3><p>对于上述得到的目标函数，求1/||w||的最大值相当于求||w||^2的最小值，因此很容易将原来的目标函数转化为：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f6978cbb.png" alt="10.png"></p><p>即变为了一个带约束的凸二次规划问题，按书上所说可以使用现成的优化计算包（QP优化包）求解，但由于SVM的特殊性，一般我们将原问题变换为它的<strong>对偶问题</strong>，接着再对其对偶问题进行求解。为什么通过对偶问题进行求解，有下面两个原因：</p><pre><code>* 一是因为使用对偶问题更容易求解；* 二是因为通过对偶问题求解出现了向量内积的形式，从而能更加自然地引出核函数。</code></pre><p>对偶问题，顾名思义，可以理解成优化等价的问题，更一般地，是将一个原始目标函数的最小化转化为它的对偶函数最大化的问题。对于当前的优化问题，首先我们写出它的朗格朗日函数：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9332be7.png" alt="11.png"></p><p>上式很容易验证：当其中有一个约束条件不满足时，L的最大值为 ∞（只需令其对应的α为 ∞即可）；当所有约束条件都满足时，L的最大值为1/2||w||^2（此时令所有的α为0），因此实际上原问题等价于：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f93321c5.png" alt="12.png"></p><p>由于这个的求解问题不好做，因此一般我们将最小和最大的位置交换一下（需满足KKT条件） ，变成原问题的对偶问题：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9330967.png" alt="13.png"></p><p>这样就将原问题的求最小变成了对偶问题求最大（用对偶这个词还是很形象），接下来便可以先求L对w和b的极小，再求L对α的极大。</p><p>（1）首先求L对w和b的极小，分别求L关于w和b的偏导，可以得出：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9333e66.png" alt="14.png"></p><p>将上述结果代入L得到：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f935ae21.png" alt="15.png"></p><p>（2）接着L关于α极大求解α（通过SMO算法求解，此处不做深入）。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9338a9d.png" alt="16.png"></p><p>（3）最后便可以根据求解出的α，计算出w和b，从而得到分类超平面函数。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f93419ca.png" alt="17.png"></p><p>在对新的点进行预测时，实际上就是将数据点x*代入分类函数f(x)=w’x+b中，若f(x)&gt;0，则为正类，f(x)&lt;0，则为负类，根据前面推导得出的w与b，分类函数如下所示，此时便出现了上面所提到的内积形式。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9353166.png" alt="18.png"></p><p>这里实际上只需计算新样本与支持向量的内积，因为对于非支持向量的数据点，其对应的拉格朗日乘子一定为0，根据最优化理论（K-T条件），对于不等式约束y(w’x+b)-1≥0，满足：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f933c947.png" alt="19.png">        </p><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>由于上述的超平面只能解决线性可分的问题，对于线性不可分的问题，例如：异或问题，我们需要使用核函数将其进行推广。一般地，解决线性不可分问题时，常常采用<strong>映射</strong>的方式，将低维原始空间映射到高维特征空间，使得数据集在高维空间中变得线性可分，从而再使用线性学习器分类。如果原始空间为有限维，即属性数有限，那么总是存在一个高维特征空间使得样本线性可分。若∅代表一个映射，则在特征空间中的划分函数变为：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f934303e.png" alt="20.png"></p><p>按照同样的方法，先写出新目标函数的拉格朗日函数，接着写出其对偶问题，求L关于w和b的极大，最后运用SOM求解α。可以得出：</p><p>（1）原对偶问题变为：</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc68b3b.png" alt="21.png"></p><p>（2）原分类函数变为：<br><img src="https://i.loli.net/2018/10/17/5bc730cc1b673.png" alt="22.png"></p><p>求解的过程中，只涉及到了高维特征空间中的内积运算，由于特征空间的维数可能会非常大，例如：若原始空间为二维，映射后的特征空间为5维，若原始空间为三维，映射后的特征空间将是19维，之后甚至可能出现无穷维，根本无法进行内积运算了，此时便引出了<strong>核函数</strong>（Kernel）的概念。</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc49adc.png" alt="23.png"></p><p>因此，核函数可以直接计算隐式映射到高维特征空间后的向量内积，而不需要显式地写出映射后的结果，它虽然完成了将特征从低维到高维的转换，但最终却是在低维空间中完成向量内积计算，与高维特征空间中的计算等效<strong>（低维计算，高维表现）</strong>，从而避免了直接在高维空间无法计算的问题。引入核函数后，原来的对偶问题与分类函数则变为：</p><p>（1）对偶问题：</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc173b2.png" alt="24.png"></p><p>（2）分类函数：</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc05959.png" alt="25.png"></p><p>因此，在线性不可分问题中，核函数的选择成了支持向量机的最大变数，若选择了不合适的核函数，则意味着将样本映射到了一个不合适的特征空间，则极可能导致性能不佳。同时，核函数需要满足以下这个必要条件：</p><p><img src="https://i.loli.net/2018/10/17/5bc730ccc468c.png" alt="26.png"></p><p>由于核函数的构造十分困难，通常我们都是从一些常用的核函数中选择，下面列出了几种常用的核函数：</p><p><img src="https://i.loli.net/2018/10/17/5bc730ccc541a.png" alt="27.png"></p><h3 id="软间隔支持向量机"><a href="#软间隔支持向量机" class="headerlink" title="软间隔支持向量机"></a>软间隔支持向量机</h3><p>前面的讨论中，我们主要解决了两个问题：当数据线性可分时，直接使用最大间隔的超平面划分；当数据线性不可分时，则通过核函数将数据映射到高维特征空间，使之线性可分。然而在现实问题中，对于某些情形还是很难处理，例如数据中有<strong>噪声</strong>的情形，噪声数据（<strong>outlier</strong>）本身就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束，如果不要这些噪声数据还好，当加入这些outlier后导致划分超平面被挤歪了，如下图所示，对支持向量机的泛化性能造成很大的影响。</p><p><img src="https://i.loli.net/2018/10/17/5bc730ccce68e.png" alt="28.png"></p><p>为了解决这一问题，我们需要允许某一些数据点不满足约束，即可以在一定程度上偏移超平面，同时使得不满足约束的数据点尽可能少，这便引出了<strong>“软间隔”支持向量机</strong>的概念</p><pre><code>* 允许某些数据点不满足约束y(w&apos;x+b)≥1；* 同时又使得不满足约束的样本尽可能少。</code></pre><p>这样优化目标变为：</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc6c9fe.png" alt="29.png"></p><p>如同阶跃函数，0/1损失函数虽然表示效果最好，但是数学性质不佳。因此常用其它函数作为“替代损失函数”。</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc5e5a9.png" alt="30.png"></p><p>支持向量机中的损失函数为<strong>hinge损失</strong>，引入<strong>“松弛变量”</strong>，目标函数与约束条件可以写为：</p><p><img src="https://i.loli.net/2018/10/17/5bc7317aa3411.png" alt="31.png"></p><p>其中C为一个参数，控制着目标函数与新引入正则项之间的权重，这样显然每个样本数据都有一个对应的松弛变量，用以表示该样本不满足约束的程度，将新的目标函数转化为拉格朗日函数得到：</p><p><img src="https://i.loli.net/2018/10/17/5bc7317a4c96e.png" alt="32.png"></p><p>按照与之前相同的方法，先让L求关于w，b以及松弛变量的极小，再使用SMO求出α，有：</p><p><img src="https://i.loli.net/2018/10/17/5bc7317a6dff2.png" alt="33.png"></p><p>将w代入L化简，便得到其对偶问题：</p><p><img src="https://i.loli.net/2018/10/17/5bc7317ab6646.png" alt="34.png"></p><p>将“软间隔”下产生的对偶问题与原对偶问题对比可以发现：新的对偶问题只是约束条件中的α多出了一个上限C，其它的完全相同，因此在引入核函数处理线性不可分问题时，便能使用与“硬间隔”支持向量机完全相同的方法。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>word-embedding</title>
      <link href="/2020/02/11/word-embedding/"/>
      <url>/2020/02/11/word-embedding/</url>
      
        <content type="html"><![CDATA[<h3 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h3><p>学习目标</p><ul><li>学习词向量的概念</li><li>用Skip-thought模型训练词向量</li><li>学习使用PyTorch dataset和dataloader</li><li>学习定义PyTorch模型</li><li>学习torch.nn中常见的Module<ul><li>Embedding</li></ul></li><li>学习常见的PyTorch operations<ul><li>bmm</li><li>logsigmoid</li></ul></li><li>保存和读取PyTorch模型</li></ul><p>使用的训练数据可以从以下链接下载到。</p><p>链接:<a href="https://pan.baidu.com/s/1tFeK3mXuVXEy3EMarfeWvg" target="_blank" rel="noopener">https://pan.baidu.com/s/1tFeK3mXuVXEy3EMarfeWvg</a> 密码:v2z5</p><p>在这一份notebook中，我们会（尽可能）尝试复现论文<a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">Distributed Representations of Words and Phrases and their Compositionality</a>中训练词向量的方法. 我们会实现Skip-gram模型，并且使用论文中noice contrastive sampling的目标函数。</p><p>这篇论文有很多模型实现的细节，这些细节对于词向量的好坏至关重要。我们虽然无法完全复现论文中的实验结果，主要是由于计算资源等各种细节原因，但是我们还是可以大致展示如何训练词向量。</p><p>以下是一些我们没有实现的细节</p><ul><li>subsampling：参考论文section 2.3</li></ul><p>In [1]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from torch import nn</span><br></pre></td></tr></table></figure><p>In [2]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn  <span class="comment">#神经网络工具箱torch.nn </span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F  <span class="comment">#神经网络函数torch.nn.functional</span></span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> tud  <span class="comment">#Pytorch读取训练集需要用到torch.utils.data类</span></span><br></pre></td></tr></table></figure><p><strong>两个模块的区别：</strong><a href="https://blog.csdn.net/hawkcici160/article/details/80140059" target="_blank" rel="noopener">torch.nn 和 torch.functional 的区别</a></p><p>In [3]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.parameter <span class="keyword">import</span> Parameter  <span class="comment">#参数更新和优化函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter <span class="comment">#Counter 计数器</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> scipy <span class="comment">#SciPy是基于NumPy开发的高级模块，它提供了许多数学算法和函数的实现</span></span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity <span class="comment">#余弦相似度函数</span></span><br></pre></td></tr></table></figure><p>开始看代码前，请确保对word2vec有了解。</p><p><a href="https://blog.csdn.net/lilong117194/article/details/81979522" target="_blank" rel="noopener">CBOW模型理解</a></p><p><a href="https://www.jianshu.com/p/da235893e4a5" target="_blank" rel="noopener">Skip-Gram模型理解</a></p><p>负例采样就是Skip-Gram模型的输出不是周围词的概率了，是正例和负例的概率</p><p>In [4]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">USE_CUDA = torch.cuda.is_available() <span class="comment">#有GPU可以用</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了保证实验结果可以复现，我们经常会把各种random seed固定在某一个值</span></span><br><span class="line">random.seed(<span class="number">53113</span>)</span><br><span class="line">np.random.seed(<span class="number">53113</span>)</span><br><span class="line">torch.manual_seed(<span class="number">53113</span>)</span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    torch.cuda.manual_seed(<span class="number">53113</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 设定一些超参数   </span></span><br><span class="line">K = <span class="number">10</span> <span class="comment"># number of negative samples 负样本随机采样数量</span></span><br><span class="line">C = <span class="number">3</span> <span class="comment"># nearby words threshold 指定周围三个单词进行预测</span></span><br><span class="line">NUM_EPOCHS = <span class="number">2</span> <span class="comment"># The number of epochs of training 迭代轮数</span></span><br><span class="line">MAX_VOCAB_SIZE = <span class="number">30000</span> <span class="comment"># the vocabulary size 词汇表多大</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span> <span class="comment"># the batch size 每轮迭代1个batch的数量</span></span><br><span class="line">LEARNING_RATE = <span class="number">0.2</span> <span class="comment"># the initial learning rate #学习率</span></span><br><span class="line">EMBEDDING_SIZE = <span class="number">100</span> <span class="comment">#词向量维度</span></span><br><span class="line">       </span><br><span class="line">    </span><br><span class="line">LOG_FILE = <span class="string">"word-embedding.log"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tokenize函数，把一篇文本转化成一个个单词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_tokenize</span><span class="params">(text)</span>:</span> </span><br><span class="line">    <span class="keyword">return</span> text.split()</span><br></pre></td></tr></table></figure><ul><li>从文本文件中读取所有的文字，通过这些文本创建一个vocabulary</li><li>由于单词数量可能太大，我们只选取最常见的MAX_VOCAB_SIZE个单词</li><li>我们添加一个UNK单词表示所有不常见的单词</li><li>我们需要记录单词到index的mapping，以及index到单词的mapping，单词的count，单词的(normalized) frequency，以及单词总数。</li></ul><p>In [5]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">"./text8/text8.train.txt"</span>, <span class="string">"r"</span>) <span class="keyword">as</span> fin: <span class="comment">#读入文件</span></span><br><span class="line">    text = fin.read() <span class="comment"># 一次性读入文件所有内容</span></span><br><span class="line">    </span><br><span class="line">text = [w <span class="keyword">for</span> w <span class="keyword">in</span> word_tokenize(text.lower())] </span><br><span class="line"><span class="comment">#分词，在这里类似于text.split()</span></span><br><span class="line"><span class="comment">#print(len(text)) # 15313011，有辣么多单词</span></span><br><span class="line"></span><br><span class="line">vocab = dict(Counter(text).most_common(MAX_VOCAB_SIZE<span class="number">-1</span>))</span><br><span class="line"><span class="comment">#字典格式，把（MAX_VOCAB_SIZE-1）个最频繁出现的单词取出来，-1是留给不常见的单词</span></span><br><span class="line"><span class="comment">#print(len(vocab)) # 29999</span></span><br><span class="line"></span><br><span class="line">vocab[<span class="string">"&lt;unk&gt;"</span>] = len(text) - np.sum(list(vocab.values()))</span><br><span class="line"><span class="comment">#unk表示不常见单词数=总单词数-常见单词数</span></span><br><span class="line"><span class="comment"># print(vocab["&lt;unk&gt;"]) # 617111</span></span><br><span class="line">print(vocab[<span class="string">"&lt;unk&gt;"</span>])</span><br><span class="line">idx_to_word = [word <span class="keyword">for</span> word <span class="keyword">in</span> vocab.keys()] </span><br><span class="line"><span class="comment">#取出字典的所有最常见30000单词</span></span><br><span class="line"></span><br><span class="line">word_to_idx = &#123;word:i <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(idx_to_word)&#125;</span><br><span class="line"><span class="comment">#取出所有单词的单词和对应的索引，索引值与单词出现次数相反，最常见单词索引为0。</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">617111</span><br></pre></td></tr></table></figure><p>In [1]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#print(vocab)</span></span><br></pre></td></tr></table></figure><p>In [2]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#print(list(word_to_idx.items())[29900:]) </span></span><br><span class="line"><span class="comment"># 敲黑板：字典是怎么像列表那样切片的</span></span><br></pre></td></tr></table></figure><p>In [9]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">word_counts = np.array([count <span class="keyword">for</span> count <span class="keyword">in</span> vocab.values()], dtype=np.float32)</span><br><span class="line"><span class="comment">#vocab所有单词的频数values</span></span><br><span class="line"></span><br><span class="line">word_freqs = word_counts / np.sum(word_counts)</span><br><span class="line"><span class="comment">#所有单词的词频概率值</span></span><br><span class="line"><span class="comment"># print(np.sum(word_freqs))=1</span></span><br><span class="line"></span><br><span class="line">word_freqs = word_freqs ** (<span class="number">3.</span>/<span class="number">4.</span>)</span><br><span class="line"><span class="comment">#论文里乘以3/4次方</span></span><br><span class="line"><span class="comment"># print(np.sum(word_freqs)) = 7.7</span></span><br><span class="line"></span><br><span class="line">word_freqs = word_freqs / np.sum(word_freqs) <span class="comment"># 用来做 negative sampling</span></span><br><span class="line"><span class="comment"># 重新计算所有单词的频率，老师这里代码好像写错了</span></span><br><span class="line"><span class="comment"># print(np.sum(word_freqs)) = 1</span></span><br><span class="line"></span><br><span class="line">VOCAB_SIZE = len(idx_to_word) <span class="comment">#词汇表单词数30000=MAX_VOCAB_SIZE</span></span><br><span class="line">VOCAB_SIZE</span><br></pre></td></tr></table></figure><p>Out[9]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">30000</span><br></pre></td></tr></table></figure><h3 id="实现Dataloader"><a href="#实现Dataloader" class="headerlink" title="实现Dataloader"></a>实现Dataloader</h3><p>一个dataloader需要以下内容：</p><ul><li>把所有text编码成数字，然后用subsampling预处理这些文字。</li><li>保存vocabulary，单词count，normalized word frequency</li><li>每个iteration sample一个中心词</li><li>根据当前的中心词返回context单词</li><li>根据中心词sample一些negative单词</li><li>返回单词的counts</li></ul><p>这里有一个好的tutorial介绍如何使用<a href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html" target="_blank" rel="noopener">PyTorch dataloader</a>. 为了使用dataloader，我们需要定义以下两个function:</p><ul><li><code>__len__</code> function需要返回整个数据集中有多少个item</li><li><code>__get__</code> 根据给定的index返回一个item</li></ul><p>有了dataloader之后，我们可以轻松随机打乱整个数据集，拿到一个batch的数据等等。</p><p>torch.utils.data.DataLoader理解：<a href="https://blog.csdn.net/qq_36653505/article/details/83351808" target="_blank" rel="noopener">https://blog.csdn.net/qq_36653505/article/details/83351808</a></p><p>In [10]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordEmbeddingDataset</span><span class="params">(tud.Dataset)</span>:</span> <span class="comment">#tud.Dataset父类</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, text, word_to_idx, idx_to_word, word_freqs, word_counts)</span>:</span></span><br><span class="line">        <span class="string">''' text: a list of words, all text from the training dataset</span></span><br><span class="line"><span class="string">            word_to_idx: the dictionary from word to idx</span></span><br><span class="line"><span class="string">            idx_to_word: idx to word mapping</span></span><br><span class="line"><span class="string">            word_freq: the frequency of each word</span></span><br><span class="line"><span class="string">            word_counts: the word counts</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super(WordEmbeddingDataset, self).__init__() <span class="comment">#初始化模型</span></span><br><span class="line">        self.text_encoded = [word_to_idx.get(t, VOCAB_SIZE<span class="number">-1</span>) <span class="keyword">for</span> t <span class="keyword">in</span> text]</span><br><span class="line">        <span class="comment">#字典 get() 函数返回指定键的值（第一个参数），如果值不在字典中返回默认值（第二个参数）。</span></span><br><span class="line">        <span class="comment">#取出text里每个单词word_to_idx字典里对应的索引,不在字典里返回"&lt;unk&gt;"的索引=29999</span></span><br><span class="line">        <span class="comment"># 这样text里的所有词都编码好了，从单词转化为了向量，</span></span><br><span class="line">        <span class="comment"># 共有15313011个单词，词向量取值范围是0～29999，0是最常见单词向量。</span></span><br><span class="line">        </span><br><span class="line">        self.text_encoded = torch.Tensor(self.text_encoded).long()</span><br><span class="line">        <span class="comment">#变成tensor类型，这里变成longtensor，也可以torch.LongTensor(self.text_encoded)</span></span><br><span class="line">        </span><br><span class="line">        self.word_to_idx = word_to_idx <span class="comment">#保存数据</span></span><br><span class="line">        self.idx_to_word = idx_to_word  <span class="comment">#保存数据</span></span><br><span class="line">        self.word_freqs = torch.Tensor(word_freqs) <span class="comment">#保存数据</span></span><br><span class="line">        self.word_counts = torch.Tensor(word_counts) <span class="comment">#保存数据</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span> <span class="comment">#数据集有多少个item </span></span><br><span class="line">        <span class="comment">#魔法函数__len__</span></span><br><span class="line">        <span class="string">''' 返回整个数据集（所有单词）的长度</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> len(self.text_encoded) <span class="comment">#所有单词的总数</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="comment">#魔法函数__getitem__，这个函数跟普通函数不一样</span></span><br><span class="line">        <span class="string">''' 这个function返回以下数据用于训练</span></span><br><span class="line"><span class="string">            - 中心词</span></span><br><span class="line"><span class="string">            - 这个单词附近的(positive)单词</span></span><br><span class="line"><span class="string">            - 随机采样的K个单词作为negative sample</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        center_word = self.text_encoded[idx] </span><br><span class="line">        <span class="comment">#中心词</span></span><br><span class="line">        <span class="comment">#这里__getitem__函数是个迭代器，idx代表了所有的单词索引。</span></span><br><span class="line">        </span><br><span class="line">        pos_indices = list(range(idx-C, idx)) + list(range(idx+<span class="number">1</span>, idx+C+<span class="number">1</span>))</span><br><span class="line">        <span class="comment">#周围词的索引，比如idx=0时。pos_indices = [-3, -2, -1, 1, 2, 3] </span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        pos_indices = [i%len(self.text_encoded) <span class="keyword">for</span> i <span class="keyword">in</span> pos_indices]</span><br><span class="line">        <span class="comment">#超出词汇总数时，需要特别处理，取余数，比如pos_indices = [15313009,15313010,15313011,1,2,3]</span></span><br><span class="line">        </span><br><span class="line">        pos_words = self.text_encoded[pos_indices]</span><br><span class="line">        <span class="comment">#周围词，就是希望出现的正例单词</span></span><br><span class="line">        </span><br><span class="line">        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[<span class="number">0</span>], <span class="literal">True</span>)</span><br><span class="line">        <span class="comment">#负例采样单词，torch.multinomial作用是按照self.word_freqs的概率做K * pos_words.shape[0]次取值，</span></span><br><span class="line">        <span class="comment">#输出的是self.word_freqs对应的下标。取样方式采用有放回的采样，并且self.word_freqs数值越大，取样概率越大。</span></span><br><span class="line">        <span class="comment">#每个正确的单词采样K个，pos_words.shape[0]是正确单词数量=6</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> center_word, pos_words, neg_words</span><br></pre></td></tr></table></figure><p>创建dataset和dataloader</p><p>In [11]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataset = WordEmbeddingDataset(text, word_to_idx, idx_to_word, word_freqs, word_counts)</span><br><span class="line"># list(dataset) 可以把尝试打印下center_word, pos_words, neg_words看看</span><br></pre></td></tr></table></figure><p>torch.utils.data.DataLoader理解：<a href="https://blog.csdn.net/qq_36653505/article/details/83351808" target="_blank" rel="noopener">https://blog.csdn.net/qq_36653505/article/details/83351808</a></p><p>In [12]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataloader = tud.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>In [13]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(next(iter(dataloader))[<span class="number">0</span>].shape) <span class="comment"># 一个batch中间词维度</span></span><br><span class="line">print(next(iter(dataloader))[<span class="number">1</span>].shape) <span class="comment"># 一个batch周围词维度</span></span><br><span class="line">print(next(iter(dataloader))[<span class="number">2</span>].shape) <span class="comment"># 一个batch负样本维度</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">128</span>])</span><br><span class="line">torch.Size([<span class="number">128</span>, <span class="number">6</span>])</span><br><span class="line">torch.Size([<span class="number">128</span>, <span class="number">60</span>])</span><br></pre></td></tr></table></figure><h3 id="定义PyTorch模型"><a href="#定义PyTorch模型" class="headerlink" title="定义PyTorch模型"></a>定义PyTorch模型</h3><p>In [14]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmbeddingModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size)</span>:</span></span><br><span class="line">        <span class="string">''' 初始化输出和输出embedding</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super(EmbeddingModel, self).__init__()</span><br><span class="line">        self.vocab_size = vocab_size  <span class="comment">#30000</span></span><br><span class="line">        self.embed_size = embed_size  <span class="comment">#100</span></span><br><span class="line">        </span><br><span class="line">        initrange = <span class="number">0.5</span> / self.embed_size</span><br><span class="line">        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment">#模型输出nn.Embedding(30000, 100)</span></span><br><span class="line">        self.out_embed.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        <span class="comment">#权重初始化的一种方法</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=<span class="literal">False</span>)</span><br><span class="line">         <span class="comment">#模型输入nn.Embedding(30000, 100)</span></span><br><span class="line">        self.in_embed.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        <span class="comment">#权重初始化的一种方法</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_labels, pos_labels, neg_labels)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        input_labels: 中心词, [batch_size]</span></span><br><span class="line"><span class="string">        pos_labels: 中心词周围 context window 出现过的单词 [batch_size * (window_size * 2)]</span></span><br><span class="line"><span class="string">        neg_labelss: 中心词周围没有出现过的单词，从 negative sampling 得到 [batch_size, (window_size * 2 * K)]</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        return: loss, [batch_size]</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        </span><br><span class="line">        batch_size = input_labels.size(<span class="number">0</span>)  <span class="comment">#input_labels是输入的标签，tud.DataLoader()返回的。已经被分成batch了。</span></span><br><span class="line">        </span><br><span class="line">        input_embedding = self.in_embed(input_labels) </span><br><span class="line">        <span class="comment"># B * embed_size</span></span><br><span class="line">        <span class="comment">#这里估计进行了运算：（128,30000）*（30000,100）= 128(Batch) * 100 (embed_size)</span></span><br><span class="line">        </span><br><span class="line">        pos_embedding = self.out_embed(pos_labels) <span class="comment"># B * (2*C=6) * embed_size </span></span><br><span class="line">        <span class="comment"># 这里估计进行了运算：（128,6,30000）*（128,30000,100）= 128(Batch) * 6 * 100 (embed_size)</span></span><br><span class="line">        <span class="comment">#同上，增加了维度(2*C)，表示一个batch有128组周围词单词，一组周围词有(2*C)个单词，每个单词有embed_size个维度。</span></span><br><span class="line">        </span><br><span class="line">        neg_embedding = self.out_embed(neg_labels) <span class="comment"># B * (2*C * K) * embed_size</span></span><br><span class="line">        <span class="comment">#同上，增加了维度(2*C*K)</span></span><br><span class="line">      </span><br><span class="line">    </span><br><span class="line">        <span class="comment">#torch.bmm()为batch间的矩阵相乘（b,n.m)*(b,m,p)=(b,n,p)</span></span><br><span class="line">        log_pos = torch.bmm(pos_embedding, input_embedding.unsqueeze(<span class="number">2</span>)).squeeze() <span class="comment"># B * (2*C)</span></span><br><span class="line">        <span class="comment"># log_pos = (128,6,100)*(128,100,1) = (128,6,1) = (128,6)</span></span><br><span class="line">        <span class="comment"># 这里如果没有负采样，只有周围单词来训练的话，每个周围单词30000个one-hot向量的维度</span></span><br><span class="line">        <span class="comment"># 而负采样大大降低了维度，每个周围单词仅仅只有一个维度。每个样本输出共有2*C个维度</span></span><br><span class="line">        log_neg = torch.bmm(neg_embedding, -input_embedding.unsqueeze(<span class="number">2</span>)).squeeze() <span class="comment"># B * (2*C*K)</span></span><br><span class="line">        <span class="comment"># log_neg = (128,6*K,100)*(128,100,1) = (128,6*K,1) = (128,6*K)，注意这里有个负号，区别与正样本</span></span><br><span class="line">        <span class="comment"># unsqueeze(2)指定位置升维，.squeeze()压缩维度。</span></span><br><span class="line">        <span class="comment"># 而负采样降低了维度，每个负例单词仅仅只有一个维度，每个样本输出共有2*C*K个维度</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#下面loss计算就是论文里的公式</span></span><br><span class="line">        log_pos = F.logsigmoid(log_pos).sum(<span class="number">1</span>)</span><br><span class="line">        log_neg = F.logsigmoid(log_neg).sum(<span class="number">1</span>) <span class="comment"># batch_size     </span></span><br><span class="line">        loss = log_pos + log_neg <span class="comment"># 正样本损失和负样本损失和尽量最大</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> -loss <span class="comment"># 最大转化成最小</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#取出self.in_embed数据参数，维度：（30000,100），就是我们要训练的词向量</span></span><br><span class="line">    <span class="comment"># 这里本来模型训练有两个矩阵的，self.in_embed和self.out_embed两个</span></span><br><span class="line">    <span class="comment"># 只是作者认为输入矩阵比较好，就舍弃了输出矩阵。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">input_embeddings</span><span class="params">(self)</span>:</span>   </span><br><span class="line">        <span class="keyword">return</span> self.in_embed.weight.data.cpu().numpy()</span><br></pre></td></tr></table></figure><p>定义一个模型以及把模型移动到GPU</p><p>In [15]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_SIZE)</span><br><span class="line"><span class="comment">#得到model，有参数，有loss，可以优化了</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    model = model.cuda()</span><br></pre></td></tr></table></figure><p>In [28]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.iloc[:, <span class="number">0</span>:<span class="number">2</span>].index</span><br></pre></td></tr></table></figure><p>Out[28]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RangeIndex(start=<span class="number">0</span>, stop=<span class="number">353</span>, step=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">"wordsim353.csv"</span>, sep=<span class="string">","</span>)</span><br><span class="line"><span class="comment"># else:</span></span><br><span class="line"><span class="comment">#     data = pd.read_csv("simlex-999.txt", sep="\t")</span></span><br><span class="line">print(data.head())</span><br><span class="line">human_similarity = []</span><br><span class="line">model_similarity = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data.iloc[:, <span class="number">0</span>:<span class="number">2</span>].index:</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure><p>下面是评估模型的代码，以及训练模型的代码</p><p>In [16]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(filename, embedding_weights)</span>:</span> </span><br><span class="line">    <span class="comment"># 传入的有三个文件课选择,两个txt，一个csv，可以先自己打开看看</span></span><br><span class="line">    <span class="comment"># embedding_weights是训练之后的embedding向量。</span></span><br><span class="line">    <span class="keyword">if</span> filename.endswith(<span class="string">".csv"</span>):</span><br><span class="line">        data = pd.read_csv(filename, sep=<span class="string">","</span>) <span class="comment"># csv文件打开</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data = pd.read_csv(filename, sep=<span class="string">"\t"</span>) <span class="comment"># txt文件打开，以\t制表符分割</span></span><br><span class="line">    human_similarity = []</span><br><span class="line">    model_similarity = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> data.iloc[:, <span class="number">0</span>:<span class="number">2</span>].index: <span class="comment"># 这里只是取出行索引，用data.index也可以</span></span><br><span class="line">        word1, word2 = data.iloc[i, <span class="number">0</span>], data.iloc[i, <span class="number">1</span>] <span class="comment"># 依次取出每行的2个单词</span></span><br><span class="line">        <span class="keyword">if</span> word1 <span class="keyword">not</span> <span class="keyword">in</span> word_to_idx <span class="keyword">or</span> word2 <span class="keyword">not</span> <span class="keyword">in</span> word_to_idx:</span><br><span class="line">            <span class="comment"># 如果取出的单词不在我们建的30000万个词汇表，就舍弃，评估不了</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            word1_idx, word2_idx = word_to_idx[word1], word_to_idx[word2]</span><br><span class="line">            <span class="comment"># 否则，分别取出这两个单词对应的向量，</span></span><br><span class="line">            word1_embed, word2_embed = embedding_weights[[word1_idx]], embedding_weights[[word2_idx]]</span><br><span class="line">            <span class="comment"># 在分别取出这两个单词对应的embedding向量，具体为啥是这种取出方式[[word1_idx]]，可以自行研究</span></span><br><span class="line">            model_similarity.append(float(sklearn.metrics.pairwise.cosine_similarity(word1_embed, word2_embed)))</span><br><span class="line">            <span class="comment"># 用余弦相似度计算这两个100维向量的相似度。这个是模型算出来的相似度</span></span><br><span class="line">            human_similarity.append(float(data.iloc[i, <span class="number">2</span>]))</span><br><span class="line">            <span class="comment"># 这个是人类统计得到的相似度</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> scipy.stats.spearmanr(human_similarity, model_similarity)<span class="comment"># , model_similarity</span></span><br><span class="line">    <span class="comment"># 因为相似度是浮点数，不是0 1 这些固定标签值，所以不能用准确度评估指标</span></span><br><span class="line">    <span class="comment"># scipy.stats.spearmanr网址：https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html</span></span><br><span class="line">    <span class="comment"># scipy.stats.spearmanr评估两个分布的相似度，有两个返回值correlation, pvalue</span></span><br><span class="line">    <span class="comment"># correlation是评估相关性的指标（-1，1），越接近1越相关，pvalue值大家可以自己搜索理解</span></span><br></pre></td></tr></table></figure><p>训练模型：</p><ul><li>模型一般需要训练若干个epoch</li><li>每个epoch我们都把所有的数据分成若干个batch</li><li>把每个batch的输入和输出都包装成cuda tensor</li><li>forward pass，通过输入的句子预测每个单词的下一个单词</li><li>用模型的预测和正确的下一个单词计算cross entropy loss</li><li>清空模型当前gradient</li><li>backward pass</li><li>更新模型参数</li><li>每隔一定的iteration输出模型在当前iteration的loss，以及在验证数据集上做模型的评估</li></ul><p>In [17]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, (input_labels, pos_labels, neg_labels) <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">        print(input_labels.shape, pos_labels.shape, neg_labels.shape)</span><br><span class="line">        <span class="keyword">if</span> i&gt;<span class="number">5</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">128</span>]) torch.Size([<span class="number">128</span>, <span class="number">6</span>]) torch.Size([<span class="number">128</span>, <span class="number">60</span>])</span><br><span class="line">torch.Size([<span class="number">128</span>]) torch.Size([<span class="number">128</span>, <span class="number">6</span>]) torch.Size([<span class="number">128</span>, <span class="number">60</span>])</span><br><span class="line">torch.Size([<span class="number">128</span>]) torch.Size([<span class="number">128</span>, <span class="number">6</span>]) torch.Size([<span class="number">128</span>, <span class="number">60</span>])</span><br><span class="line">torch.Size([<span class="number">128</span>]) torch.Size([<span class="number">128</span>, <span class="number">6</span>]) torch.Size([<span class="number">128</span>, <span class="number">60</span>])</span><br><span class="line">torch.Size([<span class="number">128</span>]) torch.Size([<span class="number">128</span>, <span class="number">6</span>]) torch.Size([<span class="number">128</span>, <span class="number">60</span>])</span><br><span class="line">torch.Size([<span class="number">128</span>]) torch.Size([<span class="number">128</span>, <span class="number">6</span>]) torch.Size([<span class="number">128</span>, <span class="number">60</span>])</span><br><span class="line">torch.Size([<span class="number">128</span>]) torch.Size([<span class="number">128</span>, <span class="number">6</span>]) torch.Size([<span class="number">128</span>, <span class="number">60</span>])</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line"><span class="comment">#随机梯度下降</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(NUM_EPOCHS): <span class="comment">#开始迭代</span></span><br><span class="line">    <span class="keyword">for</span> i, (input_labels, pos_labels, neg_labels) <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">        <span class="comment">#print(input_labels, pos_labels, neg_labels)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># TODO</span></span><br><span class="line">        input_labels = input_labels.long() <span class="comment">#longtensor</span></span><br><span class="line">        pos_labels = pos_labels.long()</span><br><span class="line">        neg_labels = neg_labels.long()</span><br><span class="line">        <span class="keyword">if</span> USE_CUDA: <span class="comment"># 变成cuda类型</span></span><br><span class="line">            input_labels = input_labels.cuda()</span><br><span class="line">            pos_labels = pos_labels.cuda()</span><br><span class="line">            neg_labels = neg_labels.cuda()</span><br><span class="line">       </span><br><span class="line">        <span class="comment">#下面第一节课都讲过的   </span></span><br><span class="line">        optimizer.zero_grad() <span class="comment">#梯度归零</span></span><br><span class="line">        loss = model(input_labels, pos_labels, neg_labels).mean() </span><br><span class="line">        <span class="comment"># model返回的是一个batch所有样本的损失，需要求个平均</span></span><br><span class="line">        </span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">       </span><br><span class="line">        <span class="comment">#打印结果。</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">with</span> open(LOG_FILE, <span class="string">"a"</span>) <span class="keyword">as</span> fout: <span class="comment"># 写进日志文件，LOG_FILE前面定义了</span></span><br><span class="line">                fout.write(<span class="string">"epoch: &#123;&#125;, iter: &#123;&#125;, loss: &#123;&#125;\n"</span>.format(e, i, loss.item()))</span><br><span class="line">                print(<span class="string">"epoch: &#123;&#125;, iter: &#123;&#125;, loss: &#123;&#125;"</span>.format(e, i, loss.item()))</span><br><span class="line">                <span class="comment"># 训练过程，我没跑，本地肯定跑不动的</span></span><br><span class="line">            </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">0</span>: <span class="comment"># 每过2000个batch就评估一次效果</span></span><br><span class="line">            embedding_weights = model.input_embeddings() </span><br><span class="line">            <span class="comment"># 取出（30000，100）训练的词向量</span></span><br><span class="line">            sim_simlex = evaluate(<span class="string">"simlex-999.txt"</span>, embedding_weights)</span><br><span class="line">            sim_men = evaluate(<span class="string">"men.txt"</span>, embedding_weights)</span><br><span class="line">            sim_353 = evaluate(<span class="string">"wordsim353.csv"</span>, embedding_weights)</span><br><span class="line">            <span class="keyword">with</span> open(LOG_FILE, <span class="string">"a"</span>) <span class="keyword">as</span> fout:</span><br><span class="line">                print(<span class="string">"epoch: &#123;&#125;, iteration: &#123;&#125;, simlex-999: &#123;&#125;, men: &#123;&#125;, sim353: &#123;&#125;, nearest to monster: &#123;&#125;\n"</span>.format(</span><br><span class="line">                    e, i, sim_simlex, sim_men, sim_353, find_nearest(<span class="string">"monster"</span>)))</span><br><span class="line">                fout.write(<span class="string">"epoch: &#123;&#125;, iteration: &#123;&#125;, simlex-999: &#123;&#125;, men: &#123;&#125;, sim353: &#123;&#125;, nearest to monster: &#123;&#125;\n"</span>.format(</span><br><span class="line">                    e, i, sim_simlex, sim_men, sim_353, find_nearest(<span class="string">"monster"</span>)))</span><br><span class="line">                </span><br><span class="line">    embedding_weights = model.input_embeddings() <span class="comment"># 调用最终训练好的embeding词向量</span></span><br><span class="line">    np.save(<span class="string">"embedding-&#123;&#125;"</span>.format(EMBEDDING_SIZE), embedding_weights) <span class="comment"># 保存参数</span></span><br><span class="line">    torch.save(model.state_dict(), <span class="string">"embedding-&#123;&#125;.th"</span>.format(EMBEDDING_SIZE)) <span class="comment"># 保存参数</span></span><br></pre></td></tr></table></figure><p>In [11]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(<span class="string">"embedding-&#123;&#125;.th"</span>.format(EMBEDDING_SIZE))) <span class="comment"># 加载模型</span></span><br></pre></td></tr></table></figure><h2 id="在-MEN-和-Simplex-999-数据集上做评估"><a href="#在-MEN-和-Simplex-999-数据集上做评估" class="headerlink" title="在 MEN 和 Simplex-999 数据集上做评估"></a>在 MEN 和 Simplex-999 数据集上做评估</h2><p>In [12]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 代码同上</span></span><br><span class="line">embedding_weights = model.input_embeddings()</span><br><span class="line">print(<span class="string">"simlex-999"</span>, evaluate(<span class="string">"simlex-999.txt"</span>, embedding_weights))</span><br><span class="line">print(<span class="string">"men"</span>, evaluate(<span class="string">"men.txt"</span>, embedding_weights))</span><br><span class="line">print(<span class="string">"wordsim353"</span>, evaluate(<span class="string">"wordsim353.csv"</span>, embedding_weights))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">simlex<span class="number">-999</span> SpearmanrResult(correlation=<span class="number">0.17251697429101504</span>, pvalue=<span class="number">7.863946056740345e-08</span>)</span><br><span class="line">men SpearmanrResult(correlation=<span class="number">0.1778096817088841</span>, pvalue=<span class="number">7.565661657312768e-20</span>)</span><br><span class="line">wordsim353 SpearmanrResult(correlation=<span class="number">0.27153702278146635</span>, pvalue=<span class="number">8.842165885381714e-07</span>)</span><br></pre></td></tr></table></figure><h2 id="寻找nearest-neighbors"><a href="#寻找nearest-neighbors" class="headerlink" title="寻找nearest neighbors"></a>寻找nearest neighbors</h2><p>In [13]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_nearest</span><span class="params">(word)</span>:</span></span><br><span class="line">    index = word_to_idx[word] </span><br><span class="line">    embedding = embedding_weights[index] <span class="comment"># 取出这个单词的embedding向量</span></span><br><span class="line">    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) <span class="keyword">for</span> e <span class="keyword">in</span> embedding_weights])</span><br><span class="line">    <span class="comment"># 计算所有30000个embedding向量与传入单词embedding向量的相似度距离</span></span><br><span class="line">    <span class="keyword">return</span> [idx_to_word[i] <span class="keyword">for</span> i <span class="keyword">in</span> cos_dis.argsort()[:<span class="number">10</span>]] <span class="comment"># 返回前10个最相似的</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> [<span class="string">"good"</span>, <span class="string">"fresh"</span>, <span class="string">"monster"</span>, <span class="string">"green"</span>, <span class="string">"like"</span>, <span class="string">"america"</span>, <span class="string">"chicago"</span>, <span class="string">"work"</span>, <span class="string">"computer"</span>, <span class="string">"language"</span>]:</span><br><span class="line">    print(word, find_nearest(word))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">good [&apos;good&apos;, &apos;bad&apos;, &apos;perfect&apos;, &apos;hard&apos;, &apos;questions&apos;, &apos;alone&apos;, &apos;money&apos;, &apos;false&apos;, &apos;truth&apos;, &apos;experience&apos;]</span><br><span class="line">fresh [&apos;fresh&apos;, &apos;grain&apos;, &apos;waste&apos;, &apos;cooling&apos;, &apos;lighter&apos;, &apos;dense&apos;, &apos;mild&apos;, &apos;sized&apos;, &apos;warm&apos;, &apos;steel&apos;]</span><br><span class="line">monster [&apos;monster&apos;, &apos;giant&apos;, &apos;robot&apos;, &apos;hammer&apos;, &apos;clown&apos;, &apos;bull&apos;, &apos;demon&apos;, &apos;triangle&apos;, &apos;storyline&apos;, &apos;slogan&apos;]</span><br><span class="line">green [&apos;green&apos;, &apos;blue&apos;, &apos;yellow&apos;, &apos;white&apos;, &apos;cross&apos;, &apos;orange&apos;, &apos;black&apos;, &apos;red&apos;, &apos;mountain&apos;, &apos;gold&apos;]</span><br><span class="line">like [&apos;like&apos;, &apos;unlike&apos;, &apos;etc&apos;, &apos;whereas&apos;, &apos;animals&apos;, &apos;soft&apos;, &apos;amongst&apos;, &apos;similarly&apos;, &apos;bear&apos;, &apos;drink&apos;]</span><br><span class="line">america [&apos;america&apos;, &apos;africa&apos;, &apos;korea&apos;, &apos;india&apos;, &apos;australia&apos;, &apos;turkey&apos;, &apos;pakistan&apos;, &apos;mexico&apos;, &apos;argentina&apos;, &apos;carolina&apos;]</span><br><span class="line">chicago [&apos;chicago&apos;, &apos;boston&apos;, &apos;illinois&apos;, &apos;texas&apos;, &apos;london&apos;, &apos;indiana&apos;, &apos;massachusetts&apos;, &apos;florida&apos;, &apos;berkeley&apos;, &apos;michigan&apos;]</span><br><span class="line">work [&apos;work&apos;, &apos;writing&apos;, &apos;job&apos;, &apos;marx&apos;, &apos;solo&apos;, &apos;label&apos;, &apos;recording&apos;, &apos;nietzsche&apos;, &apos;appearance&apos;, &apos;stage&apos;]</span><br><span class="line">computer [&apos;computer&apos;, &apos;digital&apos;, &apos;electronic&apos;, &apos;audio&apos;, &apos;video&apos;, &apos;graphics&apos;, &apos;hardware&apos;, &apos;software&apos;, &apos;computers&apos;, &apos;program&apos;]</span><br><span class="line">language [&apos;language&apos;, &apos;languages&apos;, &apos;alphabet&apos;, &apos;arabic&apos;, &apos;grammar&apos;, &apos;pronunciation&apos;, &apos;dialect&apos;, &apos;programming&apos;, &apos;chinese&apos;, &apos;spelling&apos;]</span><br></pre></td></tr></table></figure><h2 id="单词之间的关系"><a href="#单词之间的关系" class="headerlink" title="单词之间的关系"></a>单词之间的关系</h2><p>In [14]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">man_idx = word_to_idx[<span class="string">"man"</span>] </span><br><span class="line">king_idx = word_to_idx[<span class="string">"king"</span>] </span><br><span class="line">woman_idx = word_to_idx[<span class="string">"woman"</span>]</span><br><span class="line">embedding = embedding_weights[woman_idx] - embedding_weights[man_idx] + embedding_weights[king_idx]</span><br><span class="line">cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) <span class="keyword">for</span> e <span class="keyword">in</span> embedding_weights])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> cos_dis.argsort()[:<span class="number">20</span>]:</span><br><span class="line">    print(idx_to_word[i])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">king</span><br><span class="line">henry</span><br><span class="line">charles</span><br><span class="line">pope</span><br><span class="line">queen</span><br><span class="line">iii</span><br><span class="line">prince</span><br><span class="line">elizabeth</span><br><span class="line">alexander</span><br><span class="line">constantine</span><br><span class="line">edward</span><br><span class="line">son</span><br><span class="line">iv</span><br><span class="line">louis</span><br><span class="line">emperor</span><br><span class="line">mary</span><br><span class="line">james</span><br><span class="line">joseph</span><br><span class="line">frederick</span><br><span class="line">francis</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> word-embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>酒店评价情感分类与CNN模型</title>
      <link href="/2020/02/08/%E9%85%92%E5%BA%97%E8%AF%84%E4%BB%B7%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E4%B8%8ECNN%E6%A8%A1%E5%9E%8B/"/>
      <url>/2020/02/08/%E9%85%92%E5%BA%97%E8%AF%84%E4%BB%B7%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E4%B8%8ECNN%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h4 id="酒店评价情感分类与CNN模型"><a href="#酒店评价情感分类与CNN模型" class="headerlink" title="酒店评价情感分类与CNN模型"></a>酒店评价情感分类与CNN模型</h4><p>参考了<a href="https://github.com/bentrevett/pytorch-sentiment-analysis" target="_blank" rel="noopener">https://github.com/bentrevett/pytorch-sentiment-analysis</a></p><p>我们会用PyTorch模型来做情感分析(检测一段文字的情感是正面的还是负面的)。我们会使用<a href="https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/ChnSentiCorp_htl_all/intro.ipynb" target="_blank" rel="noopener">ChnSentiCorp_htl</a>数据集，即酒店评论数据集。</p><p>数据下载链接：<a href="https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv" target="_blank" rel="noopener">https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv</a></p><p>模型从简单到复杂，我们会依次构建：</p><ul><li>Word Averaging模型</li><li>RNN/LSTM模型</li><li>CNN模型</li></ul><h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><ul><li>首先让我们加载数据，来看看这一批酒店评价数据长得怎样</li></ul><p>In [1]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">path = &quot;ChnSentiCorp_htl_all.csv&quot;</span><br><span class="line">pd_all = pd.read_csv(path)</span><br><span class="line"></span><br><span class="line">print(&apos;评论数目（总体）：%d&apos; % pd_all.shape[0])</span><br><span class="line">print(&apos;评论数目（正向）：%d&apos; % pd_all[pd_all.label==1].shape[0])</span><br><span class="line">print(&apos;评论数目（负向）：%d&apos; % pd_all[pd_all.label==0].shape[0])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">评论数目（总体）：7766</span><br><span class="line">评论数目（正向）：5322</span><br><span class="line">评论数目（负向）：2444</span><br></pre></td></tr></table></figure><p>In [2]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd_all.sample(5)</span><br></pre></td></tr></table></figure><p>Out[2]:</p><table><thead><tr><th align="right"></th><th align="right">label</th><th align="right">review</th></tr></thead><tbody><tr><td align="right">914</td><td align="right">1</td><td align="right">地点看上去不错，在北京西客站对面，但出行十分不便，周边没有地铁，门口出租车倒是挺多，但就是不…</td></tr><tr><td align="right">7655</td><td align="right">0</td><td align="right">酒店位置较偏僻，环境清净，交通也方便，但酒店及周边就餐选择不多;浴场海水中有水草,水亦太浅,…</td></tr><tr><td align="right">3424</td><td align="right">1</td><td align="right">酒店给人感觉很温欣,服务员也挺有礼貌,房间内的舒适度也非常不错,离开李公递也很近,下次来苏州…</td></tr><tr><td align="right">4854</td><td align="right">1</td><td align="right">离故宫不太远，走路大概10分钟不到点，环境还好，有一点非常不好的是窗帘就只有一层，早上很早就…</td></tr><tr><td align="right">5852</td><td align="right">0</td><td align="right">宾馆背面就是省道,交通是方便的,停车场很大也很方便,但晚上尤其半夜路过的汽车声音很响,拖拉机…</td></tr></tbody></table><p>In [3]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import pkuseg</span><br><span class="line"></span><br><span class="line">seg = pkuseg.pkuseg()           # 以默认配置加载模型</span><br><span class="line">text = seg.cut(&apos;我爱北京天安门&apos;)  # 进行分词</span><br><span class="line">print(text)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&apos;我&apos;, &apos;爱&apos;, &apos;北京&apos;, &apos;天安门&apos;]</span><br></pre></td></tr></table></figure><p>下面我们先手工把数据分成train, dev, test三个部分</p><p>In [4]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">pd_all_shuf = pd_all.sample(frac=1)</span><br><span class="line"></span><br><span class="line"># 总共有多少ins</span><br><span class="line">total_num_ins = pd_all_shuf.shape[0]</span><br><span class="line">pd_train = pd_all_shuf.iloc[:int(total_num_ins*0.8)]</span><br><span class="line">pd_dev = pd_all_shuf.iloc[int(total_num_ins*0.8):int(total_num_ins*0.9)]</span><br><span class="line">pd_test = pd_all_shuf.iloc[int(total_num_ins*0.9):]</span><br><span class="line"></span><br><span class="line"># text, label</span><br><span class="line">train_text = [seg.cut(str(text)) for text in pd_train.review.tolist()]</span><br><span class="line">dev_text = [seg.cut(str(text)) for text in pd_dev.review.tolist()]</span><br><span class="line">test_text = [seg.cut(str(text)) for text in pd_test.review.tolist()]</span><br><span class="line">train_label = pd_train.label.tolist()</span><br><span class="line">dev_label = pd_dev.label.tolist()</span><br><span class="line">test_label = pd_test.label.tolist()</span><br></pre></td></tr></table></figure><p>In [6]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_label[0]</span><br></pre></td></tr></table></figure><p>Out[6]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0</span><br></pre></td></tr></table></figure><p>我们从训练数据构造出一个由单词到index的单词表</p><p>In [7]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from collections import Counter</span><br><span class="line">def build_vocab(sents, max_words=50000):</span><br><span class="line">    word_counts = Counter()</span><br><span class="line">    for sent in sents:</span><br><span class="line">        for word in sent:</span><br><span class="line">            word_counts[word] += 1</span><br><span class="line">    itos = [w for w, c in word_counts.most_common(max_words)]</span><br><span class="line">    itos = [&quot;UNK&quot;, &quot;PAD&quot;] + itos</span><br><span class="line">    stoi = &#123;w:i for i, w in enumerate(itos)&#125;</span><br><span class="line">    return itos, stoi</span><br><span class="line"></span><br><span class="line">itos, stoi = build_vocab(train_text)</span><br></pre></td></tr></table></figure><p>查看一下比较高频的单词</p><p>In [8]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">itos[:10]</span><br></pre></td></tr></table></figure><p>Out[8]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&apos;UNK&apos;, &apos;PAD&apos;, &apos;，&apos;, &apos;的&apos;, &apos;。&apos;, &apos;了&apos;, &apos;,&apos;, &apos;酒店&apos;, &apos;是&apos;, &apos;很&apos;]</span><br></pre></td></tr></table></figure><p>In [10]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stoi[&quot;酒店&quot;]</span><br></pre></td></tr></table></figure><p>Out[10]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">7</span><br></pre></td></tr></table></figure><p>我们把文本中的单词都转换成index</p><p>In [12]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_idx = [[stoi.get(word, stoi.get(&quot;UNK&quot;)) for word in text] for text in train_text ]</span><br><span class="line">dev_idx = [[stoi.get(word, stoi.get(&quot;UNK&quot;)) for word in text] for text in dev_text ]</span><br><span class="line">test_idx = [[stoi.get(word, stoi.get(&quot;UNK&quot;)) for word in text] for text in test_text ]</span><br></pre></td></tr></table></figure><p>把数据和label都转成batch</p><p>In [15]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def get_minibatches(text_idx, labels, batch_size=64, sort=True):</span><br><span class="line">    if sort:</span><br><span class="line">        text_idx_and_labels = sorted(list(zip(text_idx, labels)), key=lambda x: len(x[0]))</span><br><span class="line">        </span><br><span class="line">    text_idx_batches = []</span><br><span class="line">    label_batches = []</span><br><span class="line">    for i in range(0, len(text_idx), batch_size):</span><br><span class="line">        text_batch = [t for t, l in text_idx_and_labels[i:i+batch_size]]</span><br><span class="line">        label_batch = [l for t, l in text_idx_and_labels[i:i+batch_size]]</span><br><span class="line">        max_len = max([len(t) for t in text_batch])</span><br><span class="line">        text_batch_np = np.ones((len(text_batch), max_len), dtype=np.int) # batch_size * max_seq_ength</span><br><span class="line">        for i, t in enumerate(text_batch):</span><br><span class="line">            text_batch_np[i, :len(t)] = t</span><br><span class="line">        text_idx_batches.append(text_batch_np)</span><br><span class="line">        label_batches.append(np.array(label_batch))</span><br><span class="line">        </span><br><span class="line">    return text_idx_batches, label_batches</span><br><span class="line"></span><br><span class="line">train_batches, train_label_batches = get_minibatches(train_idx, train_label)</span><br><span class="line">dev_batches, dev_label_batches = get_minibatches(dev_idx, dev_label)</span><br><span class="line">test_batches, test_label_batches = get_minibatches(test_idx, test_label)</span><br></pre></td></tr></table></figure><p>In [17]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_batches[20]</span><br></pre></td></tr></table></figure><p>Out[17]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">array([[  80,  177,  149, ...,  191,    3,    1],</span><br><span class="line">       [  49,   18,   20, ...,   53,    4,    1],</span><br><span class="line">       [   7,   18,   17, ...,  702,    4,    1],</span><br><span class="line">       ...,</span><br><span class="line">       [1107, 2067,   10, ...,  748,  172,  442],</span><br><span class="line">       [ 241,    9,   19, ...,   17,   44,   30],</span><br><span class="line">       [3058,   20,    6, ...,    9,   19,   98]])</span><br></pre></td></tr></table></figure><p>In [18]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_label_batches[20]</span><br></pre></td></tr></table></figure><p>Out[18]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,</span><br><span class="line">       1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,</span><br><span class="line">       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1])</span><br></pre></td></tr></table></figure><ul><li>和之前一样，我们会设定random seeds使实验可以复现。</li></ul><p>In [19]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torchtext import data</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line">SEED = 1234</span><br><span class="line"></span><br><span class="line">torch.manual_seed(SEED)</span><br><span class="line">torch.cuda.manual_seed(SEED)</span><br><span class="line">torch.backends.cudnn.deterministic = True</span><br><span class="line"></span><br><span class="line">device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br></pre></td></tr></table></figure><h2 id="Word-Averaging模型"><a href="#Word-Averaging模型" class="headerlink" title="Word Averaging模型"></a>Word Averaging模型</h2><ul><li>我们首先介绍一个简单的Word Averaging模型。这个模型非常简单，我们把每个单词都通过<code>Embedding</code>层投射成word embedding vector，然后把一句话中的所有word vector做个平均，就是整个句子的vector表示了。接下来把这个sentence vector传入一个<code>Linear</code>层，做分类即可。</li></ul><p>In [32]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">class WordAVGModel(nn.Module):</span><br><span class="line">    def __init__(self, vocab_size, embedding_size, output_size, pad_idx, dropout_p=0.2):</span><br><span class="line">        super(WordAVGModel, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx=pad_idx)</span><br><span class="line">        self.linear = nn.Linear(embedding_size, output_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout_p) # 这个参数经常拿来调节</span><br><span class="line">    </span><br><span class="line">    def forward(self, text, mask):</span><br><span class="line">        # text: batch_size * max_seq_len</span><br><span class="line">        # mask: batch_size * max_seq_len</span><br><span class="line">        embedded = self.embed(text) # [batch_size, max_seq_len, embedding_size]</span><br><span class="line">        embedded = self.dropout(embedded)</span><br><span class="line">        # dropout</span><br><span class="line">        mask = (1. - mask.float()).unsqueeze(2) # [batch_size, seq_len, 1], 1 represents word, 0 represents padding</span><br><span class="line">        embedded = embedded * mask # [batch_size, seq_len, embedding_size]</span><br><span class="line">        # 求平均</span><br><span class="line">        sent_embed = embedded.sum(1) / (mask.sum(1) + 1e-9) # 防止mask.sum为0，那么不能除以零。</span><br><span class="line">        # dropout</span><br><span class="line">        return self.linear(sent_embed)</span><br></pre></td></tr></table></figure><p>In [75]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">class WordMaxModel(nn.Module):</span><br><span class="line">    def __init__(self, vocab_size, embedding_size, output_size, pad_idx, dropout_p=0.2):</span><br><span class="line">        super(WordMaxModel, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx=pad_idx)</span><br><span class="line">        self.linear = nn.Linear(embedding_size, output_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout_p) # 这个参数经常拿来调节</span><br><span class="line">    </span><br><span class="line">    def forward(self, text, mask):</span><br><span class="line">        # text: batch_size * max_seq_len</span><br><span class="line">        # mask: batch_size * max_seq_len</span><br><span class="line">        embedded = self.embed(text) # [batch_size, max_seq_len, embedding_size]</span><br><span class="line">        embedded = self.dropout(embedded)</span><br><span class="line">        embedded.masked_fill(mask.unsqueeze(2), -999999)</span><br><span class="line">        # dropout</span><br><span class="line">        sent_embed = torch.max(embedded, 1)[0]</span><br><span class="line">        # dropout</span><br><span class="line">        return self.linear(sent_embed)</span><br></pre></td></tr></table></figure><p>In [76]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">VOCAB_SIZE = len(itos)</span><br><span class="line">EMBEDDING_SIZE = 100</span><br><span class="line">OUTPUT_SIZE = 1</span><br><span class="line">PAD_IDX = stoi[&quot;PAD&quot;]</span><br><span class="line"></span><br><span class="line">model = WordMaxModel(vocab_size=VOCAB_SIZE, </span><br><span class="line">                     embedding_size=EMBEDDING_SIZE, </span><br><span class="line">                     output_size=OUTPUT_SIZE, </span><br><span class="line">                     pad_idx=PAD_IDX)</span><br></pre></td></tr></table></figure><p>In [77]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">VOCAB_SIZE</span><br></pre></td></tr></table></figure><p>Out[77]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">24001</span><br></pre></td></tr></table></figure><p>In [78]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># model</span><br><span class="line">def count_parameters(model):</span><br><span class="line">    return sum(p.numel() for p in model.parameters() if p.requires_grad)</span><br><span class="line"></span><br><span class="line">count_parameters(model)</span><br></pre></td></tr></table></figure><p>Out[78]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2400201</span><br></pre></td></tr></table></figure><p>In [79]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UNK_IDX = stoi[&quot;UNK&quot;]</span><br></pre></td></tr></table></figure><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>In [80]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(model.parameters())</span><br><span class="line">crit = nn.BCEWithLogitsLoss()</span><br><span class="line"></span><br><span class="line">model = model.to(device)</span><br><span class="line"># crit = crit.to(device)</span><br></pre></td></tr></table></figure><p>计算预测的准确率</p><p>In [81]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def binary_accuracy(preds, y):</span><br><span class="line">    rounded_preds = torch.round(torch.sigmoid(preds))</span><br><span class="line">    correct = (rounded_preds == y).float()</span><br><span class="line">    acc = correct.sum() / len(correct)</span><br><span class="line">    return acc</span><br></pre></td></tr></table></figure><p>In [82]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def train(model, text_idxs, labels, optimizer, crit):</span><br><span class="line">    epoch_loss, epoch_acc = 0., 0.</span><br><span class="line">    model.train()</span><br><span class="line">    total_len = 0.</span><br><span class="line">    for text, label in zip(text_idxs, labels):</span><br><span class="line">        text = torch.from_numpy(text).to(device)</span><br><span class="line">        label = torch.from_numpy(label).to(device)</span><br><span class="line">        mask = text == PAD_IDX</span><br><span class="line">        preds = model(text, mask).squeeze() # [batch_size, sent_length]</span><br><span class="line">        loss = crit(preds, label.float()) </span><br><span class="line">        acc = binary_accuracy(preds, label)</span><br><span class="line">        </span><br><span class="line">        # sgd</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">#         print(&quot;batch loss: &#123;&#125;&quot;.format(loss.item()))</span><br><span class="line">        </span><br><span class="line">        epoch_loss += loss.item() * len(label)</span><br><span class="line">        epoch_acc += acc.item() * len(label)</span><br><span class="line">        total_len += len(label)</span><br><span class="line">        </span><br><span class="line">    return epoch_loss / total_len, epoch_acc / total_len</span><br></pre></td></tr></table></figure><p>In [83]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def evaluate(model, text_idxs, labels, crit):</span><br><span class="line">    epoch_loss, epoch_acc = 0., 0.</span><br><span class="line">    model.eval()</span><br><span class="line">    total_len = 0.</span><br><span class="line">    for text, label in zip(text_idxs, labels):</span><br><span class="line">        text = torch.from_numpy(text).to(device)</span><br><span class="line">        label = torch.from_numpy(label).to(device)</span><br><span class="line">        mask = text == PAD_IDX</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            preds = model(text, mask).squeeze()</span><br><span class="line">        loss = crit(preds, label.float())</span><br><span class="line">        acc = binary_accuracy(preds, label)</span><br><span class="line">        </span><br><span class="line">        epoch_loss += loss.item() * len(label)</span><br><span class="line">        epoch_acc += acc.item() * len(label)</span><br><span class="line">        total_len += len(label)</span><br><span class="line">    model.train()</span><br><span class="line">        </span><br><span class="line">    return epoch_loss / total_len, epoch_acc / total_len</span><br></pre></td></tr></table></figure><p>In [84]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">N_EPOCHS = 10</span><br><span class="line">best_valid_acc = 0.</span><br><span class="line">for epoch in range(N_EPOCHS):</span><br><span class="line">    train_loss, train_acc = train(model, train_batches, train_label_batches, optimizer, crit)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, dev_batches, dev_label_batches, crit)</span><br><span class="line">    </span><br><span class="line">    if valid_acc &gt; best_valid_acc:</span><br><span class="line">        best_valid_acc = valid_acc</span><br><span class="line">        torch.save(model.state_dict(), &quot;wordavg-model.pth&quot;)</span><br><span class="line">        </span><br><span class="line">    print(&quot;Epoch&quot;, epoch, &quot;Train Loss&quot;, train_loss, &quot;Train Acc&quot;, train_acc)</span><br><span class="line">    print(&quot;Epoch&quot;, epoch, &quot;Valid Loss&quot;, valid_loss, &quot;Valid Acc&quot;, valid_acc)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0 Train Loss 0.6241851981347865 Train Acc 0.6785254346426272</span><br><span class="line">Epoch 0 Valid Loss 0.7439712684126895 Valid Acc 0.396396396434752</span><br><span class="line">Epoch 1 Train Loss 0.6111872896254639 Train Acc 0.6775595621377978</span><br><span class="line">Epoch 1 Valid Loss 0.7044879783381213 Valid Acc 0.4761904762288318</span><br><span class="line">Epoch 2 Train Loss 0.5826128212314072 Train Acc 0.7041210560206053</span><br><span class="line">Epoch 2 Valid Loss 0.667004818775172 Valid Acc 0.5791505791889348</span><br><span class="line">Epoch 3 Train Loss 0.5516750626769744 Train Acc 0.7293947198969736</span><br><span class="line">Epoch 3 Valid Loss 0.6347547087583456 Valid Acc 0.6473616476684924</span><br><span class="line">Epoch 4 Train Loss 0.5236469646921791 Train Acc 0.7512878300064392</span><br><span class="line">Epoch 4 Valid Loss 0.5953507212444928 Valid Acc 0.7348777351845799</span><br><span class="line">Epoch 5 Train Loss 0.4969042095152394 Train Acc 0.7707662588538313</span><br><span class="line">Epoch 5 Valid Loss 0.5561252224092471 Valid Acc 0.7786357789426237</span><br><span class="line">Epoch 6 Train Loss 0.46501466702892946 Train Acc 0.797005795235029</span><br><span class="line">Epoch 6 Valid Loss 0.5206214915217887 Valid Acc 0.8018018021086468</span><br><span class="line">Epoch 7 Train Loss 0.43595607032794303 Train Acc 0.8163232453316163</span><br><span class="line">Epoch 7 Valid Loss 0.4846100037776058 Valid Acc 0.8159588161889497</span><br><span class="line">Epoch 8 Train Loss 0.40671270164611334 Train Acc 0.8386992916934964</span><br><span class="line">Epoch 8 Valid Loss 0.45964578196809097 Valid Acc 0.8211068213369549</span><br><span class="line">Epoch 9 Train Loss 0.38044816804408105 Train Acc 0.8539922730199614</span><br><span class="line">Epoch 9 Valid Loss 0.4279780917953187 Valid Acc 0.8416988419289755</span><br></pre></td></tr></table></figure><p>In [85]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(&quot;wordavg-model.pth&quot;))</span><br></pre></td></tr></table></figure><p>Out[85]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;All keys matched successfully&gt;</span><br></pre></td></tr></table></figure><p>In [86]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def predict_sentiment(model, sentence):</span><br><span class="line">    model.eval()</span><br><span class="line">    indexed = [stoi.get(t, PAD_IDX) for t in seg.cut(sentence)]</span><br><span class="line">    tensor = torch.LongTensor(indexed).to(device) # seq_len</span><br><span class="line">    tensor = tensor.unsqueeze(0) # batch_size* seq_len </span><br><span class="line">    mask = tensor == PAD_IDX</span><br><span class="line">#     print(tensor, &quot;\n&quot;, mask)</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        pred = torch.sigmoid(model(tensor, mask))</span><br><span class="line">    return pred.item()</span><br></pre></td></tr></table></figure><p>In [88]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;这个酒店非常脏乱差，不推荐&quot;)</span><br></pre></td></tr></table></figure><p>Out[88]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.6831367611885071</span><br></pre></td></tr></table></figure><p>In [90]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;这个酒店非常好，强烈推荐！&quot;)</span><br></pre></td></tr></table></figure><p>Out[90]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.8252924680709839</span><br></pre></td></tr></table></figure><p>In [91]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;房间设备太破,连喷头都是不好用,空调几乎感觉不到,虽然我开了最大另外就是设备维修不及时,洗澡用品感觉都是廉价货,味道很奇怪的洗头液等等...总体感觉服务还可以,设备招待所水平...&quot;)</span><br></pre></td></tr></table></figure><p>Out[91]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.5120517611503601</span><br></pre></td></tr></table></figure><p>In [92]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;房间稍小，但清洁，非常实惠。不足之处是：双人房的洗澡用品只有一套.宾馆反馈2008年8月5日：尊敬的宾客：您好！感谢您选择入住金陵溧阳宾馆！对于酒店双人房内的洗漱用品只有一套的问题，我们已经召集酒店相关部门对此问题进行了研究和整改。努力将我们的管理与服务工作做到位，进一步关注宾客，关注细节！再次向您表示我们最衷心的感谢！期待您能再次来溧阳并入住金陵溧阳宾馆！让我们有给您提供更加优质服务的机会！顺祝您工作顺利！身体健康！金陵溧阳宾馆客务关系主任&quot;)</span><br></pre></td></tr></table></figure><p>Out[92]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.7319579124450684</span><br></pre></td></tr></table></figure><p>In [93]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;该酒店对去溧阳公务或旅游的人都很适合，自助早餐很丰富，酒店内部环境和服务很好。唯一的不足是酒店大门口在晚上时太乱，各种车辆和人在门口挤成一团。补充点评2008年5月9日：房间淋浴水压不稳，一会热、一会冷，很不好调整。宾馆反馈2008年5月13日：非常感谢您选择入住金陵溧阳宾馆。您给予我们的肯定与赞赏让我们倍受鼓舞，也使我们更加自信地去做好每一天的服务工作。正是有许多像您一样的宾客给予我们不断的鼓励和赞赏，酒店的服务品质才能得以不断提升。对于酒店大门口的秩序和房间淋浴水的问题我们已做出了相应的措施。再次向您表示我们最衷心的感谢！我们期待您的再次光临！&quot;)</span><br></pre></td></tr></table></figure><p>Out[93]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.793725311756134</span><br></pre></td></tr></table></figure><p>In [94]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;环境不错，室内色调很温馨，MM很满意！就是窗户收拾得太马虎了，拉开窗帘就觉得很凌乱的感觉。最不足的地方就是淋浴了，一是地方太小了，二是洗澡时水时大时小的，中间还停了几秒！！&quot;)</span><br></pre></td></tr></table></figure><p>Out[94]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.7605408430099487</span><br></pre></td></tr></table></figure><p>In [95]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(&quot;wordavg-model.pth&quot;))</span><br><span class="line">test_loss, test_acc = evaluate(model, test_batches, test_label_batches, crit)</span><br><span class="line">print(&quot;CNN model test loss: &quot;, test_loss, &quot;accuracy:&quot;, test_acc)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CNN model test loss:  0.44893962796897346 accuracy: 0.8133848134615247</span><br></pre></td></tr></table></figure><h2 id="RNN模型"><a href="#RNN模型" class="headerlink" title="RNN模型"></a>RNN模型</h2><ul><li><p>下面我们尝试把模型换成一个</p><p>recurrent neural network</p></li></ul><p>  (RNN)。RNN经常会被用来encode一个sequence</p><p>  ℎ𝑡=RNN(𝑥𝑡,ℎ𝑡−1)ht=RNN(xt,ht−1)</p><ul><li><p>我们使用最后一个hidden state ℎ𝑇hT来表示整个句子。</p></li><li><p>然后我们把ℎ𝑇hT通过一个线性变换𝑓f，然后用来预测句子的情感。</p></li></ul><p><img src="file:///Users/mmy/Downloads/assets/sentiment1.png" alt="img"></p><p><img src="file:///Users/mmy/Downloads/assets/sentiment7.png" alt="img"></p><p>In [57]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">class RNNModel(nn.Module):</span><br><span class="line">    def __init__(self, vocab_size, embedding_size, output_size, pad_idx, hidden_size, dropout, avg_hidden=True):</span><br><span class="line">        super(RNNModel, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx=pad_idx)</span><br><span class="line">        self.lstm = nn.LSTM(embedding_size, hidden_size, bidirectional=True, num_layers=2, batch_first=True)</span><br><span class="line">        self.linear = nn.Linear(hidden_size*2, output_size)</span><br><span class="line">            </span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.avg_hidden = avg_hidden</span><br><span class="line">    </span><br><span class="line">    def forward(self, text, mask):</span><br><span class="line">        embedded = self.embed(text) # [batch_size, seq_len, embedding_size] 其中包含一些pad</span><br><span class="line">        embedded  = self.dropout(embedded)</span><br><span class="line">        </span><br><span class="line">        # mask: batch_size * seq_length</span><br><span class="line">        seq_length = (1. - mask.float()).sum(1)</span><br><span class="line">        embedded = torch.nn.utils.rnn.pack_padded_sequence(</span><br><span class="line">            input=embedded,</span><br><span class="line">            lengths=seq_length,</span><br><span class="line">            batch_first=True,</span><br><span class="line">            enforce_sorted=False</span><br><span class="line">        ) # batch_size * seq_len * ...,    seq_len * batch_size * ...</span><br><span class="line">        output, (hidden, cell) = self.lstm(embedded)</span><br><span class="line">        output, seq_length = torch.nn.utils.rnn.pad_packed_sequence(</span><br><span class="line">            sequence=output,</span><br><span class="line">            batch_first=True,</span><br><span class="line">            padding_value=0,</span><br><span class="line">            total_length=mask.shape[1]</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        # output: [batch_size, seq_length, hidden_dim * num_directions]</span><br><span class="line">        # hidden: [num_layers * num_directions, batch_size, hidden_dim]</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        if self.avg_hidden:</span><br><span class="line">            hidden = torch.sum(output * (1. - mask.float()).unsqueeze(2), 1) / torch.sum((1. - mask.float()), 1).unsqueeze(1)</span><br><span class="line">        else:</span><br><span class="line">            # 拿最后一个hidden state作为句子的表示</span><br><span class="line">            # hidden: 2 * batch_size * hidden_size</span><br><span class="line">            hidden = torch.cat([hidden[-1], hidden[-2]], dim=1)</span><br><span class="line">            hidden = self.dropout(hidden.squeeze())</span><br><span class="line">        return self.linear(hidden)</span><br></pre></td></tr></table></figure><p>In [58]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = RNNModel(vocab_size=VOCAB_SIZE, </span><br><span class="line">                 embedding_size=EMBEDDING_SIZE, </span><br><span class="line">                 output_size=OUTPUT_SIZE, </span><br><span class="line">                 pad_idx=PAD_IDX, </span><br><span class="line">                 hidden_size=100, </span><br><span class="line">                 dropout=0.5)</span><br></pre></td></tr></table></figure><h2 id="训练RNN模型"><a href="#训练RNN模型" class="headerlink" title="训练RNN模型"></a>训练RNN模型</h2><p>In [59]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(model.parameters()) # L2</span><br><span class="line">crit = nn.BCEWithLogitsLoss()</span><br><span class="line"></span><br><span class="line">model = model.to(device)</span><br><span class="line">crit = crit.to(device)</span><br></pre></td></tr></table></figure><p>In [60]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">N_EPOCHS = 10</span><br><span class="line">best_valid_acc = 0.</span><br><span class="line">for epoch in range(N_EPOCHS):</span><br><span class="line">    train_loss, train_acc = train(model, train_batches, train_label_batches, optimizer, crit)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, dev_batches, dev_label_batches, crit)</span><br><span class="line">    </span><br><span class="line">    if valid_acc &gt; best_valid_acc:</span><br><span class="line">        best_valid_acc = valid_acc</span><br><span class="line">        torch.save(model.state_dict(), &quot;lstm-model.pth&quot;)</span><br><span class="line">        </span><br><span class="line">    print(&quot;Epoch&quot;, epoch, &quot;Train Loss&quot;, train_loss, &quot;Train Acc&quot;, train_acc)</span><br><span class="line">    print(&quot;Epoch&quot;, epoch, &quot;Valid Loss&quot;, valid_loss, &quot;Valid Acc&quot;, valid_acc)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0 Train Loss 0.5140281977456996 Train Acc 0.7472633612363168</span><br><span class="line">Epoch 0 Valid Loss 0.7321655497894631 Valid Acc 0.8133848134615247</span><br><span class="line">Epoch 1 Train Loss 0.4205178504441526 Train Acc 0.8209916291049582</span><br><span class="line">Epoch 1 Valid Loss 0.5658483397086155 Valid Acc 0.8391248392782616</span><br><span class="line">Epoch 2 Train Loss 0.3576773465620036 Train Acc 0.8473921442369607</span><br><span class="line">Epoch 2 Valid Loss 0.6089477152437777 Valid Acc 0.8545688548756996</span><br><span class="line">Epoch 3 Train Loss 0.3190276817504391 Train Acc 0.8647778493238892</span><br><span class="line">Epoch 3 Valid Loss 0.5731698980355968 Valid Acc 0.8622908625977073</span><br><span class="line">Epoch 4 Train Loss 0.2850390273336434 Train Acc 0.8881197681905988</span><br><span class="line">Epoch 4 Valid Loss 0.6073675444073966 Valid Acc 0.8622908625209961</span><br><span class="line">Epoch 5 Train Loss 0.26827128295812463 Train Acc 0.8884417256922086</span><br><span class="line">Epoch 5 Valid Loss 0.4971172449057934 Valid Acc 0.8700128701662925</span><br><span class="line">Epoch 6 Train Loss 0.23699480644442233 Train Acc 0.9059884095299421</span><br><span class="line">Epoch 6 Valid Loss 0.5370476412343549 Valid Acc 0.8635778636545748</span><br><span class="line">Epoch 7 Train Loss 0.22414902945487483 Train Acc 0.9072762395363811</span><br><span class="line">Epoch 7 Valid Loss 0.48257371315317876 Valid Acc 0.8725868726635838</span><br><span class="line">Epoch 8 Train Loss 0.2119196125996435 Train Acc 0.9162910495814552</span><br><span class="line">Epoch 8 Valid Loss 0.59562370292315 Valid Acc 0.8468468471536919</span><br><span class="line">Epoch 9 Train Loss 0.20756761220698194 Train Acc 0.9207984546039922</span><br><span class="line">Epoch 9 Valid Loss 0.6451035161122699 Valid Acc 0.8700128701662925</span><br></pre></td></tr></table></figure><p>In [62]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;沈阳市政府的酒店，比较大气，交通便利，出门往左就是北陵公园，环境好。&quot;)</span><br></pre></td></tr></table></figure><p>Out[62]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9994519352912903</span><br></pre></td></tr></table></figure><p>In [63]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;这个酒店非常脏乱差，不推荐！&quot;)</span><br></pre></td></tr></table></figure><p>Out[63]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.01588270254433155</span><br></pre></td></tr></table></figure><p>In [68]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;这个酒店不乱，非常推荐！&quot;)</span><br></pre></td></tr></table></figure><p>Out[68]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.04462616145610809</span><br></pre></td></tr></table></figure><p>在test上做模型预测</p><p>In [69]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(&quot;lstm-model.pth&quot;))</span><br><span class="line">test_loss, test_acc = evaluate(model, test_batches, test_label_batches, crit)</span><br><span class="line">print(&quot;CNN model test loss: &quot;, test_loss, &quot;accuracy:&quot;, test_acc)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CNN model test loss:  0.5639284941220376 accuracy: 0.8481338484406932</span><br></pre></td></tr></table></figure><h2 id="CNN模型"><a href="#CNN模型" class="headerlink" title="CNN模型"></a>CNN模型</h2><p>In [70]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">class CNN(nn.Module):</span><br><span class="line">    def __init__(self, vocab_size, embedding_size, output_size, pad_idx, num_filters, filter_sizes, dropout):</span><br><span class="line">        super(CNN, self).__init__()</span><br><span class="line">        self.filter_sizes = filter_sizes</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx=pad_idx)</span><br><span class="line">        self.convs = nn.ModuleList([</span><br><span class="line">            nn.Conv2d(in_channels=1, out_channels=num_filters, </span><br><span class="line">                          kernel_size=(fs, embedding_size)) </span><br><span class="line">            for fs in filter_sizes</span><br><span class="line">        ]) # 3个CNN </span><br><span class="line">        # fs实际上就是n-gram的n</span><br><span class="line">#         self.conv = nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(filter_size, embedding_size))</span><br><span class="line">        self.linear = nn.Linear(num_filters * len(filter_sizes), output_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">    </span><br><span class="line">    def forward(self, text, mask):</span><br><span class="line">        embedded = self.embed(text) # [batch_size, seq_len, embedding_size]</span><br><span class="line">        embedded = embedded.unsqueeze(1) # # [batch_size, 1, seq_len, embedding_size]</span><br><span class="line">#         conved = F.relu(self.conv(embedded)) # [batch_size, num_filters, seq_len-filter_size+1, 1]</span><br><span class="line">#         conved = conved.squeeze(3) # [batch_size, num_filters, seq_len-filter_size+1]</span><br><span class="line">        conved = [</span><br><span class="line">            F.relu(conv(embedded)).squeeze(3) for conv in self.convs</span><br><span class="line">        ] # [batch_size, num_filters, seq_len-filter_size+1]</span><br><span class="line">    </span><br><span class="line">        # [2, 5, 1, 1]</span><br><span class="line">    </span><br><span class="line">        # mask [[0, 0, 1, 1]]</span><br><span class="line">        # fs: 2</span><br><span class="line">        # [0, 0, 1]</span><br><span class="line">        conved = [</span><br><span class="line">            conv.masked_fill(mask[:, :-filter_size+1].unsqueeze(1) , -999999) for (conv, filter_size) in zip(conved, self.filter_sizes)</span><br><span class="line">        ]</span><br><span class="line">        # max over time pooling</span><br><span class="line">#         pooled = F.max_pool1d(conved, conved.shape[2]) # [batch_size, num_filters, 1]</span><br><span class="line">#         pooled = pooled.squeeze(2)</span><br><span class="line">        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]</span><br><span class="line">        pooled = torch.cat(pooled, dim=1) # batch_size, 3*num_filters</span><br><span class="line">        pooled = self.dropout(pooled)</span><br><span class="line">        </span><br><span class="line">        return self.linear(pooled)</span><br><span class="line">    </span><br><span class="line">#     Conv1d? 1x1 Conv2d?</span><br></pre></td></tr></table></figure><p>In [71]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">model = CNN(vocab_size=VOCAB_SIZE, </span><br><span class="line">           embedding_size=EMBEDDING_SIZE, </span><br><span class="line">           output_size=OUTPUT_SIZE, </span><br><span class="line">           pad_idx=PAD_IDX,</span><br><span class="line">           num_filters=100, </span><br><span class="line">           filter_sizes=[3,4,5],  # 3-gram, 4-gram, 5-gram</span><br><span class="line">           dropout=0.5)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters())</span><br><span class="line">crit = nn.BCEWithLogitsLoss()</span><br><span class="line"></span><br><span class="line">model = model.to(device)</span><br><span class="line">crit = crit.to(device)</span><br><span class="line"></span><br><span class="line">N_EPOCHS = 10</span><br><span class="line">best_valid_acc = 0.</span><br><span class="line">for epoch in range(N_EPOCHS):</span><br><span class="line">    train_loss, train_acc = train(model, train_batches, train_label_batches, optimizer, crit)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, dev_batches, dev_label_batches, crit)</span><br><span class="line">    </span><br><span class="line">    if valid_acc &gt; best_valid_acc:</span><br><span class="line">        best_valid_acc = valid_acc</span><br><span class="line">        torch.save(model.state_dict(), &quot;cnn-model.pth&quot;)</span><br><span class="line">        </span><br><span class="line">    print(&quot;Epoch&quot;, epoch, &quot;Train Loss&quot;, train_loss, &quot;Train Acc&quot;, train_acc)</span><br><span class="line">    print(&quot;Epoch&quot;, epoch, &quot;Valid Loss&quot;, valid_loss, &quot;Valid Acc&quot;, valid_acc)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0 Train Loss 0.5229088294452341 Train Acc 0.7443657437218287</span><br><span class="line">Epoch 0 Valid Loss 0.39319338566087847 Valid Acc 0.8108108110409445</span><br><span class="line">Epoch 1 Train Loss 0.3683148011498043 Train Acc 0.837894397939472</span><br><span class="line">Epoch 1 Valid Loss 0.3534783678778964 Valid Acc 0.840411840565263</span><br><span class="line">Epoch 2 Train Loss 0.3185185533801318 Train Acc 0.8644558918222794</span><br><span class="line">Epoch 2 Valid Loss 0.34023444222207233 Valid Acc 0.8545688547222771</span><br><span class="line">Epoch 3 Train Loss 0.27130810793366883 Train Acc 0.8889246619446233</span><br><span class="line">Epoch 3 Valid Loss 0.30879392936116173 Valid Acc 0.8648648650182874</span><br><span class="line">Epoch 4 Train Loss 0.24334710945314694 Train Acc 0.9034127495170637</span><br><span class="line">Epoch 4 Valid Loss 0.3020249246553718 Valid Acc 0.8790218791753015</span><br><span class="line">Epoch 5 Train Loss 0.2156534520195556 Train Acc 0.912105602060528</span><br><span class="line">Epoch 5 Valid Loss 0.326562241774575 Valid Acc 0.8571428572962797</span><br><span class="line">Epoch 6 Train Loss 0.189559489642123 Train Acc 0.9245009658725049</span><br><span class="line">Epoch 6 Valid Loss 0.28917587651095644 Valid Acc 0.885456885610308</span><br><span class="line">Epoch 7 Train Loss 0.16508568145445063 Train Acc 0.9356084996780425</span><br><span class="line">Epoch 7 Valid Loss 0.2982815937876241 Valid Acc 0.8790218791753015</span><br><span class="line">Epoch 8 Train Loss 0.14198238390007764 Train Acc 0.9452672247263362</span><br><span class="line">Epoch 8 Valid Loss 0.2929042390184513 Valid Acc 0.8880308881843105</span><br><span class="line">Epoch 9 Train Loss 0.11862559608529824 Train Acc 0.9552479072762395</span><br><span class="line">Epoch 9 Valid Loss 0.29382622203618247 Valid Acc 0.886743886820598</span><br></pre></td></tr></table></figure><p>In [72]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(&quot;cnn-model.pth&quot;))</span><br><span class="line">test_loss, test_acc = evaluate(model, test_batches, test_label_batches, crit)</span><br><span class="line">print(&quot;CNN model test loss: &quot;, test_loss, &quot;accuracy:&quot;, test_acc)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CNN model test loss:  0.32514461861537386 accuracy: 0.8674388674388674</span><br></pre></td></tr></table></figure><p>In [74]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;酒店位于昆明中心区,地理位置不错,可惜酒店服务有些差,第一天晚上可能入住的客人不多,空调根本没开,打了电话问,说是中央空调要晚上统一开,结果晚上也没开,就热了一晚上,第二天有开会的入住,晚上就有了空调,不得不说酒店经济帐作的好.房间的床太硬,睡的不好.酒店的早餐就如其他人评价一样,想法的难吃.不过携程的预订价钱还不错.&quot;)</span><br></pre></td></tr></table></figure><p>Out[74]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.893503725528717</span><br></pre></td></tr></table></figure><p>learning representation</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> RNN/LSTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP技术基础整理</title>
      <link href="/2020/01/28/NLP%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%A1%80%E6%95%B4%E7%90%86/"/>
      <url>/2020/01/28/NLP%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%A1%80%E6%95%B4%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是自然语言处理？"><a href="#什么是自然语言处理？" class="headerlink" title="什么是自然语言处理？"></a>什么是自然语言处理？</h2><p>自然语言处理（NLP）是一门融语言学、计算机科学、人工智能于一体的（实验性）科学，解决的是“让机器可以理解自然语言”。</p><p>NLP = NLU + NLG</p><h2 id="NLP问题的难点"><a href="#NLP问题的难点" class="headerlink" title="NLP问题的难点"></a>NLP问题的难点</h2><ul><li>自然语言有歧义（ambiguity），同样的含义又有不同的表达方式（variability）<ul><li>ambiguity：同样的一段表述能表示不同的意思</li><li>variability：不同的表达方式是同一个意思</li></ul></li></ul><p>coreference resolution</p><p>爸爸已经抱不动<strong>小明</strong>了，因为<strong>他</strong>太胖了。</p><p><strong>爸爸</strong>已经抱不动小明了，因为<strong>他</strong>太虚弱了。</p><p>WSC: GPT-2</p><h2 id="机器学习与NLP"><a href="#机器学习与NLP" class="headerlink" title="机器学习与NLP"></a>机器学习与NLP</h2><p>使用机器学习的方法让模型能够学到输入和输出之间的映射关系。在NLP中，输入一般都是语言文字，而输出则是各种不同的label。</p><h2 id="单词"><a href="#单词" class="headerlink" title="单词"></a>单词</h2><p>自然语言的基本构成单元。</p><h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><p>英文中的单词一般用空格隔开（标点符号等特殊情况除外），所以天然地完成了分词。中文的分词则不那么自然，需要人为分词。比较好用的分词工具：<a href="https://github.com/lancopku/pkuseg-python" target="_blank" rel="noopener">https://github.com/lancopku/pkuseg-python</a> </p><p>jieba</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pip install pkuseg</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> pkuseg</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>seg = pkuseg.pkuseg()           <span class="comment"># 以默认配置加载模型</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text = seg.cut(<span class="string">'我爱北京天安门'</span>)  <span class="comment"># 进行分词</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(text)</span><br><span class="line">[<span class="string">'我'</span>, <span class="string">'爱'</span>, <span class="string">'北京'</span>, <span class="string">'天安门'</span>]</span><br></pre></td></tr></table></figure><p>英文分词可以使用NLTK</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> nltk</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sentence = “hello, world<span class="string">"</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; tokens = nltk.word_tokenize(sentence)</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; tokens</span></span><br><span class="line"><span class="string">['hello', ‘,', 'world']</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; sents = nltk.sent_tokenize(documents)</span></span><br></pre></td></tr></table></figure><p>NLTK还有一些好用的功能，例如POS Tagging</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> nltk</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text = nltk.word_tokenize(<span class="string">'what does the fox say'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text</span><br><span class="line">[<span class="string">'what'</span>, <span class="string">'does'</span>, <span class="string">'the'</span>, <span class="string">'fox'</span>, <span class="string">'say'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>nltk.pos_tag(text)</span><br><span class="line">[(<span class="string">'what'</span>, <span class="string">'WDT'</span>), (<span class="string">'does'</span>, <span class="string">'VBZ'</span>), (<span class="string">'the'</span>, <span class="string">'DT'</span>), (<span class="string">'fox'</span>, <span class="string">'NNS'</span>), (<span class="string">'say'</span>, <span class="string">'VBP'</span>)]</span><br></pre></td></tr></table></figure><p>Named Entity Recognition</p><p><img src="https://uploader.shimo.im/f/56iORCYdcewm9en3.png!thumbnail" alt="img"></p><p>去除停用词</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="comment"># 先token一把，得到一个word_list</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="comment"># 然后filter一把</span></span><br><span class="line">filtered_words = </span><br><span class="line">[word <span class="keyword">for</span> word <span class="keyword">in</span> word_list <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stopwords.words(<span class="string">'english'</span>)]</span><br></pre></td></tr></table></figure><p>one hot vector [0, 0, 0, 1, 0, 0…]</p><h2 id="Bag-of-Words和TF-IDF"><a href="#Bag-of-Words和TF-IDF" class="headerlink" title="Bag of Words和TF-IDF"></a>Bag of Words和TF-IDF</h2><p>词包模型</p><p>vocab: 50000个单词</p><p>文本–&gt; 50000维向量</p><p>{a: 0, an: 1, the:2, ….}</p><p>[100, 50, 30, …]</p><p><strong>TF: Term Frequency</strong>, 衡量一个term在文档中出现得有多频繁。</p><p>TF(t) = (t出现在文档中的次数) / (文档中的term总数)</p><p>文档一个10000个单词，100个the</p><p>TF(the) = 0.01</p><p><strong>IDF: Inverse Document Frequency</strong>, 衡量一个term有多重要。</p><p>有些词出现的很多，但是信息量可能不大，比如’is’，’the‘，’and‘之类。</p><p>为了平衡，我们把罕见的词的重要性（weight）调高，把常见词的重要性调低。</p><p>IDF(t) = lg(文档总数 / 含有t的文档总数 + 1)</p><p>语料一共在3篇文章中出现，但是我们一共有100,000篇文章。IDF(julyedu) = log(100,000/3)</p><p><strong>TF-IDF = TF * IDF</strong></p><p>TFIDF词包</p><p>a, 100*0.000001</p><p>[0.0001, ]</p><h2 id="Distributional-Word-Vectors-词向量"><a href="#Distributional-Word-Vectors-词向量" class="headerlink" title="Distributional Word Vectors 词向量"></a>Distributional Word Vectors 词向量</h2><p>distributional semantics</p><p>“The distributional hypothesis in linguistics is derived from the semantic theory of language usage, i.e. words that are used and occur in the same contexts tend to purport similar meanings.”</p><p>如果两个单词总是在同样的语境下出现，那么表示他们之间存在某种相关性/相似性。</p><p>Counting Context Words</p><p><img src="https://uploader.shimo.im/f/OClAoRm660cjqZR3.png!thumbnail" alt="img"></p><p>50000 * 50000</p><p>50000*300  </p><p>300*300</p><p>300*50000</p><p>在我们定义的固定大小的context window下出现的单词组，就是co-occuring word pairs。</p><p>对于句子的开头和结尾，我们可以定义两个特殊的符号  &lt;s&gt;   和  &lt;/s&gt;。</p><h2 id="单词相似度"><a href="#单词相似度" class="headerlink" title="单词相似度"></a>单词相似度</h2><p>使用词向量间的cosine相似度（cosine 夹角）, u, v是两个词向量</p><p><img src="https://uploader.shimo.im/f/tQdVXIYvqV4KYDft.png!thumbnail" alt="img">= cosine(u, v)</p><p>单词”cooked”周围context windows最常见的单词</p><p><img src="https://uploader.shimo.im/f/5yQkBDvdSLMwl5D7.png!thumbnail" alt="img"></p><p>Pointwise Mutual Information (PMI)</p><p><img src="https://uploader.shimo.im/f/5y0BeYWd6FcaIvsY.png!thumbnail" alt="img"></p><p>独立 P(x)*P(y) = P(x, y)</p><p>PMI表示了事件 x 和事件 y 之间是否存在相关性。</p><p>与 “cooked” PMI值最高的单词</p><p><img src="https://uploader.shimo.im/f/ywl7Mm0fkBYm9phX.png!thumbnail" alt="img"></p><h2 id="如何评估词向量的好坏？"><a href="#如何评估词向量的好坏？" class="headerlink" title="如何评估词向量的好坏？"></a>如何评估词向量的好坏？</h2><p>标准化的单词相似度数据集</p><ul><li>Assign a numerical similarity score between 0 and 10 (0 = words are totally    unrelated, 10 = words are VERY closely related).</li></ul><p><img src="https://uploader.shimo.im/f/ToYFke24QkcZ1Nnc.png!thumbnail" alt="img"><img src="https://uploader.shimo.im/f/QTXGcqteVe4Gw2ly.png!thumbnail" alt="img"></p><p>cosine(journey, voyage) = </p><p>cosine(king, queen) = </p><p>spearman’s R 分数</p><p><strong>Sparse vs. dense vectors</strong></p><p>根据context window定义的词向量非常长，<strong>很多位置上都是0.</strong> 表示我们的信息密度是很低的</p><ul><li><p>低维度词向量更容易训练模型，占用的内存/硬盘也会比较小。</p></li><li><p>低维度词向量能够学到一些单词间的关系，例如有些单词之间是近义词。</p></li></ul><p>降维算法</p><ul><li><p>PCA</p></li><li><p>SVD</p></li><li><p>Brown cluster</p></li><li><p><strong>Word2Vec</strong></p></li></ul><h2 id="Contextualized-Word-Vectors"><a href="#Contextualized-Word-Vectors" class="headerlink" title="Contextualized Word Vectors"></a>Contextualized Word Vectors</h2><p>近两年非常流行的做法，不仅仅是针对单个单词训练词向量，而是根据单词出现的语境给出词向量，是的该词向量既包含当前单词的信息，又包含单词周围context的信息。</p><ul><li><p>BERT, RoBERTa, ALBERT, T5</p></li><li><p>GPT2</p></li></ul><h2 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h2><p>NLP数据集</p><ul><li>NLP数据集一般包含输入（inputs，一般是文字）和输出（outputs，一般是某种标注）。</li></ul><p>标注</p><ul><li><p>监督学习需要标注过的数据集，这些标注一般被称为ground truth。</p></li><li><p>在自然语言处理数据集中，标注往往是由人手动标注的</p></li><li><p>人们往往会对数据的标注有不同的意见，因为很多时候不同的人对同样的语言会有不同的理解。所以我们也会把这些标注称为gold standard，而不是ground truth。</p></li></ul><p>NLP数据集如何构建</p><ul><li><p>付钱请人标注</p><ul><li>比较传统的做法</li><li>研究员写下标注的guideline，然后花钱请人标注（以前一般请一些专业的语言学家）</li><li>标注的质量会比较高，但是成本也高</li><li>例如，Penn Treebank(1993)</li></ul></li><li><p>Croudsourcing</p><ul><li>现在比较流行</li><li>一般不专门训练标注者（annotator），但是可以对同一条数据取得多条样本</li><li>例如，Stanford Sentiment Treebank</li></ul></li><li><p>自然拥有标注的数据集</p><ul><li>法律文件的中英文版本，可以用于训练翻译模型</li><li>报纸的内容分类</li><li>聊天记录</li><li>文本摘要（新闻的全文和摘要）</li></ul></li></ul><p>标注者同意度 Annotator Agreement</p><ul><li>给定两个标注者给出的所有标注，如何计算他们之间的标注是否比较统一？<ul><li>相同标注的百分比？</li><li>Cohen’s Kappa</li></ul></li></ul><p><img src="https://uploader.shimo.im/f/tPj4cDeD0qwzoRl8.png!thumbnail" alt="img"></p><p>来自维基百科</p><ul><li>也有更多别的测量方法</li></ul><h2 id="常见的文本分类数据集"><a href="#常见的文本分类数据集" class="headerlink" title="常见的文本分类数据集"></a>常见的文本分类数据集</h2><p>英文：</p><ul><li>AGNEWS, DBPedia, TREC: <a href="http://nlpprogress.com/english/text_classification.html" target="_blank" rel="noopener">http://nlpprogress.com/english/text_classification.html</a></li></ul><p>中文：</p><ul><li><a href="https://github.com/SophonPlus/ChineseNlpCorpus" target="_blank" rel="noopener">https://github.com/SophonPlus/ChineseNlpCorpus</a></li></ul><h2 id="文本分类模型"><a href="#文本分类模型" class="headerlink" title="文本分类模型"></a>文本分类模型</h2><p>什么是一个分类器？</p><ul><li><p>一个从输入（inputs）特征x投射到标注y的函数</p></li><li><p>一个简单的分类器：</p><ul><li>对于输入<strong>x</strong>，给每一个label y打一个分数，score(<strong>x</strong>, y, <strong>w</strong>)，其中<strong>w</strong>是模型的参数</li><li>分类问题也就是选出分数最高的y：classify(<strong>x, w</strong>) = argmax_y score(<strong>x</strong>, y, <strong>w</strong>)</li></ul></li></ul><p>Modeling, Inference, Learning</p><p><img src="https://uploader.shimo.im/f/ZMjjEt9ljYYZOe0M.png!thumbnail" alt="img"></p><h2 id="Modeling"><a href="#Modeling" class="headerlink" title="Modeling"></a>Modeling</h2><p>二元情感分类</p><p>classify(<strong>x, w</strong>)  = argmax_y score(<strong>x,</strong> y, <strong>w</strong>)</p><p>如果我们采用线性模型，那么模型可以被写为</p><p><img src="https://uploader.shimo.im/f/OS6m4GnrJW8nxTQx.png!thumbnail" alt="img"></p><p>现在的问题是，我们如何定义f？对于比较常见的机器学习问题，我们的输入往往是格式固定的，但是NLP的输入一般是长度不固定的文本。这里就涉及到如何把文本转换成特征（feature）。</p><ul><li><p>过去25年：特征工程（feature engineering），人为定制，比较复杂，只适用于某一类问题</p></li><li><p>过去5年：表示学习（representation learning）, ICLR， international conference for learning representations</p></li></ul><p>常见的features：</p><p>f1: 文本是正面情感，文本包含“好”</p><p>f2: 文本是负面情感，文本包含“好”</p><p>。。。</p><h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><p>比较直观，给定一段话，在每个Label上打分，然后取分数最大的label作为预测。</p><h2 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h2><ul><li><p>根据训练数据得到模型权重<strong>w</strong></p></li><li><p>把数据分为训练集（train），验证集（dev, val）测试集（test）</p></li><li><p>在NLP中，我们常常使用一种learning framework: Empirical Risk Minimization</p><ul><li>损失函数（cost function）：对比模型的预测和gold standard，计算一个分数<img src="https://uploader.shimo.im/f/d4DeJ9HiUWo9s7Lm.png!thumbnail" alt="img"></li><li>损失函数与我们真正优化的目标要尽量保持一致</li><li>一般来说如果cost为0，表示我们的模型预测完全正确</li><li>对于文本分类来说，我们应该使用怎样的损失函数呢？</li></ul></li></ul><p>错误率：<img src="https://uploader.shimo.im/f/E9r4QuAPgOgrMABu.png!thumbnail" alt="img"></p><p>Risk Minimization:</p><p>给定训练数据 <img src="https://uploader.shimo.im/f/osePhB4L13wW9hVW.png!thumbnail" alt="img">x表示输入，y表示label</p><p>我们的目标是<img src="https://uploader.shimo.im/f/HsQ3wAnDSF05627e.png!thumbnail" alt="img"></p><p>Empirical Risk Minimization <img src="https://uploader.shimo.im/f/j8m6S8mQx20BXvuG.png!thumbnail" alt="img"></p><p>我们之前定义的0-1损失函数是很难优化的，因为0-1loss不连续，所以无法使用基于梯度的优化方法。</p><p>loss.backward() # \d loss / \d w = gradient</p><p>optimizer.step() # w - learning_rate*gradient</p><p>cost = -score(x, y_label, w) 问题：没有考虑到label之间的关系！</p><p>一些其他的损失函数</p><p><strong>perceptron loss</strong></p><p><img src="https://uploader.shimo.im/f/Z0tFJCnDmosbrqSl.png!thumbnail" alt="img"></p><p><strong>hinge loss</strong></p><p><img src="https://uploader.shimo.im/f/4NtGiIfAhsUB1CD0.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/IYZsqDJtKqE1xTNS.png!thumbnail" alt="img"></p><h3 id="Log-Loss-Cross-Entropy-Loss"><a href="#Log-Loss-Cross-Entropy-Loss" class="headerlink" title="Log Loss/Cross Entropy Loss"></a>Log Loss/Cross Entropy Loss</h3><p><img src="https://uploader.shimo.im/f/t5msLvPFMsQMSZ7n.png!thumbnail" alt="img"></p><p>我们之前只有score(x, y, w)，怎么样定义p_w(y | z)</p><ul><li><p>让gold standard label的条件概率尽可能大</p></li><li><p>使用softmax把score转化成概率</p></li><li><p>其中的score function可以是各种函数，例如一个神经网络</p></li></ul><p>损失函数往往会结合regularization 正则项</p><ul><li><p>L2 regularization <img src="https://uploader.shimo.im/f/pZ2L5AVPvWE5Hksz.png!thumbnail" alt="img"></p></li><li><p>L1 regularization <img src="https://uploader.shimo.im/f/fG2yM4re1x8ZURLk.png!thumbnail" alt="img"></p></li></ul><p>模型训练</p><ul><li>(stochastic, batch) gradient descent</li></ul><h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p>语言模型：给句子计算一个概率</p><p>为什么会有这样一个奇怪的任务？</p><ul><li><p>机器翻译：P（我喜欢吃水果）&gt; P（我喜欢喝水果）</p></li><li><p>拼写检查：P（我想吃饭）&gt; P（我像吃饭）</p></li><li><p>语音识别：P （我看见了一架飞机）&gt; P（我看见了一架斐济）</p></li><li><p>summarizaton, question answering, etc. </p></li></ul><p>文本自动补全。。。</p><h2 id="概率语言模型（probablistic-language-modeling）"><a href="#概率语言模型（probablistic-language-modeling）" class="headerlink" title="概率语言模型（probablistic language modeling）"></a>概率语言模型（probablistic language modeling）</h2><ul><li><p>目标：计算一串单词连成一个句子的概率 P(<strong>w</strong>) = P(w_1, …, w_n)</p></li><li><p>相关的任务 P(w_4|w_1, …, w_3) </p></li><li><p>这两个任务的模型都称之为语言模型</p></li></ul><p>条件概率</p><p><img src="https://uploader.shimo.im/f/jnAscu6ZX6Ua0iKb.png!thumbnail" alt="img"></p><p>马尔科夫假设</p><ul><li><p>上述条件概率公式只取决于最近的n-1个单词 P(w_i|w_1, …, w_{i-1}) = P(w_i | w_{i-n+1}, …, w_{i-1})</p></li><li><p>我们创建出了n-gram模型</p></li><li><p>简单的案例，bigram模型</p></li></ul><p><img src="https://uploader.shimo.im/f/I2HL7hp9YtQpnrCY.png!thumbnail" alt="img"></p><p>一些Smoothing方法</p><ul><li>“Add-1” estimation</li></ul><p><img src="https://uploader.shimo.im/f/K8GCxNZ0tvAHbT6k.png!thumbnail" alt="img"></p><ul><li><p>Backoff，如果一些trigram存在，就是用trigram，如果不存在，就是用bigram，如果bigram也不存在，就退而求其次使用unigram。</p></li><li><p>interpolation：混合使用unigram, bigram, trigram</p></li></ul><p>Perplexity: 用于评估语言模型的好坏。评估的语言，我现在给你一套比较好的语言，我希望自己的语言模型能够给这段话尽可能高的分数。</p><p><img src="https://uploader.shimo.im/f/tq4vTXGrt80lNPky.png!thumbnail" alt="img"> l 越大越好，-l 越小越好</p><p><img src="https://uploader.shimo.im/f/QGDnJuSZy4U48KFA.png!thumbnail" alt="img">PP 越小越好 困惑度</p><p>perplexity越低 = 模型越好</p><h2 id="简单的Trigram神经网络语言模型"><a href="#简单的Trigram神经网络语言模型" class="headerlink" title="简单的Trigram神经网络语言模型"></a>简单的Trigram神经网络语言模型</h2><p><img src="https://uploader.shimo.im/f/CJrPG8AsEUktoMmU.png!thumbnail" alt="img"></p><p>这个模型可以使用log loss来训练。</p><p>我们还可以在这个模型的基础上增加hidden layer </p><p><img src="https://uploader.shimo.im/f/bV1ttfAFazcOcXpj.png!thumbnail" alt="img"></p><h2 id="循环神经网络（Recurrent-Neural-Networks）"><a href="#循环神经网络（Recurrent-Neural-Networks）" class="headerlink" title="循环神经网络（Recurrent Neural Networks）"></a>循环神经网络（Recurrent Neural Networks）</h2><p><img src="https://uploader.shimo.im/f/WUuDdC1NvcoEvYtG.png!thumbnail" alt="img"></p><p>基于循环神经网络的语言模型</p><p><img src="https://uploader.shimo.im/f/xveC6xbloAUcZNsk.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/4OYGZmwTiZIi45JQ.png!thumbnail" alt="img"></p><p>Long Short-term Memory</p><p><img src="https://uploader.shimo.im/f/LAcBJG4tRCEhosvb.png!thumbnail" alt="img"></p><p>Gates</p><p><img src="https://uploader.shimo.im/f/ALMg6pQ29vIRto5c.png!thumbnail" alt="img"></p><p>Gated Recurrent Unit (GRU)</p><p><img src="https://uploader.shimo.im/f/F7xsa6NCDqQwhXLD.png!thumbnail" alt="img"></p><p><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p><h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><p><img src="https://uploader.shimo.im/f/w3BqZxsJscgnTdOo.png!thumbnail" alt="img"></p><p>我们的目标是利用没有标注过的纯文本训练有用的词向量（word vectors）</p><p>skip-gram (window size = 5)</p><blockquote><p>agriculture is the tradional mainstay of the cambodian economy . but benares has been destroyed by an earthquake.</p></blockquote><p><img src="https://uploader.shimo.im/f/HCdEvmDFLjQaKEjy.png!thumbnail" alt="img"></p><p>skip-gram中使用的score function</p><p><img src="https://uploader.shimo.im/f/2zIZmqQGMHspT70Y.png!thumbnail" alt="img"></p><p>模型参数，所有的单词的词向量，包括输入向量和输出向量</p><h2 id="learning"><a href="#learning" class="headerlink" title="learning"></a>learning</h2><p><img src="https://uploader.shimo.im/f/N7MJfS38rIYSTgPJ.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/gONaJMt6zNYKXwOw.png!thumbnail" alt="img"></p><p>注意这个概率模型需要汇总单词表中的所有单词，计算量非常之大</p><h2 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h2><p><img src="https://uploader.shimo.im/f/3z40P9PADe4YxhYr.png!thumbnail" alt="img"></p><p>随机生成一些负例，然后优化以上损失函数</p><p><img src="https://uploader.shimo.im/f/sa2fnlo4tvA5zP4I.png!thumbnail" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语言模型 </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN-Image-Classification</title>
      <link href="/2020/01/18/CNN-Image-Classification/"/>
      <url>/2020/01/18/CNN-Image-Classification/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line">print(<span class="string">"PyTorch Version: "</span>,torch.__version__)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PyTorch Version:  1.0.0</span><br></pre></td></tr></table></figure><p>首先我们定义一个基于ConvNet的简单神经网络</p><p>In [4]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">50</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">4</span>*<span class="number">4</span>*<span class="number">50</span>, <span class="number">500</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">500</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        x = F.relu(self.conv2(x))</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">4</span>*<span class="number">4</span>*<span class="number">50</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>NLL loss的定义</p><p>ℓ(𝑥,𝑦)=𝐿={𝑙1,…,𝑙𝑁}⊤,𝑙𝑛=−𝑤𝑦𝑛𝑥𝑛,𝑦𝑛,𝑤𝑐=weight[𝑐]⋅𝟙{𝑐≠ignore_index}ℓ(x,y)=L={l1,…,lN}⊤,ln=−wynxn,yn,wc=weight[c]⋅1{c≠ignore_index}</p><p>In [7]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, device, train_loader, optimizer, epoch, log_interval=<span class="number">100</span>)</span>:</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        data, target = data.to(device), target.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data)</span><br><span class="line">        loss = F.nll_loss(output, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % log_interval == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:0f&#125;%)]\tLoss: &#123;:.6f&#125;"</span>.format(</span><br><span class="line">                epoch, batch_idx * len(data), len(train_loader.dataset), </span><br><span class="line">                <span class="number">100.</span> * batch_idx / len(train_loader), loss.item()</span><br><span class="line">            ))</span><br></pre></td></tr></table></figure><p>In [8]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(model, device, test_loader)</span>:</span></span><br><span class="line">    model.eval()</span><br><span class="line">    test_loss = <span class="number">0</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line">            data, target = data.to(device), target.to(device)</span><br><span class="line">            output = model(data)</span><br><span class="line">            test_loss += F.nll_loss(output, target, reduction=<span class="string">'sum'</span>).item() <span class="comment"># sum up batch loss</span></span><br><span class="line">            pred = output.argmax(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) <span class="comment"># get the index of the max log-probability</span></span><br><span class="line">            correct += pred.eq(target.view_as(pred)).sum().item()</span><br><span class="line"></span><br><span class="line">    test_loss /= len(test_loader.dataset)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\n'</span>.format(</span><br><span class="line">        test_loss, correct, len(test_loader.dataset),</span><br><span class="line">        <span class="number">100.</span> * correct / len(test_loader.dataset)))</span><br></pre></td></tr></table></figure><p>In [13]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">53113</span>)</span><br><span class="line"></span><br><span class="line">use_cuda = torch.cuda.is_available()</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> use_cuda <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">batch_size = test_batch_size = <span class="number">32</span></span><br><span class="line">kwargs = &#123;<span class="string">'num_workers'</span>: <span class="number">1</span>, <span class="string">'pin_memory'</span>: <span class="literal">True</span>&#125; <span class="keyword">if</span> use_cuda <span class="keyword">else</span> &#123;&#125;</span><br><span class="line">train_loader = torch.utils.data.DataLoader(</span><br><span class="line">    datasets.MNIST(<span class="string">'./mnist_data'</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>,</span><br><span class="line">                   transform=transforms.Compose([</span><br><span class="line">                       transforms.ToTensor(),</span><br><span class="line">                       transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">                   ])),</span><br><span class="line">    batch_size=batch_size, shuffle=<span class="literal">True</span>, **kwargs)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">    datasets.MNIST(<span class="string">'./mnist_data'</span>, train=<span class="literal">False</span>, transform=transforms.Compose([</span><br><span class="line">                       transforms.ToTensor(),</span><br><span class="line">                       transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">                   ])),</span><br><span class="line">    batch_size=test_batch_size, shuffle=<span class="literal">True</span>, **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line">momentum = <span class="number">0.5</span></span><br><span class="line">model = Net().to(device)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">2</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">    train(model, device, train_loader, optimizer, epoch)</span><br><span class="line">    test(model, device, test_loader)</span><br><span class="line"></span><br><span class="line">save_model = <span class="literal">True</span></span><br><span class="line"><span class="keyword">if</span> (save_model):</span><br><span class="line">    torch.save(model.state_dict(),<span class="string">"mnist_cnn.pt"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">Train Epoch: 1 [0/60000 (0.000000%)]Loss: 2.297938</span><br><span class="line">Train Epoch: 1 [3200/60000 (5.333333%)]Loss: 0.567845</span><br><span class="line">Train Epoch: 1 [6400/60000 (10.666667%)]Loss: 0.206370</span><br><span class="line">Train Epoch: 1 [9600/60000 (16.000000%)]Loss: 0.094653</span><br><span class="line">Train Epoch: 1 [12800/60000 (21.333333%)]Loss: 0.180530</span><br><span class="line">Train Epoch: 1 [16000/60000 (26.666667%)]Loss: 0.041645</span><br><span class="line">Train Epoch: 1 [19200/60000 (32.000000%)]Loss: 0.135092</span><br><span class="line">Train Epoch: 1 [22400/60000 (37.333333%)]Loss: 0.054001</span><br><span class="line">Train Epoch: 1 [25600/60000 (42.666667%)]Loss: 0.111863</span><br><span class="line">Train Epoch: 1 [28800/60000 (48.000000%)]Loss: 0.059039</span><br><span class="line">Train Epoch: 1 [32000/60000 (53.333333%)]Loss: 0.089227</span><br><span class="line">Train Epoch: 1 [35200/60000 (58.666667%)]Loss: 0.186015</span><br><span class="line">Train Epoch: 1 [38400/60000 (64.000000%)]Loss: 0.093208</span><br><span class="line">Train Epoch: 1 [41600/60000 (69.333333%)]Loss: 0.077090</span><br><span class="line">Train Epoch: 1 [44800/60000 (74.666667%)]Loss: 0.038075</span><br><span class="line">Train Epoch: 1 [48000/60000 (80.000000%)]Loss: 0.036247</span><br><span class="line">Train Epoch: 1 [51200/60000 (85.333333%)]Loss: 0.052358</span><br><span class="line">Train Epoch: 1 [54400/60000 (90.666667%)]Loss: 0.013201</span><br><span class="line">Train Epoch: 1 [57600/60000 (96.000000%)]Loss: 0.036660</span><br><span class="line"></span><br><span class="line">Test set: Average loss: 0.0644, Accuracy: 9802/10000 (98%)</span><br><span class="line"></span><br><span class="line">Train Epoch: 2 [0/60000 (0.000000%)]Loss: 0.054402</span><br><span class="line">Train Epoch: 2 [3200/60000 (5.333333%)]Loss: 0.032239</span><br><span class="line">Train Epoch: 2 [6400/60000 (10.666667%)]Loss: 0.092350</span><br><span class="line">Train Epoch: 2 [9600/60000 (16.000000%)]Loss: 0.058544</span><br><span class="line">Train Epoch: 2 [12800/60000 (21.333333%)]Loss: 0.029762</span><br><span class="line">Train Epoch: 2 [16000/60000 (26.666667%)]Loss: 0.012521</span><br><span class="line">Train Epoch: 2 [19200/60000 (32.000000%)]Loss: 0.101891</span><br><span class="line">Train Epoch: 2 [22400/60000 (37.333333%)]Loss: 0.127773</span><br><span class="line">Train Epoch: 2 [25600/60000 (42.666667%)]Loss: 0.009259</span><br><span class="line">Train Epoch: 2 [28800/60000 (48.000000%)]Loss: 0.013482</span><br><span class="line">Train Epoch: 2 [32000/60000 (53.333333%)]Loss: 0.039676</span><br><span class="line">Train Epoch: 2 [35200/60000 (58.666667%)]Loss: 0.016707</span><br><span class="line">Train Epoch: 2 [38400/60000 (64.000000%)]Loss: 0.168691</span><br><span class="line">Train Epoch: 2 [41600/60000 (69.333333%)]Loss: 0.056318</span><br><span class="line">Train Epoch: 2 [44800/60000 (74.666667%)]Loss: 0.008174</span><br><span class="line">Train Epoch: 2 [48000/60000 (80.000000%)]Loss: 0.075149</span><br><span class="line">Train Epoch: 2 [51200/60000 (85.333333%)]Loss: 0.205798</span><br><span class="line">Train Epoch: 2 [54400/60000 (90.666667%)]Loss: 0.019762</span><br><span class="line">Train Epoch: 2 [57600/60000 (96.000000%)]Loss: 0.012056</span><br><span class="line"></span><br><span class="line">Test set: Average loss: 0.0464, Accuracy: 9850/10000 (98%)</span><br></pre></td></tr></table></figure><p>In [15]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">53113</span>)</span><br><span class="line"></span><br><span class="line">use_cuda = torch.cuda.is_available()</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> use_cuda <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">batch_size = test_batch_size = <span class="number">32</span></span><br><span class="line">kwargs = &#123;<span class="string">'num_workers'</span>: <span class="number">1</span>, <span class="string">'pin_memory'</span>: <span class="literal">True</span>&#125; <span class="keyword">if</span> use_cuda <span class="keyword">else</span> &#123;&#125;</span><br><span class="line">train_loader = torch.utils.data.DataLoader(</span><br><span class="line">    datasets.FashionMNIST(<span class="string">'./fashion_mnist_data'</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>,</span><br><span class="line">                   transform=transforms.Compose([</span><br><span class="line">                       transforms.ToTensor(),</span><br><span class="line">                       transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">                   ])),</span><br><span class="line">    batch_size=batch_size, shuffle=<span class="literal">True</span>, **kwargs)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">    datasets.FashionMNIST(<span class="string">'./fashion_mnist_data'</span>, train=<span class="literal">False</span>, transform=transforms.Compose([</span><br><span class="line">                       transforms.ToTensor(),</span><br><span class="line">                       transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">                   ])),</span><br><span class="line">    batch_size=test_batch_size, shuffle=<span class="literal">True</span>, **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line">momentum = <span class="number">0.5</span></span><br><span class="line">model = Net().to(device)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">2</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">    train(model, device, train_loader, optimizer, epoch)</span><br><span class="line">    test(model, device, test_loader)</span><br><span class="line"></span><br><span class="line">save_model = <span class="literal">True</span></span><br><span class="line"><span class="keyword">if</span> (save_model):</span><br><span class="line">    torch.save(model.state_dict(),<span class="string">"fashion_mnist_cnn.pt"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz</span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz</span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz</span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz</span><br><span class="line">Processing...</span><br><span class="line">Done!</span><br><span class="line">Train Epoch: 1 [0/60000 (0.000000%)]Loss: 2.279603</span><br><span class="line">Train Epoch: 1 [3200/60000 (5.333333%)]Loss: 0.962251</span><br><span class="line">Train Epoch: 1 [6400/60000 (10.666667%)]Loss: 1.019635</span><br><span class="line">Train Epoch: 1 [9600/60000 (16.000000%)]Loss: 0.544330</span><br><span class="line">Train Epoch: 1 [12800/60000 (21.333333%)]Loss: 0.629807</span><br><span class="line">Train Epoch: 1 [16000/60000 (26.666667%)]Loss: 0.514437</span><br><span class="line">Train Epoch: 1 [19200/60000 (32.000000%)]Loss: 0.555741</span><br><span class="line">Train Epoch: 1 [22400/60000 (37.333333%)]Loss: 0.528186</span><br><span class="line">Train Epoch: 1 [25600/60000 (42.666667%)]Loss: 0.656440</span><br><span class="line">Train Epoch: 1 [28800/60000 (48.000000%)]Loss: 0.294654</span><br><span class="line">Train Epoch: 1 [32000/60000 (53.333333%)]Loss: 0.293626</span><br><span class="line">Train Epoch: 1 [35200/60000 (58.666667%)]Loss: 0.227645</span><br><span class="line">Train Epoch: 1 [38400/60000 (64.000000%)]Loss: 0.473842</span><br><span class="line">Train Epoch: 1 [41600/60000 (69.333333%)]Loss: 0.724678</span><br><span class="line">Train Epoch: 1 [44800/60000 (74.666667%)]Loss: 0.519580</span><br><span class="line">Train Epoch: 1 [48000/60000 (80.000000%)]Loss: 0.465854</span><br><span class="line">Train Epoch: 1 [51200/60000 (85.333333%)]Loss: 0.378200</span><br><span class="line">Train Epoch: 1 [54400/60000 (90.666667%)]Loss: 0.503832</span><br><span class="line">Train Epoch: 1 [57600/60000 (96.000000%)]Loss: 0.616502</span><br><span class="line"></span><br><span class="line">Test set: Average loss: 0.4365, Accuracy: 8425/10000 (84%)</span><br><span class="line"></span><br><span class="line">Train Epoch: 2 [0/60000 (0.000000%)]Loss: 0.385171</span><br><span class="line">Train Epoch: 2 [3200/60000 (5.333333%)]Loss: 0.329045</span><br><span class="line">Train Epoch: 2 [6400/60000 (10.666667%)]Loss: 0.308792</span><br><span class="line">Train Epoch: 2 [9600/60000 (16.000000%)]Loss: 0.360471</span><br><span class="line">Train Epoch: 2 [12800/60000 (21.333333%)]Loss: 0.445865</span><br><span class="line">Train Epoch: 2 [16000/60000 (26.666667%)]Loss: 0.357145</span><br><span class="line">Train Epoch: 2 [19200/60000 (32.000000%)]Loss: 0.376523</span><br><span class="line">Train Epoch: 2 [22400/60000 (37.333333%)]Loss: 0.389735</span><br><span class="line">Train Epoch: 2 [25600/60000 (42.666667%)]Loss: 0.308655</span><br><span class="line">Train Epoch: 2 [28800/60000 (48.000000%)]Loss: 0.352300</span><br><span class="line">Train Epoch: 2 [32000/60000 (53.333333%)]Loss: 0.499613</span><br><span class="line">Train Epoch: 2 [35200/60000 (58.666667%)]Loss: 0.282398</span><br><span class="line">Train Epoch: 2 [38400/60000 (64.000000%)]Loss: 0.330232</span><br><span class="line">Train Epoch: 2 [41600/60000 (69.333333%)]Loss: 0.430427</span><br><span class="line">Train Epoch: 2 [44800/60000 (74.666667%)]Loss: 0.406084</span><br><span class="line">Train Epoch: 2 [48000/60000 (80.000000%)]Loss: 0.443538</span><br><span class="line">Train Epoch: 2 [51200/60000 (85.333333%)]Loss: 0.348947</span><br><span class="line">Train Epoch: 2 [54400/60000 (90.666667%)]Loss: 0.424920</span><br><span class="line">Train Epoch: 2 [57600/60000 (96.000000%)]Loss: 0.231494</span><br><span class="line"></span><br><span class="line">Test set: Average loss: 0.3742, Accuracy: 8652/10000 (87%)</span><br></pre></td></tr></table></figure><h3 id="CNN模型的迁移学习"><a href="#CNN模型的迁移学习" class="headerlink" title="CNN模型的迁移学习"></a>CNN模型的迁移学习</h3><ul><li>很多时候当我们需要训练一个新的图像分类任务，我们不会完全从一个随机的模型开始训练，而是利用_预训练_的模型来加速训练的过程。我们经常使用在<code>ImageNet</code>上的预训练模型。</li><li>这是一种transfer learning的方法。我们常用以下两种方法做迁移学习。<ul><li>fine tuning: 从一个预训练模型开始，我们改变一些模型的架构，然后继续训练整个模型的参数。</li><li>feature extraction: 我们不再改变与训练模型的参数，而是只更新我们改变过的部分模型参数。我们之所以叫它feature extraction是因为我们把预训练的CNN模型当做一个特征提取模型，利用提取出来的特征做来完成我们的训练任务。</li></ul></li></ul><p>以下是构建和训练迁移学习模型的基本步骤：</p><ul><li>初始化预训练模型</li><li>把最后一层的输出层改变成我们想要分的类别总数</li><li>定义一个optimizer来更新参数</li><li>模型训练</li></ul><p>In [87]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms, models</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">print(<span class="string">"Torchvision Version: "</span>,torchvision.__version__)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Torchvision Version:  0.2.0</span><br></pre></td></tr></table></figure><h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>我们会使用<em>hymenoptera_data</em>数据集，<a href="https://download.pytorch.org/tutorial/hymenoptera_data.zip" target="_blank" rel="noopener">下载</a>.</p><p>这个数据集包括两类图片, <strong>bees</strong> 和 <strong>ants</strong>, 这些数据都被处理成了可以使用<code>ImageFolder &lt;https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder&gt;</code>来读取的格式。我们只需要把<code>data_dir</code>设置成数据的根目录，然后把<code>model_name</code>设置成我们想要使用的与训练模型： :: [resnet, alexnet, vgg, squeezenet, densenet, inception]</p><p>其他的参数有：</p><ul><li><code>num_classes</code>表示数据集分类的类别数</li><li><code>batch_size</code></li><li><code>num_epochs</code></li><li><code>feature_extract</code>表示我们训练的时候使用fine tuning还是feature extraction方法。如果<code>feature_extract = False</code>，整个模型都会被同时更新。如果<code>feature_extract = True</code>，只有模型的最后一层被更新。</li></ul><p>In [36]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Top level data directory. Here we assume the format of the directory conforms </span><br><span class="line">#   to the ImageFolder structure</span><br><span class="line">data_dir = &quot;./hymenoptera_data&quot;</span><br><span class="line"># Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]</span><br><span class="line">model_name = &quot;resnet&quot;</span><br><span class="line"># Number of classes in the dataset</span><br><span class="line">num_classes = 2</span><br><span class="line"># Batch size for training (change depending on how much memory you have)</span><br><span class="line">batch_size = 32</span><br><span class="line"># Number of epochs to train for </span><br><span class="line">num_epochs = 15</span><br><span class="line"># Flag for feature extracting. When False, we finetune the whole model, </span><br><span class="line">#   when True we only update the reshaped layer params</span><br><span class="line">feature_extract = True</span><br></pre></td></tr></table></figure><p>In [120]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model, dataloaders, criterion, optimizer, num_epochs=<span class="number">5</span>)</span>:</span></span><br><span class="line">    since = time.time()</span><br><span class="line">    val_acc_history = []</span><br><span class="line">    best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line">    best_acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        print(<span class="string">"Epoch &#123;&#125;/&#123;&#125;"</span>.format(epoch, num_epochs<span class="number">-1</span>))</span><br><span class="line">        print(<span class="string">"-"</span>*<span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">"train"</span>, <span class="string">"val"</span>]:</span><br><span class="line">            running_loss = <span class="number">0.</span></span><br><span class="line">            running_corrects = <span class="number">0.</span></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">"train"</span>:</span><br><span class="line">                model.train()</span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                model.eval()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[phase]:</span><br><span class="line">                inputs = inputs.to(device)</span><br><span class="line">                labels = labels.to(device)</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">with</span> torch.autograd.set_grad_enabled(phase==<span class="string">"train"</span>):</span><br><span class="line">                    outputs = model(inputs)</span><br><span class="line">                    loss = criterion(outputs, labels)</span><br><span class="line">                    </span><br><span class="line">                _, preds = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">if</span> phase == <span class="string">"train"</span>:</span><br><span class="line">                    optimizer.zero_grad()</span><br><span class="line">                    loss.backward()</span><br><span class="line">                    optimizer.step()</span><br><span class="line">                    </span><br><span class="line">                running_loss += loss.item() * inputs.size(<span class="number">0</span>)</span><br><span class="line">                running_corrects += torch.sum(preds.view(<span class="number">-1</span>) == labels.view(<span class="number">-1</span>)).item()</span><br><span class="line">            </span><br><span class="line">            epoch_loss = running_loss / len(dataloaders[phase].dataset)</span><br><span class="line">            epoch_acc = running_corrects / len(dataloaders[phase].dataset)</span><br><span class="line">       </span><br><span class="line">            print(<span class="string">"&#123;&#125; Loss: &#123;&#125; Acc: &#123;&#125;"</span>.format(phase, epoch_loss, epoch_acc))</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">"val"</span> <span class="keyword">and</span> epoch_acc &gt; best_acc:</span><br><span class="line">                best_acc = epoch_acc</span><br><span class="line">                best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">"val"</span>:</span><br><span class="line">                val_acc_history.append(epoch_acc)</span><br><span class="line">            </span><br><span class="line">        print()</span><br><span class="line">    </span><br><span class="line">    time_elapsed = time.time() - since</span><br><span class="line">    print(<span class="string">"Training compete in &#123;&#125;m &#123;&#125;s"</span>.format(time_elapsed // <span class="number">60</span>, time_elapsed % <span class="number">60</span>))</span><br><span class="line">    print(<span class="string">"Best val Acc: &#123;&#125;"</span>.format(best_acc))</span><br><span class="line">    </span><br><span class="line">    model.load_state_dict(best_model_wts)</span><br><span class="line">    <span class="keyword">return</span> model, val_acc_history</span><br></pre></td></tr></table></figure><p>In [121]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># it = iter(dataloaders_dict[&quot;train&quot;])</span><br><span class="line"># inputs, labels = next(it)</span><br><span class="line"># for inputs, labels in dataloaders_dict[&quot;train&quot;]:</span><br><span class="line">#     print(labels.size())</span><br></pre></td></tr></table></figure><p>In [122]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(dataloaders_dict[&quot;train&quot;].dataset.imgs)</span><br></pre></td></tr></table></figure><p>Out[122]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">244</span><br></pre></td></tr></table></figure><p>In [123]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(dataloaders_dict[&quot;train&quot;].dataset)</span><br></pre></td></tr></table></figure><p>Out[123]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">244</span><br></pre></td></tr></table></figure><p>In [124]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def set_parameter_requires_grad(model, feature_extracting):</span><br><span class="line">    if feature_extracting:</span><br><span class="line">        for param in model.parameters():</span><br><span class="line">            param.requires_grad = False</span><br></pre></td></tr></table></figure><p>In [125]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_model</span><span class="params">(model_name, num_classes, feature_extract, use_pretrained=True)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> model_name == <span class="string">"resnet"</span>:</span><br><span class="line">        model_ft = models.resnet18(pretrained=use_pretrained)</span><br><span class="line">        set_parameter_requires_grad(model_ft, feature_extract)</span><br><span class="line">        num_ftrs = model_ft.fc.in_features</span><br><span class="line">        model_ft.fc = nn.Linear(num_ftrs, num_classes)</span><br><span class="line">        input_size = <span class="number">224</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> model_ft, input_size</span><br><span class="line">model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=<span class="literal">True</span>)</span><br><span class="line">print(model_ft)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">ResNet(</span><br><span class="line">  (conv1): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">7</span>, <span class="number">7</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">3</span>, <span class="number">3</span>), bias=<span class="literal">False</span>)</span><br><span class="line">  (bn1): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">  (relu): ReLU(inplace)</span><br><span class="line">  (maxpool): MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">  (layer1): Sequential(</span><br><span class="line">    (<span class="number">0</span>): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (relu): ReLU(inplace)</span><br><span class="line">      (conv2): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (relu): ReLU(inplace)</span><br><span class="line">      (conv2): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer2): Sequential(</span><br><span class="line">    (<span class="number">0</span>): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (relu): ReLU(inplace)</span><br><span class="line">      (conv2): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (relu): ReLU(inplace)</span><br><span class="line">      (conv2): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer3): Sequential(</span><br><span class="line">    (<span class="number">0</span>): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (relu): ReLU(inplace)</span><br><span class="line">      (conv2): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (relu): ReLU(inplace)</span><br><span class="line">      (conv2): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer4): Sequential(</span><br><span class="line">    (<span class="number">0</span>): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (relu): ReLU(inplace)</span><br><span class="line">      (conv2): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (relu): ReLU(inplace)</span><br><span class="line">      (conv2): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AvgPool2d(kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">  (fc): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">2</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="读入数据"><a href="#读入数据" class="headerlink" title="读入数据"></a>读入数据</h2><p>现在我们知道了模型输入的size，我们就可以把数据预处理成相应的格式。</p><p>In [126]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">data_transforms = &#123;</span><br><span class="line">    <span class="string">"train"</span>: transforms.Compose([</span><br><span class="line">        transforms.RandomResizedCrop(input_size),</span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">    <span class="string">"val"</span>: transforms.Compose([</span><br><span class="line">        transforms.Resize(input_size),</span><br><span class="line">        transforms.CenterCrop(input_size),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Initializing Datasets and Dataloaders..."</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create training and validation datasets</span></span><br><span class="line">image_datasets = &#123;x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br><span class="line"><span class="comment"># Create training and validation dataloaders</span></span><br><span class="line">dataloaders_dict = &#123;x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Detect if we have a GPU available</span></span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Initializing Datasets and Dataloaders...</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In [127]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Send the model to GPU</span></span><br><span class="line">model_ft = model_ft.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Gather the parameters to be optimized/updated in this run. If we are</span></span><br><span class="line"><span class="comment">#  finetuning we will be updating all parameters. However, if we are </span></span><br><span class="line"><span class="comment">#  doing feature extract method, we will only update the parameters</span></span><br><span class="line"><span class="comment">#  that we have just initialized, i.e. the parameters with requires_grad</span></span><br><span class="line"><span class="comment">#  is True.</span></span><br><span class="line">params_to_update = model_ft.parameters()</span><br><span class="line">print(<span class="string">"Params to learn:"</span>)</span><br><span class="line"><span class="keyword">if</span> feature_extract:</span><br><span class="line">    params_to_update = []</span><br><span class="line">    <span class="keyword">for</span> name,param <span class="keyword">in</span> model_ft.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> param.requires_grad == <span class="literal">True</span>:</span><br><span class="line">            params_to_update.append(param)</span><br><span class="line">            print(<span class="string">"\t"</span>,name)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">for</span> name,param <span class="keyword">in</span> model_ft.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> param.requires_grad == <span class="literal">True</span>:</span><br><span class="line">            print(<span class="string">"\t"</span>,name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Observe that all parameters are being optimized</span></span><br><span class="line">optimizer_ft = optim.SGD(params_to_update, lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Params to learn:</span><br><span class="line"> fc.weight</span><br><span class="line"> fc.bias</span><br></pre></td></tr></table></figure><p>In [133]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Setup the loss fxn</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train and evaluate</span></span><br><span class="line">model_ft, ohist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.2623850886450439 Acc: 0.8975409836065574</span><br><span class="line">val Loss: 0.22199168762350394 Acc: 0.9215686274509803</span><br><span class="line"></span><br><span class="line">Epoch 1/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.20775875546893136 Acc: 0.9262295081967213</span><br><span class="line">val Loss: 0.21329789413930544 Acc: 0.9215686274509803</span><br><span class="line"></span><br><span class="line">Epoch 2/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.24463887243974405 Acc: 0.9098360655737705</span><br><span class="line">val Loss: 0.2308054333613589 Acc: 0.9215686274509803</span><br><span class="line"></span><br><span class="line">Epoch 3/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.2108444703406975 Acc: 0.930327868852459</span><br><span class="line">val Loss: 0.20637644174831365 Acc: 0.954248366013072</span><br><span class="line"></span><br><span class="line">Epoch 4/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.22102872954040279 Acc: 0.9221311475409836</span><br><span class="line">val Loss: 0.19902625017695957 Acc: 0.9281045751633987</span><br><span class="line"></span><br><span class="line">Epoch 5/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.22044393127081824 Acc: 0.9221311475409836</span><br><span class="line">val Loss: 0.2212505256818011 Acc: 0.9281045751633987</span><br><span class="line"></span><br><span class="line">Epoch 6/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.1636357441788814 Acc: 0.9467213114754098</span><br><span class="line">val Loss: 0.1969745449380937 Acc: 0.934640522875817</span><br><span class="line"></span><br><span class="line">Epoch 7/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.1707800094221459 Acc: 0.9385245901639344</span><br><span class="line">val Loss: 0.20569930824578977 Acc: 0.934640522875817</span><br><span class="line"></span><br><span class="line">Epoch 8/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.18224841185280535 Acc: 0.9344262295081968</span><br><span class="line">val Loss: 0.192565394480244 Acc: 0.9411764705882353</span><br><span class="line"></span><br><span class="line">Epoch 9/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.17762072372143387 Acc: 0.9385245901639344</span><br><span class="line">val Loss: 0.19549715163466197 Acc: 0.9411764705882353</span><br><span class="line"></span><br><span class="line">Epoch 10/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.19314993575948183 Acc: 0.9180327868852459</span><br><span class="line">val Loss: 0.2000840900380627 Acc: 0.934640522875817</span><br><span class="line"></span><br><span class="line">Epoch 11/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.21551114418467537 Acc: 0.9057377049180327</span><br><span class="line">val Loss: 0.18960770005299374 Acc: 0.934640522875817</span><br><span class="line"></span><br><span class="line">Epoch 12/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.1847396502729322 Acc: 0.9426229508196722</span><br><span class="line">val Loss: 0.1871058808432685 Acc: 0.9411764705882353</span><br><span class="line"></span><br><span class="line">Epoch 13/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.17342406132670699 Acc: 0.9508196721311475</span><br><span class="line">val Loss: 0.20636656588199093 Acc: 0.9215686274509803</span><br><span class="line"></span><br><span class="line">Epoch 14/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.16013679030488748 Acc: 0.9508196721311475</span><br><span class="line">val Loss: 0.18491691759988374 Acc: 0.9411764705882353</span><br><span class="line"></span><br><span class="line">Training compete in 0.0m 14.700076580047607s</span><br><span class="line">Best val Acc: 0.954248366013072</span><br></pre></td></tr></table></figure><p>In [130]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the non-pretrained version of the model used for this run</span></span><br><span class="line">scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=<span class="literal">False</span>, use_pretrained=<span class="literal">False</span>)</span><br><span class="line">scratch_model = scratch_model.to(device)</span><br><span class="line">scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">scratch_criterion = nn.CrossEntropyLoss()</span><br><span class="line">_,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.7185551504619786 Acc: 0.4426229508196721</span><br><span class="line">val Loss: 0.6956208067781785 Acc: 0.45751633986928103</span><br><span class="line"></span><br><span class="line">Epoch 1/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.6852761008700387 Acc: 0.5778688524590164</span><br><span class="line">val Loss: 0.6626271987273022 Acc: 0.6601307189542484</span><br><span class="line"></span><br><span class="line">Epoch 2/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.6603062289660094 Acc: 0.5942622950819673</span><br><span class="line">val Loss: 0.6489538297154545 Acc: 0.5816993464052288</span><br><span class="line"></span><br><span class="line">Epoch 3/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.6203305486772881 Acc: 0.639344262295082</span><br><span class="line">val Loss: 0.6013184107986151 Acc: 0.673202614379085</span><br><span class="line"></span><br><span class="line">Epoch 4/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.5989709232674271 Acc: 0.6680327868852459</span><br><span class="line">val Loss: 0.5929347966231552 Acc: 0.6993464052287581</span><br><span class="line"></span><br><span class="line">Epoch 5/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.5821619336722327 Acc: 0.6557377049180327</span><br><span class="line">val Loss: 0.5804777059679717 Acc: 0.6928104575163399</span><br><span class="line"></span><br><span class="line">Epoch 6/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.6114685896967278 Acc: 0.6270491803278688</span><br><span class="line">val Loss: 0.5674225290616354 Acc: 0.7189542483660131</span><br><span class="line"></span><br><span class="line">Epoch 7/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.5681056575696977 Acc: 0.6680327868852459</span><br><span class="line">val Loss: 0.5602688086188696 Acc: 0.7189542483660131</span><br><span class="line"></span><br><span class="line">Epoch 8/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.5701596453541615 Acc: 0.7090163934426229</span><br><span class="line">val Loss: 0.5554519264526616 Acc: 0.7450980392156863</span><br><span class="line"></span><br><span class="line">Epoch 9/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.5476810380083615 Acc: 0.7254098360655737</span><br><span class="line">val Loss: 0.5805927063125411 Acc: 0.7189542483660131</span><br><span class="line"></span><br><span class="line">Epoch 10/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.5508710468401674 Acc: 0.6926229508196722</span><br><span class="line">val Loss: 0.5859468777974447 Acc: 0.7058823529411765</span><br><span class="line"></span><br><span class="line">Epoch 11/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.5344281519045595 Acc: 0.7172131147540983</span><br><span class="line">val Loss: 0.5640550851821899 Acc: 0.7058823529411765</span><br><span class="line"></span><br><span class="line">Epoch 12/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.5125471890949812 Acc: 0.7295081967213115</span><br><span class="line">val Loss: 0.5665123891207128 Acc: 0.7058823529411765</span><br><span class="line"></span><br><span class="line">Epoch 13/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.496260079204059 Acc: 0.7254098360655737</span><br><span class="line">val Loss: 0.5820710787586137 Acc: 0.7058823529411765</span><br><span class="line"></span><br><span class="line">Epoch 14/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.49067981907578767 Acc: 0.7704918032786885</span><br><span class="line">val Loss: 0.5722863315756804 Acc: 0.7058823529411765</span><br><span class="line"></span><br><span class="line">Training compete in 0.0m 18.418847799301147s</span><br><span class="line">Best val Acc: 0.7450980392156863</span><br></pre></td></tr></table></figure><p>In [134]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the training curves of validation accuracy vs. number </span></span><br><span class="line"><span class="comment">#  of training epochs for the transfer learning method and</span></span><br><span class="line"><span class="comment">#  the model trained from scratch</span></span><br><span class="line"><span class="comment"># ohist = []</span></span><br><span class="line"><span class="comment"># shist = []</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ohist = [h.cpu().numpy() for h in ohist]</span></span><br><span class="line"><span class="comment"># shist = [h.cpu().numpy() for h in scratch_hist]</span></span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">"Validation Accuracy vs. Number of Training Epochs"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Training Epochs"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Validation Accuracy"</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,num_epochs+<span class="number">1</span>),ohist,label=<span class="string">"Pretrained"</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,num_epochs+<span class="number">1</span>),scratch_hist,label=<span class="string">"Scratch"</span>)</span><br><span class="line">plt.ylim((<span class="number">0</span>,<span class="number">1.</span>))</span><br><span class="line">plt.xticks(np.arange(<span class="number">1</span>, num_epochs+<span class="number">1</span>, <span class="number">1.0</span>))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>丘吉尔的人物传记char级别的文本生成代码注释</title>
      <link href="/2019/11/08/%E4%B8%98%E5%90%89%E5%B0%94%E7%9A%84%E4%BA%BA%E7%89%A9%E4%BC%A0%E8%AE%B0char%E7%BA%A7%E5%88%AB%E7%9A%84%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E4%BB%A3%E7%A0%81%E6%B3%A8%E9%87%8A/"/>
      <url>/2019/11/08/%E4%B8%98%E5%90%89%E5%B0%94%E7%9A%84%E4%BA%BA%E7%89%A9%E4%BC%A0%E8%AE%B0char%E7%BA%A7%E5%88%AB%E7%9A%84%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E4%BB%A3%E7%A0%81%E6%B3%A8%E9%87%8A/</url>
      
        <content type="html"><![CDATA[<h1 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h1><p>举个小小的例子，来看看LSTM是怎么玩的</p><p>我们这里用温斯顿丘吉尔的人物传记作为我们的学习语料。</p><p>第一步，一样，先导入各种库</p><p>关于LSTM的详细原理先看博客：<a href="https://www.julyedu.com/question/big/kp_id/26/ques_id/1851" target="_blank" rel="noopener">如何从RNN起步，一步一步通俗理解LSTM</a></p><p>In [68]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dropout</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> LSTM</span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ModelCheckpoint</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br></pre></td></tr></table></figure><p>In [69]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">raw_text = open(<span class="string">'./input/Winston_Churchil.txt'</span>).read()</span><br><span class="line"><span class="comment"># .read() 读入整个文件为一个字符串</span></span><br><span class="line">raw_text = raw_text.lower() <span class="comment"># 小写</span></span><br><span class="line">print(raw_text[:<span class="number">100</span>]) <span class="comment">#打印前100个字符</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">﻿project gutenberg’s real soldiers of fortune, by richard harding davis</span><br><span class="line"></span><br><span class="line">this ebook <span class="keyword">is</span> <span class="keyword">for</span> the use o</span><br></pre></td></tr></table></figure><p>既然我们是以每个字母为层级，字母总共才26个，所以我们可以很方便的用One-Hot来编码出所有的字母（当然，可能还有些标点符号和其他noise）</p><p>In [70]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">chars = sorted(list(set(raw_text))) </span><br><span class="line"><span class="comment"># 这里去重不是单词去重，是字母去重，去重后只有英文字母和标点符号等字符</span></span><br><span class="line">print(len(raw_text))</span><br><span class="line">print(len(chars))</span><br><span class="line">char_to_int = dict((c, i) <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(chars)) </span><br><span class="line"><span class="comment"># 去重后的字符排序好后，xi</span></span><br><span class="line">int_to_char = dict((i, c) <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(chars))</span><br><span class="line">print(char_to_int)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">276830</span><br><span class="line">61</span><br><span class="line">&#123;&apos;\n&apos;: 0, &apos; &apos;: 1, &apos;!&apos;: 2, &apos;#&apos;: 3, &apos;$&apos;: 4, &apos;%&apos;: 5, &apos;(&apos;: 6, &apos;)&apos;: 7, &apos;*&apos;: 8, &apos;,&apos;: 9, &apos;-&apos;: 10, &apos;.&apos;: 11, &apos;/&apos;: 12, &apos;0&apos;: 13, &apos;1&apos;: 14, &apos;2&apos;: 15, &apos;3&apos;: 16, &apos;4&apos;: 17, &apos;5&apos;: 18, &apos;6&apos;: 19, &apos;7&apos;: 20, &apos;8&apos;: 21, &apos;9&apos;: 22, &apos;:&apos;: 23, &apos;;&apos;: 24, &apos;?&apos;: 25, &apos;@&apos;: 26, &apos;[&apos;: 27, &apos;]&apos;: 28, &apos;_&apos;: 29, &apos;a&apos;: 30, &apos;b&apos;: 31, &apos;c&apos;: 32, &apos;d&apos;: 33, &apos;e&apos;: 34, &apos;f&apos;: 35, &apos;g&apos;: 36, &apos;h&apos;: 37, &apos;i&apos;: 38, &apos;j&apos;: 39, &apos;k&apos;: 40, &apos;l&apos;: 41, &apos;m&apos;: 42, &apos;n&apos;: 43, &apos;o&apos;: 44, &apos;p&apos;: 45, &apos;q&apos;: 46, &apos;r&apos;: 47, &apos;s&apos;: 48, &apos;t&apos;: 49, &apos;u&apos;: 50, &apos;v&apos;: 51, &apos;w&apos;: 52, &apos;x&apos;: 53, &apos;y&apos;: 54, &apos;z&apos;: 55, &apos;‘&apos;: 56, &apos;’&apos;: 57, &apos;“&apos;: 58, &apos;”&apos;: 59, &apos;\ufeff&apos;: 60&#125;</span><br></pre></td></tr></table></figure><h1 id="构造训练集"><a href="#构造训练集" class="headerlink" title="构造训练集"></a>构造训练集</h1><p>我们这里简单的文本预测就是，给了前置的字母以后，下一个字母是谁？<br>我们需要把我们的raw text变成可以用来训练的x,y:</p><p>x 是前置字母们 y 是后一个字母</p><p>In [71]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">seq_length = <span class="number">100</span> </span><br><span class="line"><span class="comment"># 输入的字符的长度，一个字符对应一个神经元，总共100个神经元</span></span><br><span class="line"><span class="comment"># 输入是100个字符，输出是预测的一个字符</span></span><br><span class="line"></span><br><span class="line">x = []</span><br><span class="line">y = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(raw_text) - seq_length): <span class="comment"># 每次循环都滑动一个字符距离</span></span><br><span class="line">    given = raw_text[i:i + seq_length] <span class="comment"># 从零先取前100个字符作为输入</span></span><br><span class="line">    predict = raw_text[i + seq_length] <span class="comment"># y是后一个字符</span></span><br><span class="line">    x.append([char_to_int[char] <span class="keyword">for</span> char <span class="keyword">in</span> given]) <span class="comment"># 把字符转化为向量</span></span><br><span class="line">    y.append(char_to_int[predict]) <span class="comment"># # 把字符转化为向量</span></span><br></pre></td></tr></table></figure><p>In [72]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(x[:<span class="number">3</span>]) </span><br><span class="line">print(y[:<span class="number">3</span>])</span><br><span class="line">print(set(y)) <span class="comment"># 这里注意下，'\ufeff': 60这个字符是文本的首字符，只有这么一个，y是取不到的，所以y值的可能只有60种</span></span><br><span class="line">print(len(set(y)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[60, 45, 47, 44, 39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 57, 48, 1, 47, 34, 30, 41, 1, 48, 44, 41, 33, 38, 34, 47, 48, 1, 44, 35, 1, 35, 44, 47, 49, 50, 43, 34, 9, 1, 31, 54, 1, 47, 38, 32, 37, 30, 47, 33, 1, 37, 30, 47, 33, 38, 43, 36, 1, 33, 30, 51, 38, 48, 0, 0, 49, 37, 38, 48, 1, 34, 31, 44, 44, 40, 1, 38, 48, 1, 35, 44, 47, 1, 49, 37, 34, 1, 50, 48, 34, 1, 44], [45, 47, 44, 39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 57, 48, 1, 47, 34, 30, 41, 1, 48, 44, 41, 33, 38, 34, 47, 48, 1, 44, 35, 1, 35, 44, 47, 49, 50, 43, 34, 9, 1, 31, 54, 1, 47, 38, 32, 37, 30, 47, 33, 1, 37, 30, 47, 33, 38, 43, 36, 1, 33, 30, 51, 38, 48, 0, 0, 49, 37, 38, 48, 1, 34, 31, 44, 44, 40, 1, 38, 48, 1, 35, 44, 47, 1, 49, 37, 34, 1, 50, 48, 34, 1, 44, 35], [47, 44, 39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 57, 48, 1, 47, 34, 30, 41, 1, 48, 44, 41, 33, 38, 34, 47, 48, 1, 44, 35, 1, 35, 44, 47, 49, 50, 43, 34, 9, 1, 31, 54, 1, 47, 38, 32, 37, 30, 47, 33, 1, 37, 30, 47, 33, 38, 43, 36, 1, 33, 30, 51, 38, 48, 0, 0, 49, 37, 38, 48, 1, 34, 31, 44, 44, 40, 1, 38, 48, 1, 35, 44, 47, 1, 49, 37, 34, 1, 50, 48, 34, 1, 44, 35, 1]]</span><br><span class="line">[35, 1, 30]</span><br><span class="line">&#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59&#125;</span><br><span class="line">60</span><br></pre></td></tr></table></figure><p>此刻，楼上这些表达方式，类似就是一个词袋，或者说 index。</p><p>接下来我们做两件事：</p><p>1.我们已经有了一个input的数字表达（index），我们要把它变成LSTM需要的数组格式： [样本数，时间步伐，特征]</p><p>2.对于output，我们在Word2Vec里学过，用one-hot做output的预测可以给我们更好的效果，相对于直接预测一个准确的y数值的话。</p><p>In [73]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">n_patterns = len(x) <span class="comment"># 样本数量</span></span><br><span class="line">n_vocab = len(chars) <span class="comment"># </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把x变成LSTM需要的样子</span></span><br><span class="line">x = numpy.reshape(x, (n_patterns, seq_length, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># n_patterns 样本的数量</span></span><br><span class="line"><span class="comment"># seq_length 每个样本每次训练100个字符</span></span><br><span class="line"><span class="comment"># 1 代表我们的一个字符的维度是1维，这里1维是特殊情况，如果我们的输入不是一个字符，</span></span><br><span class="line"><span class="comment">#是一个单词的话，单词是可以embedding成100维向量，那这里就是100.</span></span><br><span class="line"></span><br><span class="line">x = x / float(n_vocab) <span class="comment"># 简单normal到0-1之间，归一化，防止梯度爆炸</span></span><br><span class="line">y = np_utils.to_categorical(y) </span><br><span class="line"><span class="comment"># y值总共取值61种，直接做个onehot编码</span></span><br><span class="line"><span class="comment"># 对类别进行onehot编码一个很重要的原因在于计算loss时的问题。loss一般用距离来表示，</span></span><br><span class="line"><span class="comment"># 如果用1~5来表示，那么1和2的距离是1，而1和5的距离是4，但是按道理1和2、1和5的距离应该一样。</span></span><br><span class="line"><span class="comment"># 如果用one hot编码表示，那么1和2的距离跟1和5的距离时一样的。</span></span><br><span class="line"></span><br><span class="line">print(x[<span class="number">10</span>][:<span class="number">10</span>]) <span class="comment"># 第10个样本的前10个字符向量</span></span><br><span class="line">print(y[<span class="number">10</span>])</span><br><span class="line">print(y.shape) <span class="comment">#</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[[0.81967213]</span><br><span class="line"> [0.80327869]</span><br><span class="line"> [0.55737705]</span><br><span class="line"> [0.70491803]</span><br><span class="line"> [0.50819672]</span><br><span class="line"> [0.55737705]</span><br><span class="line"> [0.7704918 ]</span><br><span class="line"> [0.59016393]</span><br><span class="line"> [0.93442623]</span><br><span class="line"> [0.78688525]]</span><br><span class="line">[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.</span><br><span class="line"> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.</span><br><span class="line"> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</span><br><span class="line">(276730, 60)</span><br></pre></td></tr></table></figure><h1 id="构建和训练模型"><a href="#构建和训练模型" class="headerlink" title="构建和训练模型"></a>构建和训练模型</h1><p>In [74]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># Sequential()：序贯模型是多个网络层的线性堆叠，可以通过.add()一个一个添加层</span></span><br><span class="line">model.add(LSTM(<span class="number">256</span>, input_shape=(x.shape[<span class="number">1</span>], x.shape[<span class="number">2</span>])))</span><br><span class="line"><span class="comment"># 256是LSTM的隐藏层的维度。</span></span><br><span class="line"><span class="comment"># input_shape，不需要考虑样本数量，对模型本身结构没有意义</span></span><br><span class="line">model.add(Dropout(<span class="number">0.2</span>)) <span class="comment"># 去掉20%的神经元</span></span><br><span class="line">model.add(Dense(y.shape[<span class="number">1</span>], activation=<span class="string">'softmax'</span>)) </span><br><span class="line"><span class="comment"># y.shape[1] = 60，'softmax'转化为概率，概率和为1</span></span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>)</span><br><span class="line"><span class="comment"># 多分类交叉熵损失，adam优化器</span></span><br></pre></td></tr></table></figure><p>In [75]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.fit(x, y, nb_epoch=<span class="number">1</span>, batch_size=<span class="number">1024</span>)</span><br><span class="line"><span class="comment"># 这里本地跑的太慢了，nb_epoch设置为1了</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.</span><br><span class="line">  &quot;&quot;&quot;Entry point for launching an IPython kernel.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/1</span><br><span class="line">276730/276730 [==============================] - 1152s 4ms/step - loss: 3.0591</span><br></pre></td></tr></table></figure><p>Out[75]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;keras.callbacks.History at 0xb27cb07f0&gt;</span><br></pre></td></tr></table></figure><h2 id="预测模型"><a href="#预测模型" class="headerlink" title="预测模型"></a>预测模型</h2><p>In [95]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_next</span><span class="params">(input_array)</span>:</span></span><br><span class="line">    x = numpy.reshape(input_array, (<span class="number">1</span>, seq_length, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 同上，只有一个样本，reshape成lstm需要的维度。</span></span><br><span class="line">    x = x / float(n_vocab) <span class="comment"># 归一化</span></span><br><span class="line">    y = model.predict(x) <span class="comment"># y是60维的向量</span></span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">string_to_index</span><span class="params">(raw_input)</span>:</span></span><br><span class="line">    res = [] </span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> raw_input[(len(raw_input)-seq_length):]:</span><br><span class="line"><span class="comment"># 这步一个问题是len(raw_input)一定要大于seq_length：100,不然会报错</span></span><br><span class="line">        res.append(char_to_int[c]) <span class="comment"># 得到输入句子中后100个字符的向量表示</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">y_to_char</span><span class="params">(y)</span>:</span> </span><br><span class="line">    largest_index = y.argmax() <span class="comment"># 取出概率最大的值对应的索引</span></span><br><span class="line">    c = int_to_char[largest_index] <span class="comment"># 根据索引得到对应的字符</span></span><br><span class="line">    <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure><p>In [96]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_article</span><span class="params">(init, rounds=<span class="number">200</span>)</span>:</span> </span><br><span class="line">    <span class="comment"># rounds=200 代表预测生成200个新的字符</span></span><br><span class="line">    <span class="comment"># init是输入的字符串</span></span><br><span class="line">    in_string = init.lower() <span class="comment"># 跟上面一样的预处理步骤</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(rounds): <span class="comment"># 每次模型预测一个字符</span></span><br><span class="line">        n = y_to_char(predict_next(string_to_index(in_string)))</span><br><span class="line">        <span class="comment"># n是预测的新的字符</span></span><br><span class="line">        in_string += n <span class="comment"># 把新的字符拼接，重新作为新的输入</span></span><br><span class="line">    <span class="keyword">return</span> in_string</span><br></pre></td></tr></table></figure><p>In [97]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">init = <span class="string">'His object in coming to New York was to engage officers for that service. He came at an opportune moment'</span></span><br><span class="line">article = generate_article(init)</span><br><span class="line">print(article) <span class="comment"># 这里上面只是迭代了一次，导致后面所有的预测都是空格</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">his object in coming to new york was to engage officers for that service. he came at an opportune moment</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LSTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用朴素贝叶斯完成语种检测</title>
      <link href="/2019/11/08/%E7%94%A8%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%8C%E6%88%90%E8%AF%AD%E7%A7%8D%E6%A3%80%E6%B5%8B/"/>
      <url>/2019/11/08/%E7%94%A8%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%8C%E6%88%90%E8%AF%AD%E7%A7%8D%E6%A3%80%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="用朴素贝叶斯完成语种检测"><a href="#用朴素贝叶斯完成语种检测" class="headerlink" title="用朴素贝叶斯完成语种检测"></a>用朴素贝叶斯完成语种检测</h1><p>我们试试用朴素贝叶斯完成一个语种检测的分类器，说起来，用朴素贝叶斯完成这个任务，其实准确度还不错。</p><p><img src="data:image/png;base64,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" alt="image.png"></p><h2 id="查看数据，并划分训练集和验证集"><a href="#查看数据，并划分训练集和验证集" class="headerlink" title="查看数据，并划分训练集和验证集"></a>查看数据，并划分训练集和验证集</h2><p>In [87]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">in_f = open(<span class="string">'data.csv'</span>)</span><br><span class="line">lines = in_f.readlines() <span class="comment"># 一次读取整个文件，每行的内容放在一个字符串变量中作为列表的一个元素。</span></span><br><span class="line">in_f.close()</span><br><span class="line">dataset = [(line.strip()[:<span class="number">-3</span>], line.strip()[<span class="number">-2</span>:]) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"><span class="comment"># 把句子和语言分开，这里.strip()把\n去掉了。</span></span><br><span class="line">print(lines[<span class="number">0</span>]) <span class="comment"># 打印第一个句子</span></span><br><span class="line">print(dataset[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 december wereld aids dag voorlichting in zuidafrika over bieten taboes en optimisme,nl</span><br><span class="line"></span><br><span class="line">(&apos;1 december wereld aids dag voorlichting in zuidafrika over bieten taboes en optimisme&apos;, &apos;nl&apos;)</span><br></pre></td></tr></table></figure><p>In [88]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x, y = zip(*dataset) <span class="comment"># 解压缩</span></span><br><span class="line">x, y = list(x),list(y) </span><br><span class="line">print(x[<span class="number">0</span>],y[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">"-----"</span>*<span class="number">10</span>)</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 默认0.25的比例</span></span><br><span class="line"></span><br><span class="line">print(len(x_train),x_train[<span class="number">0</span>],y_train[<span class="number">0</span>])</span><br><span class="line">print(len(x_test),x_test[<span class="number">0</span>],y_test[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1 december wereld aids dag voorlichting in zuidafrika over bieten taboes en optimisme nl</span><br><span class="line">--------------------------------------------------</span><br><span class="line">6799 io non ho ipad ma mi sa che è fatta un po meglio sfrutta meglio la superficie it</span><br><span class="line">2267 ook jullie bedankt voor jullie steun nl</span><br></pre></td></tr></table></figure><h2 id="数据去燥，设置正则化规则"><a href="#数据去燥，设置正则化规则" class="headerlink" title="数据去燥，设置正则化规则"></a>数据去燥，设置正则化规则</h2><p>In [89]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="comment">#因为是twitter的数据，有时候会有一些@ # http等字符。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_noise</span><span class="params">(document)</span>:</span></span><br><span class="line">    noise_pattern = re.compile(<span class="string">"|"</span>.join([<span class="string">"http\S+"</span>, <span class="string">"\@\w+"</span>, <span class="string">"\#\w+"</span>]))</span><br><span class="line">    <span class="comment"># 关于正则表达式的使用方法，看第一节课</span></span><br><span class="line">    </span><br><span class="line">    clean_text = re.sub(noise_pattern, <span class="string">""</span>, document)</span><br><span class="line">    <span class="comment"># 使用空白""替换document中每一个匹配noise_pattern规则的子串。</span></span><br><span class="line">    <span class="keyword">return</span> clean_text.strip()</span><br><span class="line"></span><br><span class="line"><span class="comment">#举例</span></span><br><span class="line">remove_noise(<span class="string">"Trump images are now more popular than cat gifs. @trump a #trends http://www.trumptrends.html"</span>)</span><br><span class="line"><span class="comment"># http\S+ 匹配的是http://www.trumptrends.html</span></span><br><span class="line"><span class="comment"># \@\w+ 匹配的是@trump</span></span><br><span class="line"><span class="comment"># \#\w+ 匹配的是#trends</span></span><br></pre></td></tr></table></figure><p>Out[89]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;Trump images are now more popular than cat gifs.  a&apos;</span><br></pre></td></tr></table></figure><h2 id="统计词频，并选取1000个特征作为输入的特征维度"><a href="#统计词频，并选取1000个特征作为输入的特征维度" class="headerlink" title="统计词频，并选取1000个特征作为输入的特征维度"></a>统计词频，并选取1000个特征作为输入的特征维度</h2><p>In [94]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="comment"># 关于CountVectorizer参数可以看这个：https://blog.csdn.net/weixin_38278334/article/details/82320307</span></span><br><span class="line"></span><br><span class="line">vec = CountVectorizer(</span><br><span class="line">    lowercase=<span class="literal">True</span>,     <span class="comment"># 将所有字符变成小写</span></span><br><span class="line">    analyzer=<span class="string">'char_wb'</span>, <span class="comment"># 按ngram_range：1～2个字符划分特征</span></span><br><span class="line">    ngram_range=(<span class="number">1</span>,<span class="number">2</span>),  <span class="comment"># 考虑两个单词的的关联顺序</span></span><br><span class="line">    max_features=<span class="number">1000</span>,  <span class="comment"># 选取出现次数最多的1000个特征</span></span><br><span class="line">    preprocessor=remove_noise  <span class="comment"># 调用前面去燥的函数，这里是说计数前需要先做这一步</span></span><br><span class="line">)</span><br><span class="line">vec.fit(x_train)</span><br><span class="line">print(vec.transform(x_train).toarray()) <span class="comment"># 训练集</span></span><br><span class="line">print(vec.transform(x_train).toarray().shape) <span class="comment"># 训练集维度</span></span><br><span class="line">vec.get_feature_names()[:<span class="number">50</span>] <span class="comment"># 看下选取的1000个特征前100个长啥样，可以看到按1～2个字符不等划分特征。</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[[34  0  0 ...  0  0  0]</span><br><span class="line"> [22  0  0 ...  0  0  0]</span><br><span class="line"> [18  0  0 ...  0  0  0]</span><br><span class="line"> ...</span><br><span class="line"> [32  0  0 ...  0  0  0]</span><br><span class="line"> [18  0  0 ...  0  0  0]</span><br><span class="line"> [ 8  0  0 ...  0  0  0]]</span><br><span class="line">(6799, 1000)</span><br></pre></td></tr></table></figure><p>Out[94]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">[&apos; &apos;,</span><br><span class="line"> &apos; 0&apos;,</span><br><span class="line"> &apos; 1&apos;,</span><br><span class="line"> &apos; 2&apos;,</span><br><span class="line"> &apos; 3&apos;,</span><br><span class="line"> &apos; 4&apos;,</span><br><span class="line"> &apos; 5&apos;,</span><br><span class="line"> &apos; 6&apos;,</span><br><span class="line"> &apos; 7&apos;,</span><br><span class="line"> &apos; 8&apos;,</span><br><span class="line"> &apos; 9&apos;,</span><br><span class="line"> &apos; a&apos;,</span><br><span class="line"> &apos; b&apos;,</span><br><span class="line"> &apos; c&apos;,</span><br><span class="line"> &apos; d&apos;,</span><br><span class="line"> &apos; e&apos;,</span><br><span class="line"> &apos; f&apos;,</span><br><span class="line"> &apos; g&apos;,</span><br><span class="line"> &apos; h&apos;,</span><br><span class="line"> &apos; i&apos;,</span><br><span class="line"> &apos; j&apos;,</span><br><span class="line"> &apos; k&apos;,</span><br><span class="line"> &apos; l&apos;,</span><br><span class="line"> &apos; m&apos;,</span><br><span class="line"> &apos; n&apos;,</span><br><span class="line"> &apos; o&apos;,</span><br><span class="line"> &apos; p&apos;,</span><br><span class="line"> &apos; q&apos;,</span><br><span class="line"> &apos; r&apos;,</span><br><span class="line"> &apos; s&apos;,</span><br><span class="line"> &apos; t&apos;,</span><br><span class="line"> &apos; u&apos;,</span><br><span class="line"> &apos; v&apos;,</span><br><span class="line"> &apos; w&apos;,</span><br><span class="line"> &apos; x&apos;,</span><br><span class="line"> &apos; y&apos;,</span><br><span class="line"> &apos; z&apos;,</span><br><span class="line"> &apos; ä&apos;,</span><br><span class="line"> &apos; è&apos;,</span><br><span class="line"> &apos; é&apos;,</span><br><span class="line"> &apos; ê&apos;,</span><br><span class="line"> &apos; ú&apos;,</span><br><span class="line"> &apos; ü&apos;,</span><br><span class="line"> &apos;0&apos;,</span><br><span class="line"> &apos;0 &apos;,</span><br><span class="line"> &apos;00&apos;,</span><br><span class="line"> &apos;01&apos;,</span><br><span class="line"> &apos;02&apos;,</span><br><span class="line"> &apos;04&apos;,</span><br><span class="line"> &apos;05&apos;]</span><br></pre></td></tr></table></figure><h2 id="训练及预测"><a href="#训练及预测" class="headerlink" title="训练及预测"></a>训练及预测</h2><p>In [91]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line">classifier = MultinomialNB()</span><br><span class="line">classifier.fit(vec.transform(x_train), y_train)</span><br><span class="line"><span class="comment">#classifier.fit(vec.transform(x_train).toarray(), y_train) 也可以</span></span><br><span class="line">print(classifier.score(vec.transform(x_test), y_test))</span><br><span class="line"><span class="comment"># 打印准确率</span></span><br><span class="line">print(classifier.predict(vec.transform([<span class="string">'This is an English sentence'</span>])))</span><br><span class="line"><span class="comment"># 测试下</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0.9770621967357741</span><br><span class="line">[&apos;en&apos;]</span><br></pre></td></tr></table></figure><h2 id="重新封装下"><a href="#重新封装下" class="headerlink" title="重新封装下"></a>重新封装下</h2><p>In [92]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LanguageDetector</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, classifier=MultinomialNB<span class="params">()</span>)</span>:</span></span><br><span class="line">        self.classifier = classifier</span><br><span class="line">        self.vectorizer = CountVectorizer(ngram_range=(<span class="number">1</span>,<span class="number">2</span>), max_features=<span class="number">1000</span>, preprocessor=self._remove_noise)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_remove_noise</span><span class="params">(self, document)</span>:</span></span><br><span class="line">        noise_pattern = re.compile(<span class="string">"|"</span>.join([<span class="string">"http\S+"</span>, <span class="string">"\@\w+"</span>, <span class="string">"\#\w+"</span>]))</span><br><span class="line">        clean_text = re.sub(noise_pattern, <span class="string">""</span>, document)</span><br><span class="line">        <span class="keyword">return</span> clean_text</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">features</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.vectorizer.transform(X)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        self.vectorizer.fit(X)</span><br><span class="line">        self.classifier.fit(self.features(X), y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.classifier.predict(self.features([x]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.classifier.score(self.features(X), y)</span><br></pre></td></tr></table></figure><p>In [93]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">in_f = open(<span class="string">'data.csv'</span>)</span><br><span class="line">lines = in_f.readlines()</span><br><span class="line">in_f.close()</span><br><span class="line">dataset = [(line.strip()[:<span class="number">-3</span>], line.strip()[<span class="number">-2</span>:]) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">x, y = zip(*dataset)</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">language_detector = LanguageDetector()</span><br><span class="line">language_detector.fit(x_train, y_train)</span><br><span class="line">print(language_detector.predict(<span class="string">'This is an English sentence'</span>))</span><br><span class="line">print(language_detector.score(x_test, y_test))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&apos;en&apos;]</span><br><span class="line">0.9770621967357741</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 朴素贝叶斯 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习速查表</title>
      <link href="/2019/09/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%80%9F%E6%9F%A5%E8%A1%A8/"/>
      <url>/2019/09/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%80%9F%E6%9F%A5%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p>请参考</p><ul><li><a href="https://github.com/fengdu78/deeplearning_ai_books" target="_blank" rel="noopener">吴恩达老师的深度学习课程笔记及资源</a></li><li><a href="https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning" target="_blank" rel="noopener">CS229深度学习速查表</a></li><li><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks" target="_blank" rel="noopener">CS230深度学习速查表</a></li></ul><h2 id="深度学习各种模型与知识速查表"><a href="#深度学习各种模型与知识速查表" class="headerlink" title="深度学习各种模型与知识速查表"></a>深度学习各种模型与知识速查表</h2><p><img src="/blog_picture/dl-0001.jpg" alt="avatar"><br><img src="/blog_picture/dl-0002.jpg" alt="avatar"><br><img src="/blog_picture/dl-0003.jpg" alt="avatar"><br><img src="/blog_picture/dl-0004.jpg" alt="avatar"><br><img src="/blog_picture/dl-0005.jpg" alt="avatar"><br><img src="/blog_picture/dl-0006.jpg" alt="avatar"><br><img src="/blog_picture/dl-0007.jpg" alt="avatar"><br><img src="/blog_picture/dl-0008.jpg" alt="avatar"><br><img src="/blog_picture/dl-0009.jpg" alt="avatar"><br><img src="/blog_picture/dl-0010.jpg" alt="avatar"><br><img src="/blog_picture/dl-0011.jpg" alt="avatar"><br><img src="/blog_picture/dl-0012.jpg" alt="avatar"><br><img src="/blog_picture/dl-0013.jpg" alt="avatar"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习速查表 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模型调优</title>
      <link href="/2019/09/01/%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98/"/>
      <url>/2019/09/01/%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<p>参考阅读材料：</p><ul><li><a href="https://www.zhihu.com/question/34470160" target="_blank" rel="noopener">机器学习各种算法怎么调参?</a></li><li><a href="https://blog.csdn.net/mozhizun/article/details/60966354" target="_blank" rel="noopener">机器学习模型应用以及模型优化的一些思路</a></li><li><a href="https://blog.csdn.net/mozhizun/article/details/71438821" target="_blank" rel="noopener">机器学习模型优化中常见问题和解决思路</a></li><li><a href="https://blog.csdn.net/bitcarmanlee/article/details/71753056" target="_blank" rel="noopener">机器学习中模型优化不得不思考的几个问题</a></li><li><a href="https://www.jianshu.com/p/9fe84a7a5ba8" target="_blank" rel="noopener">机器学习中模型优化的两个问题</a></li></ul><h2 id="模型选择与调优"><a href="#模型选择与调优" class="headerlink" title="模型选择与调优"></a>模型选择与调优</h2><h3 id="不同模型的选择"><a href="#不同模型的选择" class="headerlink" title="不同模型的选择"></a>不同模型的选择</h3><p><img src="/blog_picture/model_tuning1.png" alt="avatar"></p><h3 id="模型超参数的选择"><a href="#模型超参数的选择" class="headerlink" title="模型超参数的选择"></a>模型超参数的选择</h3><p><img src="/blog_picture/model_tuning2.png" alt="avatar"></p><h3 id="评估方法-超参数产出方法"><a href="#评估方法-超参数产出方法" class="headerlink" title="评估方法+超参数产出方法"></a>评估方法+超参数产出方法</h3><p><img src="/blog_picture/model_tuning4.png" alt="avatar"><br><img src="/blog_picture/model_tuning5.png" alt="avatar"><br><img src="/blog_picture/model_tuning6.png" alt="avatar"></p><h3 id="模型状态"><a href="#模型状态" class="headerlink" title="模型状态"></a>模型状态</h3><p><img src="/blog_picture/model_tuning3.png" alt="avatar"><br><img src="/blog_picture/model_tuning7.png" alt="avatar"><br><img src="/blog_picture/model_tuning8.png" alt="avatar"><br><img src="/blog_picture/model_tuning9.png" alt="avatar"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模型调优 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>集成学习与boosting模型</title>
      <link href="/2019/09/01/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B8%8Eboosting%E6%A8%A1%E5%9E%8B/"/>
      <url>/2019/09/01/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B8%8Eboosting%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="机器学习中的集成学习"><a href="#机器学习中的集成学习" class="headerlink" title="机器学习中的集成学习"></a>机器学习中的集成学习</h2><p>顾名思义，集成学习（ensemble learning）指的是将多个学习器进行有效地结合，组建一个“学习器委员会”，其中每个学习器担任委员会成员并行使投票表决权，使得委员会最后的决定更能够四方造福普度众生<del>…</del>，即其泛化性能要能优于其中任何一个学习器。</p><h3 id="个体与集成"><a href="#个体与集成" class="headerlink" title="个体与集成"></a>个体与集成</h3><p>集成学习的基本结构为：先产生一组个体学习器，再使用某种策略将它们结合在一起。集成模型如下图所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0c15683.png" alt="1.png"></p><p>在上图的集成模型中，若个体学习器都属于同一类别，例如都是决策树或都是神经网络，则称该集成为同质的（homogeneous）;若个体学习器包含多种类型的学习算法，例如既有决策树又有神经网络，则称该集成为异质的（heterogenous）。</p><blockquote><p><strong>同质集成</strong>：个体学习器称为“基学习器”（base learner），对应的学习算法为“基学习算法”（base learning algorithm）。</p></blockquote><blockquote><p><strong>异质集成</strong>：个体学习器称为“组件学习器”（component learner）或直称为“个体学习器”。</p></blockquote><p>上面我们已经提到要让集成起来的泛化性能比单个学习器都要好，虽说团结力量大但也有木桶短板理论调皮捣蛋，那如何做到呢？这就引出了集成学习的两个重要概念：<strong>准确性</strong>和<strong>多样性</strong>（diversity）。准确性指的是个体学习器不能太差，要有一定的准确度；多样性则是个体学习器之间的输出要具有差异性。通过下面的这三个例子可以很容易看出这一点，准确度较高，差异度也较高，可以较好地提升集成性能。</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0d23e13.png" alt="2.png"></p><p>现在考虑二分类的简单情形，假设基分类器之间相互独立（能提供较高的差异度），且错误率相等为 ε，则可以将集成器的预测看做一个伯努利实验，易知当所有基分类器中不足一半预测正确的情况下，集成器预测错误，所以集成器的错误率可以计算为：</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0cce0bb.png" alt="3.png"></p><p>此时，集成器错误率随着基分类器的个数的增加呈指数下降，但前提是基分类器之间相互独立，在实际情形中显然是不可能的，假设训练有A和B两个分类器，对于某个测试样本，显然满足：P（A=1 | B=1）&gt; P（A=1），因为A和B为了解决相同的问题而训练，因此在预测新样本时存在着很大的联系。因此，<strong>个体学习器的“准确性”和“差异性”本身就是一对矛盾的变量</strong>，准确性高意味着牺牲多样性，所以产生“<strong>好而不同</strong>”的个体学习器正是集成学习研究的核心。现阶段有三种主流的集成学习方法：Boosting、Bagging以及随机森林（Random Forest），接下来将进行逐一介绍。</p><h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>Boosting是一种串行的工作机制，即个体学习器的训练存在依赖关系，必须一步一步序列化进行。其基本思想是：增加前一个基学习器在训练训练过程中预测错误样本的权重，使得后续基学习器更加关注这些打标错误的训练样本，尽可能纠正这些错误，一直向下串行直至产生需要的T个基学习器，Boosting最终对这T个学习器进行加权结合，产生学习器委员会。</p><p>Boosting族算法最著名、使用最为广泛的就是AdaBoost，因此下面主要是对AdaBoost算法进行介绍。AdaBoost使用的是<strong>指数损失函数</strong>，因此AdaBoost的权值与样本分布的更新都是围绕着最小化指数损失函数进行的。看到这里回想一下之前的机器学习算法，<strong>不难发现机器学习的大部分带参模型只是改变了最优化目标中的损失函数</strong>：如果是Square loss，那就是最小二乘了；如果是Hinge Loss，那就是著名的SVM了；如果是log-Loss，那就是Logistic Regression了。</p><p>定义基学习器的集成为加权结合，则有：</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0ca2ca5.png" alt="4.png"></p><p>AdaBoost算法的指数损失函数定义为：</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0d10461.png" alt="5.png"></p><p>具体说来，整个Adaboost 迭代算法分为3步：</p><ul><li>初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。</li><li>训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。</li><li>将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。</li></ul><p>整个AdaBoost的算法流程如下所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0d7c057.png" alt="6.png"></p><p>可以看出：<strong>AdaBoost的核心步骤就是计算基学习器权重和样本权重分布</strong>，那为何是上述的计算公式呢？这就涉及到了我们之前为什么说大部分带参机器学习算法只是改变了损失函数，就是因为<strong>大部分模型的参数都是通过最优化损失函数（可能还加个规则项）而计算（梯度下降，坐标下降等）得到</strong>，这里正是通过最优化指数损失函数从而得到这两个参数的计算公式，具体的推导过程此处不进行展开。</p><p>Boosting算法要求基学习器能对特定分布的数据进行学习，即每次都更新样本分布权重，这里书上提到了两种方法：“重赋权法”（re-weighting）和“重采样法”（re-sampling），书上的解释有些晦涩，这里进行展开一下：</p><blockquote><p><strong>重赋权法</strong> : 对每个样本附加一个权重，这时涉及到样本属性与标签的计算，都需要乘上一个权值。<br><strong>重采样法</strong> : 对于一些无法接受带权样本的及学习算法，适合用“重采样法”进行处理。方法大致过程是，根据各个样本的权重，对训练数据进行重采样，初始时样本权重一样，每个样本被采样到的概率一致，每次从N个原始的训练样本中按照权重有放回采样N个样本作为训练集，然后计算训练集错误率，然后调整权重，重复采样，集成多个基学习器。</p></blockquote><p>从偏差-方差分解来看：Boosting算法主要关注于降低偏差，每轮的迭代都关注于训练过程中预测错误的样本，将弱学习提升为强学习器。从AdaBoost的算法流程来看，标准的AdaBoost只适用于二分类问题。在此，当选为数据挖掘十大算法之一的AdaBoost介绍到这里，能够当选正是说明这个算法十分婀娜多姿，背后的数学证明和推导充分证明了这一点，限于篇幅不再继续展开。</p><h3 id="Bagging与Random-Forest"><a href="#Bagging与Random-Forest" class="headerlink" title="Bagging与Random Forest"></a>Bagging与Random Forest</h3><p>相比之下，Bagging与随机森林算法就简洁了许多，上面已经提到产生“好而不同”的个体学习器是集成学习研究的核心，即在保证基学习器准确性的同时增加基学习器之间的多样性。而这两种算法的基本思（tao）想（lu）都是通过“自助采样”的方法来增加多样性。</p><h4 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h4><p>Bagging是一种并行式的集成学习方法，即基学习器的训练之间没有前后顺序可以同时进行，Bagging使用“有放回”采样的方式选取训练集，对于包含m个样本的训练集，进行m次有放回的随机采样操作，从而得到m个样本的采样集，这样训练集中有接近36.8%的样本没有被采到。按照相同的方式重复进行，我们就可以采集到T个包含m个样本的数据集，从而训练出T个基学习器，最终对这T个基学习器的输出进行结合。</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0ce62fc.png" alt="7.png"></p><p>Bagging算法的流程如下所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc84d0d0e761.png" alt="8.png"></p><p>可以看出Bagging主要通过<strong>样本的扰动</strong>来增加基学习器之间的多样性，因此Bagging的基学习器应为那些对训练集十分敏感的不稳定学习算法，例如：神经网络与决策树等。从偏差-方差分解来看，Bagging算法主要关注于降低方差，即通过多次重复训练提高稳定性。不同于AdaBoost的是，Bagging可以十分简单地移植到多分类、回归等问题。总的说起来则是：<strong>AdaBoost关注于降低偏差，而Bagging关注于降低方差。</strong></p><h4 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h4><p>随机森林（Random Forest）是Bagging的一个拓展体，它的基学习器固定为决策树，多棵树也就组成了森林，而“随机”则在于选择划分属性的随机，随机森林在训练基学习器时，也采用有放回采样的方式添加样本扰动，同时它还引入了一种<strong>属性扰动</strong>，即在基决策树的训练过程中，在选择划分属性时，RF先从候选属性集中随机挑选出一个包含K个属性的子集，再从这个子集中选择最优划分属性，一般推荐K=log2（d）。</p><p>这样随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，从而进一步提升了基学习器之间的差异度。相比决策树的Bagging集成，随机森林的起始性能较差（由于属性扰动，基决策树的准确度有所下降），但随着基学习器数目的增多，随机森林往往会收敛到更低的泛化误差。同时不同于Bagging中决策树从所有属性集中选择最优划分属性，随机森林只在属性集的一个子集中选择划分属性，因此训练效率更高。<br><img src="../img/random-forest.png" alt><br><img src="https://i.loli.net/2018/10/18/5bc84d0d7a4fd.png" alt="9.png"></p><h3 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h3><p>结合策略指的是在训练好基学习器后，如何将这些基学习器的输出结合起来产生集成模型的最终输出，下面将介绍一些常用的结合策略：</p><h4 id="平均法（回归问题）"><a href="#平均法（回归问题）" class="headerlink" title="平均法（回归问题）"></a>平均法（回归问题）</h4><p><img src="https://i.loli.net/2018/10/18/5bc84d0d07983.png" alt="10.png"></p><p><img src="https://i.loli.net/2018/10/18/5bc84de1b74ff.png" alt="11.png"></p><p>易知简单平均法是加权平均法的一种特例，加权平均法可以认为是集成学习研究的基本出发点。由于各个基学习器的权值在训练中得出，<strong>一般而言，在个体学习器性能相差较大时宜使用加权平均法，在个体学习器性能相差较小时宜使用简单平均法</strong>。</p><h4 id="投票法（分类问题）"><a href="#投票法（分类问题）" class="headerlink" title="投票法（分类问题）"></a>投票法（分类问题）</h4><p><img src="https://i.loli.net/2018/10/18/5bc84de2629c4.png" alt="12.png"></p><p><img src="https://i.loli.net/2018/10/18/5bc84de25a74b.png" alt="13.png"></p><p><img src="https://i.loli.net/2018/10/18/5bc84de1bacc4.png" alt="14.png"></p><p>绝对多数投票法（majority voting）提供了拒绝选项，这在可靠性要求很高的学习任务中是一个很好的机制。同时，对于分类任务，各个基学习器的输出值有两种类型，分别为类标记和类概率。</p><p><img src="https://i.loli.net/2018/10/18/5bc84de2768c1.png" alt="15.png"></p><p>一些在产生类别标记的同时也生成置信度的学习器，置信度可转化为类概率使用，<strong>一般基于类概率进行结合往往比基于类标记进行结合的效果更好</strong>，需要注意的是对于异质集成，其类概率不能直接进行比较，此时需要将类概率转化为类标记输出，然后再投票。</p><h4 id="学习法"><a href="#学习法" class="headerlink" title="学习法"></a>学习法</h4><p>学习法是一种更高级的结合策略，即学习出一种“投票”的学习器，Stacking是学习法的典型代表。Stacking的基本思想是：首先训练出T个基学习器，对于一个样本它们会产生T个输出，将这T个基学习器的输出与该样本的真实标记作为新的样本，m个样本就会产生一个m<em>T的样本集，来训练一个新的“投票”学习器。投票学习器的输入属性与学习算法对Stacking集成的泛化性能有很大的影响，书中已经提到：*</em>投票学习器采用类概率作为输入属性，选用多响应线性回归（MLR）一般会产生较好的效果**。</p><p><img src="https://i.loli.net/2018/10/18/5bc84de25cbaf.png" alt="16.png"></p><h4 id="多样性（diversity）"><a href="#多样性（diversity）" class="headerlink" title="多样性（diversity）"></a>多样性（diversity）</h4><p>在集成学习中，基学习器之间的多样性是影响集成器泛化性能的重要因素。因此增加多样性对于集成学习研究十分重要，一般的思路是在学习过程中引入随机性，常见的做法主要是对数据样本、输入属性、输出表示、算法参数进行扰动。</p><blockquote><p><strong>数据样本扰动</strong>，即利用具有差异的数据集来训练不同的基学习器。例如：有放回自助采样法，但此类做法只对那些不稳定学习算法十分有效，例如：决策树和神经网络等，训练集的稍微改变能导致学习器的显著变动。<br><strong>输入属性扰动</strong>，即随机选取原空间的一个子空间来训练基学习器。例如：随机森林，从初始属性集中抽取子集，再基于每个子集来训练基学习器。但若训练集只包含少量属性，则不宜使用属性扰动。<br><strong>输出表示扰动</strong>，此类做法可对训练样本的类标稍作变动，或对基学习器的输出进行转化。<br><strong>算法参数扰动</strong>，通过随机设置不同的参数，例如：神经网络中，随机初始化权重与随机设置隐含层节点数。</p></blockquote><h2 id="机器学习中的Boosting模型：GBDT-vs-Xgboost-vs-LightGBM"><a href="#机器学习中的Boosting模型：GBDT-vs-Xgboost-vs-LightGBM" class="headerlink" title="机器学习中的Boosting模型：GBDT vs Xgboost vs LightGBM"></a>机器学习中的Boosting模型：GBDT vs Xgboost vs LightGBM</h2><p>见资料<strong>GBDT_wepon.pdf</strong></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 集成学习 </tag>
            
            <tag> boosting </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>聚类与降维</title>
      <link href="/2019/09/01/%E8%81%9A%E7%B1%BB%E4%B8%8E%E9%99%8D%E7%BB%B4/"/>
      <url>/2019/09/01/%E8%81%9A%E7%B1%BB%E4%B8%8E%E9%99%8D%E7%BB%B4/</url>
      
        <content type="html"><![CDATA[<h2 id="机器学习中的聚类算法"><a href="#机器学习中的聚类算法" class="headerlink" title="机器学习中的聚类算法"></a>机器学习中的聚类算法</h2><p>聚类是一种经典的<strong>无监督学习</strong>方法，<strong>无监督学习的目标是通过对无标记训练样本的学习，发掘和揭示数据集本身潜在的结构与规律</strong>，即不依赖于训练数据集的类标记信息。聚类则是试图将数据集的样本划分为若干个互不相交的类簇，从而每个簇对应一个潜在的类别。</p><p>聚类直观上来说是将相似的样本聚在一起，从而形成一个<strong>类簇（cluster）</strong>。那首先的问题是如何来<strong>度量相似性</strong>（similarity measure）呢？这便是<strong>距离度量</strong>，在生活中我们说差别小则相似，对应到多维样本，每个样本可以对应于高维空间中的一个数据点，若它们的距离相近，我们便可以称它们相似。那接着如何来评价聚类结果的好坏呢？这便是<strong>性能度量</strong>，性能度量为评价聚类结果的好坏提供了一系列有效性指标。</p><h3 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h3><p>谈及距离度量，最熟悉的莫过于欧式距离了，从年头一直用到年尾的距离计算公式：即对应属性之间相减的平方和再开根号。度量距离还有其它的很多经典方法，通常它们需要满足一些基本性质：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed4c0390.png" alt="1.png"></p><p>最常用的距离度量方法是<strong>“闵可夫斯基距离”（Minkowski distance)</strong>：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed49e31f.png" alt="2.png"></p><p>当p=1时，闵可夫斯基距离即<strong>曼哈顿距离（Manhattan distance）</strong>：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed49c31f.png" alt="3.png"></p><p>当p=2时，闵可夫斯基距离即<strong>欧氏距离（Euclidean distance）</strong>：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed497613.png" alt="4.png"></p><p>我们知道属性分为两种：<strong>连续属性</strong>和<strong>离散属性</strong>（有限个取值）。对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；而对于离散值的属性，需要作下面进一步的处理：</p><blockquote><p>若属性值之间<strong>存在序关系</strong>，则可以将其转化为连续值，例如：身高属性“高”“中等”“矮”，可转化为{1, 0.5, 0}。<br>若属性值之间<strong>不存在序关系</strong>，则通常将其转化为向量的形式，例如：性别属性“男”“女”，可转化为{（1,0），（0,1）}。</p></blockquote><p>在进行距离度量时，易知<strong>连续属性和存在序关系的离散属性都可以直接参与计算</strong>，因为它们都可以反映一种程度，我们称其为“<strong>有序属性</strong>”；而对于不存在序关系的离散属性，我们称其为：“<strong>无序属性</strong>”，显然无序属性再使用闵可夫斯基距离就行不通了。</p><p><strong>对于无序属性，我们一般采用VDM进行距离的计算</strong>，例如：对于离散属性的两个取值a和b，定义：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed4e9560.png" alt="5.png"></p><p>于是，在计算两个样本之间的距离时，我们可以将闵可夫斯基距离和VDM混合在一起进行计算：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed507bc7.png" alt="6.png"></p><p>若我们定义的距离计算方法是用来度量相似性，例如下面将要讨论的聚类问题，即距离越小，相似性越大，反之距离越大，相似性越小。这时距离的度量方法并不一定需要满足前面所说的四个基本性质，这样的方法称为：<strong>非度量距离（non-metric distance）</strong>。</p><h3 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h3><p>由于聚类算法不依赖于样本的真实类标，就不能像监督学习的分类那般，通过计算分对分错（即精确度或错误率）来评价学习器的好坏或作为学习过程中的优化目标。一般聚类有两类性能度量指标：<strong>外部指标</strong>和<strong>内部指标</strong>。</p><h4 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h4><p>即将聚类结果与某个参考模型的结果进行比较，<strong>以参考模型的输出作为标准，来评价聚类好坏</strong>。假设聚类给出的结果为λ，参考模型给出的结果是λ*，则我们将样本进行两两配对，定义：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed59160e.png" alt="7.png"></p><p>显然a和b代表着聚类结果好坏的正能量，b和c则表示参考结果和聚类结果相矛盾，基于这四个值可以导出以下常用的外部评价指标：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed587438.png" alt="8.png"></p><h4 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h4><p>内部指标即不依赖任何外部模型，直接对聚类的结果进行评估，聚类的目的是想将那些相似的样本尽可能聚在一起，不相似的样本尽可能分开，直观来说：<strong>簇内高内聚紧紧抱团，簇间低耦合老死不相往来</strong>。定义：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed581852.png" alt="9.png"></p><p>基于上面的四个距离，可以导出下面这些常用的内部评价指标：</p><p><img src="https://i.loli.net/2018/10/18/5bc84ed582854.png" alt="10.png"></p><h3 id="原型聚类"><a href="#原型聚类" class="headerlink" title="原型聚类"></a>原型聚类</h3><p>原型聚类即“<strong>基于原型的聚类</strong>”（prototype-based clustering），原型表示模板的意思，就是通过参考一个模板向量或模板分布的方式来完成聚类的过程，常见的K-Means便是基于簇中心来实现聚类，混合高斯聚类则是基于簇分布来实现聚类。</p><h4 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h4><p>K-Means的思想十分简单，<strong>首先随机指定类中心，根据样本与类中心的远近划分类簇，接着重新计算类中心，迭代直至收敛</strong>。但是其中迭代的过程并不是主观地想象得出，事实上，若将样本的类别看做为“隐变量”（latent variable），类中心看作样本的分布参数，这一过程正是通过<strong>EM算法</strong>的两步走策略而计算出，其根本的目的是为了最小化平方误差函数E：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb82b5d3.png" alt="11.png"></p><p>K-Means的算法流程如下所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb9c0817.png" alt="12.png"></p><p><img src="../img/K-Means.png" alt></p><h4 id="高斯混合聚类"><a href="#高斯混合聚类" class="headerlink" title="高斯混合聚类"></a>高斯混合聚类</h4><p>现在可以看出K-Means与LVQ都试图以类中心作为原型指导聚类，高斯混合聚类则采用高斯分布来描述原型。现假设<strong>每个类簇中的样本都服从一个多维高斯分布，那么空间中的样本可以看作由k个多维高斯分布混合而成</strong>。</p><p>对于多维高斯分布，其概率密度函数如下所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb870d98.png" alt="14.png"></p><p>其中u表示均值向量，∑表示协方差矩阵，可以看出一个多维高斯分布完全由这两个参数所确定。接着定义高斯混合分布为：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb876794.png" alt="15.png"></p><p>α称为混合系数，这样空间中样本的采集过程则可以抽象为：<strong>（1）先选择一个类簇（高斯分布），（2）再根据对应高斯分布的密度函数进行采样</strong>，这时候贝叶斯公式又能大展身手了：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb9191d9.png" alt="16.png"></p><p>此时只需要选择PM最大时的类簇并将该样本划分到其中，看到这里很容易发现：这和那个传说中的贝叶斯分类不是神似吗，都是通过贝叶斯公式展开，然后计算类先验概率和类条件概率。但遗憾的是：<strong>这里没有真实类标信息，对于类条件概率，并不能像贝叶斯分类那样通过最大似然法美好地计算出来</strong>，因为这里的样本可能属于所有的类簇，这里的似然函数变为：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb871d4a.png" alt="17.png"></p><p>可以看出：简单的最大似然法根本无法求出所有的参数，这样PM也就没法计算。<strong>这里就要召唤出之前的EM大法，首先对高斯分布的参数及混合系数进行随机初始化，计算出各个PM（即γji，第i个样本属于j类），再最大化似然函数（即LL（D）分别对α、u和∑求偏导 ），对参数进行迭代更新</strong>。</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb8a6f32.png" alt="18.png"></p><p>高斯混合聚类的算法流程如下图所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb9c4fa4.png" alt="19.png"></p><h4 id="密度聚类"><a href="#密度聚类" class="headerlink" title="密度聚类"></a>密度聚类</h4><p>密度聚类则是基于密度的聚类，它从样本分布的角度来考察样本之间的可连接性，并基于可连接性（密度可达）不断拓展疆域（类簇）。其中最著名的便是<strong>DBSCAN</strong>算法，首先定义以下概念：</p><p><img src="https://i.loli.net/2018/10/18/5bc84fb9bd69c.png" alt="20.png"></p><p><img src="https://i.loli.net/2018/10/18/5bc8509f8d619.png" alt="21.png"></p><p>简单来理解DBSCAN便是：<strong>找出一个核心对象所有密度可达的样本集合形成簇</strong>。首先从数据集中任选一个核心对象A，找出所有A密度可达的样本集合，将这些样本形成一个密度相连的类簇，直到所有的核心对象都遍历完。DBSCAN算法的流程如下图所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc8509feb587.png" alt="22.png"></p><h4 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h4><p>层次聚类是一种基于树形结构的聚类方法，常用的是<strong>自底向上</strong>的结合策略（<strong>AGNES算法</strong>）。假设有N个待聚类的样本，其基本步骤是：</p><ul><li>1.初始化–&gt;把每个样本归为一类，计算每两个类之间的距离，也就是样本与样本之间的相似度；</li><li>2.寻找各个类之间最近的两个类，把他们归为一类（这样类的总数就少了一个）；</li><li>3.重新计算新生成的这个<strong>类与各个旧类之间的相似度</strong>；</li><li>4.重复2和3直到所有样本点都归为一类，结束。</li></ul><p>可以看出其中最关键的一步就是<strong>计算两个类簇的相似度</strong>，这里有多种度量方法：</p><pre><code>* 单链接（single-linkage）:取类间最小距离。</code></pre><p><img src="https://i.loli.net/2018/10/18/5bc8509ebb022.png" alt="23.png"></p><pre><code>* 全链接（complete-linkage）:取类间最大距离</code></pre><p><img src="https://i.loli.net/2018/10/18/5bc8509eb2b30.png" alt="24.png"></p><pre><code>* 均链接（average-linkage）:取类间两两的平均距离</code></pre><p><img src="https://i.loli.net/2018/10/18/5bc8509f089a7.png" alt="25.png"></p><p>很容易看出：<strong>单链接的包容性极强，稍微有点暧昧就当做是自己人了，全链接则是坚持到底，只要存在缺点就坚决不合并，均连接则是从全局出发顾全大局</strong>。层次聚类法的算法流程如下所示：</p><p><img src="https://i.loli.net/2018/10/18/5bc8509f9d4a0.png" alt="26.png"><br><img src="/blog_picture/clustering.png" alt="avatar"></p><h2 id="机器学习中的PCA降维"><a href="#机器学习中的PCA降维" class="headerlink" title="机器学习中的PCA降维"></a>机器学习中的PCA降维</h2><p>资料from<a href="http://blog.codinglabs.org/articles/pca-tutorial.html" target="_blank" rel="noopener">PCA的数学原理</a></p><p><img src="/blog_picture/PCA_.jpg" alt="avatar"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 聚类 </tag>
            
            <tag> 降维 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>贝叶斯分类器</title>
      <link href="/2019/09/01/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/"/>
      <url>/2019/09/01/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<p>参考阅读材料：</p><ul><li><a href="https://blog.csdn.net/han_xiaoyang/article/details/50616559" target="_blank" rel="noopener">NLP系列(2)_用朴素贝叶斯进行文本分类(上)</a></li><li><a href="https://blog.csdn.net/han_xiaoyang/article/details/50629587" target="_blank" rel="noopener">NLP系列(3)_用朴素贝叶斯进行文本分类(下)</a></li></ul><p>朴素贝叶斯</p><ul><li>贝叶斯公式 + 条件独立假设</li><li>平滑算法</li></ul><h2 id="机器学习中的贝叶斯分类器"><a href="#机器学习中的贝叶斯分类器" class="headerlink" title="机器学习中的贝叶斯分类器"></a>机器学习中的贝叶斯分类器</h2><p>贝叶斯分类器是一种概率框架下的统计学习分类器，对分类任务而言，假设在相关概率都已知的情况下，贝叶斯分类器考虑如何基于这些概率为样本判定最优的类标。在开始介绍贝叶斯决策论之前，我们首先来回顾下概率论委员会常委–贝叶斯公式。</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd7a2575.png" alt="1.png"></p><h3 id="贝叶斯决策论"><a href="#贝叶斯决策论" class="headerlink" title="贝叶斯决策论"></a>贝叶斯决策论</h3><p>若将上述定义中样本空间的划分Bi看做为类标，A看做为一个新的样本，则很容易将条件概率理解为样本A是类别Bi的概率。在机器学习训练模型的过程中，往往我们都试图去优化一个风险函数，因此在概率框架下我们也可以为贝叶斯定义“<strong>条件风险</strong>”（conditional risk）。</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd15db94.png" alt="2.png"></p><p>我们的任务就是寻找一个判定准则最小化所有样本的条件风险总和，因此就有了<strong>贝叶斯判定准则</strong>（Bayes decision rule）:为最小化总体风险，只需在每个样本上选择那个使得条件风险最小的类标。</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd308600.png" alt="3.png"></p><p>若损失函数λ取0-1损失，则有：</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd37c502.png" alt="4.png"></p><p>即对于每个样本x，选择其后验概率P（c | x）最大所对应的类标，能使得总体风险函数最小，从而将原问题转化为估计后验概率P（c | x）。一般这里有两种策略来对后验概率进行估计：</p><pre><code>* 判别式模型：直接对 P（c | x）进行建模求解。例我们前面所介绍的决策树、神经网络、SVM都是属于判别式模型。* 生成式模型：通过先对联合分布P（x,c）建模，从而进一步求解 P（c | x）。</code></pre><p>贝叶斯分类器就属于生成式模型，基于贝叶斯公式对后验概率P（c | x） 进行一项神奇的变换，巴拉拉能量…. P（c | x）变身：</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd501ad3.png" alt="5.png"></p><p>对于给定的样本x，P（x）与类标无关，P（c）称为类先验概率，p（x | c ）称为类条件概率。这时估计后验概率P（c | x）就变成为估计类先验概率和类条件概率的问题。对于先验概率和后验概率，在看这章之前也是模糊了我好久，这里普及一下它们的基本概念。</p><pre><code>* 先验概率： 根据以往经验和分析得到的概率。* 后验概率：后验概率是基于新的信息，修正原来的先验概率后所获得的更接近实际情况的概率估计。</code></pre><p>实际上先验概率就是在没有任何结果出来的情况下估计的概率，而后验概率则是在有一定依据后的重新估计，直观意义上后验概率就是条件概率。下面直接上Wiki上的一个例子，简单粗暴快速完事…</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd799610.png" alt="6.png"></p><p>回归正题，对于类先验概率P（c），p（c）就是样本空间中各类样本所占的比例，根据大数定理（当样本足够多时，频率趋于稳定等于其概率），这样当训练样本充足时，p(c)可以使用各类出现的频率来代替。因此只剩下类条件概率p（x | c ），它表达的意思是在类别c中出现x的概率，它涉及到属性的联合概率问题，若只有一个离散属性还好，当属性多时采用频率估计起来就十分困难，因此这里一般采用极大似然法进行估计。</p><h3 id="极大似然法"><a href="#极大似然法" class="headerlink" title="极大似然法"></a>极大似然法</h3><p>极大似然估计（Maximum Likelihood Estimation，简称MLE），是一种根据数据采样来估计概率分布的经典方法。常用的策略是先假定总体具有某种确定的概率分布，再基于训练样本对概率分布的参数进行估计。运用到类条件概率p（x | c ）中，假设p（x | c ）服从一个参数为θ的分布，问题就变为根据已知的训练样本来估计θ。极大似然法的核心思想就是：估计出的参数使得已知样本出现的概率最大，即使得训练数据的似然最大。</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd70fb73.png" alt="7.png"></p><p>所以，贝叶斯分类器的训练过程就是参数估计。总结最大似然法估计参数的过程，一般分为以下四个步骤：</p><pre><code>* 1.写出似然函数；* 2.对似然函数取对数，并整理；* 3.求导数，令偏导数为0，得到似然方程组；* 4.解似然方程组，得到所有参数即为所求。</code></pre><p>例如：假设样本属性都是连续值，p（x | c ）服从一个多维高斯分布，则通过MLE计算出的参数刚好分别为：</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd705729.png" alt="8.png"></p><p>上述结果看起来十分合乎实际，但是采用最大似然法估计参数的效果很大程度上依赖于作出的假设是否合理，是否符合潜在的真实数据分布。这就需要大量的经验知识，搞统计越来越值钱也是这个道理，大牛们掐指一算比我们搬砖几天更有效果。</p><h3 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h3><p>不难看出：原始的贝叶斯分类器最大的问题在于联合概率密度函数的估计，首先需要根据经验来假设联合概率分布，其次当属性很多时，训练样本往往覆盖不够，参数的估计会出现很大的偏差。为了避免这个问题，朴素贝叶斯分类器（naive Bayes classifier）采用了“属性条件独立性假设”，即样本数据的所有属性之间相互独立。这样类条件概率p（x | c ）可以改写为：</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd55e102.png" alt="9.png"></p><p>这样，为每个样本估计类条件概率变成为每个样本的每个属性估计类条件概率。</p><p><img src="https://i.loli.net/2018/10/18/5bc83fd6678cd.png" alt="10.png"></p><p>相比原始贝叶斯分类器，朴素贝叶斯分类器基于单个的属性计算类条件概率更加容易操作，需要注意的是：若某个属性值在训练集中和某个类别没有一起出现过，这样会抹掉其它的属性信息，因为该样本的类条件概率被计算为0。因此在估计概率值时，常常用进行平滑（smoothing）处理，拉普拉斯修正（Laplacian correction）就是其中的一种经典方法，具体计算方法如下：</p><p><img src="https://i.loli.net/2018/10/18/5bc83fe54aaed.png" alt="11.png"></p><p>当训练集越大时，拉普拉斯修正引入的影响越来越小。对于贝叶斯分类器，模型的训练就是参数估计，因此可以事先将所有的概率储存好，当有新样本需要判定时，直接查表计算即可。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 贝叶斯分类器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>决策树与随机森林</title>
      <link href="/2019/09/01/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
      <url>/2019/09/01/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/</url>
      
        <content type="html"><![CDATA[<h2 id="机器学习中的决策树模型"><a href="#机器学习中的决策树模型" class="headerlink" title="机器学习中的决策树模型"></a>机器学习中的决策树模型</h2><ul><li>① 树模型不用做scaling</li><li>② 树模型不太需要做离散化</li><li>③ 用Xgboost等工具库，是不需要做缺失值填充</li><li>④ 树模型是非线性模型，有非线性的表达能力</li></ul><h3 id="决策树基本概念"><a href="#决策树基本概念" class="headerlink" title="决策树基本概念"></a>决策树基本概念</h3><ul><li>决策时是一种树形结构，其中每个内部节点表示在一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别</li><li>决策树学习是以实例为基础的归纳学习</li><li>决策树学习采用的是自顶向下的归纳方法，其基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子节点处的熵值为0，此时每个叶子节点中的实例都属于同一类。</li></ul><p>顾名思义，决策树是基于树结构来进行决策的，，在网上看到一个例子十分有趣，放在这里正好合适。现想象一位捉急的母亲想要给自己的女娃介绍一个男朋友，于是有了下面的对话：</p><hr><pre><code>女儿：多大年纪了？母亲：26。女儿：长的帅不帅？母亲：挺帅的。女儿：收入高不？母亲：不算很高，中等情况。女儿：是公务员不？母亲：是，在税务局上班呢。女儿：那好，我去见见。</code></pre><hr><p>这个女孩的挑剔过程就是一个典型的决策树，即相当于通过年龄、长相、收入和是否公务员将男童鞋分为两个类别：见和不见。假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么使用下图就能很好地表示女孩的决策逻辑（即一颗决策树）。</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec84a77.png" alt="1.png"></p><p>在上图的决策树中，决策过程的每一次判定都是对某一属性的“测试”，决策最终结论则对应最终的判定结果。一般一颗决策树包含：一个根节点、若干个内部节点和若干个叶子节点，易知：</p><pre><code>* 每个非叶节点表示一个特征属性测试。* 每个分支代表这个特征属性在某个值域上的输出。* 每个叶子节点存放一个类别。* 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。</code></pre><h3 id="决策树的构造"><a href="#决策树的构造" class="headerlink" title="决策树的构造"></a>决策树的构造</h3><p>决策树的构造是一个递归的过程，有三种情形会导致递归返回：(1) 当前结点包含的样本全属于同一类别，这时直接将该节点标记为叶节点，并设为相应的类别；(2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别；(3) 当前结点包含的样本集合为空，不能划分，这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。算法的基本流程如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc728ecc27fe.png" alt="2.png"></p><p>可以看出：决策树学习的关键在于如何选择划分属性，不同的划分属性得出不同的分支结构，从而影响整颗决策树的性能。属性划分的目标是让各个划分出来的子节点尽可能地“纯”，即属于同一类别。因此下面便是介绍量化纯度的具体方法，决策树最常用的算法有三种：ID3，C4.5和CART。##</p><h4 id="决策树学习算法的特点"><a href="#决策树学习算法的特点" class="headerlink" title="决策树学习算法的特点"></a>决策树学习算法的特点</h4><p>决策树学习算法最大的优点是，它可以自学习。在学习的过程中，不需要使用者了解过多背景知识，只需要对训练实例进行较好的标注，就能进行学习。显然，属于有监督学习。从一类无序、无规则的事物中推理出决策树表示分类的规则。</p><h4 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h4><p>ID3算法使用信息增益为准则来选择划分属性，“信息熵”(information entropy)是度量样本结合纯度的常用指标，假定当前样本集合D中第k类样本所占比例为pk，则样本集合D的信息熵定义为：</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec515a5.png" alt="3.png"></p><p>假定通过属性划分样本集D，产生了V个分支节点，v表示其中第v个分支节点，易知：分支节点包含的样本数越多，表示该分支节点的影响力越大。故可以计算出划分后相比原始数据集D获得的“信息增益”（information gain）。</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec3e067.png" alt="4.png"></p><p>Ent(D)划分前的信息增益 -划分后的信息增益</p><p>DV/D表示第V个分支的权重，样本越多越重要</p><p>信息增益越大，表示使用该属性划分样本集D的效果越好，因此ID3算法在递归过程中，每次选择最大信息增益的属性作为当前的划分属性。</p><h4 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h4><p>ID3算法存在一个问题，就是偏向于取值数目较多的属性，例如：如果存在一个唯一标识，这样样本集D将会被划分为|D|个分支，每个分支只有一个样本，这样划分后的信息熵为零，十分纯净，但是对分类毫无用处。因此C4.5算法使用了“增益率”（gain ratio）来选择划分属性，来避免这个问题带来的困扰。首先使用ID3算法计算出信息增益高于平均水平的候选属性，接着C4.5计算这些候选属性的增益率，增益率定义为：</p><p>启发式：先从候选划分属性中找出信息增益高于平均水平的，再从中选取增益率最高的</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec69647.png" alt="5.png"></p><h4 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h4><p>CART决策树使用“基尼指数”（Gini index）来选择划分属性，基尼指数反映的是从样本集D中随机抽取两个样本，其类别标记不一致的概率，因此Gini(D)越小越好，数据集D的纯度越高。基尼指数定义如下：</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec5a2ff.png" alt="6.png"></p><p>进而，使用属性α划分后的基尼指数为：</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec62eaf.png" alt="7.png"></p><p>二分类视角看CART</p><ul><li>每一个产生分支的过程是一个二分类过程</li><li>这个过程叫作“决策树桩”</li><li>一棵CART是由许多决策树桩拼接起来的</li><li>决策树桩是只有一层的决策树</li></ul><h4 id="三种不同的决策树"><a href="#三种不同的决策树" class="headerlink" title="三种不同的决策树"></a>三种不同的决策树</h4><ul><li>ID3:取值多的属性，更容易使数据更纯，其信息增益更大；训练得到的是一棵庞大且深度浅的树：不合理</li><li>C4.5:采用信息增益率替代信息增益</li><li>CART：以基尼系数替代熵；最小化不纯度，而不是最大化信息增益</li></ul><h4 id="剪枝处理"><a href="#剪枝处理" class="headerlink" title="剪枝处理"></a>剪枝处理</h4><p>从决策树的构造流程中我们可以直观地看出：不管怎么样的训练集，决策树总是能很好地将各个类别分离开来，这时就会遇到之前提到过的问题：过拟合（overfitting），即太依赖于训练样本。剪枝（pruning）则是决策树算法对付过拟合的主要手段，剪枝的策略有两种如下：</p><pre><code>* 预剪枝（prepruning）：在构造的过程中先评估，再考虑是否分支。* 后剪枝（post-pruning）：在构造好一颗完整的决策树后，自底向上，评估分支的必要性。</code></pre><p>评估指的是性能度量，即决策树的泛化性能。之前提到：可以使用测试集作为学习器泛化性能的近似，因此可以将数据集划分为训练集和测试集。预剪枝表示在构造数的过程中，对一个节点考虑是否分支时，首先计算决策树不分支时在测试集上的性能，再计算分支之后的性能，若分支对性能没有提升，则选择不分支（即剪枝）。后剪枝则表示在构造好一颗完整的决策树后，从最下面的节点开始，考虑该节点分支对模型的性能是否有提升，若无则剪枝，即将该节点标记为叶子节点，类别标记为其包含样本最多的类别。</p><p><img src="https://i.loli.net/2018/10/17/5bc728ec80d34.png" alt="8.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc728ec9e330.png" alt="9.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc728ec9d497.png" alt="10.png"></p><p>上图分别表示不剪枝处理的决策树、预剪枝决策树和后剪枝决策树。预剪枝处理使得决策树的很多分支被剪掉，因此大大降低了训练时间开销，同时降低了过拟合的风险，但另一方面由于剪枝同时剪掉了当前节点后续子节点的分支，因此预剪枝“贪心”的本质阻止了分支的展开，在一定程度上带来了欠拟合的风险。而后剪枝则通常保留了更多的分支，因此采用后剪枝策略的决策树性能往往优于预剪枝，但其自底向上遍历了所有节点，并计算性能，训练时间开销相比预剪枝大大提升。</p><h4 id="连续值与缺失值处理"><a href="#连续值与缺失值处理" class="headerlink" title="连续值与缺失值处理"></a>连续值与缺失值处理</h4><p>对于连续值的属性，若每个取值作为一个分支则显得不可行，因此需要进行离散化处理，常用的方法为二分法，基本思想为：给定样本集D与连续属性α，二分法试图找到一个划分点t将样本集D在属性α上分为≤t与＞t。</p><pre><code>* 首先将α的所有取值按升序排列，所有相邻属性的均值作为候选划分点（n-1个，n为α所有的取值数目）。* 计算每一个划分点划分集合D（即划分为两个分支）后的信息增益。* 选择最大信息增益的划分点作为最优划分点。</code></pre><p><img src="https://i.loli.net/2018/10/17/5bc72a0968fad.png" alt="11.png"></p><p>现实中常会遇到不完整的样本，即某些属性值缺失。有时若简单采取剔除，则会造成大量的信息浪费，因此在属性值缺失的情况下需要解决两个问题：（1）如何选择划分属性。（2）给定划分属性，若某样本在该属性上缺失值，如何划分到具体的分支上。假定为样本集中的每一个样本都赋予一个权重，根节点中的权重初始化为1，则定义：</p><p><img src="https://i.loli.net/2018/10/17/5bc72a098f3be.png" alt="12.png"></p><p>对于（1）：通过在样本集D中选取在属性α上没有缺失值的样本子集，计算在该样本子集上的信息增益，最终的信息增益等于该样本子集划分后信息增益乘以样本子集占样本集的比重。即：</p><p><img src="https://i.loli.net/2018/10/17/5bc72a096ccc3.png" alt="13.png"></p><p>对于（2）：若该样本子集在属性α上的值缺失，则将该样本以不同的权重（即每个分支所含样本比例）划入到所有分支节点中。该样本在分支节点中的权重变为：</p><p><img src="https://i.loli.net/2018/10/17/5bc72a093ed3c.png" alt="14.png"></p><h4 id="Bootstraping"><a href="#Bootstraping" class="headerlink" title="Bootstraping"></a>Bootstraping</h4><p>称为自助法，它是一种有放回的抽样方法</p><p>####Bagging的策略</p><ul><li>bootstrap aggregation</li><li>从样本集中重采样(有重复的)选出n个样本</li><li>在所有属性上，对这n个样本建立分类器(ID3、C4.5、CART、SVM、Logistic回归等)</li><li>重复以上两步m次，即获得了m个分类器</li><li>将数据放在这m个分类器上，最后根据这m个分类器的投票结果，决定数据属于哪一类</li></ul><h4 id="OOB数据"><a href="#OOB数据" class="headerlink" title="OOB数据"></a>OOB数据</h4><p>可以发现，Bootstrap每次约有36.79%的样本不会出现在Bootstrap所采集的样本集合中，将未参与模型训练的数据称为袋外数据(out of bag)。它可以用于取代测试集用于误差估计。得到的模型参数是无偏估计。</p><h4 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h4><p>随机森林在bagging基础上做了修改</p><ul><li>从样本集中用bootstrap采样选出n个样本</li><li>从所有属性中随机选择k个属性，选择最佳分割属性作为节点建立CART决策树</li><li>重复以上两步m次，即建立m课CART决策树</li><li>这m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类</li></ul><h4 id="随机森林-bagging和决策树的关系"><a href="#随机森林-bagging和决策树的关系" class="headerlink" title="随机森林/bagging和决策树的关系"></a>随机森林/bagging和决策树的关系</h4><ul><li>当然可以使用决策树作为基本分类器</li><li>但也可以使用SVM、Logistics回归等其他分类，习惯上，这些分类器组成的“总分类器”，仍然叫做随机森林</li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 决策树 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习逻辑回归与softmax</title>
      <link href="/2019/08/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8Esoftmax/"/>
      <url>/2019/08/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8Esoftmax/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习逻辑回归与softmax"><a href="#机器学习逻辑回归与softmax" class="headerlink" title="机器学习逻辑回归与softmax"></a>机器学习逻辑回归与softmax</h1><h2 id="机器学习中的线性模型"><a href="#机器学习中的线性模型" class="headerlink" title="机器学习中的线性模型"></a>机器学习中的线性模型</h2><p>谈及线性模型，其实我们很早就已经与它打过交道，还记得高中数学必修3课本中那个顽皮的“最小二乘法”吗？这就是线性模型的经典算法之一：根据给定的（x，y）点对，求出一条与这些点拟合效果最好的直线y=ax+b，之前我们利用下面的公式便可以计算出拟合直线的系数a,b（3.1中给出了具体的计算过程），从而对于一个新的x，可以预测它所对应的y值。前面我们提到：在机器学习的术语中，当预测值为连续值时，称为“回归问题”，离散值时为“分类问题”。本篇先从线性回归任务开始，接着讨论分类和多分类问题。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b068e48.png" alt="1.png"></p><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>线性回归问题就是试图学到一个线性模型尽可能准确地预测新样本的输出值，例如：通过历年的人口数据预测2017年人口数量。在这类问题中，往往我们会先得到一系列的有标记数据，例如：2000–&gt;13亿…2016–&gt;15亿，这时输入的属性只有一个，即年份；也有输入多属性的情形，假设我们预测一个人的收入，这时输入的属性值就不止一个了，例如：（学历，年龄，性别，颜值，身高，体重）–&gt;15k。</p><p>有时这些输入的属性值并不能直接被我们的学习模型所用，需要进行相应的处理，对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；对于离散值的属性，可作下面的处理：</p><ul><li><p>若属性值之间存在“序关系”，则可以将其转化为连续值，例如：身高属性分为“高”“中等”“矮”，可转化为数值：{1， 0.5， 0}。</p></li><li><p>若属性值之间不存在“序关系”，则通常将其转化为向量的形式，例如：性别属性分为“男”“女”，可转化为二维向量：{（1，0），（0，1）}。</p></li></ul><p>（1）当输入属性只有一个的时候，就是最简单的情形，也就是我们高中时最熟悉的“最小二乘法”（Euclidean distance），首先计算出每个样本预测值与真实值之间的误差并求和，通过最小化均方误差MSE，使用求偏导等于零的方法计算出拟合直线y=wx+b的两个参数w和b，计算过程如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0ccec4.png" alt="2.png"></p><p>（2）当输入属性有多个的时候，例如对于一个样本有d个属性{（x1,x2…xd）,y}，则y=wx+b需要写成：</p><p><img src="https://i.loli.net/2018/10/17/5bc72567b8bcd.png" alt="0.png"></p><p>通常对于多元问题，常常使用矩阵的形式来表示数据。在本问题中，将具有m个样本的数据集表示成矩阵X，将系数w与b合并成一个列向量，这样每个样本的预测值以及所有样本的均方误差最小化就可以写成下面的形式：</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0ad8f7.png" alt="3.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc722b0af652.png" alt="4.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc722b090543.png" alt="5.png"></p><p><img src="/blog_picture/line04.jpg" alt="avatar"></p><p><img src="/blog_picture/line05.jpg" alt="avatar"></p><p><img src="/blog_picture/line06.jpg" alt="avatar"></p><p><img src="/blog_picture/line07.jpg" alt="avatar"></p><h3 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h3><p>同样地，我们使用最小二乘法对w和b进行估计，令均方误差的求导等于0，需要注意的是，当一个矩阵的行列式不等于0时，我们才可能对其求逆，因此对于下式，我们需要考虑矩阵（X的转置*X）的行列式是否为0，若不为0，则可以求出其解，若为0，则需要使用其它的方法进行计算，书中提到了引入正则化，此处不进行深入。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0cde33.png" alt="6.png"></p><p>然而现实任务中当特征数量大于样本数时，XTX不满秩，此时θ有多个解；而且当数据量大时，求矩阵的逆非常耗时；对于不可逆矩阵（特征之间不相互独立），这种正规方程方法是不能用的。所以，还可以采用梯度下降法，利用迭代的方式求解θ。</p><h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>梯度下降法是按下面的流程进行的：<br>1）首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。<br>2）改变θ的值，使得θ按梯度下降的方向进行减少。</p><p><img src="/blog_picture/line01.jpg" alt="avatar"></p><p>对于只有两维属性的样本，J(θ)即J(θ0,θ1)的等高线图</p><p><img src="/blog_picture/line02.jpg" alt="avatar"></p><p><img src="/blog_picture/line03.jpg" alt="avatar"></p><p>迭代更新的方式有多种</p><ul><li>批量梯度下降（batch gradient descent），也就是是梯度下降法最原始的形式，对全部的训练数据求得误差后再对θ<br>进行更新，优点是每步都趋向全局最优解；缺点是对于大量数据，由于每步要计算整体数据，训练过程慢；</li><li>随机梯度下降（stochastic gradient descent），每一步随机选择一个样本对θ<br>进行更新，优点是训练速度快；缺点是每次的前进方向不好确定，容易陷入局部最优；</li><li>微型批量梯度下降（mini-batch gradient descent），每步选择一小批数据进行批量梯度下降更新θ<br>，属于批量梯度下降和随机梯度下降的一种折中，非常适合并行处理。</li></ul><p>另一方面，有时像上面这种原始的线性回归可能并不能满足需求，例如：y值并不是线性变化，而是在指数尺度上变化。这时我们可以采用线性模型来逼近y的衍生物，例如lny，这时衍生的线性模型如下所示，实际上就是相当于将指数曲线投影在一条直线上，如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc722b103cbf.png" alt="7.png"></p><p>更一般地，考虑所有y的衍生物的情形，就得到了“广义的线性模型”（generalized linear model），其中，g（*）称为联系函数（link function）。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0a2841.png" alt="8.png"></p><h3 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a>对数几率回归</h3><p>回归就是通过输入的属性值得到一个预测值，利用上述广义线性模型的特征，是否可以通过一个联系函数，将预测值转化为离散值从而进行分类呢？线性几率回归正是研究这样的问题。对数几率引入了一个对数几率函数（logistic function）,将预测值投影到0-1之间，从而将线性回归问题转化为二分类问题。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0c7748.png" alt="9.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc722b0a655d.png" alt="10.png"></p><p>若将y看做样本为正例的概率，（1-y）看做样本为反例的概率，则上式实际上使用线性回归模型的预测结果器逼近真实标记的对数几率。因此这个模型称为“对数几率回归”（logistic regression），也有一些书籍称之为“逻辑回归”。下面使用最大似然估计的方法来计算出w和b两个参数的取值，下面只列出求解的思路，不列出具体的计算过程。</p><p><img src="https://i.loli.net/2018/10/17/5bc723b824f0c.png" alt="11.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc723b817961.png" alt="12.png"></p><h3 id="线性判别分析"><a href="#线性判别分析" class="headerlink" title="线性判别分析"></a>线性判别分析</h3><p>线性判别分析（Linear Discriminant Analysis，简称LDA）,其基本思想是：将训练样本投影到一条直线上，使得同类的样例尽可能近，不同类的样例尽可能远。如图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc723b863ebb.png" alt="13.png"><img src="https://i.loli.net/2018/10/17/5bc723b85bfa9.png" alt="14.png"></p><p>想让同类样本点的投影点尽可能接近，不同类样本点投影之间尽可能远，即：让各类的协方差之和尽可能小，不用类之间中心的距离尽可能大。基于这样的考虑，LDA定义了两个散度矩阵。</p><ul><li>类内散度矩阵（within-class scatter matrix）</li></ul><p><img src="https://i.loli.net/2018/10/17/5bc723b8156e1.png" alt="15.png"></p><ul><li>类间散度矩阵(between-class scaltter matrix)</li></ul><p><img src="https://i.loli.net/2018/10/17/5bc723b7e9db3.png" alt="16.png"></p><p>因此得到了LDA的最大化目标：“广义瑞利商”（generalized Rayleigh quotient）。</p><p><img src="https://i.loli.net/2018/10/17/5bc723b7e8a61.png" alt="17.png"></p><p>从而分类问题转化为最优化求解w的问题，当求解出w后，对新的样本进行分类时，只需将该样本点投影到这条直线上，根据与各个类别的中心值进行比较，从而判定出新样本与哪个类别距离最近。求解w的方法如下所示，使用的方法为λ乘子。</p><p><img src="https://i.loli.net/2018/10/17/5bc723b83d5e0.png" alt="18.png"></p><p>若将w看做一个投影矩阵，类似PCA的思想，则LDA可将样本投影到N-1维空间（N为类簇数），投影的过程使用了类别信息（标记信息），因此LDA也常被视为一种经典的监督降维技术。    </p><h3 id="回归与欠-过拟合"><a href="#回归与欠-过拟合" class="headerlink" title="回归与欠/过拟合"></a>回归与欠/过拟合</h3><p><img src="/blog_picture/line08.jpg" alt="avatar">    </p><h3 id="线性回归与正则化"><a href="#线性回归与正则化" class="headerlink" title="线性回归与正则化"></a>线性回归与正则化</h3><p><img src="/blog_picture/line09.jpg" alt="avatar">     </p><h3 id="多分类学习"><a href="#多分类学习" class="headerlink" title="多分类学习"></a>多分类学习</h3><p>现实中我们经常遇到不只两个类别的分类问题，即多分类问题，在这种情形下，我们常常运用“拆分”的策略，通过多个二分类学习器来解决多分类问题，即将多分类问题拆解为多个二分类问题，训练出多个二分类学习器，最后将多个分类结果进行集成得出结论。最为经典的拆分策略有三种：“一对一”（OvO）、“一对其余”（OvR）和“多对多”（MvM），核心思想与示意图如下所示。</p><ul><li><p>OvO：给定数据集D，假定其中有N个真实类别，将这N个类别进行两两配对（一个正类/一个反类），从而产生N（N-1）/2个二分类学习器，在测试阶段，将新样本放入所有的二分类学习器中测试，得出N（N-1）个结果，最终通过投票产生最终的分类结果。</p></li><li><p>OvM：给定数据集D，假定其中有N个真实类别，每次取出一个类作为正类，剩余的所有类别作为一个新的反类，从而产生N个二分类学习器，在测试阶段，得出N个结果，若仅有一个学习器预测为正类，则对应的类标作为最终分类结果。</p></li><li><p>MvM：给定数据集D，假定其中有N个真实类别，每次取若干个类作为正类，若干个类作为反类（通过ECOC码给出，编码），若进行了M次划分，则生成了M个二分类学习器，在测试阶段（解码），得出M个结果组成一个新的码，最终通过计算海明/欧式距离选择距离最小的类别作为最终分类结果。</p></li></ul><p><img src="https://i.loli.net/2018/10/17/5bc723b862bfb.png" alt="19.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc723b8300d5.png" alt="20.png"></p><h3 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h3><p>类别不平衡（class-imbanlance）就是指分类问题中不同类别的训练样本相差悬殊的情况，例如正例有900个，而反例只有100个，这个时候我们就需要进行相应的处理来平衡这个问题。常见的做法有三种：</p><ol><li>在训练样本较多的类别中进行“欠采样”（undersampling）,比如从正例中采出100个，常见的算法有：EasyEnsemble。</li><li>在训练样本较少的类别中进行“过采样”（oversampling）,例如通过对反例中的数据进行插值，来产生额外的反例，常见的算法有SMOTE。</li><li>直接基于原数据集进行学习，对预测值进行“再缩放”处理。其中再缩放也是代价敏感学习的基础。<img src="https://i.loli.net/2018/10/17/5bc726fe87ae2.png" alt="21.png"></li></ol><h3 id="LR应用经验"><a href="#LR应用经验" class="headerlink" title="LR应用经验"></a>LR应用经验</h3><p>LR实现简单高效易解释，计算速度快，易并行，在大规模数据情况下非常适用，更适合于应对数值型和标称型数据，主要适合解决线性可分的问题，但容易欠拟合，大多数情况下需要手动进行特征工程，构建组合特征，分类精度不高。</p><p>LR直接对分类可能性进行建模，无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题<br>LR能以概率的形式输出，而非知识0，1判定，对许多利用概率辅助决策的任务很有用<br>对率函数任意阶可导，具有很好的数学性质，许多现有的数值优化算法都可以用来求最优解，训练速度快<br>适用情景：LR是很多分类算法的基础组件，它的好处是输出值自然地落在0到1之间，并且有概率意义。因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。虽然效果一般，却胜在模型清晰，背后的概率学经得住推敲。它拟合出来的参数就代表了每一个特征(feature)对结果的影响。也是一个理解数据的好工具。</p><p>应用上： </p><ul><li>CTR预估，推荐系统的learning to rank，各种分类场景 </li><li>某搜索引擎厂的广告CTR预估基线版是LR </li><li>某电商搜索排序基线版是LR </li><li>某新闻app排序基线版是LR</li></ul><p>大规模工业实时数据，需要可解释性的金融数据，需要快速部署低耗时数据<br>LR就是简单，可解释，速度快，消耗资源少，分布式性能好</p><p>ADMM-LR:用ADMM求解LogisticRegression的优化方法称作ADMM_LR。ADMM算法是一种求解约束问题的最优化方法，它适用广泛。相比于SGD，ADMM在精度要求不高的情况下，在少数迭代轮数时就达到一个合理的精度，但是收敛到很精确的解则需要很多次迭代。</p><p><img src="/blog_picture/line10.jpg" alt="avatar">   </p><p><img src="/blog_picture/line11.jpg" alt="avatar">   </p><p><img src="/blog_picture/line12.jpg" alt="avatar">   </p><h2 id="LR多分类推广-Softmax回归"><a href="#LR多分类推广-Softmax回归" class="headerlink" title="LR多分类推广 - Softmax回归"></a>LR多分类推广 - Softmax回归</h2><p>LR是一个传统的二分类模型，它也可以用于多分类任务，其基本思想是：将多分类任务拆分成若干个二分类任务，然后对每个二分类任务训练一个模型，最后将多个模型的结果进行集成以获得最终的分类结果。一般来说，可以采取的拆分策略有：</p><h3 id="one-vs-one策略"><a href="#one-vs-one策略" class="headerlink" title="one vs one策略"></a>one vs one策略</h3><p>　　假设我们有N个类别，该策略基本思想就是不同类别两两之间训练一个分类器，这时我们一共会训练出<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171104175530607-1392543504.png" alt="img">种不同的分类器。在预测时，我们将样本提交给所有的分类器，一共会获得N(N-1)个结果，最终结果通过<strong>投票</strong>产生。</p><h3 id="one-vs-all策略"><a href="#one-vs-all策略" class="headerlink" title="one vs all策略"></a>one vs all策略</h3><p>　　该策略基本思想就是将第i种类型的所有样本作为正例，将剩下的所有样本作为负例，进行训练得到一个分类器。这样我们就一共可以得到N个分类器。在预测时，我们将样本提交给所有的分类器，一共会获得N个结果，我们<strong>选择其中概率值最大</strong>的那个作为最终分类结果。 <img src="https://images2017.cnblogs.com/blog/1251096/201710/1251096-20171021171313943-1199609768.png" alt="img"></p><h2 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h2><p>　　softmax是LR在多分类的推广。与LR一样，同属于广义线性模型。什么是Softmax函数？假设我们有一个数组A，<img src="https://images2017.cnblogs.com/blog/1251096/201710/1251096-20171021164616881-992414484.png" alt="img">表示的是数组A中的第i个元素，那么这个元素的Softmax值就是</p><p>　　　　　　　　　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201710/1251096-20171021165228865-866731732.png" alt="img"></p><p>也就是说，是该元素的指数，与所有元素指数和的比值。那么 softmax回归模型的假设函数又是怎么样的呢？</p><p>　　　　　　　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105100956795-1587348606.png" alt="img"></p><p>由上式很明显可以得出，假设函数的分母其实就是对概率分布进行了归一化，使得所有类别的概率之和为1；也可以看出LR其实就是K=2时的Softmax。在参数获得上，我们可以采用one vs all策略获得K个不同的训练数据集进行训练，进而针对每一类别都会得到一组参数向量<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102153701-629755133.png" alt="img">。当测试样本特征向量<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102416560-1451219507.png" alt="img">输入时，我们先用假设函数针对每一个类别<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102622045-1005416234.png" alt="img">估算出概率值<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102706623-369597312.png" alt="img">。因此我们的假设函数将要输出一个K维的向量（向量元素和为1）来表示K个类别的估计概率，我们选择其中得分最大的类别作为该输入的预测类别。Softmax看起来和one vs all 的LR很像，它们最大的不同在与Softmax得到的K个类别的得分和为1，而one vs all的LR并不是。</p><h3 id="softmax的代价函数"><a href="#softmax的代价函数" class="headerlink" title="softmax的代价函数"></a>softmax的代价函数</h3><p>　　类似于LR，其似然函数我们采用对数似然，故：</p><p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105113747779-692061991.png" alt="img"></p><p>加入<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171104162333248-539020480.png" alt="img">正则项的损失函数为：</p><p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105114132232-517057992.png" alt="img"></p><p>此处的<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105110553560-1026190635.png" alt="img">为符号函数。对于其参数的求解过程，我们依然采用梯度下降法。</p><h3 id="softmax的梯度的求解"><a href="#softmax的梯度的求解" class="headerlink" title="softmax的梯度的求解"></a>softmax的梯度的求解</h3><p>　　正则化项的求导很简单，就等于<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105120226607-495282914.png" alt="img">，下面我们主要讨论没有加正则项的损失函数的梯度求解，即</p><p>　　　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105113747779-692061991.png" alt="img"></p><p>的导数（梯度）。为了使得求解过程看起来简便、易于理解，我们仅仅只对于一个样本（x,y）情况（SGD）进行讨论，</p><p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105161912482-1607069737.png" alt="img"></p><p>此时，我们令</p><p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105162625888-1575402902.png" alt="img"></p><p>可以得到</p><p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105163457810-492161690.png" alt="img"></p><p>故：</p><p><img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105170233232-810575386.png" alt="img"></p><p>所以，正则化之后的损失函数的梯度为</p><p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105171748341-1281292385.png" alt="img"></p><p>然后通过梯度下降法最小化 <img src="http://ufldl.stanford.edu/wiki/images/math/c/e/0/ce027336c1cb3c0cd461406c81369ebf.png" alt="\textstyle J(\theta)">，我们就能实现一个可用的 softmax 回归模型了。</p><h3 id="多分类LR与Softmax回归"><a href="#多分类LR与Softmax回归" class="headerlink" title="多分类LR与Softmax回归"></a>多分类LR与Softmax回归</h3><p>　　有了多分类的处理方法，那么我们什么时候该用多分类LR？什么时候要用softmax呢？</p><p>总的来说，若待分类的<strong>类别互斥</strong>，我们就使用Softmax方法；若待分类的<strong>类别有相交</strong>，我们则要选用多分类LR，然后投票表决。</p><h2 id="Softmax分类器"><a href="#Softmax分类器" class="headerlink" title="Softmax分类器"></a>Softmax分类器</h2><p>SVM是最常用的两个分类器之一，而另一个就是<strong>Softmax分类器，</strong>它的损失函数与SVM的损失函数不同。对于学习过二元逻辑回归分类器的读者来说，Softmax分类器就可以理解为逻辑回归分类器面对多个分类的一般化归纳。SVM将输出<img src="https://www.zhihu.com/equation?tex=f%28x_i%2CW%29" alt="[公式]">作为每个分类的评分（因为无定标，所以难以直接解释）。与SVM不同，Softmax的输出（归一化的分类概率）更加直观，并且从概率上可以解释，这一点后文会讨论。在Softmax分类器中，函数映射<img src="https://www.zhihu.com/equation?tex=f%28x_i%3BW%29%3DWx_i" alt="[公式]">保持不变，但将这些评分值视为每个分类的未归一化的对数概率，并且将<em>折叶损失（hinge loss）</em>替换为<strong>交叉熵损失</strong>（<strong>cross-entropy loss）</strong>。公式如下：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+Li%3D-log%28%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%29" alt="[公式]"> 或等价的 <img src="https://www.zhihu.com/equation?tex=L_i%3D-f_%7By_i%7D%2Blog%28%5Csum_je%5E%7Bf_j%7D%29" alt="[公式]"></p><p>在上式中，使用<img src="https://www.zhihu.com/equation?tex=f_j" alt="[公式]">来表示分类评分向量<img src="https://www.zhihu.com/equation?tex=f" alt="[公式]">中的第j个元素。和之前一样，整个数据集的损失值是数据集中所有样本数据的损失值<img src="https://www.zhihu.com/equation?tex=L_i" alt="[公式]">的均值与正则化损失<img src="https://www.zhihu.com/equation?tex=R%28W%29" alt="[公式]">之和。其中函数<img src="https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D" alt="[公式]">被称作<strong>softmax 函数</strong>：其输入值是一个向量，向量中元素为任意实数的评分值（<img src="https://www.zhihu.com/equation?tex=z" alt="[公式]">中的），函数对其进行压缩，输出一个向量，其中每个元素值在0到1之间，且所有元素之和为1。所以，包含softmax函数的完整交叉熵损失看起唬人，实际上还是比较容易理解的。</p><p><strong>信息理论视角</strong>：在“真实”分布<img src="https://www.zhihu.com/equation?tex=p" alt="[公式]">和估计分布<img src="https://www.zhihu.com/equation?tex=q" alt="[公式]">之间的<em>交叉熵</em>定义如下：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29" alt="[公式]"></p><p><strong>*译者注</strong>：Kullback-Leibler差异（Kullback-Leibler Divergence）也叫做相对熵（Relative Entropy），它衡量的是相同事件空间里的两个概率分布的差异情况。*</p><p><strong>概率论解释</strong>：先看下面的公式：</p><p><img src="https://www.zhihu.com/equation?tex=P%28y_i%7Cx_i%2CW%29%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D" alt="[公式]"></p><p><strong>实操事项：数值稳定。</strong>编程实现softmax函数计算的时候，中间项<img src="https://www.zhihu.com/equation?tex=e%5E%7Bf_%7By_i%7D%7D" alt="[公式]">和<img src="https://www.zhihu.com/equation?tex=%5Csum_j+e%5E%7Bf_j%7D" alt="[公式]">因为存在指数函数，所以数值可能非常大。除以大数值可能导致数值计算的不稳定，所以学会使用归一化技巧非常重要。如果在分式的分子和分母都乘以一个常数<img src="https://www.zhihu.com/equation?tex=C" alt="[公式]">，并把它变换到求和之中，就能得到一个从数学上等价的公式：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7BCe%5E%7Bf_%7By_i%7D%7D%7D%7BC%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%2BlogC%7D%7D%7B%5Csum_je%5E%7Bf_j%2BlogC%7D%7D" alt="[公式]"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">f = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>]) <span class="comment"># 例子中有3个分类，每个评分的数值都很大</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 不妙：数值问题，可能导致数值爆炸</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 那么将f中的值平移到最大值为0：</span></span><br><span class="line">f -= np.max(f) <span class="comment"># f becomes [-666, -333, 0]</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 现在OK了，将给出正确结果</span></span><br></pre></td></tr></table></figure><p><strong>让人迷惑的命名规则</strong>：精确地说，SVM分类器使用的是<em>折叶损失（hinge loss）</em>，有时候又被称为<em>最大边界损失（max-margin loss）</em>。Softmax分类器使用的是<em>交叉熵损失（corss-entropy loss）</em>。Softmax分类器的命名是从<em>softmax函数</em>那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。注意从技术上说“softmax损失（softmax loss）”是没有意义的，因为softmax只是一个压缩数值的函数。但是在这个说法常常被用来做简称。</p><h2 id="SVM和Softmax的比较"><a href="#SVM和Softmax的比较" class="headerlink" title="SVM和Softmax的比较"></a>SVM和Softmax的比较</h2><p>下图有助于区分这 Softmax和SVM这两种分类器：</p><p>————————————————————————————————————————</p><p><img src="https://pic1.zhimg.com/80/a90ce9e0ff533f3efee4747305382064_hd.png" alt="img"></p><p>针对一个数据点，SVM和Softmax分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量<strong>f</strong>（本节中是通过矩阵乘来实现）。不同之处在于对<strong>f</strong>中分值的解释：SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别2）的分值比其他分类的分值高出至少一个边界值。Softmax分类器将这些数值看做是每个分类没有归一化的<strong>对数概率</strong>，鼓励正确分类的归一化的对数概率变高，其余的变低。SVM的最终的损失值是1.58，Softmax的最终的损失值是0.452，但要注意这两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。</p><p>————————————————————————————————————————</p><p><strong>Softmax分类器为每个分类提供了“可能性”</strong>：SVM的计算是无标定的，而且难以针对所有分类的评分值给出直观解释。Softmax分类器则不同，它允许我们计算出对于所有分类标签的可能性。举个例子，针对给出的图像，SVM分类器可能给你的是一个[12.5, 0.6, -23.0]对应分类“猫”，“狗”，“船”。而softmax分类器可以计算出这三个标签的”可能性“是[0.9, 0.09, 0.01]，这就让你能看出对于不同分类准确性的把握。为什么我们要在”可能性“上面打引号呢？这是因为可能性分布的集中或离散程度是由正则化参数λ直接决定的，λ是你能直接控制的一个输入参数。举个例子，假设3个分类的原始分数是[1, -2, 0]，那么softmax函数就会计算：</p><p><img src="https://www.zhihu.com/equation?tex=%5B1%2C-2%2C0%5D%5Cto%5Be%5E1%2Ce%5E%7B-2%7D%2Ce%5E0%5D%3D%5B2.71%2C0.14%2C1%5D%5Cto%5B0.7%2C0.04%2C0.26%5D" alt="[公式]"></p><p>现在，如果正则化参数λ更大，那么权重W就会被惩罚的更多，然后他的权重数值就会更小。这样算出来的分数也会更小，假设小了一半吧[0.5, -1, 0]，那么softmax函数的计算就是：</p><p><img src="https://www.zhihu.com/equation?tex=%5B0.5%2C-1%2C0%5D%5Cto%5Be%5E%7B0.5%7D%2Ce%5E%7B-1%7D%2Ce%5E0%5D%3D%5B1.65%2C0.73%2C1%5D%5Cto%5B0.55%2C0.12%2C0.33%5D" alt="[公式]"></p><p>现在看起来，概率的分布就更加分散了。还有，随着正则化参数λ不断增强，权重数值会越来越小，最后输出的概率会接近于均匀分布。这就是说，softmax分类器算出来的概率最好是看成一种对于分类正确性的自信。和SVM一样，数字间相互比较得出的大小顺序是可以解释的，但其绝对值则难以直观解释<strong>。</strong></p><p><strong>在实际使用中，SVM和Softmax经常是相似的</strong>：通常说来，两种分类器的表现差别很小，不同的人对于哪个分类器更好有不同的看法。相对于Softmax分类器，SVM更加“局部目标化（local objective）”，这既可以看做是一个特性，也可以看做是一个劣势。考虑一个评分是[10, -2, 3]的数据，其中第一个分类是正确的。那么一个SVM（<img src="https://www.zhihu.com/equation?tex=%5CDelta+%3D1" alt="[公式]">）会看到正确分类相较于不正确分类，已经得到了比边界值还要高的分数，它就会认为损失值是0。SVM对于数字个体的细节是不关心的：如果分数是[10, -100, -100]或者[10, 9, 9]，对于SVM来说没设么不同，只要满足超过边界值等于1，那么损失值就等于0。</p><p>对于softmax分类器，情况则不同。对于[10, 9, 9]来说，计算出的损失值就远远高于[10, -100, -100]的。换句话来说，softmax分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。但是，SVM只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。这可以被看做是SVM的一种特性。举例说来，一个汽车的分类器应该把他的大量精力放在如何分辨小轿车和大卡车上，而不应该纠结于如何与青蛙进行区分，因为区分青蛙得到的评分已经足够低了。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> LR </tag>
            
            <tag> softmax </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文本分类问题</title>
      <link href="/2019/08/24/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/"/>
      <url>/2019/08/24/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1 id="文本分类问题"><a href="#文本分类问题" class="headerlink" title="文本分类问题"></a>文本分类问题</h1><p>下面我们来看一个文本分类问题，经典的新闻主题分类，用朴素贝叶斯怎么做。</p><p>In [193]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">seg_list = jieba.cut(<span class="string">"他来到东海识别区"</span>,cut_all=<span class="literal">False</span>)</span><br><span class="line">print(<span class="string">"/"</span> .join(seg_list) )</span><br><span class="line">jieba.add_word(<span class="string">'东海识别区'</span>)</span><br><span class="line">seg_list = jieba.cut(<span class="string">"他来到东海识别区"</span>,cut_all=<span class="literal">False</span>)</span><br><span class="line">print(<span class="string">"/ "</span> .join(seg_list) )</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">他/来到/东海/识别区</span><br><span class="line">他/ 来到/ 东海识别区</span><br></pre></td></tr></table></figure><p>In [175]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> jieba  <span class="comment">#处理中文</span></span><br><span class="line"><span class="keyword">import</span> nltk  <span class="comment">#处理英文</span></span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> pl</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> nltk</span><br></pre></td></tr></table></figure><h2 id="文本处理¶"><a href="#文本处理¶" class="headerlink" title="文本处理¶"></a>文本处理<a href="file:///Users/mmy/Downloads/朴素贝叶斯新闻分类.html#文本处理" target="_blank" rel="noopener">¶</a></h2><p>1、把训练样本划分为训练集和测试集</p><p>2、统计了词频，按词频降序生成词袋</p><p>In [137]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文本处理，也就是样本生成过程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_processing</span><span class="params">(folder_path, test_size=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">    folder_list = os.listdir(folder_path)</span><br><span class="line">    <span class="comment"># os.listdir 方法用于返回路径下包含的文件或文件夹的名字的列表</span></span><br><span class="line">    <span class="comment"># folder_list = ['C000008', 'C000014', 'C000013', 'C000022', 'C000023', 'C000024', 'C000010', 'C000020', 'C000016']</span></span><br><span class="line">    </span><br><span class="line">    data_list = []</span><br><span class="line">    class_list = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历文件夹，每个文件夹里是一个新闻的类别</span></span><br><span class="line">    <span class="keyword">for</span> folder <span class="keyword">in</span> folder_list:</span><br><span class="line">        new_folder_path = os.path.join(folder_path, folder)</span><br><span class="line">        <span class="comment"># os.path.join就是把两个路径拼接</span></span><br><span class="line">        <span class="comment"># new_folder_path = ./Database/SogouC/Sample/C000008</span></span><br><span class="line">        </span><br><span class="line">        files = os.listdir(new_folder_path)</span><br><span class="line">        <span class="comment"># 读取new_folder_path路径下的文件名</span></span><br><span class="line">        <span class="comment"># ['15.txt', '14.txt', '16.txt', '17.txt', '13.txt', '12.txt', '10.txt', '11.txt', '19.txt', '18.txt']</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 读取文件</span></span><br><span class="line">        j = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">            <span class="keyword">if</span> j &gt; <span class="number">100</span>: </span><br><span class="line">            <span class="comment"># 怕内存爆掉，只取100个样本文件，你可以注释掉取完，</span></span><br><span class="line">            <span class="comment"># 这里每个类别下只有10个样本，没事</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">with</span> open(os.path.join(new_folder_path, file), <span class="string">'r'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">                raw = fp.read()</span><br><span class="line">                <span class="comment"># read() 返回值为str，每次读取整个文件，将文件所有内容放到一个字符串变量中</span></span><br><span class="line">                <span class="comment"># readline() 返回值为str，每次只读取一行,每行的内容放在一个字符串变量中</span></span><br><span class="line">                <span class="comment"># readlines() 返回值为list，一次读取整个文件，每行的内容放在一个字符串变量中作为列表的一个元素。</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">## 是的，随处可见的jieba中文分词</span></span><br><span class="line">            jieba.enable_parallel(<span class="number">4</span>) <span class="comment"># 开启并行分词模式，参数为并行进程数，不支持windows</span></span><br><span class="line">            word_cut = jieba.cut(raw, cut_all=<span class="literal">False</span>) <span class="comment"># 精确模式，返回的结构是一个可迭代的genertor</span></span><br><span class="line">            word_list = list(word_cut) <span class="comment"># genertor转化为list，每个词unicode格式</span></span><br><span class="line">            jieba.disable_parallel() <span class="comment"># 关闭并行分词模式</span></span><br><span class="line">            </span><br><span class="line">            data_list.append(word_list) <span class="comment">#训练集list</span></span><br><span class="line">            <span class="comment">#class_list.append(folder.decode('utf-8')) #类别,str.decode会报错</span></span><br><span class="line">            class_list.append(folder) <span class="comment">#训练集的标签类别</span></span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">    <span class="comment">## 下面手动粗暴地划分训练集和测试集</span></span><br><span class="line">    data_class_list = zip(data_list, class_list) <span class="comment"># zip 函数返回一个zip对象</span></span><br><span class="line">    data_class_list = list(data_class_list) <span class="comment"># 需要用list转换成列表</span></span><br><span class="line">    </span><br><span class="line">    random.shuffle(data_class_list) <span class="comment"># shuffle随机打乱样本</span></span><br><span class="line">    index = int(len(data_class_list)*test_size)+<span class="number">1</span></span><br><span class="line">    train_list = data_class_list[index:]</span><br><span class="line">    test_list = data_class_list[:index]</span><br><span class="line">    </span><br><span class="line">    train_data_list, train_class_list = zip(*train_list) </span><br><span class="line">    <span class="comment"># 解压缩,文本和类别分开返回的是元组格式，可以在用list转换</span></span><br><span class="line">    test_data_list, test_class_list = zip(*test_list) </span><br><span class="line">    <span class="comment">#以上划分训练集和测试集其实可以用sklearn自带的部分做</span></span><br><span class="line">    <span class="comment">#train_data_list, test_data_list, train_class_list, test_class_list = \</span></span><br><span class="line">    <span class="comment">#sklearn.model_selection.train_test_split(data_list, class_list, test_size=test_size) </span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">   </span><br><span class="line">    <span class="comment"># 统计词频放入all_words_dict</span></span><br><span class="line">    all_words_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> word_list <span class="keyword">in</span> train_data_list:</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> word_list:</span><br><span class="line">            <span class="comment">#if all_words_dict.has_key(word):已删除此方法</span></span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> all_words_dict:</span><br><span class="line">                all_words_dict[word] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                all_words_dict[word] = <span class="number">1</span></span><br><span class="line">    <span class="comment"># all_words_dict=&#123;'\n': 1257, '\u3000': 1986, '有': 175, '江湖': 1,.....&#125;</span></span><br><span class="line">    <span class="comment"># 同样以上有现成的统计词频的API可以调用</span></span><br><span class="line">    <span class="comment"># from collections import Counter </span></span><br><span class="line">    <span class="comment"># Counter(train_data_list)</span></span><br><span class="line"></span><br><span class="line">    all_words_tuple_list = sorted(all_words_dict.items(), key=<span class="keyword">lambda</span> f:f[<span class="number">1</span>], reverse=<span class="literal">True</span>) </span><br><span class="line">    <span class="comment"># 内建函数sorted第一个参数需为list，all_words_dict.items()转化为列表，键和值为元组</span></span><br><span class="line">    <span class="comment"># key函数代表按元组的的词频排序，并降序返回结果</span></span><br><span class="line">    <span class="comment"># all_words_tuple_list = [('，', 3424), ('的', 2527), ('\u3000', 1734), ('。', 1482),.....]</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#all_words_list = list(zip(*all_words_tuple_list)[0]) 报错，需要修改</span></span><br><span class="line">    all_words_list,_ = zip(*all_words_tuple_list) <span class="comment"># 解压缩</span></span><br><span class="line">    all_words_list = list(all_words_list)</span><br><span class="line">    <span class="comment"># all_words_list = ['，', '的', '\u3000', '。', '\n', '在', ' ', '、', '了', '“',.....]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> all_words_list, train_data_list, test_data_list, train_class_list, test_class_list</span><br></pre></td></tr></table></figure><p>In [138]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (<span class="string">"start"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 文本预处理</span></span><br><span class="line">folder_path = <span class="string">'./Database/SogouC/Sample'</span></span><br><span class="line">all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = \</span><br><span class="line">text_processing(folder_path, test_size=<span class="number">0.2</span>)</span><br><span class="line">print(len(all_words_list)) <span class="comment"># 9748个不重复单词</span></span><br><span class="line">print(all_words_list[:<span class="number">100</span>])</span><br><span class="line">print(len(train_data_list)) <span class="comment"># 71个训练集样本</span></span><br><span class="line">print(len(test_data_list))  <span class="comment"># 19个测试集样本</span></span><br><span class="line">print(len(train_class_list))  <span class="comment"># 71个训练集标签</span></span><br><span class="line">print(len(test_class_list))  <span class="comment"># 19个测试集标签</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">start</span><br><span class="line">9481</span><br><span class="line">[&apos;，&apos;, &apos;的&apos;, &apos;\u3000&apos;, &apos;。&apos;, &apos;\n&apos;, &apos; &apos;, &apos;;&apos;, &apos;&amp;&apos;, &apos;nbsp&apos;, &apos;、&apos;, &apos;在&apos;, &apos;了&apos;, &apos;“&apos;, &apos;是&apos;, &apos;”&apos;, &apos;\x00&apos;, &apos;：&apos;, &apos;和&apos;, &apos;中国&apos;, &apos;有&apos;, &apos;也&apos;, &apos;我&apos;, &apos;对&apos;, &apos;就&apos;, &apos;将&apos;, &apos;—&apos;, &apos;上&apos;, &apos;这&apos;, &apos;游客&apos;, &apos;都&apos;, &apos;旅游&apos;, &apos;中&apos;, &apos;不&apos;, &apos;为&apos;, &apos;要&apos;, &apos;与&apos;, &apos;年&apos;, &apos;等&apos;, &apos;而&apos;, &apos;；&apos;, &apos;可以&apos;, &apos;月&apos;, &apos;（&apos;, &apos;）&apos;, &apos;导弹&apos;, &apos;大陆&apos;, &apos;一个&apos;, &apos;从&apos;, &apos;人&apos;, &apos;3&apos;, &apos;到&apos;, &apos;但&apos;, &apos;你&apos;, &apos;公司&apos;, &apos;说&apos;, &apos;火炮&apos;, &apos;日&apos;, &apos;(&apos;, &apos;)&apos;, &apos;他&apos;, &apos;考生&apos;, &apos;台军&apos;, &apos;认为&apos;, &apos;北京&apos;, &apos;时&apos;, &apos;多&apos;, &apos;还&apos;, &apos;个&apos;, &apos;1&apos;, &apos;.&apos;, &apos;能&apos;, &apos;《&apos;, &apos;》&apos;, &apos;已经&apos;, &apos;解放军&apos;, &apos;一种&apos;, &apos;会&apos;, &apos;时间&apos;, &apos;自己&apos;, &apos;来&apos;, &apos;新&apos;, &apos;各种&apos;, &apos;大&apos;, &apos;０&apos;, &apos;5&apos;, &apos;进行&apos;, &apos;市场&apos;, &apos;主要&apos;, &apos;我们&apos;, &apos;以&apos;, &apos;后&apos;, &apos;美国&apos;, &apos;五一&apos;, &apos;让&apos;, &apos;支付&apos;, &apos;黄金周&apos;, &apos;增长&apos;, &apos;并&apos;, &apos;成为&apos;, &apos;最&apos;]</span><br><span class="line">71</span><br><span class="line">19</span><br><span class="line">71</span><br><span class="line">19</span><br></pre></td></tr></table></figure><h2 id="停用词文件去重"><a href="#停用词文件去重" class="headerlink" title="停用词文件去重"></a>停用词文件去重</h2><p>这个停用词文件不是很官方，所以需要清洗下</p><p>In [140]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 粗暴的词去重</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_word_set</span><span class="params">(words_file)</span>:</span></span><br><span class="line">    words_set = set() <span class="comment"># 集合格式</span></span><br><span class="line">    <span class="keyword">with</span> open(words_file, <span class="string">'r'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fp.readlines(): <span class="comment"># 循环取出每一行</span></span><br><span class="line">            word = line.strip()</span><br><span class="line">            <span class="comment"># line.strip() 当()为空时，默认删除空白符（包括'\n','\r','\t',' ')</span></span><br><span class="line">            <span class="keyword">if</span> len(word)&gt;<span class="number">0</span> <span class="keyword">and</span> word <span class="keyword">not</span> <span class="keyword">in</span> words_set: <span class="comment"># 去重</span></span><br><span class="line">                words_set.add(word)</span><br><span class="line">    <span class="keyword">return</span> words_set</span><br></pre></td></tr></table></figure><p>In [145]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成stopwords_set</span></span><br><span class="line">stopwords_file = <span class="string">'./stopwords_cn.txt'</span> <span class="comment"># 停用词列表文件</span></span><br><span class="line">stopwords_set = make_word_set(stopwords_file) <span class="comment"># 首先停用词去重</span></span><br><span class="line">print(len(stopwords_set))</span><br><span class="line">print(stopwords_set)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">428</span><br><span class="line">&#123;&apos;得了&apos;, &apos;还是&apos;, &apos;所在&apos;, &apos;为此&apos;, &apos;如同下&apos;, &apos;并且&apos;, &apos;许多&apos;, &apos;但&apos;, &apos;不尽然&apos;, &apos;无&apos;, &apos;却&apos;, &apos;所&apos;, &apos;据此&apos;, &apos;分别&apos;, &apos;向&apos;, &apos;遵照&apos;, &apos;多会&apos;, &apos;而后&apos;, &apos;如下&apos;, &apos;再有&apos;, &apos;的确&apos;, &apos;此外&apos;, &apos;距&apos;, &apos;而已&apos;, &apos;何处&apos;, &apos;在于&apos;, &apos;说来&apos;, &apos;不料&apos;, &apos;且&apos;, &apos;于&apos;, &apos;亦&apos;, &apos;不单&apos;, &apos;而是&apos;, &apos;本人&apos;, &apos;正如&apos;, &apos;前者&apos;, &apos;别&apos;, &apos;才能&apos;, &apos;啦&apos;, &apos;只因&apos;, &apos;受到&apos;, &apos;甚至于&apos;, &apos;另一方面&apos;, &apos;此&apos;, &apos;只有&apos;, &apos;可是&apos;, &apos;您&apos;, &apos;别的&apos;, &apos;别处&apos;, &apos;些&apos;, &apos;或&apos;, &apos;不论&apos;, &apos;这会&apos;, &apos;其它&apos;, &apos;况且&apos;, &apos;有&apos;, &apos;此次&apos;, &apos;因此&apos;, &apos;去&apos;, &apos;毋宁&apos;, &apos;它们&apos;, &apos;根据&apos;, &apos;基于&apos;, &apos;当地&apos;, &apos;依据&apos;, &apos;然而&apos;, &apos;虽然&apos;, &apos;因为&apos;, &apos;从而&apos;, &apos;对比&apos;, &apos;怎&apos;, &apos;以上&apos;, &apos;诸如&apos;, &apos;倘若&apos;, &apos;一些&apos;, &apos;否则&apos;, &apos;所以&apos;, &apos;那边&apos;, &apos;每&apos;, &apos;并非&apos;, &apos;之&apos;, &apos;另&apos;, &apos;简言之&apos;, &apos;只限&apos;, &apos;连同&apos;, &apos;反之&apos;, &apos;这般&apos;, &apos;几&apos;, &apos;往&apos;, &apos;是&apos;, &apos;既&apos;, &apos;各自&apos;, &apos;该&apos;, &apos;因之&apos;, &apos;得&apos;, &apos;来&apos;, &apos;不然&apos;, &apos;么&apos;, &apos;甚至&apos;, &apos;为&apos;, &apos;处在&apos;, &apos;全部&apos;, &apos;如上&apos;, &apos;按照&apos;, &apos;加之&apos;, &apos;介于&apos;, &apos;正巧&apos;, &apos;好&apos;, &apos;似的&apos;, &apos;不过&apos;, &apos;有些&apos;, &apos;那样&apos;, &apos;此地&apos;, &apos;凡是&apos;, &apos;每当&apos;, &apos;不是&apos;, &apos;万一&apos;, &apos;由于&apos;, &apos;曾&apos;, &apos;而&apos;, &apos;照着&apos;, &apos;依照&apos;, &apos;彼时&apos;, &apos;就要&apos;, &apos;然后&apos;, &apos;以为&apos;, &apos;只需&apos;, &apos;为何&apos;, &apos;谁&apos;, &apos;到&apos;, &apos;不仅仅&apos;, &apos;既然&apos;, &apos;当然&apos;, &apos;起&apos;, &apos;嗡&apos;, &apos;此间&apos;, &apos;果然&apos;, &apos;与否&apos;, &apos;随&apos;, &apos;因&apos;, &apos;值此&apos;, &apos;即便&apos;, &apos;格里斯&apos;, &apos;趁着&apos;, &apos;要不然&apos;, &apos;那个&apos;, &apos;至于&apos;, &apos;乃&apos;, &apos;随后&apos;, &apos;有时&apos;, &apos;何况&apos;, &apos;当&apos;, &apos;使&apos;, &apos;只是&apos;, &apos;为着&apos;, &apos;可见&apos;, &apos;即使&apos;, &apos;以来&apos;, &apos;着&apos;, &apos;吧&apos;, &apos;截至&apos;, &apos;个&apos;, &apos;为什么&apos;, &apos;用&apos;, &apos;除外&apos;, &apos;他们&apos;, &apos;随时&apos;, &apos;这些&apos;, &apos;还&apos;, &apos;了&apos;, &apos;还有&apos;, &apos;啥&apos;, &apos;甚而&apos;, &apos;他&apos;, &apos;但是&apos;, &apos;并不&apos;, &apos;某某&apos;, &apos;如果说&apos;, &apos;从&apos;, &apos;各&apos;, &apos;别人&apos;, &apos;趁&apos;, &apos;这里&apos;, &apos;别说&apos;, &apos;以&apos;, &apos;不光&apos;, &apos;其次&apos;, &apos;就算&apos;, &apos;沿着&apos;, &apos;如果&apos;, &apos;大家&apos;, &apos;朝着&apos;, &apos;正是&apos;, &apos;对待&apos;, &apos;自己&apos;, &apos;不尽&apos;, &apos;虽&apos;, &apos;有关&apos;, &apos;替代&apos;, &apos;哟&apos;, &apos;对于&apos;, &apos;以及&apos;, &apos;这儿&apos;, &apos;加以&apos;, &apos;一&apos;, &apos;不外乎&apos;, &apos;尽管如此&apos;, &apos;个别&apos;, &apos;再则&apos;, &apos;哪&apos;, &apos;她&apos;, &apos;除此&apos;, &apos;也&apos;, &apos;自身&apos;, &apos;用来&apos;, &apos;不&apos;, &apos;什么&apos;, &apos;人们&apos;, &apos;便于&apos;, &apos;又&apos;, &apos;此处&apos;, &apos;非但&apos;, &apos;如何&apos;, &apos;哇&apos;, &apos;那里&apos;, &apos;只消&apos;, &apos;既是&apos;, &apos;凭&apos;, &apos;的&apos;, &apos;故而&apos;, &apos;打&apos;, &apos;嘻嘻&apos;, &apos;对方&apos;, &apos;谁人&apos;, &apos;乃至&apos;, &apos;以至&apos;, &apos;再&apos;, &apos;什么样&apos;, &apos;何&apos;, &apos;任何&apos;, &apos;最&apos;, &apos;由此&apos;, &apos;而且&apos;, &apos;自&apos;, &apos;直到&apos;, &apos;为了&apos;, &apos;固然&apos;, &apos;除了&apos;, &apos;假如&apos;, &apos;人&apos;, &apos;才是&apos;, &apos;据&apos;, &apos;的话&apos;, &apos;来自&apos;, &apos;有的&apos;, &apos;我们&apos;, &apos;从此&apos;, &apos;关于&apos;, &apos;向着&apos;, &apos;那么&apos;, &apos;给&apos;, &apos;或者&apos;, &apos;某个&apos;, &apos;等等&apos;, &apos;光是&apos;, &apos;怎么样&apos;, &apos;嘿嘿&apos;, &apos;如此&apos;, &apos;只&apos;, &apos;多少&apos;, &apos;来说&apos;, &apos;那般&apos;, &apos;哪个&apos;, &apos;那时&apos;, &apos;首先&apos;, &apos;赖以&apos;, &apos;这边&apos;, &apos;我&apos;, &apos;于是&apos;, &apos;另外&apos;, &apos;她们&apos;, &apos;已&apos;, &apos;不如&apos;, &apos;哪儿&apos;, &apos;及&apos;, &apos;及至&apos;, &apos;很&apos;, &apos;多么&apos;, &apos;哪些&apos;, &apos;又及&apos;, &apos;其&apos;, &apos;还要&apos;, &apos;既往&apos;, &apos;以致&apos;, &apos;或者说&apos;, &apos;如是&apos;, &apos;不仅&apos;, &apos;为止&apos;, &apos;本着&apos;, &apos;鉴于&apos;, &apos;什么的&apos;, &apos;而外&apos;, &apos;譬如&apos;, &apos;那儿&apos;, &apos;咱们&apos;, &apos;只要&apos;, &apos;凭借&apos;, &apos;后者&apos;, &apos;则&apos;, &apos;比如&apos;, &apos;一切&apos;, &apos;个人&apos;, &apos;何以&apos;, &apos;那&apos;, &apos;咱&apos;, &apos;上&apos;, &apos;在&apos;, &apos;如若&apos;, &apos;他人&apos;, &apos;一旦&apos;, &apos;哪怕&apos;, &apos;这样&apos;, &apos;只限于&apos;, &apos;仍&apos;, &apos;之所以&apos;, &apos;所有&apos;, &apos;或是&apos;, &apos;诸位&apos;, &apos;总之&apos;, &apos;怎么办&apos;, &apos;其中&apos;, &apos;怎么&apos;, &apos;若&apos;, &apos;作为&apos;, &apos;怎样&apos;, &apos;本身&apos;, &apos;凡&apos;, &apos;连带&apos;, &apos;由&apos;, &apos;不管&apos;, &apos;不但&apos;, &apos;只怕&apos;, &apos;和&apos;, &apos;看&apos;, &apos;同&apos;, &apos;把&apos;, &apos;宁可&apos;, &apos;那些&apos;, &apos;彼此&apos;, &apos;不只&apos;, &apos;唯有&apos;, &apos;继而&apos;, &apos;呵呵&apos;, &apos;正值&apos;, &apos;至&apos;, &apos;你们&apos;, &apos;下&apos;, &apos;跟&apos;, &apos;针对&apos;, &apos;并&apos;, &apos;以免&apos;, &apos;不至于&apos;, &apos;经过&apos;, &apos;你&apos;, &apos;就是&apos;, &apos;虽说&apos;, &apos;小&apos;, &apos;与其&apos;, &apos;至今&apos;, &apos;一来&apos;, &apos;让&apos;, &apos;们&apos;, &apos;即&apos;, &apos;诸&apos;, &apos;要不&apos;, &apos;沿&apos;, &apos;出来&apos;, &apos;两者&apos;, &apos;此时&apos;, &apos;遵循&apos;, &apos;如&apos;, &apos;这&apos;, &apos;这么&apos;, &apos;出于&apos;, &apos;较之&apos;, &apos;比&apos;, &apos;嘛&apos;, &apos;某些&apos;, &apos;以便&apos;, &apos;可以&apos;, &apos;若非&apos;, &apos;各位&apos;, &apos;今&apos;, &apos;逐步&apos;, &apos;这个&apos;, &apos;它&apos;, &apos;例如&apos;, &apos;其他&apos;, &apos;反而&apos;, &apos;就是说&apos;, &apos;随着&apos;, &apos;致&apos;, &apos;同时&apos;, &apos;可&apos;, &apos;被&apos;, &apos;接着&apos;, &apos;靠&apos;, &apos;除非&apos;, &apos;某&apos;, &apos;后&apos;, &apos;尔&apos;, &apos;其余&apos;, &apos;与&apos;, &apos;全体&apos;, &apos;仍旧&apos;, &apos;进而&apos;, &apos;儿&apos;, &apos;自从&apos;, &apos;开外&apos;, &apos;拿&apos;, &apos;要是&apos;, &apos;无论&apos;, &apos;要么&apos;, &apos;若是&apos;, &apos;因而&apos;, &apos;本地&apos;, &apos;尽管&apos;, &apos;何时&apos;&#125;</span><br></pre></td></tr></table></figure><h2 id="词袋中选取有代表性的特征词"><a href="#词袋中选取有代表性的特征词" class="headerlink" title="词袋中选取有代表性的特征词"></a>词袋中选取有代表性的特征词</h2><p>第一步生成的词袋里有很多通用的、无意义的词语，需要去掉。<br>有代表性的词语很大概率是一些对最终类别区分有作用的词语。并且后面这些词语会作为特征作为模型的输入。</p><p>In [125]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">words_dict</span><span class="params">(all_words_list, deleteN, stopwords_set=set<span class="params">()</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 选取特征词</span></span><br><span class="line">    feature_words = []</span><br><span class="line">    n = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(deleteN, len(all_words_list), <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 循环时从第20个开始，也就是舍弃前20个词语</span></span><br><span class="line">        <span class="keyword">if</span> n &gt; <span class="number">1000</span>: <span class="comment"># feature_words的维度1000</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> all_words_list[t].isdigit() <span class="keyword">and</span> \</span><br><span class="line">        all_words_list[t] <span class="keyword">not</span> <span class="keyword">in</span> stopwords_set <span class="keyword">and</span> \</span><br><span class="line">        <span class="number">1</span>&lt;len(all_words_list[t])&lt;<span class="number">5</span>:</span><br><span class="line">        <span class="comment"># isdigit() 方法检测字符串是否只由数字组成,返回True和False</span></span><br><span class="line">        <span class="comment"># 满足三个条件：不是数字；不在停用词表；长度2～4</span></span><br><span class="line">            feature_words.append(all_words_list[t])</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> feature_words</span><br></pre></td></tr></table></figure><p>In [149]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">deleteN = <span class="number">20</span> </span><br><span class="line"><span class="comment"># 删除前20个词语,可以调整这个数值</span></span><br><span class="line"><span class="comment"># 越靠前的词语出现的越频繁，有可能所有类别中都出现很多次，这类词语是可以去掉的。</span></span><br><span class="line">feature_words = words_dict(all_words_list, deleteN, stopwords_set)=</span><br><span class="line">print(feature_words[:<span class="number">100</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&apos;游客&apos;, &apos;旅游&apos;, &apos;导弹&apos;, &apos;大陆&apos;, &apos;一个&apos;, &apos;公司&apos;, &apos;火炮&apos;, &apos;考生&apos;, &apos;台军&apos;, &apos;认为&apos;, &apos;北京&apos;, &apos;已经&apos;, &apos;解放军&apos;, &apos;一种&apos;, &apos;时间&apos;, &apos;各种&apos;, &apos;进行&apos;, &apos;市场&apos;, &apos;主要&apos;, &apos;美国&apos;, &apos;五一&apos;, &apos;支付&apos;, &apos;黄金周&apos;, &apos;增长&apos;, &apos;成为&apos;, &apos;复习&apos;, &apos;很多&apos;, &apos;目前&apos;, &apos;没有&apos;, &apos;记者&apos;, &apos;问题&apos;, &apos;分析&apos;, &apos;远程&apos;, &apos;万人次&apos;, &apos;射程&apos;, &apos;接待&apos;, &apos;基础&apos;, &apos;部分&apos;, &apos;部署&apos;, &apos;作战&apos;, &apos;一定&apos;, &apos;选择&apos;, &apos;辅导班&apos;, &apos;考试&apos;, &apos;词汇&apos;, &apos;技术&apos;, &apos;比赛&apos;, &apos;文章&apos;, &apos;完全&apos;, &apos;可能&apos;, &apos;收入&apos;, &apos;工作&apos;, &apos;时候&apos;, &apos;今年&apos;, &apos;表示&apos;, &apos;期间&apos;, &apos;企业&apos;, &apos;VS&apos;, &apos;能力&apos;, &apos;达到&apos;, &apos;毕业生&apos;, &apos;上海&apos;, &apos;表现&apos;, &apos;影响&apos;, &apos;比较&apos;, &apos;人数&apos;, &apos;用户&apos;, &apos;相对&apos;, &apos;专家&apos;, &apos;服务&apos;, &apos;重要&apos;, &apos;拥有&apos;, &apos;需要&apos;, &apos;训练&apos;, &apos;开始&apos;, &apos;销售&apos;, &apos;通过&apos;, &apos;阵地&apos;, &apos;资料&apos;, &apos;情况&apos;, &apos;要求&apos;, &apos;阅读&apos;, &apos;老师&apos;, &apos;新浪&apos;, &apos;坦克&apos;, &apos;网络&apos;, &apos;军事&apos;, &apos;英语&apos;, &apos;项目&apos;, &apos;历史&apos;, &apos;设计&apos;, &apos;几乎&apos;, &apos;这是&apos;, &apos;写作&apos;, &apos;日本&apos;, &apos;考古&apos;, &apos;不同&apos;, &apos;提高&apos;, &apos;活动&apos;, &apos;公里&apos;]</span><br></pre></td></tr></table></figure><h2 id="训练和测试集生成固定长度的词向量特征"><a href="#训练和测试集生成固定长度的词向量特征" class="headerlink" title="训练和测试集生成固定长度的词向量特征"></a>训练和测试集生成固定长度的词向量特征</h2><p>这步为后面数据输入进贝叶斯模型训练做准备。</p><p>因为文本长度不一，所以每个样本需要固定好维度，才能喂给模型训练。</p><p>In [153]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文本特征</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_features</span><span class="params">(train_data_list, test_data_list, feature_words, flag=<span class="string">'nltk'</span>)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">text_features</span><span class="params">(text, feature_words)</span>:</span> <span class="comment"># text的定义在下面</span></span><br><span class="line">        text_words = set(text) <span class="comment"># 样本去重</span></span><br><span class="line">        <span class="comment">## -----------------------------------------------------------------------------------</span></span><br><span class="line">        <span class="keyword">if</span> flag == <span class="string">'nltk'</span>:</span><br><span class="line">            <span class="comment">## nltk特征 dict</span></span><br><span class="line">            features = &#123;word:<span class="number">1</span> <span class="keyword">if</span> word <span class="keyword">in</span> text_words <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> word <span class="keyword">in</span> feature_words&#125;</span><br><span class="line"><span class="comment"># 遍历每个样本词语，凡是样本的词语出现在1000个特征词里，就记录下来，保存为字典格式，键为词语，值为1，否则值为0。</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">elif</span> flag == <span class="string">'sklearn'</span>:</span><br><span class="line">            <span class="comment">## sklearn特征 list</span></span><br><span class="line">            features = [<span class="number">1</span> <span class="keyword">if</span> word <span class="keyword">in</span> text_words <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> word <span class="keyword">in</span> feature_words] </span><br><span class="line"><span class="comment"># 同上，遍历每个样本词语，结果不是字典，出现即为1，不出现为0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            features = []</span><br><span class="line">        <span class="comment">## -----------------------------------------------------------------------------------</span></span><br><span class="line">        <span class="keyword">return</span> features</span><br><span class="line">    train_feature_list = [text_features(text, feature_words) <span class="keyword">for</span> text <span class="keyword">in</span> train_data_list]</span><br><span class="line">    <span class="comment"># text为每一个训练的样本，返回值是二维列表</span></span><br><span class="line">    </span><br><span class="line">    test_feature_list = [text_features(text, feature_words) <span class="keyword">for</span> text <span class="keyword">in</span> test_data_list]</span><br><span class="line">    <span class="comment"># train为每一个测试样本，返回值是二维列表</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> train_feature_list, test_feature_list</span><br></pre></td></tr></table></figure><p>In [169]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">flag = <span class="string">'sklearn'</span></span><br><span class="line">train_feature_list, test_feature_list = \</span><br><span class="line">text_features(train_data_list, test_data_list, feature_words, flag)</span><br><span class="line">print(len(train_feature_list)) <span class="comment">#</span></span><br><span class="line">print(len(test_feature_list))  <span class="comment"># </span></span><br><span class="line">print(len(test_feature_list[<span class="number">5</span>])) <span class="comment"># 每个样本的维度都是1000 </span></span><br><span class="line">print(test_feature_list[<span class="number">5</span>][<span class="number">0</span>:<span class="number">100</span>]) <span class="comment"># 打印测试集的第5个样本的前100个值</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">71</span><br><span class="line">19</span><br><span class="line">1000</span><br><span class="line">[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</span><br></pre></td></tr></table></figure><h2 id="贝叶斯模型开始训练和预测"><a href="#贝叶斯模型开始训练和预测" class="headerlink" title="贝叶斯模型开始训练和预测"></a>贝叶斯模型开始训练和预测</h2><p>In [176]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分类，同时输出准确率等</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_classifier</span><span class="params">(train_feature_list, test_feature_list, </span></span></span><br><span class="line"><span class="function"><span class="params">                    train_class_list, test_class_list, flag=<span class="string">'nltk'</span>)</span>:</span></span><br><span class="line">    <span class="comment">## -----------------------------------------------------------------------------------</span></span><br><span class="line">    <span class="keyword">if</span> flag == <span class="string">'nltk'</span>:</span><br><span class="line">        <span class="comment">## 使用nltk分类器</span></span><br><span class="line">        train_flist = zip(train_feature_list, train_class_list)</span><br><span class="line">        train_flist = list(train_flist) </span><br><span class="line">        test_flist = zip(test_feature_list, test_class_list)</span><br><span class="line">        train_flist = list(test_flist) </span><br><span class="line">        classifier = nltk.classify.NaiveBayesClassifier.train(train_flist)</span><br><span class="line">        test_accuracy = nltk.classify.accuracy(classifier, test_flist)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">elif</span> flag == <span class="string">'sklearn'</span>:</span><br><span class="line">        <span class="comment">## sklearn分类器</span></span><br><span class="line">        classifier = MultinomialNB().fit(train_feature_list, train_class_list)</span><br><span class="line">        <span class="comment"># MultinomialNB()的使用方法和参数见：https://www.cnblogs.com/pinard/p/6074222.html</span></span><br><span class="line">        </span><br><span class="line">        test_accuracy = classifier.score(test_feature_list, test_class_list)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        test_accuracy = []</span><br><span class="line">    <span class="keyword">return</span> test_accuracy</span><br></pre></td></tr></table></figure><p>In [177]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">flag=&apos;sklearn&apos;</span><br><span class="line">test_accuracy = text_classifier(train_feature_list, test_feature_list, </span><br><span class="line">                                    train_class_list, test_class_list, flag)</span><br><span class="line">print(test_accuracy)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.7368421052631579</span><br></pre></td></tr></table></figure><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>这步调参，查看不同的deleteNs对模型效果的影响</p><p>In [179]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (<span class="string">"start"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 文本预处理</span></span><br><span class="line">folder_path = <span class="string">'./Database/SogouC/Sample'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = text_processing(folder_path, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成stopwords_set</span></span><br><span class="line">stopwords_file = <span class="string">'./stopwords_cn.txt'</span></span><br><span class="line">stopwords_set = make_word_set(stopwords_file)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 文本特征提取和分类</span></span><br><span class="line"><span class="comment"># flag = 'nltk'</span></span><br><span class="line">flag = <span class="string">'sklearn'</span></span><br><span class="line">deleteNs = range(<span class="number">0</span>, <span class="number">1000</span>, <span class="number">20</span>)</span><br><span class="line">test_accuracy_list = []</span><br><span class="line"><span class="keyword">for</span> deleteN <span class="keyword">in</span> deleteNs:</span><br><span class="line">    <span class="comment"># feature_words = words_dict(all_words_list, deleteN)</span></span><br><span class="line">    feature_words = words_dict(all_words_list, deleteN, stopwords_set)</span><br><span class="line">    train_feature_list, test_feature_list = text_features(train_data_list, test_data_list, feature_words, flag)</span><br><span class="line">    test_accuracy = text_classifier(train_feature_list, test_feature_list, train_class_list, test_class_list, flag)</span><br><span class="line">    test_accuracy_list.append(test_accuracy)</span><br><span class="line"><span class="keyword">print</span> (test_accuracy_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果评价</span></span><br><span class="line"><span class="comment">#plt.figure()</span></span><br><span class="line">plt.plot(deleteNs, test_accuracy_list)</span><br><span class="line">plt.title(<span class="string">'Relationship of deleteNs and test_accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'deleteNs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'test_accuracy'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#plt.savefig('result.png')</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"finished"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start</span><br><span class="line">[0.6842105263157895, 0.6842105263157895, 0.7368421052631579, 0.7368421052631579, 0.7368421052631579, 0.7368421052631579, 0.7368421052631579, 0.6842105263157895, 0.6842105263157895, 0.6842105263157895, 0.6842105263157895, 0.6842105263157895, 0.6842105263157895, 0.7368421052631579, 0.7368421052631579, 0.7894736842105263, 0.7368421052631579, 0.7368421052631579, 0.7368421052631579, 0.7368421052631579, 0.7368421052631579, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.7368421052631579, 0.6842105263157895, 0.6842105263157895, 0.6842105263157895, 0.7368421052631579, 0.7368421052631579, 0.6842105263157895, 0.7368421052631579, 0.6842105263157895, 0.6842105263157895, 0.7368421052631579, 0.6842105263157895, 0.631578947368421, 0.6842105263157895, 0.6842105263157895, 0.7368421052631579, 0.7894736842105263, 0.7894736842105263, 0.7368421052631579, 0.7368421052631579, 0.7894736842105263]</span><br></pre></td></tr></table></figure><p><img src="data:image/png;base64,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" alt="img"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">finished</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jiaba </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习基本概念</title>
      <link href="/2019/08/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
      <url>/2019/08/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<h2 id="机器学习基本概念"><a href="#机器学习基本概念" class="headerlink" title="机器学习基本概念"></a>机器学习基本概念</h2><p><strong>1  机器学习</strong></p><p><strong>1.1 机器学习的定义</strong></p><p>正如我们根据过去的经验来判断明天的天气，吃货们希望从购买经验中挑选一个好瓜，那能不能让计算机帮助人类来实现这个呢？机器学习正是这样的一门学科，人的“经验”对应计算机中的“数据”，让计算机来学习这些经验数据，生成一个算法模型，在面对新的情况中，计算机便能作出有效的判断，这便是机器学习。</p><p>另一本经典教材的作者Mitchell给出了一个形式化的定义，假设：</p><ul><li>P：计算机程序在某任务类T上的性能。</li><li>T：计算机程序希望实现的任务类。</li><li>E：表示经验，即历史的数据集。</li></ul><p>若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习。</p><p><strong>1.2 机器学习的一些基本术语</strong><br><img src="../img/ml_concepts.png" alt><br>假设我们收集了一批西瓜的数据，例如：（色泽=青绿;根蒂=蜷缩;敲声=浊响)， (色泽=乌黑;根蒂=稍蜷;敲声=沉闷)， (色泽=浅自;根蒂=硬挺;敲声=清脆)……每对括号内是一个西瓜的记录，定义：     </p><ul><li><p>所有记录的集合为：数据集。</p></li><li><p>每一条记录为：一个实例（instance）或样本（sample）。</p></li><li><p>例如：色泽或敲声，单个的特点为特征（feature）或属性（attribute）。</p></li><li><p>对于一条记录，如果在坐标轴上表示，每个西瓜都可以用坐标轴中的一个点表示，一个点也是一个向量，例如（青绿，蜷缩，浊响），即每个西瓜为：一个特征向量（feature vector）。</p></li><li><p>一个样本的特征数为：维数（dimensionality），该西瓜的例子维数为3，当维数非常大时，也就是现在说的“维数灾难”。</p><p> 计算机程序学习经验数据生成算法模型的过程中，每一条记录称为一个“训练样本”，同时在训练好模型后，我们希望使用新的样本来测试模型的效果，则每一个新的样本称为一个“测试样本”。定义：    </p></li><li><p>所有训练样本的集合为：训练集（trainning set），[特殊]。</p></li><li><p>所有测试样本的集合为：测试集（test set），[一般]。  </p></li><li><p>机器学习出来的模型适用于新样本的能力为：泛化能力（generalization），即从特殊到一般。</p><p> 西瓜的例子中，我们是想计算机通过学习西瓜的特征数据，训练出一个决策模型，来判断一个新的西瓜是否是好瓜。可以得知我们预测的是：西瓜是好是坏，即好瓜与差瓜两种，是离散值。同样地，也有通过历年的人口数据，来预测未来的人口数量，人口数量则是连续值。定义：    </p></li><li><p>预测值为离散值的问题为：分类（classification）。</p></li><li><p>预测值为连续值的问题为：回归（regression）。</p><p> 我们预测西瓜是否是好瓜的过程中，很明显对于训练集中的西瓜，我们事先已经知道了该瓜是否是好瓜，学习器通过学习这些好瓜或差瓜的特征，从而总结出规律，即训练集中的西瓜我们都做了标记，称为标记信息。但也有没有标记信息的情形，例如：我们想将一堆西瓜根据特征分成两个小堆，使得某一堆的西瓜尽可能相似，即都是好瓜或差瓜，对于这种问题，我们事先并不知道西瓜的好坏，样本没有标记信息。定义：    </p></li><li><p>训练数据有标记信息的学习任务为：监督学习（supervised learning），容易知道上面所描述的分类和回归都是监督学习的范畴。</p></li><li><p>训练数据没有标记信息的学习任务为：无监督学习（unsupervised learning），常见的有聚类和关联规则。</p></li></ul><p><strong>2  模型的评估与选择</strong></p><p><strong>2.1 误差与过拟合</strong></p><p>我们将学习器对样本的实际预测结果与样本的真实值之间的差异成为：误差（error）。定义：    </p><ul><li>在训练集上的误差称为训练误差（training error）或经验误差（empirical error）。</li><li>在测试集上的误差称为测试误差（test error）。</li><li>学习器在所有新样本上的误差称为泛化误差（generalization error）。</li></ul><p>显然，我们希望得到的是在新样本上表现得很好的学习器，即泛化误差小的学习器。因此，我们应该让学习器尽可能地从训练集中学出普适性的“一般特征”，这样在遇到新样本时才能做出正确的判别。然而，当学习器把训练集学得“太好”的时候，即把一些训练样本的自身特点当做了普遍特征；同时也有学习能力不足的情况，即训练集的基本特征都没有学习出来。我们定义：</p><ul><li>学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了，称为：过拟合（overfitting）。</li><li>学习能太差，训练样本的一般性质尚未学好，称为：欠拟合（underfitting）。</li></ul><p>可以得知：在过拟合问题中，训练误差十分小，但测试误差教大；在欠拟合问题中，训练误差和测试误差都比较大。目前，欠拟合问题比较容易克服，例如增加迭代次数等，但过拟合问题还没有十分好的解决方案，过拟合是机器学习面临的关键障碍。</p><p><img src="https://i.loli.net/2018/10/17/5bc7181172996.png" alt></p><p><strong>2.2 评估方法</strong></p><p>在现实任务中，我们往往有多种算法可供选择，那么我们应该选择哪一个算法才是最适合的呢？如上所述，我们希望得到的是泛化误差小的学习器，理想的解决方案是对模型的泛化误差进行评估，然后选择泛化误差最小的那个学习器。但是，泛化误差指的是模型在所有新样本上的适用能力，我们无法直接获得泛化误差。</p><p>因此，通常我们采用一个“测试集”来测试学习器对新样本的判别能力，然后以“测试集”上的“测试误差”作为“泛化误差”的近似。显然：我们选取的测试集应尽可能与训练集互斥，下面用一个小故事来解释why：</p><p>假设老师出了10 道习题供同学们练习，考试时老师又用同样的这10道题作为试题，可能有的童鞋只会做这10 道题却能得高分，很明显：这个考试成绩并不能有效地反映出真实水平。回到我们的问题上来，我们希望得到泛化性能好的模型，好比希望同学们课程学得好并获得了对所学知识”举一反三”的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试。显然，若测试样本被用作训练了，则得到的将是过于”乐观”的估计结果。</p><p><strong>2.3 训练集与测试集的划分方法</strong></p><p>如上所述：我们希望用一个“测试集”的“测试误差”来作为“泛化误差”的近似，因此我们需要对初始数据集进行有效划分，划分出互斥的“训练集”和“测试集”。下面介绍几种常用的划分方法：</p><p><strong>2.3.1 留出法</strong></p><p>将数据集D划分为两个互斥的集合，一个作为训练集S，一个作为测试集T，满足D=S∪T且S∩T=∅，常见的划分为：大约2/3-4/5的样本用作训练，剩下的用作测试。需要注意的是：训练/测试集的划分要尽可能保持数据分布的一致性，以避免由于分布的差异引入额外的偏差，常见的做法是采取分层抽样。同时，由于划分的随机性，单次的留出法结果往往不够稳定，一般要采用若干次随机划分，重复实验取平均值的做法。</p><p><strong>2.3.2 交叉验证法</strong></p><p>将数据集D划分为k个大小相同的互斥子集，满足D=D1∪D2∪…∪Dk，Di∩Dj=∅（i≠j），同样地尽可能保持数据分布的一致性，即采用分层抽样的方法获得这些子集。交叉验证法的思想是：每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就有K种训练集/测试集划分的情况，从而可进行k次训练和测试，最终返回k次测试结果的均值。交叉验证法也称“k折交叉验证”，k最常用的取值是10，下图给出了10折交叉验证的示意图。</p><p><img src="https://i.loli.net/2018/10/17/5bc718115d224.png" alt></p><p>与留出法类似，将数据集D划分为K个子集的过程具有随机性，因此K折交叉验证通常也要重复p次，称为p次k折交叉验证，常见的是10次10折交叉验证，即进行了100次训练/测试。特殊地当划分的k个子集的每个子集中只有一个样本时，称为“留一法”，显然，留一法的评估结果比较准确，但对计算机的消耗也是巨大的。</p><p><strong>2.3.3 自助法</strong></p><p>我们希望评估的是用整个D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。“自助法”正是解决了这样的问题。</p><p>自助法的基本思想是：给定包含m个样本的数据集D，每次随机从D 中挑选一个样本，将其拷贝放入D’，然后再将该样本放回初始数据集D 中，使得该样本在下次采样时仍有可能被采到。重复执行m 次，就可以得到了包含m个样本的数据集D’。可以得知在m次采样中，样本始终不被采到的概率取极限为：</p><p><img src="https://i.loli.net/2018/10/17/5bc71811246dd.png" alt></p><p>这样，通过自助采样，初始样本集D中大约有36.8%的样本没有出现在D’中，于是可以将D’作为训练集，D-D’作为测试集。自助法在数据集较小，难以有效划分训练集/测试集时很有用，但由于自助法产生的数据集（随机抽样）改变了初始数据集的分布，因此引入了估计偏差。在初始数据集足够时，留出法和交叉验证法更加常用。</p><p><strong>2.4 调参</strong></p><p>大多数学习算法都有些参数(parameter) 需要设定，参数配置不同，学得模型的性能往往有显著差别，这就是通常所说的”参数调节”或简称”调参” (parameter tuning)。</p><p>学习算法的很多参数是在实数范围内取值，因此，对每种参数取值都训练出模型来是不可行的。常用的做法是：对每个参数选定一个范围和步长λ，这样使得学习的过程变得可行。例如：假定算法有3 个参数，每个参数仅考虑5 个候选值，这样对每一组训练/测试集就有5<em>5</em>5= 125 个模型需考察，由此可见：拿下一个参数（即经验值）对于算法人员来说是有多么的happy。</p><p>最后需要注意的是：当选定好模型和调参完成后，我们需要使用初始的数据集D重新训练模型，即让最初划分出来用于评估的测试集也被模型学习，增强模型的学习效果。用上面考试的例子来比喻：就像高中时大家每次考试完，要将考卷的题目消化掉（大多数题目都还是之前没有见过的吧？），这样即使考差了也能开心的玩耍了~。</p><p><strong>3  机器学习评估与度量指标</strong></p><p>这里的内容主要包括：性能度量、比较检验和偏差与方差。在上一个notebook中，我们解决了评估学习器泛化性能的方法，即用测试集的“测试误差”作为“泛化误差”的近似，当我们划分好训练/测试集后，那如何计算“测试误差”呢？这就是性能度量，例如：均方差，错误率等，即“测试误差”的一个评价标准。有了评估方法和性能度量，就可以计算出学习器的“测试误差”，但由于“测试误差”受到很多因素的影响，例如：算法随机性或测试集本身的选择，那如何对两个或多个学习器的性能度量结果做比较呢？这就是比较检验。最后偏差与方差是解释学习器泛化性能的一种重要工具。</p><p><strong>3.1 性能度量</strong></p><p>性能度量（performance measure）是衡量模型泛化能力的评价标准，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。本节除2.5.1外，其它主要介绍分类模型的性能度量。</p><p><strong>3.1.1 最常见的性能度量</strong></p><p>在回归任务中，即预测连续值的问题，最常用的性能度量是“均方误差”（mean squared error）,很多的经典算法都是采用了MSE作为评价函数，想必大家都十分熟悉。</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf76276.png" alt="1.png"></p><p>在分类任务中，即预测离散值的问题，最常用的是错误率和精度，错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例，易知：错误率+精度=1。</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf4c704.png" alt="2.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71daf6fb84.png" alt="3.png"></p><p><strong>3.1.2 查准率/查全率/F1</strong></p><p>错误率和精度虽然常用，但不能满足所有的需求，例如：在推荐系统中，我们只关心推送给用户的内容用户是否感兴趣（即查准率），或者说所有用户感兴趣的内容我们推送出来了多少（即查全率）。因此，使用查准/查全率更适合描述这类问题。对于二分类问题，分类结果混淆矩阵与查准/查全率定义如下：</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf885a4.png" alt="4.png"></p><p>初次接触时，FN与FP很难正确的理解，按照惯性思维容易把FN理解成：False-&gt;Negtive，即将错的预测为错的，这样FN和TN就反了，后来找到一张图，描述得很详细，为方便理解，把这张图也贴在了下边：</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf871a6.png" alt="5.png"></p><p>正如天下没有免费的午餐，查准率和查全率是一对矛盾的度量。例如我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容，这样就漏掉了一些用户感兴趣的内容，查全率就低了；如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上，宁可错杀一千，不可放过一个，这样查准率就很低了。</p><p>“P-R曲线”正是描述查准/查全率变化的曲线，P-R曲线定义如下：根据学习器的预测结果（一般为一个实值或概率）对测试样本进行排序，将最可能是“正例”的样本排在前面，最不可能是“正例”的排在后面，按此顺序逐个把样本作为“正例”进行预测，每次计算出当前的P值和R值，如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc71dafc4411.png" alt="6.png"></p><p>P-R曲线如何评估呢？若一个学习器A的P-R曲线被另一个学习器B的P-R曲线完全包住，则称：B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的，所以衍生出了“平衡点”（Break-Event Point，简称BEP），即当P=R时的取值，平衡点的取值越高，性能更优。</p><p>P和R指标有时会出现矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的加权调和平均，即：</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf40ff6.png" alt="7.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71daf75407.png" alt="8.png"></p><p>特别地，当β=1时，也就是常见的F1度量，是P和R的调和平均，当F1较高时，模型的性能越好。</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf20885.png" alt="9.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71daf4b90a.png" alt="10.png"></p><p>有时候我们会有多个二分类混淆矩阵，例如：多次训练或者在多个数据集上训练，那么估算全局性能的方法有两种，分为宏观和微观。简单理解，宏观就是先算出每个混淆矩阵的P值和R值，然后取得平均P值macro-P和平均R值macro-R，在算出Fβ或F1，而微观则是计算出混淆矩阵的平均TP、FP、TN、FN，接着进行计算P、R，进而求出Fβ或F1。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed70230e.png" alt="11.png"></p><p><strong>3.1.3 ROC与AUC</strong></p><p>如上所述：学习器对测试样本的评估结果一般为一个实值或概率，设定一个阈值，大于阈值为正例，小于阈值为负例，因此这个实值的好坏直接决定了学习器的泛化性能，若将这些实值排序，则排序的好坏决定了学习器的性能高低。ROC曲线正是从这个角度出发来研究学习器的泛化性能，ROC曲线与P-R曲线十分类似，都是按照排序的顺序逐一按照正例预测，不同的是ROC曲线以“真正例率”（True Positive Rate，简称TPR）为横轴，纵轴为“假正例率”（False Positive Rate，简称FPR），ROC偏重研究基于测试样本评估值的排序好坏。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6bee91.png" alt="12.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71ed75cefe.png" alt="13.png"></p><p>简单分析图像，可以得知：当FN=0时，TN也必须0，反之也成立，我们可以画一个队列，试着使用不同的截断点（即阈值）去分割队列，来分析曲线的形状，（0,0）表示将所有的样本预测为负例，（1,1）则表示将所有的样本预测为正例，（0,1）表示正例全部出现在负例之前的理想情况，（1,0）则表示负例全部出现在正例之前的最差情况。限于篇幅，这里不再论述。</p><p>现实中的任务通常都是有限个测试样本，因此只能绘制出近似ROC曲线。绘制方法：首先根据测试样本的评估值对测试样本排序，接着按照以下规则进行绘制。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed740a24.png" alt="14.png"></p><p>同样地，进行模型的性能比较时，若一个学习器A的ROC曲线被另一个学习器B的ROC曲线完全包住，则称B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。ROC曲线下的面积定义为AUC（Area Uder ROC Curve），不同于P-R的是，这里的AUC是可估算的，即AOC曲线下每一个小矩形的面积之和。易知：AUC越大，证明排序的质量越好，AUC为1时，证明所有正例排在了负例的前面，AUC为0时，所有的负例排在了正例的前面。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6e2c57.png" alt="15.png"></p><p><strong>3.1.4 代价敏感错误率与代价曲线</strong></p><p>上面的方法中，将学习器的犯错同等对待，但在现实生活中，将正例预测成假例与将假例预测成正例的代价常常是不一样的，例如：将无疾病–&gt;有疾病只是增多了检查，但有疾病–&gt;无疾病却是增加了生命危险。以二分类为例，由此引入了“代价矩阵”（cost matrix）。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6ed582.png" alt="16.png"></p><p>在非均等错误代价下，我们希望的是最小化“总体代价”，这样“代价敏感”的错误率（2.5.1节介绍）为：</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed70bebe.png" alt="17.png"></p><p>同样对于ROC曲线，在非均等错误代价下，演变成了“代价曲线”，代价曲线横轴是取值在[0,1]之间的正例概率代价，式中p表示正例的概率，纵轴是取值为[0,1]的归一化代价。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6e952e.png" alt="18.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6eee7b.png" alt="19.png"></p><p>代价曲线的绘制很简单：设ROC曲线上一点的坐标为(TPR，FPR) ，则可相应计算出FNR，然后在代价平面上绘制一条从(0，FPR) 到(1，FNR) 的线段，线段下的面积即表示了该条件下的期望总体代价；如此将ROC 曲线土的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价，如图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed716e0d.png" alt="20.png"></p><p><strong>4  机器学习指标ROC与AUC</strong></p><p>AUC是一种模型分类指标，且仅仅是二分类模型的评价指标。AUC是Area Under Curve的简称，那么Curve就是ROC（Receiver Operating Characteristic），翻译为”接受者操作特性曲线”。</p><h3 id="ROC"><a href="#ROC" class="headerlink" title="ROC"></a>ROC</h3><p>曲线由两个变量TPR和FPR组成，这个组合以FPR对TPR，即是以代价(costs)对收益(benefits)。</p><ul><li><p>x轴为假阳性率（FPR）：在所有的负样本中，分类器预测错误的比例</p><p>$$FPR = \frac {FP}{FP+TN}$$</p></li><li><p>y轴为真阳性率（TPR）：在所有的正样本中，分类器预测正确的比例（等于Recall）</p><p>$$TPR = \frac {TP}{TP+FN}$$</p></li></ul><p>为了更好地理解ROC曲线，我们使用具体的实例来说明：</p><p>如在医学诊断中,判断有病的样本。那么尽量把有病的揪出来是主要任务，也就是第一个指标TPR，要越高越好。而把没病的样本误诊为有病的，也就是第二个指标FPR，要越低越好。</p><p>不难发现,这两个指标之间是相互制约的。如果某个医生对于有病的症状比较敏感，稍微的小症状都判断为有病,那么他的第一个指标应该会很高，但是第二个指标也就相应地变高。最极端的情况下,他把所有的样本都看做有病,那么第一个指标达到1,第二个指标也为1。</p><p>我们以FPR为横轴,TPR为纵轴,得到如下ROC空间。</p><img src="/blog_picture/1.7.png" width="60%"><p>我们可以看出，左上角的点(TPR=1，FPR=0)，为完美分类，也就是这个医生医术高明，诊断全对。点A(TPR&gt;FPR),医生A的判断大体是正确的。中线上的点B(TPR=FPR),也就是医生B全都是蒙的，蒙对一半，蒙错一半；下半平面的点C(TPR&lt;FPR)，这个医生说你有病，那么你很可能没有病，医生C的话我们要反着听，为真庸医。上图中一个阈值，得到一个点。现在我们需要一个独立于阈值的评价指标来衡量这个医生的医术如何，也就是遍历所有的阈值,得到ROC曲线。</p><p>假设如下就是某个医生的诊断统计图，直线代表阈值。通过改变不同的阈值$1.0 \rightarrow 0$，从而绘制出ROC曲线。下图为未得病人群（蓝色）和得病人群（红色）的模型输出概率分布图（横坐标表示模型输出概率，纵坐标表示概率对应的人群的数量）。阈值为1时，不管你什么症状，医生均未诊断出疾病（预测值都为N），此时FPR=TPR=0，位于左下。阈值为0时，不管你什么症状，医生都诊断结果都是得病（预测值都为P），此时FPR=TPR=1，位于右上。</p><img src="/blog_picture/1.8.png" width="50%"><p>曲线距离左上角越近,证明分类器效果越好。</p><img src="/blog_picture/1.9.png" width="60%"><p>如上，是三条ROC曲线，在0.23处取一条直线。那么，在同样的低FPR=0.23的情况下，红色分类器得到更高的PTR。也就表明，ROC越往左上，分类器效果越好。我们用一个标量值AUC来量化它。</p><h3 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h3><p><strong>AUC定义：</strong></p><p>AUC值为ROC曲线所覆盖的区域面积，显然，AUC越大，分类器分类效果越好。</p><p>AUC = 1，是完美分类器。绝大多数预测的场合，不存在完美分类器。</p><p>0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。</p><p>AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。</p><p>AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。</p><p>注：对于AUC小于0.5的模型，我们可以考虑取反（模型预测为positive，那我们就取negtive），这样就可以保证模型的性能不可能比随机猜测差。</p><p>以下为ROC曲线和AUC值的实例：</p><img src="/blog_picture/1.12.png" width="70%"><p><strong>AUC的物理意义</strong></p><p>AUC的物理意义正样本的预测结果大于负样本的预测结果的概率。所以AUC反应的是分类器对样本的排序能力。  </p><p>另外值得注意的是，AUC对样本类别是否均衡并不敏感，这也是不均衡样本通常用AUC评价分类器性能的一个原因。</p><p>下面从一个小例子解释AUC的含义：小明一家四口，小明5岁，姐姐10岁，爸爸35岁，妈妈33岁建立一个逻辑回归分类器，来预测小明家人为成年人概率，假设分类器已经对小明的家人做过预测，得到每个人为成人的概率。</p><ol><li>AUC更多的是关注对计算概率的排序，关注的是概率值的相对大小，与阈值和概率值的绝对大小没有关系</li></ol><p>例子中并不关注小明是不是成人，而关注的是，预测为成人的概率的排序。</p><p><strong>问题⑪：</strong>以下为三种模型的输出结果，求三种模型的AUC。</p><table><thead><tr><th></th><th>小明</th><th>姐姐</th><th>妈妈</th><th>爸爸</th></tr></thead><tbody><tr><td>a</td><td>0.12</td><td>0.35</td><td>0.76</td><td>0.85</td></tr><tr><td>b</td><td>0.12</td><td>0.35</td><td>0.44</td><td>0.49</td></tr><tr><td>c</td><td>0.52</td><td>0.65</td><td>0.76</td><td>0.85</td></tr></tbody></table><p>AUC只与概率的相对大小（概率排序）有关，和绝对大小没关系。由于三个模型概率排序的前两位都是未成年人（小明，姐姐），后两位都是成年人（妈妈，爸爸），因此三个模型的AUC都等于。</p><ol><li><p>AUC只关注正负样本之间的排序，并不关心正样本内部，或者负样本内部的排序。这也体现了AUC的本质：任意个正样本的概率都大于负样本的概率的能力  </p><p>例子中AUC只需要保证（小明和姐姐）（爸爸和妈妈），小明和姐姐在前2个排序，爸爸和妈妈在后2个排序，而不会考虑小明和姐姐谁在前，或者爸爸和妈妈谁在前。</p><p><strong>问题⑫：</strong>以下已经对分类器输出概率从小到大进行了排列，哪些情况的AUC等于1， 情况的AUC为0（其中背景色表示True value，红色表示成年人，蓝色表示未成年人）。</p><img src="/blog_picture/1.10.png" width="70%"><p>D 模型, E模型和F模型的AUC值为1，C模型的AUC值为0（爸妈为成年人的概率小于小明和姐姐，显然这个模型预测反了）。</p></li></ol><p><strong>AUC的计算：</strong></p><ul><li><p>法1：AUC为ROC曲线下的面积，那我们直接计算面积可得。面积为一个个小的梯形面积（曲线）之和。计算的精度与阈值的精度有关。</p></li><li><p>法2：根据AUC的物理意义，我们计算正样本预测结果大于负样本预测结果的概率。取n1*n0(n1为正样本数，n0为负样本数)个二元组，比较score（预测结果），最后得到AUC。时间复杂度为O(N*M)。</p></li><li><p>法3：我们首先把所有样本按照score排序，依次用rank表示他们，如最大score的样本，rank=n (n=n0+n1，其中n0为负样本个数，n1为正样本个数)，其次为n-1。那么对于正样本中rank最大的样本，rank_max，有n1-1个其他正样本比他score小,那么就有(rank_max-1)-(n1-1)个负样本比他score小。其次为(rank_second-1)-(n1-2)。最后我们得到正样本大于负样本的概率为</p><p><img src="/blog_picture/auc.jpg" alt="avatar"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> ROC </tag>
            
            <tag> AUC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>简洁版机器学习速查表</title>
      <link href="/2019/08/23/%E7%AE%80%E6%B4%81%E7%89%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%9F%E6%9F%A5%E8%A1%A8/"/>
      <url>/2019/08/23/%E7%AE%80%E6%B4%81%E7%89%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%9F%E6%9F%A5%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p><img src="/blog_picture/ml_1.jpg" alt="avatar"><br><img src="/blog_picture/ml_2.jpg" alt="avatar"><br><img src="/blog_picture/ml_3.jpg" alt="avatar"><br><img src="/blog_picture/ml_4.jpg" alt="avatar"><br><img src="/blog_picture/ml_5.jpg" alt="avatar"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS229版机器学习速查表</title>
      <link href="/2019/08/23/CS229%E7%89%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%9F%E6%9F%A5%E8%A1%A8/"/>
      <url>/2019/08/23/CS229%E7%89%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%9F%E6%9F%A5%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p><img src="https://s1.ax1x.com/2020/04/24/JBZ8U0.jpg" alt="avatar"></p><p><img src="https://s1.ax1x.com/2020/04/24/JBZg2D.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBZHG8.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBZzaq.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBePRU.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBeAsJ.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBmBh6.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBm2Bd.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBmhNt.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBmo38.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBm7jg.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBmv40.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBniDJ.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBnAER.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBneC6.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBnYPP.jpg" alt="avatar"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>葫芦书学习笔记</title>
      <link href="/2019/08/08/%E8%91%AB%E8%8A%A6%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2019/08/08/%E8%91%AB%E8%8A%A6%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="1-为什么需要对数值类型的特征做归一化处理？"><a href="#1-为什么需要对数值类型的特征做归一化处理？" class="headerlink" title="1.为什么需要对数值类型的特征做归一化处理？"></a>1.为什么需要对数值类型的特征做归一化处理？</h1><ul><li>为了方便后续进行梯度下降的时候加速收敛</li><li>归一化通常主要分为两种：min-max(线性函数归一化)，Z-Score(零均值归一化)</li><li>需要进行归一化的模型：线性回归，LR，SVM，神经网络等</li><li>决策树模型不适用归一化处理，因为决策树在进行节点分裂时主要依据数据集D关于特征x的信息增益比，而信息增益比和特征是否进行归一化无关，因为归一化并不改变样本在特征x上的信息增益。</li></ul><h1 id="2-􏳉􏴔􏳓在数据进行预处理时，应该怎样处理类别型特征？"><a href="#2-􏳉􏴔􏳓在数据进行预处理时，应该怎样处理类别型特征？" class="headerlink" title="2.􏳉􏴔􏳓在数据进行预处理时，应该怎样处理类别型特征？"></a>2.􏳉􏴔􏳓在数据进行预处理时，应该怎样处理类别型特征？</h1><ul><li>类别型特征原始输入形式通常是字符串形式、除了决策树等少量模型能直接处理字符串形式的输入，对于LR、SVM等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作</li><li>序号编码(ordinal)：类别间具有大小关系</li><li>独热编码(one-hot)：类别间没有大小关系，特征的每个值作为一列。维度过高可能导致维度灾难，产生过拟合问题。</li><li>二进制编码：先用序号编码给每个类别赋予一个类别ID，然后将ID转为二进制编码作为结果。</li></ul><h1 id="3-什么是组合特征？如何处理高维组合特征-解决高维组合特征维度过高的问题-？"><a href="#3-什么是组合特征？如何处理高维组合特征-解决高维组合特征维度过高的问题-？" class="headerlink" title="3.什么是组合特征？如何处理高维组合特征(解决高维组合特征维度过高的问题)？"></a>3.什么是组合特征？如何处理高维组合特征(解决高维组合特征维度过高的问题)？</h1><ul><li>可以将M * N矩阵分解为M * K和K * N两个矩阵相乘的形式，这样参数从M * N降到 K * (M+N)</li><li>为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。</li></ul><p>#4.怎样有效的找到组合特征？</p><ul><li>基于决策树的特征组合寻找。（原始输入构建决策树可以采用梯度提升决策树即每次都在之前构建的决策树的残差上构建下一课决策树）</li></ul><h1 id="5-有哪些文本模型？他们各有什么优缺点？"><a href="#5-有哪些文本模型？他们各有什么优缺点？" class="headerlink" title="5.有哪些文本模型？他们各有什么优缺点？"></a>5.有哪些文本模型？他们各有什么优缺点？</h1><ul><li>词袋模型(bag of words)：最基础的文本表示模型是词袋模型。顾名思义，就是将每篇文章看成一袋子词，并忽略每个词出现的顺序。将整段文本以词为单位切分开，然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对应得权重则则反映了这个词在原文章的重要程度。但是词袋忽略了由几个词组成一个意思这种情况（“如NBA吐槽大会”这种，分解成了NBA和吐槽大会，结果匹配了很多李诞这样和NBA完全不相关的物料）</li><li>N-gram模型：词袋模型的改进，N-gram将连续出现的N个词组成的词组也作为一维放到向量表示中去。但是N-gram不能识别两个不同的词有相同的主题</li><li>TF-TDF：TF-IDF(t,d) = TF(t,d)*IDF(t)其中，TF(t,d)为单词t在文档d中出现的频率，IDF(t) = log(文章总数/(包含单词t的文章总数+1)) ，IDF公式可理解为如果一个词出现的文章数越多那么说明它越是一个通用词，通用词对文档内容贡献度比较小</li><li>主题模型：主题模型用于从文本库发现有代表性的主题（得到每个主题上面词的分布特性），并且能够计算出每篇文章的主题分布。</li><li>词嵌入与深度学习模型：词嵌入是一类将词向量化的模型的统称，核心思想是将每个词都映射成低维空间（通常50-300维)上的一个稠密向量。K维空间中的每一维都可以看作是一个隐含的主题，只不过不像主题模型中的主题那么直观。由于词嵌入将每个词映射成一个K维的向量，如果一篇文章有N个词，就可以用一个N*K维的矩阵来表示这篇文档，但是这样表示过去底层。在实际应用中，如果仅仅把这个矩阵作为源文本的表示特征输入到机器学习模型中，通常很难得到满意的结果。因此，还需要在此基础上加工出更高层的特征。在传统的浅层机器学习模型中，一个好的特征工程往往可以带来算法效果的显著提示。深度学习模型正好为我们提供了一种自动 的进行特征工程的方式，模型中的每个隐层都可以认为对应着不同抽象层次的特征。卷积神经网络和循环神经网络的结构在文本表示中取得很好的效果，主要是由于他们能够更好的对文本进行建模，抽取出更高层的语义特征。。与全链接网络结构相比，卷积神经网络和RNN一方面很好的抓住了文本的特征，另一方面又减少了网络学习中待学习的参数，提高了训练速度，并且降低了过拟合的风险。</li></ul><h1 id="6-Word2Vec是如何工作的？它和LDA有什么区别与联系？"><a href="#6-Word2Vec是如何工作的？它和LDA有什么区别与联系？" class="headerlink" title="6.Word2Vec是如何工作的？它和LDA有什么区别与联系？"></a>6.Word2Vec是如何工作的？它和LDA有什么区别与联系？</h1><ul><li>word2vec实际上一种浅层的神经网络模型，它有两种网络结构，分别是CBOW(continues bag of words)和Skip-gram</li><li>CBOW的目标是根据上下文出现的词语来预测当前词的生成概率；skip-gram是根据当前词来预测上下文中各词的生成概率。</li><li>word2vec是google开发的一种词向量嵌入的模型，主要分为CBOW和skip-gram两种，最后得到得词向量是dense vector。</li><li>LDA是一种生成模型，最后可以得到文档与主题，主题与词之间的概率分布。</li></ul><h1 id="7-在图像分类任务中，训练数据不足会带来什么问题？如何缓解数据量不足带来的问题？"><a href="#7-在图像分类任务中，训练数据不足会带来什么问题？如何缓解数据量不足带来的问题？" class="headerlink" title="7.在图像分类任务中，训练数据不足会带来什么问题？如何缓解数据量不足带来的问题？"></a>7.在图像分类任务中，训练数据不足会带来什么问题？如何缓解数据量不足带来的问题？</h1><ul><li>训练数据不足主要表现在过拟合方面。</li><li>两类处理方法：一是基于模型的方法，主要是采用降低过拟合风险的措施包括简化模型(非线性简化为线性)、添加约束项以缩小假设空间、集成学习、Dropout超参数等。二是基于数据的的方法，主要是通过数据扩充</li></ul><h1 id="8-准确率的局限性"><a href="#8-准确率的局限性" class="headerlink" title="8.准确率的局限性"></a>8.准确率的局限性</h1><ul><li>不同类别的样本比例非常不均匀时，占比大的类别往往成为影响准确率的最主要因素</li></ul><h1 id="9-精确率与召回率的权衡"><a href="#9-精确率与召回率的权衡" class="headerlink" title="9.精确率与召回率的权衡"></a>9.精确率与召回率的权衡</h1><ul><li>只用某个点对应的精确率和召回率不能全面地衡量模型的性能，只有通过P-R曲线的整体表现，才能够对模型进行更为全面的评估</li></ul><h1 id="10-平方根误差的意外"><a href="#10-平方根误差的意外" class="headerlink" title="10.平方根误差的意外"></a>10.平方根误差的意外</h1><ul><li>一般情况下，RMSE能够很好的反映回归模型预测值与真实值的偏离程度。但在实际问题中，如果存在个别偏离程度非常大的离群点时，即使离群点数量非常少，也会让RMSE指标变得很差。</li><li>解决方法：一，在数据预处理时过滤这些噪声点。二，如果不认为这些离群点是噪声的话就要进一步提高模型的预测能力，将离群点产生的机制建模进去。三，找一个更合适的指标来评估该模型。</li></ul><h1 id="11-什么是ROC曲线"><a href="#11-什么是ROC曲线" class="headerlink" title="11.什么是ROC曲线"></a>11.什么是ROC曲线</h1><ul><li>ROC􏵻􏵲􏰋曲线是Receiver Operating Characteristic Curve􏰊􏵱􏲒􏰢􏱟的简称，中文名为“受试者工作特征曲线”。ROC曲线的横坐标为假阳性率FPR；纵坐标为真阳性率TPR。</li></ul><p>#12.如果绘制ROC􏵻􏵲􏰋曲线</p><ul><li>ROC􏵻􏵲􏰋曲线是通过不断移动分类器的“截断点”来生成曲线上一组关键点的。</li><li>首先根据样本标签统计出正负样本的数量，假设正样本数量为p，负样本数量为n；接下来，把横轴的刻度间隔设置为1/n,纵轴的刻度间隔设置为1/p；再根据模型输出的预测概率对样本进行排序依次遍历样本，同时从零点开始绘制ROC􏵻􏵲􏰋曲线，每遇到一个正样本就沿纵轴方向绘制一个刻度间隔的曲线，每遇到一个负样本就沿着横轴方向绘制一个刻度间隔的曲线，直到遍历完所有样本，曲线最终停在（1，1）这个点，整个ROC曲线绘制完成。</li></ul><h1 id="13-如何计算AUC"><a href="#13-如何计算AUC" class="headerlink" title="13.如何计算AUC"></a>13.如何计算AUC</h1><ul><li>沿着ROC横轴做积分。</li></ul><h1 id="14-ROC曲线相比P-R曲线有什么特点"><a href="#14-ROC曲线相比P-R曲线有什么特点" class="headerlink" title="14.ROC曲线相比P-R曲线有什么特点"></a>14.ROC曲线相比P-R曲线有什么特点</h1><ul><li>当正负样本的分布发生变化时，ROC曲线的形状能够基本保持不变，而P-R曲线的形状一般会发生较剧烈变化。</li><li>ROC曲线能够尽量降低不同测试集带来的干扰，更加客观地衡量模型本身的性能。ROC曲线的适用范围更广，适用于排序、推荐、广告。选择ROC曲线还是P-R曲线因实际问题而异，如果希望更多的看到模型在特定数据集上的表现，P-R曲线能够更直观地反映其性能。</li></ul><h1 id="15-结合你的学习和研究经历，探讨为什么在一些场景中要使用余弦相似度而不是欧氏距离"><a href="#15-结合你的学习和研究经历，探讨为什么在一些场景中要使用余弦相似度而不是欧氏距离" class="headerlink" title="15.结合你的学习和研究经历，探讨为什么在一些场景中要使用余弦相似度而不是欧氏距离"></a>15.结合你的学习和研究经历，探讨为什么在一些场景中要使用余弦相似度而不是欧氏距离</h1><ul><li>当一对文本相似度的长度差距很大、但内容相近时，如果采用词频或词向量作为特征，它们在特征空间中的欧式距离通常很大；而余弦相似度，它们之间的夹角可能很小，因而相似度更高。此外，在文本、图像、视频等领域，研究的对象的特征维度往往很高，余弦相似度在高维情况下依然保持“ 相同为1，正交是为0，相反时为-1”的性质，而欧式距离的数值则受维度的影响，范围不固定，并且含义也比较模糊。</li><li>在一些场景中，例如Word2Vec中，其向量的模长是经过归一化的，此时欧式距离与余弦距离有着单调的关系。此场景下余弦相似度和欧式距离的结果是相同的</li><li>欧式距离体现数值上的绝对差异，余弦距离体现方向上的相对差异。分析两个不同用户对于不同视频的偏好，更关注相对差异，显然应当用余弦距离。分析用户活跃度，以登陆次数和平均观看时长作为特征，余弦距离为认为(1,10)􏱤(10,100)两个用户距离很近；但显然这两个用户活跃度是有着极大的差异的，此时更关注数值绝对差异，应当使用欧式距离。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 面试 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 面试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数学基础知识整理</title>
      <link href="/2019/08/06/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/"/>
      <url>/2019/08/06/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>##线性代数</p><h4 id="线性相关与线性无关"><a href="#线性相关与线性无关" class="headerlink" title="线性相关与线性无关"></a>线性相关与线性无关</h4><ul><li><img src="/blog_picture/linear.jpg" alt="avatar"></li></ul><p>线性相关的判定：根据观察，利用定义即可判断。</p><p>线性相关的判断定理：</p><ul><li><p><img src="/blog_picture/2.jpg" alt="avatar"></p></li><li><p><img src="/blog_picture/3.jpg" alt="avatar"></p></li></ul><h4 id="矩阵的秩"><a href="#矩阵的秩" class="headerlink" title="矩阵的秩"></a>矩阵的秩</h4><p>一个向量组A的秩是A的线性无关的向量的个数</p><p>如果把一个向量组看成一个矩阵，则向量组的秩就是矩阵的秩</p><ul><li><p><img src="/blog_picture/4.jpg" alt="avatar"></p></li><li><p><img src="/blog_picture/5.jpg" alt="avatar"></p></li></ul><h4 id="向量的范数"><a href="#向量的范数" class="headerlink" title="向量的范数"></a>向量的范数</h4><ul><li><img src="/blog_picture/6.jpg" alt="avatar"></li></ul><p>####常用的向量范数</p><p>1-范数 $||x||<em>1=\sum</em>{i=1}^{n}|x|$</p><p>2-范数  $||x||<em>2=\sqrt{\sum</em>{i=1}^{n}{x_i}^2}$欧式范数</p><p>无穷范数   $||x||_n=max|x_i|$</p><h4 id="矩阵的范数"><a href="#矩阵的范数" class="headerlink" title="矩阵的范数"></a>矩阵的范数</h4><p><img src="/blog_picture/8.jpg" alt="avatar"></p><p>####常用的矩阵范数</p><p><img src="/blog_picture/9.jpg" alt="avatar"></p><p><img src="/blog_picture/10.jpg" alt="avatar"></p><p>范数的作用：机器学习的分类问题中，使用范数可以判断两个特征向量和矩阵的相似性</p><p>####矩阵的迹</p><p><img src="/blog_picture/11.jpg" alt="avatar"></p><h4 id="线性变换及其矩阵表示"><a href="#线性变换及其矩阵表示" class="headerlink" title="线性变换及其矩阵表示"></a>线性变换及其矩阵表示</h4><p><img src="/blog_picture/12.jpg" alt="avatar"></p><h4 id="特征值、特征向量"><a href="#特征值、特征向量" class="headerlink" title="特征值、特征向量"></a>特征值、特征向量</h4><p><img src="/blog_picture/13.jpg" alt="avatar"></p><p>####特征值的性质</p><p><img src="/blog_picture/14.jpg" alt="avatar"></p><p>####特征值和特征向量的求法</p><p><img src="/blog_picture/15.jpg" alt="avatar"></p><p><img src="/blog_picture/16.jpg" alt="avatar"></p><p>特征值和特征向量在机器学习中的应用：• 主成分分析• 流行学习• LDA</p><h4 id="正交投影"><a href="#正交投影" class="headerlink" title="正交投影"></a>正交投影</h4><p>在线性代数和泛函分析中，投影是从向量空间映射到自身的一种线性变换。具体来说，正交投影是指像空间U和零空间W相互正交子空间的投影</p><p>从解方程角度看，A x = b 可能无解，因为对任意 的 x , Ax 总是在A的列子空间里，若 向量 b 不在 列空间里，则方程无解。但是我们可以将 b 利用正 交投影矩阵投影到 A 的列子空间里得到正交投影 y， 然后求解A x = y，寻找一个最佳近似解 x。</p><p><img src="/blog_picture/17.jpg" alt="avatar"></p><h4 id="二次型"><a href="#二次型" class="headerlink" title="二次型"></a>二次型</h4><p><img src="/blog_picture/18.jpg" alt="avatar"></p><p><img src="/blog_picture/19.jpg" alt="avatar"></p><p>二次型补充知识点</p><p><img src="/blog_picture/20.jpg" alt="avatar"></p><h4 id="矩阵的QR分解"><a href="#矩阵的QR分解" class="headerlink" title="矩阵的QR分解"></a>矩阵的QR分解</h4><p><img src="/blog_picture/21.jpg" alt="avatar"></p><h4 id="SVD奇异值分解"><a href="#SVD奇异值分解" class="headerlink" title="SVD奇异值分解"></a>SVD奇异值分解</h4><p><img src="/blog_picture/22.jpg" alt="avatar"></p><p>##微积分</p><h4 id="集合的定义"><a href="#集合的定义" class="headerlink" title="集合的定义"></a>集合的定义</h4><p><img src="/blog_picture/23.jpg" alt="avatar"></p><p>####集合的表示方法</p><p><img src="/blog_picture/24.jpg" alt="avatar"></p><p>####集合的分类</p><p><img src="/blog_picture/25.jpg" alt="avatar"></p><h4 id="集合运算"><a href="#集合运算" class="headerlink" title="集合运算"></a>集合运算</h4><p><img src="/blog_picture/26.jpg" alt="avatar"></p><h4 id="Venn图"><a href="#Venn图" class="headerlink" title="Venn图"></a>Venn图</h4><p>表示集合的另一种形式</p><p><img src="/blog_picture/27.jpg" alt="avatar"></p><h4 id="函数定义"><a href="#函数定义" class="headerlink" title="函数定义"></a>函数定义</h4><p><img src="/blog_picture/28.jpg" alt="avatar"></p><h4 id="领域的定义"><a href="#领域的定义" class="headerlink" title="领域的定义"></a>领域的定义</h4><p><img src="/blog_picture/29.jpg" alt="avatar"></p><p>####函数的极限性质</p><h5 id="四则运算"><a href="#四则运算" class="headerlink" title="四则运算"></a>四则运算</h5><p><img src="/blog_picture/30.jpg" alt="avatar"></p><p>#####复合函数的极限</p><p><img src="/blog_picture/31.jpg" alt="avatar"></p><p>#####保号性</p><p><img src="/blog_picture/32.jpg" alt="avatar"></p><p>#####夹逼定理</p><p><img src="/blog_picture/33.jpg" alt="avatar"></p><p>#####洛必达法则</p><p><img src="/blog_picture/34.jpg" alt="avatar"></p><p>####函数的连续性</p><p><img src="/blog_picture/35.jpg" alt="avatar"></p><p>####间断的定义</p><p><img src="/blog_picture/36.jpg" alt="avatar"></p><p><img src="/blog_picture/37.jpg" alt="avatar"></p><h4 id="函数的导数"><a href="#函数的导数" class="headerlink" title="函数的导数"></a>函数的导数</h4><p><img src="/blog_picture/38.jpg" alt="avatar"></p><p>####导数的常用公式</p><p><img src="/blog_picture/39.jpg" alt="avatar"></p><h4 id="导数的性质"><a href="#导数的性质" class="headerlink" title="导数的性质"></a>导数的性质</h4><p>#####四则运算</p><p><img src="/blog_picture/40.jpg" alt="avatar"></p><p>#####复合函数求导</p><p><img src="/blog_picture/41.jpg" alt="avatar"></p><h5 id="导数作用"><a href="#导数作用" class="headerlink" title="导数作用"></a>导数作用</h5><p>链式求导法则:神经网络反向传播基础 </p><p>梯度下降法:最简单的优化方法</p><h4 id="函数的微分"><a href="#函数的微分" class="headerlink" title="函数的微分"></a>函数的微分</h4><p><img src="/blog_picture/42.jpg" alt="avatar"></p><h4 id="原函数"><a href="#原函数" class="headerlink" title="原函数"></a>原函数</h4><p><img src="/blog_picture/43.jpg" alt="avatar"></p><h4 id="不定积分"><a href="#不定积分" class="headerlink" title="不定积分"></a>不定积分</h4><p><img src="/blog_picture/44.jpg" alt="avatar"></p><p><img src="/blog_picture/45.jpg" alt="avatar"></p><p><img src="/blog_picture/46.jpg" alt="avatar"></p><p>####不定积分性质</p><p><img src="/blog_picture/47.jpg" alt="avatar"></p><p>####不定积分的基本公式</p><p><img src="/blog_picture/48.jpg" alt="avatar"></p><h4 id="定积分"><a href="#定积分" class="headerlink" title="定积分"></a>定积分</h4><p><img src="/blog_picture/49.jpg" alt="avatar"></p><p><img src="/blog_picture/50.jpg" alt="avatar"></p><p><img src="/blog_picture/51.jpg" alt="avatar"></p><p><img src="/blog_picture/52.jpg" alt="avatar"></p><p><img src="/blog_picture/53.jpg" alt="avatar"></p><p>####定积分的性质</p><p><img src="/blog_picture/54.jpg" alt="avatar"></p><p>牛顿-莱布尼兹公式</p><p><img src="/blog_picture/55.jpg" alt="avatar"></p><p><img src="/blog_picture/56.jpg" alt="avatar"></p><p>####二重积分</p><p><img src="/blog_picture/57.jpg" alt="avatar"></p><p><img src="/blog_picture/58.jpg" alt="avatar"></p><p><img src="/blog_picture/59.jpg" alt="avatar"></p><p><img src="/blog_picture/60.jpg" alt="avatar"></p><p>####导数</p><p><img src="/blog_picture/61.jpg" alt="avatar"></p><p>#####标量关于标量X的求导</p><p><img src="/blog_picture/62.jpg" alt="avatar"></p><p>#####向量关于标量X的求导</p><p><img src="/blog_picture/63.jpg" alt="avatar"></p><p>####矩阵关于标量X的求导</p><p><img src="/blog_picture/64.jpg" alt="avatar"></p><p>####标量关于向量x的导数</p><p><img src="/blog_picture/65.jpg" alt="avatar"></p><p>####向量关于向量x的导数</p><p><img src="/blog_picture/66.jpg" alt="avatar"></p><p>####矩阵关于向量 x 的导数</p><p><img src="/blog_picture/67.jpg" alt="avatar"></p><p>####标量关于矩阵的导数</p><p><img src="/blog_picture/68.jpg" alt="avatar"></p><p>####向量关于矩阵的导数</p><p><img src="/blog_picture/69.jpg" alt="avatar"></p><h4 id="矩阵关于矩阵的导数"><a href="#矩阵关于矩阵的导数" class="headerlink" title="矩阵关于矩阵的导数"></a>矩阵关于矩阵的导数</h4><p><img src="/blog_picture/70.jpg" alt="avatar"></p><p>####分子布局法与分母局部法区别</p><p><img src="/blog_picture/71.jpg" alt="avatar"></p><p>####Hessian矩阵</p><p><img src="/blog_picture/72.jpg" alt="avatar"></p><p><img src="/blog_picture/73.jpg" alt="avatar"></p><p>##概率论基础</p><p>####概率论基础</p><p>概率论与数理统计是研究什么的?</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">随机现象:不确定性与统计规律性 </span><br><span class="line">概率论:从数量上研究随机现象的统计规律性的科学</span><br><span class="line">数理统计:从应用角度研究处理随机性数据，建立有效的统计方法，进行统计推理</span><br></pre></td></tr></table></figure><p>随机试验</p><p>在概率论中，将具有下述三个特点的试验称为<strong>随机试验</strong>，简称试验。 随机试验常用E表示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.试验的可重复性 —— 在相同条件下可重复进行;</span><br><span class="line">2.一次试验结果的随机性 —— 一次试验的可能结果不止一个，且试验之前无法确定具体是哪种结果出现; </span><br><span class="line">3.全部试验结果的可知性 —— 所有可能的结果是预先可知的，且每次试验有且仅有一个结果出现。</span><br></pre></td></tr></table></figure><p>####样本空间与样本点</p><p><img src="/blog_picture/74.jpg" alt="avatar"></p><p>####随机事件</p><p><img src="/blog_picture/75.jpg" alt="avatar"></p><p>####事件的性质与运算</p><p>事件的本质是集合，集合的一切性质和运算都适用与事件</p><p>####频率与概率</p><p><img src="/blog_picture/76.jpg" alt="avatar"></p><h4 id="概率的性质"><a href="#概率的性质" class="headerlink" title="概率的性质"></a>概率的性质</h4><p><img src="/blog_picture/77.jpg" alt="avatar"></p><h4 id="古典概型"><a href="#古典概型" class="headerlink" title="古典概型"></a>古典概型</h4><p><img src="/blog_picture/78.jpg" alt="avatar"></p><p><img src="/blog_picture/79.jpg" alt="avatar"></p><h4 id="几何概型"><a href="#几何概型" class="headerlink" title="几何概型"></a>几何概型</h4><p><img src="/blog_picture/80.jpg" alt="avatar"></p><p><img src="/blog_picture/1565069627088.jpg" alt="avatar"></p><h4 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h4><p><img src="/blog_picture/1565069696978.jpg" alt="avatar"></p><p><img src="/blog_picture/1565069726956.jpg" alt="avatar"></p><p>####条件概率的几何意义</p><p><img src="/blog_picture/1565069854934.jpg" alt="avatar"></p><p>####加法公式</p><p><img src="/blog_picture/1565069969698.jpg" alt="avatar"></p><p>####乘法公式</p><p><img src="/blog_picture/1565070039368.jpg" alt="avatar"></p><p>####排列组合</p><p><img src="/blog_picture/1565070090722.jpg" alt="avatar"></p><p>####全概率公式</p><p><img src="/blog_picture/1565070149616.jpg" alt="avatar"></p><p>####离散分布 vs 连续分布</p><p><img src="/blog_picture/1565070338442.jpg" alt="avatar"></p><p>####伯努利分布</p><p><img src="/blog_picture/1565070378085.jpg" alt="avatar"></p><p>####二项分布</p><p><img src="/blog_picture/1565070413168.jpg" alt="avatar"></p><p>####期望</p><p><img src="/blog_picture/1565070571975.jpg" alt="avatar"></p><h5 id="期望的性质"><a href="#期望的性质" class="headerlink" title="期望的性质"></a>期望的性质</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">1、E (C ) = C</span><br><span class="line">2、E (aX ) = a E (X )</span><br><span class="line">3、E (X + Y ) = E (X ) + E (Y )</span><br><span class="line">4、当X ,Y 相互独立时，E (X Y ) = E (X )E (Y )</span><br></pre></td></tr></table></figure><h5 id="期望的数学含义"><a href="#期望的数学含义" class="headerlink" title="期望的数学含义"></a>期望的数学含义</h5><p>反应了数据的平均取值情况</p><p>####方差</p><p><img src="/blog_picture/1565070731625.jpg" alt="avatar"></p><p><img src="/blog_picture/1565070796641.jpg" alt="avatar"></p><p>####数据归一化</p><p><img src="/blog_picture/1565070856261.jpg" alt="avatar"></p><p>####高斯分布</p><p><img src="/blog_picture/1565070907312.jpg" alt="avatar"></p><p><img src="/blog_picture/1565070933026.jpg" alt="avatar"></p><p>####分布函数</p><p><img src="/blog_picture/1565071005463.jpg" alt="avatar"></p><p><img src="/blog_picture/1565071040441.jpg" alt="avatar"></p><p>####均匀分布</p><p><img src="/blog_picture/1565071087634.jpg" alt="avatar"></p><p><img src="/blog_picture/1565071126393.jpg" alt="avatar"></p><p>####指数分布</p><p><img src="/blog_picture/1565071160548.jpg" alt="avatar"></p><p><img src="/blog_picture/1565071258034.jpg" alt="avatar"></p><p>####二维随机变量</p><p><img src="/blog_picture/1565071336855.jpg" alt="avatar"></p><p>####联合分布函数</p><p><img src="/blog_picture/1565071371964.jpg" alt="avatar"></p><p><img src="/blog_picture/1565071397963.jpg" alt="avatar"></p><p>####联合分布列</p><p><img src="/blog_picture/1565071449242.jpg" alt="avatar"></p><p>####二维连续型随机变量及其密度函数</p><p><img src="/blog_picture/1565071492637.jpg" alt="avatar"></p><p>####联合密度性质</p><p><img src="/blog_picture/1565071532659.jpg" alt="avatar"></p><p><img src="/blog_picture/1565071741870.jpg" alt="avatar"></p><p><img src="/blog_picture/1565071745834.jpg" alt="avatar"></p><p><img src="/blog_picture/1565071749581.jpg" alt="avatar"></p><p>####边缘分布</p><p><img src="/blog_picture/1565071827339.jpg" alt="avatar"></p><p><img src="/blog_picture/1565071830825.jpg" alt="avatar"></p><p><img src="/blog_picture/1565072132127.jpg" alt="avatar"></p><p><img src="/blog_picture/1565072287072.jpg" alt="avatar"></p><p>####多维分布</p><p>在机器学习中，一个 样本有多个特征，研究多个特征的概率分布与统计情况</p><p>####二维随机变量</p><p><img src="/blog_picture/1565072474902.jpg" alt="avatar"></p><p>####为什么需要协方差?</p><p><img src="/blog_picture/1565072501064.jpg" alt="avatar"></p><p>####协方差</p><p><img src="/blog_picture/1565072546171.jpg" alt="avatar"></p><p>####协方差的性质</p><p><img src="/blog_picture/1565072627923.jpg" alt="avatar"></p><p>####协方差矩阵</p><p><img src="/blog_picture/1565072672838.jpg" alt="avatar"></p><h4 id="主成分分析法"><a href="#主成分分析法" class="headerlink" title="主成分分析法"></a>主成分分析法</h4><p>#####PCA的意义</p><p><img src="/blog_picture/1565072734381.jpg" alt="avatar"></p><p>#####PCA的数学模型</p><p><img src="/blog_picture/1565072773199.jpg" alt="avatar"></p><p>####PCA推导</p><p><img src="/blog_picture/1565072881626.jpg" alt="avatar"></p><p><img src="/blog_picture/1565072885469.jpg" alt="avatar"></p><p><img src="/blog_picture/1565072888797.jpg" alt="avatar"></p><p>####PCA实施</p><p><img src="/blog_picture/1565072955696.jpg" alt="avatar"></p><p><img src="/blog_picture/1565072959543.jpg" alt="avatar"></p><h2 id="概率论与信息论"><a href="#概率论与信息论" class="headerlink" title="概率论与信息论"></a>概率论与信息论</h2><p>####切比雪夫不等式</p><p><img src="/blog_picture/1565073174283.jpg" alt="avatar"></p><p>####中心极限定理</p><p><img src="/blog_picture/1565073177541.jpg" alt="avatar"></p><p><img src="/blog_picture/1565073180613.jpg" alt="avatar"></p><p>####关于正态分布计算的补充</p><p><img src="/blog_picture/1565073253138.jpg" alt="avatar"></p><p>####矩的概念</p><p><img src="/blog_picture/1565073324278.jpg" alt="avatar"></p><p><img src="/blog_picture/1565073327543.jpg" alt="avatar"></p><p>####矩估计</p><p><img src="/blog_picture/1565073407133.jpg" alt="avatar"></p><p><img src="/blog_picture/1565073410261.jpg" alt="avatar"></p><p>####极大似然估计的思想</p><p><img src="/blog_picture/1565073555795.jpg" alt="avatar"></p><p>####极大似然估计</p><p><img src="/blog_picture/1565073575910.jpg" alt="avatar"></p><p><img src="/blog_picture/1565073579745.jpg" alt="avatar"></p><p>####极大似然估计求法</p><p><img src="/blog_picture/1565073673751.jpg" alt="avatar"></p><p><img src="/blog_picture/1565073677603.jpg" alt="avatar"></p><p><img src="/blog_picture/1565073681686.jpg" alt="avatar"></p><h4 id="MLE在机器学习中的应用"><a href="#MLE在机器学习中的应用" class="headerlink" title="MLE在机器学习中的应用"></a>MLE在机器学习中的应用</h4><p>参数估计 逻辑回归的参数估计</p><h4 id="最大后验估MAP"><a href="#最大后验估MAP" class="headerlink" title="最大后验估MAP"></a>最大后验估MAP</h4><p>####先验信息</p><p><img src="/blog_picture/1565073949228.jpg" alt="avatar"></p><p>####先验分布</p><p><img src="/blog_picture/1565073979451.jpg" alt="avatar"></p><p>####如何利用先验信息?</p><p>在样本少的情况下，如何 加入先验信息? 后验概率</p><p>####后验概率</p><p><img src="/blog_picture/1565074058577.jpg" alt="avatar"></p><p>####最大后验估计</p><p><img src="/blog_picture/1565074072524.jpg" alt="avatar"></p><p>####贝叶斯法则</p><p><img src="/blog_picture/1565074197261.jpg" alt="avatar"></p><p><img src="/blog_picture/1565074201110.jpg" alt="avatar"></p><p><img src="/blog_picture/1565074204451.jpg" alt="avatar"></p><p>####贝叶斯意义</p><p><img src="/blog_picture/1565074264162.jpg" alt="avatar"></p><p>####贝叶斯公式的密度函数形式</p><p><img src="/blog_picture/1565074294912.jpg" alt="avatar"></p><p><img src="/blog_picture/1565074306035.jpg" alt="avatar"></p><p>####共轭分布</p><p><img src="/blog_picture/1565074406347.jpg" alt="avatar"></p><p><img src="/blog_picture/1565074410909.jpg" alt="avatar"></p><p><img src="/blog_picture/1565074415028.jpg" alt="avatar"></p><p>####如何度量信息的多少?</p><p><img src="/blog_picture/1565074557282.jpg" alt="avatar"></p><p>####自信息量</p><p><img src="/blog_picture/1565074568602.jpg" alt="avatar"></p><p>####信息熵</p><p><img src="/blog_picture/1565074580286.jpg" alt="avatar"></p><p><img src="/blog_picture/1565074642251.jpg" alt="avatar"></p><p>####交叉熵</p><p><img src="/blog_picture/1565074679965.jpg" alt="avatar"></p><p>####交叉熵在机器学习中的应用</p><p>交叉熵损失函数   衡量两个随机变量之间的相似度</p><p>####互信息</p><p><img src="/blog_picture/1565074818018.jpg" alt="avatar"></p><p>####KL散度</p><p><img src="/blog_picture/1565074821449.jpg" alt="avatar"></p><p>####KL散度的性质</p><p><img src="/blog_picture/1565074832615.jpg" alt="avatar"></p><p><img src="/blog_picture/1565074846596.jpg" alt="avatar"></p><p>##优化方法</p><h4 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h4><p>所谓最优化问题，指在某些约束条件下，决定某些可选择的变量应该取何值，使所选定的目标函数达到最优的问题。即运用最新科技手段和处理方法，使系统达到总体最优，从而为系统提出 设计、施工、管理、运行的最优方案。</p><p>为什么要用优化算法?</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">求导找函数的最小(大)值不行吗?</span><br><span class="line">考虑:1、多元函数</span><br><span class="line">2、局部最大最小值</span><br></pre></td></tr></table></figure><p>####线性规划</p><p><img src="/blog_picture/1565076518810.jpg" alt="avatar"></p><p><img src="/blog_picture/1565076541128.jpg" alt="avatar"></p><h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><p><img src="/blog_picture/1565076888078.jpg" alt="avatar"></p><h4 id="一维函数梯度"><a href="#一维函数梯度" class="headerlink" title="一维函数梯度"></a>一维函数梯度</h4><p><img src="/blog_picture/1565077155760.jpg" alt="avatar"></p><p>####梯度下降法</p><p><img src="/blog_picture/1565077206152.jpg" alt="avatar"></p><p><img src="/blog_picture/1565077232963.jpg" alt="avatar"></p><p><img src="/blog_picture/1565077254742.jpg" alt="avatar"></p><p>####梯度法的迭代过程</p><p><img src="/blog_picture/1565077296818.jpg" alt="avatar"></p><p>####批量梯度下降BGD</p><p><img src="/blog_picture/1565077637320.jpg" alt="avatar"></p><p><img src="/blog_picture/1565077674463.jpg" alt="avatar"></p><p>####随机梯度下降SGD</p><p><img src="/blog_picture/1565078179419.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078182517.jpg" alt="avatar"></p><p>####小批量梯度下降法MBGD</p><p><img src="/blog_picture/1565078253357.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078257221.jpg" alt="avatar"></p><h4 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h4><p>求解无约束极值问题得最古老算法之一，已发展成为一类算法:Newton型方法。<br>在局部，用一个二次函数近似代替目标函数 f(x)，然后用近似函数的极小 点作为f(x) 的近似极小点。</p><p><img src="/blog_picture/1565078413953.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078417533.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078421199.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078424695.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078428776.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078432769.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078436246.jpg" alt="avatar"></p><h4 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h4><p>拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用 正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。 拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化， 构造一个目标函数的模型使之足以产生超线性收敛性。 这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的 信息，所以有时比牛顿法更为有效。</p><p><strong>用不包含二阶导数的矩阵近似<em>Hesse*</em></strong>矩阵的*</p><p><img src="/blog_picture/1565078600068.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078603177.jpg" alt="avatar"></p><p>####常用的拟牛顿法</p><p><img src="/blog_picture/1565078654456.jpg" alt="avatar"></p><h4 id="共轭方向法"><a href="#共轭方向法" class="headerlink" title="共轭方向法"></a>共轭方向法</h4><p><strong>共轭方向法</strong>是介于最速下降法与牛顿法之间的一类方法。</p><p>它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了存储和 计算牛顿法所需要的二阶导数信息。</p><p><img src="/blog_picture/1565078732264.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078735596.jpg" alt="avatar"></p><p>####共轭方向法的几何意义</p><p><img src="/blog_picture/1565078861764.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078864870.jpg" alt="avatar"></p><p><img src="/blog_picture/1565078867892.jpg" alt="avatar"></p><h4 id="共轭梯度法"><a href="#共轭梯度法" class="headerlink" title="共轭梯度法"></a>共轭梯度法</h4><p>⚫ <strong>共轭梯度法</strong>(conjugate gradient method, CG)是以共轭方向(conjugate direction)作为 搜索方向的一类算法。</p><p>⚫ CG法是由Hesteness和Stiefel于1952年为求解线性方程组而提出的。后来用于求解无约束最优 化问题，它是一种重要的数学优化方法。这种方法具有<strong>二次终止性</strong></p><p>CG的基本思想是把共轭性与最速下降法相结合，利用已知迭代点的梯度方向 构造一组共轭方向，并沿着此组方向进行搜索，求出目标函数的极小点。</p><p><strong>什么是二次终止性?</strong></p><p>如果某算法用于求解目标函数为二次函数的无约束问题时，只需要经过有限迭代就能 达到最优解，则该算法具有二次终止性。<br>共轭梯度法就有二次终止性</p><p><img src="/blog_picture/1565078980017.jpg" alt="avatar"></p><p><img src="/blog_picture/1565079002662.jpg" alt="avatar"></p><p><img src="/blog_picture/1565079045462.jpg" alt="avatar"></p><p><img src="/blog_picture/1565079049180.jpg" alt="avatar"></p><p>####动量梯度下降法法Momentum</p><p><img src="/blog_picture/1565079107750.jpg" alt="avatar"></p><p><img src="/blog_picture/1565079110804.jpg" alt="avatar"></p><p>####均方根优化法RMSp</p><p><img src="/blog_picture/1565079198345.jpg" alt="avatar"></p><p><img src="/blog_picture/1565079201526.jpg" alt="avatar"></p><p>####自适应矩估计法Adam</p><p><img src="/blog_picture/1565079255381.jpg" alt="avatar"></p><p><img src="/blog_picture/1565079258315.jpg" alt="avatar"></p><p>####学习率衰减</p><p><img src="/blog_picture/1565079349701.jpg" alt="avatar"></p><p><img src="/blog_picture/1565079353025.jpg" alt="avatar"></p><p>####早停</p><p><img src="/blog_picture/1565079408798.jpg" alt="avatar"></p><p><strong>核心思想:</strong></p><p>如果训练数轮后准确率(损失函数)没有上升(下降)，就停止训练</p><p><strong>应用场景:</strong></p><p>大批量数据，训练时间长</p><p>####局部最优值</p><p><img src="/blog_picture/1565079488243.jpg" alt="avatar"></p><p>####鞍点问题</p><p><img src="/blog_picture/1565079501278.jpg" alt="avatar"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性代数 </tag>
            
            <tag> 优化方法 </tag>
            
            <tag> 微积分 </tag>
            
            <tag> 概率 </tag>
            
            <tag> 统计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据基础</title>
      <link href="/2019/07/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80/"/>
      <url>/2019/07/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h1 id="pyspark-RDD基础"><a href="#pyspark-RDD基础" class="headerlink" title="pyspark-RDD基础"></a>pyspark-RDD基础</h1><h3 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyspark是spark的python API,允许python调用spark编程模型</span><br></pre></td></tr></table></figure><h3 id="初始化spark"><a href="#初始化spark" class="headerlink" title="初始化spark"></a>初始化spark</h3><h4 id="SparkContext"><a href="#SparkContext" class="headerlink" title="SparkContext"></a>SparkContext</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line">sc = SparkContext(master=<span class="string">'local[2]'</span>)</span><br></pre></td></tr></table></figure><h4 id="核查SparkContext"><a href="#核查SparkContext" class="headerlink" title="核查SparkContext"></a>核查SparkContext</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sc.version获取SparkContext的版本</span><br><span class="line">sc.pythonVer获取python版本</span><br><span class="line">sc.master要连接的Master URL</span><br><span class="line">str(sc.sparkHome)spark工作节点的安装路径</span><br><span class="line">str(sc.sparkUser())获取SparkContext的spark用户名</span><br><span class="line">sc.appName返回应用名称</span><br><span class="line">sc.applicationId返回应用程序ID</span><br><span class="line">sc.defaultParallelism返回默认并行级别</span><br><span class="line">sc.defaultMinPatitionsRDD默认最小分区数</span><br></pre></td></tr></table></figure><h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf,SparkContext</span><br><span class="line">conf = (SparkConf().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"my APP"</span>).set(<span class="string">"spark.executor.memory"</span>,<span class="string">"1g"</span>))</span><br><span class="line">sc = SparkContext(conf=conf)</span><br></pre></td></tr></table></figure><h4 id="使用shell"><a href="#使用shell" class="headerlink" title="使用shell"></a>使用shell</h4><p>pyspark shell已经为SparkContext创建了名为sc的变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-shell --master local[2]</span><br><span class="line">./bin/pyspark --master local[4] --py-files code.py</span><br></pre></td></tr></table></figure><p>用—master参数设定Context连接到哪个Master服务器，通过传递逗号分隔列表至—py-files添加Python.zip、egg或.py文件到Runtime路径</p><h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><h4 id="并行集合"><a href="#并行集合" class="headerlink" title="并行集合"></a>并行集合</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">'a'</span>,<span class="number">7</span>),(<span class="string">'a'</span>,<span class="number">2</span>),(<span class="string">'b'</span>,<span class="number">2</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">'a'</span>,<span class="number">2</span>),(<span class="string">'d'</span>,<span class="number">1</span>),(<span class="string">'b'</span>,<span class="number">1</span>)])</span><br><span class="line">rdd3 = sc.parallelize(range(<span class="number">100</span>))</span><br><span class="line">rdd4 = sc.parallelize([(<span class="string">"a"</span>,[<span class="string">"x"</span>,<span class="string">"y"</span>,<span class="string">"z"</span>]),(<span class="string">"b"</span>,[<span class="string">"p"</span>,<span class="string">"r"</span>])])</span><br></pre></td></tr></table></figure><h4 id="外部数据"><a href="#外部数据" class="headerlink" title="外部数据"></a>外部数据</h4><p>使用textFile()函数从HDFS、本地文件或其它支持hadoop的文件系统里读取文件，或使用wholeTextFiles()函数读取目录下所有文本文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">textFile = sc.textFile(<span class="string">'a.txt'</span>)</span><br><span class="line">textFile2 = sc.wholeTextFiles(/aa)</span><br></pre></td></tr></table></figure><h3 id="提取RDD信息"><a href="#提取RDD信息" class="headerlink" title="提取RDD信息"></a>提取RDD信息</h3><h4 id="基础信息"><a href="#基础信息" class="headerlink" title="基础信息"></a>基础信息</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">rdd.getNumPatitions()列出分区数</span><br><span class="line">rdd.count()计算RDD的实例数量</span><br><span class="line">rdd.countByKey()按键计算RDD实例数量</span><br><span class="line">defaultdict(&lt;type <span class="string">'int'</span>&gt;,(<span class="string">'a'</span>:<span class="number">2</span>,<span class="string">'b'</span>:<span class="number">1</span>))</span><br><span class="line">rdd.countByValue()按值计算RDD实例数量</span><br><span class="line">defaultdict(&lt;type <span class="string">'int'</span>&gt;,((<span class="string">'b'</span>,<span class="number">2</span>):<span class="number">1</span>,(<span class="string">'a'</span>,<span class="number">2</span>):<span class="number">1</span>,(<span class="string">'a'</span>,<span class="number">7</span>):<span class="number">1</span>))</span><br><span class="line">rdd.collectAsMap()以字典的形式返回键值</span><br><span class="line">(<span class="string">'a'</span>:<span class="number">2</span>,<span class="string">'b'</span>:<span class="number">2</span>)</span><br><span class="line">rdd.sum()汇总RDD元素</span><br><span class="line"><span class="number">4959</span></span><br><span class="line">sc.parallelize([]).isEmpty()检查RDD是否为空</span><br></pre></td></tr></table></figure><h4 id="汇总"><a href="#汇总" class="headerlink" title="汇总"></a>汇总</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rdd.max()RDD元素的最大值</span><br><span class="line">rdd.min()RDD元素的最小值</span><br><span class="line">rdd.mean()RDD元素的平均值</span><br><span class="line">rdd.stdev()RDD元素的标准差</span><br><span class="line">rdd.variance()RDD元素的方差</span><br><span class="line">rdd.histogram(<span class="number">3</span>)分箱（bin）生成直方图</span><br><span class="line">rdd.stats()综合统计包括：计数、平均值、标准差、最大值和最小值</span><br></pre></td></tr></table></figure><h4 id="应用函数"><a href="#应用函数" class="headerlink" title="应用函数"></a>应用函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd.map(<span class="keyword">lambda</span> x:x+(x[<span class="number">1</span>],x[<span class="number">0</span>])).collect()对每个RDD元素执行函数</span><br><span class="line">rdd.flatMap(<span class="keyword">lambda</span> x:x+(x[<span class="number">1</span>],x[<span class="number">0</span>]))对每个RDD元素执行函数，并拉平结果</span><br><span class="line">rdd.collect()</span><br><span class="line">rdd.flatMapValues(<span class="keyword">lambda</span> x:x).collect()不改变键，对rdd的每个键值对执行flatMap函数</span><br></pre></td></tr></table></figure><h4 id="选择数据"><a href="#选择数据" class="headerlink" title="选择数据"></a>选择数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">获取</span><br><span class="line">rdd.collect()返回包含所以RDD元素的列表</span><br><span class="line">rdd.take(<span class="number">4</span>)提取前<span class="number">4</span>个RDD元素</span><br><span class="line">rdd.first()提取第一个RDD元素</span><br><span class="line">rdd.top(<span class="number">2</span>)提取前两个RDD元素</span><br><span class="line">抽样</span><br><span class="line">rdd.sample(<span class="literal">False</span>,<span class="number">0.15</span>,<span class="number">81</span>)返回RDD的采样子集</span><br><span class="line">筛选</span><br><span class="line">rdd.filter(<span class="keyword">lambda</span> x:<span class="string">'a'</span> <span class="keyword">in</span> x)筛选RDD</span><br><span class="line">rdd.distinct()返回RDD里的唯一值</span><br><span class="line">rdd.keys()返回RDD键值对里的键</span><br></pre></td></tr></table></figure><h4 id="迭代"><a href="#迭代" class="headerlink" title="迭代"></a>迭代</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">(x)</span>:</span>print(x)     </span><br><span class="line">rdd.foreach(g)</span><br></pre></td></tr></table></figure><h4 id="改变数据形状"><a href="#改变数据形状" class="headerlink" title="改变数据形状"></a>改变数据形状</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">规约</span><br><span class="line">rdd.reduceByKey(<span class="keyword">lambda</span> x,y:x+y)合并每个键的值</span><br><span class="line">rdd.reduce(<span class="keyword">lambda</span> x,y:x+y)合并RDD的值</span><br><span class="line">分组</span><br><span class="line">rdd.groupBy(<span class="keyword">lambda</span> x:x%<span class="number">2</span>).mapValues(list)返回RDD的分组值</span><br><span class="line">rdd.groupByKey().mapValues(list)按键分组RDD</span><br><span class="line">集合</span><br><span class="line">seqOp = (<span class="keyword">lambda</span> x,y:(x[<span class="number">0</span>]+y,x[<span class="number">1</span>]+<span class="number">1</span>))</span><br><span class="line">combOP = (<span class="keyword">lambda</span> x,y:(x[<span class="number">0</span>]+y[<span class="number">0</span>],x[<span class="number">1</span>]+y[<span class="number">1</span>]))</span><br><span class="line">rdd.aggregate((<span class="number">0</span>,<span class="number">0</span>),seqOp,combOP) 汇总每个分区里的RDD元素，并输出结果</span><br><span class="line">rdd.aggregeteByKey((<span class="number">0</span>,<span class="number">0</span>),seqOp,combOP)汇总每个RDD的键的值</span><br><span class="line">rdd.fold(<span class="number">0</span>,add)汇总每个分区里的RDD元素，并输出结果</span><br><span class="line">rdd.foldByKey(<span class="number">0</span>,add)合并每个键的值</span><br><span class="line">rdd,keyBy(<span class="keyword">lambda</span> x:x+x)通过执行函数，创建RDD元素的元组</span><br></pre></td></tr></table></figure><h4 id="数学运算"><a href="#数学运算" class="headerlink" title="数学运算"></a>数学运算</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd.subtract(rdd2)返回RDD2里没有匹配键的rdd的兼职对</span><br><span class="line">rdd2.subtractByKey(rdd)返回rdd2里的每个（键、值）对，rdd中，没有匹配的键</span><br><span class="line">rdd.cartesian(rdd2)返回rdd和rdd2的笛卡尔积</span><br></pre></td></tr></table></figure><h4 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd.sortBy(lambda x:x[1])按给定函数排序RDD</span><br><span class="line">rdd.sortByKey()按键排序RDD的键值对</span><br></pre></td></tr></table></figure><h4 id="重分区"><a href="#重分区" class="headerlink" title="重分区"></a>重分区</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd.repartition(4)新建一个含4个分区的RDD</span><br><span class="line">rdd.coalesce(1)将RDD中的分区数缩减为1个</span><br></pre></td></tr></table></figure><h4 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd.saveAsTextFile(&quot;rdd.txt&quot;)</span><br><span class="line">rdd.saveAsHadoopFile(&quot;hdfs://namenodehost/parent/child&quot;,&apos;org.apache.hadoop.mapred.TextOutputFormat&apos;)</span><br></pre></td></tr></table></figure><h4 id="终止SparkContext"><a href="#终止SparkContext" class="headerlink" title="终止SparkContext"></a>终止SparkContext</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.stop()</span><br></pre></td></tr></table></figure><h4 id="执行程序"><a href="#执行程序" class="headerlink" title="执行程序"></a>执行程序</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit examples/src/main/python/pi.py</span><br></pre></td></tr></table></figure><h1 id="Pyspark-sql"><a href="#Pyspark-sql" class="headerlink" title="Pyspark_sql"></a>Pyspark_sql</h1><h4 id="Pyspark与Spark-SQL"><a href="#Pyspark与Spark-SQL" class="headerlink" title="Pyspark与Spark SQL"></a>Pyspark与Spark SQL</h4><p>Spark SQL是Apache Spark处理结构化数据的模块</p><h4 id="初始化SparkSession"><a href="#初始化SparkSession" class="headerlink" title="初始化SparkSession"></a>初始化SparkSession</h4><p>SparkSession用于创建数据框，将数据框注册为表，执行SQL查询，缓存表及读取Parquet文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">spark = SparkSession.builder.appName(&quot;my app&quot;).config(&quot;spark.some.config.option&quot;,&quot;some-value&quot;).getOrCreate()</span><br></pre></td></tr></table></figure><h4 id="创建数据框"><a href="#创建数据框" class="headerlink" title="创建数据框"></a>创建数据框</h4><h5 id="从RDD创建"><a href="#从RDD创建" class="headerlink" title="从RDD创建"></a>从RDD创建</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql.types import *</span><br><span class="line">推断Schema</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">lines = sc.textFile(&quot;people.txt&quot;)</span><br><span class="line">parts = lines.map(lambda l:l.split(&quot;,&quot;))</span><br><span class="line">people = parts.map(lambda p:Row(name=p[0],age=int(p[1])))</span><br><span class="line">peopledf = spark.createDataFrame(people)</span><br><span class="line">指定Schema</span><br><span class="line">people = parts.map(lambda p:Row(name=p[0],age=int(p[1].strip())))</span><br><span class="line">schemaString = &quot;name age&quot;</span><br><span class="line">fields = [StructField(field_name,StringType(),True) for field_name in schemaString.split()]</span><br><span class="line">schema = StructType(fields)</span><br><span class="line">spark.createDataFrame(people,schema).show()</span><br></pre></td></tr></table></figure><h5 id="从spark数据源创建"><a href="#从spark数据源创建" class="headerlink" title="从spark数据源创建"></a>从spark数据源创建</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">json</span><br><span class="line">df = spark.read.json(<span class="string">"customer.json"</span>)</span><br><span class="line">df.show()</span><br><span class="line">df2 = spark.read.load(<span class="string">"people.json"</span>,format = <span class="string">"json"</span>)</span><br><span class="line">Parquet文件</span><br><span class="line">df3 = spark.read.load(<span class="string">"users.parquet"</span>)</span><br><span class="line">文本文件</span><br><span class="line">df4 = spark.read.text(<span class="string">"people.txt"</span>)</span><br></pre></td></tr></table></figure><h5 id="查阅数据信息"><a href="#查阅数据信息" class="headerlink" title="查阅数据信息"></a>查阅数据信息</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">df.dtypes返回df的列名与数据类型</span><br><span class="line">df.show()显示df内容</span><br><span class="line">df.head()返回前n行数据</span><br><span class="line">df.first()返回第一行数据</span><br><span class="line">df.take(2)返回前两行数据</span><br><span class="line">df.schema返回df的schema</span><br><span class="line">df.describe().show()汇总统计数据</span><br><span class="line">df.columns返回df列名</span><br><span class="line">df.count()返回df的行数</span><br><span class="line">df.distinct().count()返回df中不重复的行数</span><br><span class="line">df.printSchema()返回df的Schema</span><br><span class="line">df.explain()返回逻辑与实体方案</span><br></pre></td></tr></table></figure><h5 id="重复值"><a href="#重复值" class="headerlink" title="重复值"></a>重复值</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = df.dropDuplicates()</span><br></pre></td></tr></table></figure><h5 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line">Select</span><br><span class="line">df.select(<span class="string">"firstName"</span>).show()显示firstName列的所有条目</span><br><span class="line">df.select(<span class="string">"firstName"</span>,<span class="string">"lastName"</span>.show())</span><br><span class="line">df.select(<span class="string">"firstName"</span>,<span class="string">"age"</span>,\</span><br><span class="line">          explode(<span class="string">"phoneNumber"</span>)\显示firstName、age的所有条目和类型</span><br><span class="line">          .alias(<span class="string">"contactInfo"</span>))\</span><br><span class="line">.select(<span class="string">"ContactInfo.type"</span>,<span class="string">"firstName"</span>,<span class="string">"age"</span>)</span><br><span class="line">df.select(df[<span class="string">"firstName"</span>],df[<span class="string">"age"</span>]+<span class="number">1</span>).show()显示firstName和age列的所有记录添加</span><br><span class="line">df.select(df[<span class="string">"age"</span>]&gt;<span class="number">24</span>).show()显示所有小于<span class="number">24</span>的记录</span><br><span class="line">When</span><br><span class="line">df.select(<span class="string">"firstName"</span>,F.when(df.age&gt;<span class="number">30</span>,<span class="number">1</span>))\显示firstName，且大于<span class="number">30</span>岁显示<span class="number">1</span>，小于<span class="number">30</span>显示<span class="number">0</span></span><br><span class="line">.otherwise(<span class="number">0</span>).show()</span><br><span class="line">df[df.firstName.isin(<span class="string">"Jane"</span>,<span class="string">"Boris"</span>)].collect()显示符合特定条件的firstName列的记录</span><br><span class="line">Like</span><br><span class="line">df.select(<span class="string">"firstName"</span>,df.lastName,\显示lastName列中包含Smith的firstName列的记录</span><br><span class="line">          like(<span class="string">"Smith"</span>)).show()</span><br><span class="line">Startswith-Endwith</span><br><span class="line">df.select(<span class="string">"firstName"</span>,df.lastName.\显示lastName列中以Sm开头的firstName列的记录</span><br><span class="line">          startswith(<span class="string">"Sm"</span>)).show()</span><br><span class="line">df.select(df.lastName.endswith(<span class="string">"th"</span>)).show()显示以th结尾的lastName</span><br><span class="line">Substring</span><br><span class="line">df.select(df.firstName.substr(<span class="number">1</span>,<span class="number">3</span>).alias(<span class="string">"name"</span>))返回firstName的子字符串</span><br><span class="line">Between</span><br><span class="line">df.select(df.age.between(<span class="number">22</span>,<span class="number">24</span>)).show()显示介于<span class="number">22</span>到<span class="number">24</span>直接的age列的所有记录</span><br></pre></td></tr></table></figure><h4 id="添加、修改、删除列"><a href="#添加、修改、删除列" class="headerlink" title="添加、修改、删除列"></a>添加、修改、删除列</h4><h5 id="添加列"><a href="#添加列" class="headerlink" title="添加列"></a>添加列</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df = df.withColumn(<span class="string">'city'</span>,df.address.city) \</span><br><span class="line">       .withColumn(<span class="string">'postalCode'</span>,df.address.postalCode) \</span><br><span class="line">    .withColumn(<span class="string">'state'</span>,df.address.state) \</span><br><span class="line">    .withColumn(<span class="string">'streetAddress'</span>,df.address.streetAddress) \</span><br><span class="line">    .withColumn(<span class="string">'telePhoneNumber'</span>,explode(df.phoneNumber.number)) \</span><br><span class="line">    .withColumn(<span class="string">'telePhoneType'</span>,explode(df.phoneNumber.type)) \</span><br></pre></td></tr></table></figure><h5 id="修改列"><a href="#修改列" class="headerlink" title="修改列"></a>修改列</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = df.withColumnRenamed(<span class="string">'telePhoneNumber'</span>,<span class="string">'phoneNumber'</span>)</span><br></pre></td></tr></table></figure><h5 id="删除列"><a href="#删除列" class="headerlink" title="删除列"></a>删除列</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = df.drop(&quot;address&quot;,&quot;phoneNumber&quot;)</span><br><span class="line">df = df.drop(df.address).drop(df.phoneNumber)</span><br></pre></td></tr></table></figure><h5 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.groupBy(<span class="string">"age"</span>).count().show()按age列分组，统计每组人数</span><br></pre></td></tr></table></figure><h5 id="筛选"><a href="#筛选" class="headerlink" title="筛选"></a>筛选</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.filter(df[<span class="string">"age"</span>]&gt;<span class="number">24</span>).show()按age列筛选，保留年龄大于<span class="number">24</span>岁的</span><br></pre></td></tr></table></figure><h5 id="排序-1"><a href="#排序-1" class="headerlink" title="排序"></a>排序</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">peopledf.sort(peopledf.age.desc()).collect()</span><br><span class="line">df.sort(&quot;age&quot;,ascending=False).collect()</span><br><span class="line">df.orderBy([&quot;age&quot;,&quot;city&quot;],ascending=[0,1]).collect()</span><br></pre></td></tr></table></figure><h5 id="替换缺失值"><a href="#替换缺失值" class="headerlink" title="替换缺失值"></a>替换缺失值</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.na.fill(<span class="number">50</span>).show()用一个值替换空值</span><br><span class="line">df.na.drop().show()去除df中为空值的行</span><br><span class="line">df.na.replace(<span class="number">10</span>,<span class="number">20</span>).show()用一个值去替换另一个值</span><br></pre></td></tr></table></figure><h5 id="重分区-1"><a href="#重分区-1" class="headerlink" title="重分区"></a>重分区</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.repartition(<span class="number">10</span>).rdd.getNumPartitions()将df拆分为<span class="number">10</span>个分区</span><br><span class="line">df.coalesce(<span class="number">1</span>).rdd.getNumPartitions()将df合并为<span class="number">1</span>个分区</span><br></pre></td></tr></table></figure><h4 id="运行SQL查询"><a href="#运行SQL查询" class="headerlink" title="运行SQL查询"></a>运行SQL查询</h4><h5 id="将数据框注册为视图"><a href="#将数据框注册为视图" class="headerlink" title="将数据框注册为视图"></a>将数据框注册为视图</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">peopledf.createGlobalTempView(<span class="string">"people"</span>)</span><br><span class="line">df.createTempView(<span class="string">"customer"</span>)</span><br><span class="line">df.createOrReplaceTempView(<span class="string">"customer"</span>)</span><br></pre></td></tr></table></figure><h5 id="查询视图"><a href="#查询视图" class="headerlink" title="查询视图"></a>查询视图</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = spark.sql(<span class="string">"select * from customer"</span>).show()</span><br><span class="line">peopledf = spark.sql(<span class="string">"select * from global_temp.people"</span>).show()</span><br></pre></td></tr></table></figure><h4 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h4><h5 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd1 = df.rdd将df转为rdd</span><br><span class="line">df.toJSON().first()将df转为rdd字符串</span><br><span class="line">df.toPandas()将df的内容转为Pandas的数据框</span><br></pre></td></tr></table></figure><h5 id="保存至文件"><a href="#保存至文件" class="headerlink" title="保存至文件"></a>保存至文件</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.select(<span class="string">"firstName"</span>,<span class="string">"city"</span>).write.save(<span class="string">"nameAndCity.parquet"</span>)</span><br><span class="line">df.select(<span class="string">"firstName"</span>,<span class="string">"age"</span>).write.save(<span class="string">"nameAndAges.json"</span>,format=<span class="string">"json"</span>)</span><br></pre></td></tr></table></figure><h5 id="终止SparkSession"><a href="#终止SparkSession" class="headerlink" title="终止SparkSession"></a>终止SparkSession</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pyspark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据分析常用工具总结</title>
      <link href="/2019/07/22/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E6%80%BB%E7%BB%93/"/>
      <url>/2019/07/22/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1. 优点:向量化数据操作比for循环,速度大大加强，numpy array比list好的地方在于切片</span><br><span class="line">2. array属性</span><br><span class="line">    np.random.random((2,2)) # 0-1随机数</span><br><span class="line">    np.random.randint(1,10,(3,3)) # 随机整数</span><br><span class="line">    array.shape, array.dtype # numpy两个属性</span><br><span class="line">    array.astype(np.float64) # 类型转换</span><br><span class="line">3. array切片操作</span><br><span class="line">    a[0,1] # 第一个维度为0,第二个维度1,第三个维度全选,类似于a[0,1,:]</span><br><span class="line">    a[a&gt;2] # boolean indexing, 利用broadcasting进行判断, 之后可以作为index进行数据的提取</span><br><span class="line">    a[a&gt;2]=0 # 也可以对满足条件的元素进行赋值</span><br><span class="line">4. array数学运算</span><br><span class="line">    broadcasting, 对不匹配的数据在高维上进行扩展,在取最小公倍数</span><br><span class="line">    np.sum(array) # 统计运算</span><br><span class="line">    np.dot # 矩阵乘法,点乘</span><br><span class="line">    np.multiply  # 逐个元素乘法,对应相乘</span><br></pre></td></tr></table></figure><p>#pandas</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">基于Numpy构建，利用它的高级数据结构和操作工具，可使数据分析工作变得更加便捷高效。</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br></pre></td></tr></table></figure><h2 id="基本数据结构"><a href="#基本数据结构" class="headerlink" title="基本数据结构"></a>基本数据结构</h2><h3 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">1. 基本概念</span><br><span class="line">    pd.__version__ # 查看版本</span><br><span class="line">    pd.Series # 可以使用不同类型,和list区别在于有index, 可以指定index</span><br><span class="line"></span><br><span class="line">2. Series构建</span><br><span class="line">    pd.Series([1,2,3], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;])</span><br><span class="line">    pd.Series(&#123;...&#125;, name=&quot;xxx&quot;) # 通过对dictionary进行构建pandas, 给Series赋予名字</span><br><span class="line"></span><br><span class="line">3. 切片</span><br><span class="line">    aseries[[1,4,3]]; aseries[1:]; aseries[:-1]  # 数字下标切片,即使index不是数字也ok</span><br><span class="line"></span><br><span class="line">4. 运算规则</span><br><span class="line">    series的相加是根据index对应相加的</span><br><span class="line"></span><br><span class="line">5. 取值</span><br><span class="line">    数学运算也是broadcasting方式</span><br><span class="line">    &apos;xxx&apos; in aseries # 判断xxx是否在aseries的index中</span><br><span class="line">    aseries.get(&apos;xxx&apos;, 0) # 类似于字典</span><br><span class="line">    aseries[aseries&lt;20] # boolean index也可以</span><br><span class="line">    aseries.median() # 除去缺失值之后进行统计运算</span><br><span class="line">    aseries[&apos;xxx&apos;] = 1000 # 对aseries[&apos;xxx&apos;]重新赋值</span><br><span class="line">    np.square(aseries) # 对每个运算进行计算平方</span><br></pre></td></tr></table></figure><h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">1. 基本概念</span><br><span class="line">    一组Series集合在一起</span><br><span class="line"></span><br><span class="line">2. DataFrame的构建</span><br><span class="line">    - pd.DataFrame(&#123;&apos;a&apos;:[1,2,3], &apos;b&apos;:[1,4,3]&#125;, columns = [&apos;b&apos;, &apos;a&apos;], index = [&apos;one&apos;, &apos;two&apos;, &apos;three&apos;]) # 构建DF, 指定列名以及index名</span><br><span class="line">    - pd.DataFrame([&#123;&apos;a&apos;:100,&apos;b&apos;:200&#125;, &#123;&apos;a&apos;:200, &apos;b&apos;:300&#125;], index=[&apos;one&apos;, &apos;two&apos;]) # 按照一行一行构建DF</span><br><span class="line">    - pd.DataFrame(&#123;&apos;a&apos;:seriesa, &apos;b&apos;:seriesb&#125; # 记住按照index对齐, 缺失值直接Nan填充</span><br><span class="line"></span><br><span class="line">3. 元素的提取以及增加及逻辑操作及转置</span><br><span class="line">    - aDF[&apos;xxx&apos;]/aDF.xxx # 取出来的是一个Series</span><br><span class="line">    - aDF[[&apos;xxx&apos;]] # 取出来的是一个DF</span><br><span class="line">    - aDF.loc([&apos;a&apos;,&apos;b&apos;],[&apos;c&apos;,&apos;d&apos;]) # 取对应的数据</span><br><span class="line">    - aDF.loc[:, &apos;newcol&apos;] = 2000 # 如果没有newcol那么就新加一列</span><br><span class="line">    - aDF.loc[(aDF[&apos;a&apos;]&gt;10) &amp; (aDF[&apos;b&apos;]&lt;100), :] # 也可以给条件进行筛选,&amp; | ~进行逻辑运算</span><br><span class="line">    - aDF.T # 进行转置</span><br><span class="line">4. 数据读入以及基本信息以及删除</span><br><span class="line">    - pd.read_csv(path, sep=&apos;\t&apos;, index_col=&apos;&apos;/int, usecols=[...], header=0, parse_dates=[0]/[&apos;Date&apos;]) # 读文件，第一列作为日期型，日期型处理参照: http://hshsh.me/post/2016-04-12-python-pandas-notes-01/</span><br><span class="line">    - aDF.to_csv(&apos;xxx.csv&apos;, sep=&apos;\t&apos;, index=True, header=True) # 写文件</span><br><span class="line">    - aDF.describe(include=[np.float64...]) / aDF.info() # 对数据进行统计，查看缺失值</span><br><span class="line">    - aDF.shape</span><br><span class="line">    - aDF.isnull() # 判断是是否为空</span><br><span class="line">    - aDF[aDF[&apos;xxx&apos;].isnull(), :] = 10 # 对空值赋值</span><br><span class="line">    - aDF.notnull() # 查看是否有值</span><br><span class="line">    - aDF.drop([&apos;one&apos;, &apos;two&apos;], axis=0) # 对index为one和two的两行进行删除, axis=1删除列</span><br><span class="line"></span><br><span class="line">5. 数据分组聚合</span><br><span class="line">    - aDF.groupby(&apos;name&apos;, sort=False).sum() # 对DF进行聚合操作,同时对相应聚合的列进行排序,然后计算其他值的和</span><br><span class="line">    - groupbyname=aDF.groupby(&apos;name&apos;); groupbyname.groups; len(groupbyname) # 得到对应的各个组别包含的index, 并且可以获取对应的group长度</span><br><span class="line">    - aDF.groupby(&apos;name&apos;).agg([np.sum, np.mean, np.std]) # 对不同类别的数据进行各类运算, 每个name对应三列分别是分组之后np.sum, np.mean, np.std计算</span><br><span class="line">    - aDF.groupby(&apos;name&apos;).agg([&apos;sum&apos;, &apos;median&apos;, &apos;mean&apos;]) # 和上面的作用相同</span><br><span class="line">    - aDF.groupby(&apos;name&apos;).agg([&apos;a&apos;:np.sum, &apos;b&apos;:median, &apos;c&apos;:np.mean]) # 对不同列进行不同操作</span><br><span class="line">    - aDF.groupby([&apos;name&apos;, &apos;year&apos;]).sum()/mean()/median()/describe() # 多组分类</span><br><span class="line">    - aDF.groupby([&apos;name&apos;, &apos;year&apos;]).size() # 多组分类, 每一组有多少个记录</span><br><span class="line">    - 提取group类别名称以及类别对应的数据行</span><br><span class="line">        for name,group in groupbyname:</span><br><span class="line">            print(name) # 类别名称</span><br><span class="line">            print(group) # 名称对应的数据行</span><br><span class="line">        groupbyname.get_group(&apos;jason&apos;) # 可以得到对应组别的数据行,DF格式</span><br><span class="line">6. transform/apply/filter 数据变换</span><br><span class="line">    transfrom可以对分组进行变换, apply对整个DF进行分类,filter对分组进行判断</span><br><span class="line">    - aDF[&apos;Date&apos;].dt.dayofweek # 可以得到对应的日期中的第几天</span><br><span class="line">    - aDF.groupby(aDF.index.year).mean() # 可以对相应的日期型的年进行分组聚合</span><br><span class="line">    - aDF.groupby(aDF.index.year).transform(lambda x: (x-x.mean())/x.std()) # 对每一年的数据求均值以及标准差,并对每个数据进行操作,之所以没以每年为单位进行展示主要是跟function有关,因为之前的是mean之类的</span><br><span class="line">    - aDF.groupby(aDF.index.year).apply(lambda x: (x-x.mean())/x.std()) # 可以起到相同的效果</span><br><span class="line">    - aDF.loc[:,&apos;new&apos;] = aDF[&apos;xxx&apos;].apply(afunc) # 可以对xxx这一列进行操作按照afunc进行操作,然后创建新的列</span><br><span class="line">    - aSer = pd.Series([1,1,2,2,2,3,3,4,5,5]); sSer.groupby(sSer).filter(lambda x:x.sum()&gt;4) # 对ser进行过滤,留下那些和大于4的类别</span><br><span class="line"></span><br><span class="line">7. 表格的拼接与合并(concat/append/merge/join)</span><br><span class="line">    - df1.append(df2, sort=False, ignore_index=True) # 追加在行上,同时忽略原先df1和df2的index,合并为新的index</span><br><span class="line">    - df1.append([df2, df3])  # 也可以追加两个DF, 参考: https://zhuanlan.zhihu.com/p/38184619</span><br><span class="line">    - pd.concat([df1.set_index(&apos;a&apos;), df2.set_index(&apos;a&apos;)], sort=False, axis=1, join=&apos;inner&apos;) # 和上述利用merge在a字段上进行内连接的效果类似,因为concat是基于index进行连接的,merge可以不基于index,指定字段</span><br><span class="line">    - pd.concat([df1, df2, df3], keys=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;], axis=0, join=&apos;outer&apos;, sort=False) #列对齐的方式对行进行拼接,缺少值则补充为None,可以对拼接的每个df进行key的命名,axis=1的时候行对齐列拼接; join指定连接方式,outer表示外连接,inner表示内连接,sort是否对合并的数据进行排序</span><br><span class="line">    - merge # 基于某个字段进行连接,之前的append和concat都是在行上或者列上进行连接的,merge类似于SQL里面的连接,可以指定某个字段或某几个字段,体现在on上,on接list就是多个字段为key</span><br><span class="line">    - pd.merge(df1, df4, on=&apos;city&apos;, how=&apos;outer&apos;/&apos;inner&apos;/&apos;left&apos;/&apos;right&apos;) # 基于两个表中的city字段进行表格的连接,把其他的列进行combine到一起,不指定on的话就会找字段相同的那个进行拼接,注意concat是基于index进行拼接的</span><br><span class="line">    - pd.merge(df1, df2, how=&apos;inner&apos;, left_index=True, right_on=&apos;id&apos;) # 对数据进行merge,左表以index作为连接关键字,右表用id作为关键字</span><br><span class="line">8. 链家Case study流程</span><br><span class="line">    - pd.to_datetime() # 日期类型转换</span><br><span class="line">    - df.drop(droplist, inplace=True, axis=1) # 删除一些列</span><br><span class="line">    - aDF.describe(include=&apos;all&apos;) # 字符串变量也会同时统计</span><br><span class="line">    - aDF.sort_values(by = &apos;xxx&apos;).tail() # 找出更新最晚的20套,但是有可能同一天超过20套</span><br><span class="line">    - 如果对数据进行处理发现转换未果可能是因为数据有缺失,做异常处理,缺失值作为Nan</span><br><span class="line">    - aDF.nsmallest(columns=&apos;age&apos;, n=20) # 取出年龄最小的20个数据</span><br><span class="line">    - groupby().agg() 之后一般会使用reset_index() 对数据进行归置然后再进行操作,ascending=False</span><br><span class="line">    - adf.value_counts(normalize=True) # 默认是按照value进行排序的</span><br><span class="line">    - aDF.apply(lambda x: &apos;xxx&apos; in x) # 筛选出xxx在某列的值中与否,返回Ture, False，正则表达式的字符串匹配</span><br><span class="line">    - 可以定义正则表达式对文本信息进行提取</span><br><span class="line">        def get_info(s, pattern, n):</span><br><span class="line">            result = re.search(pattern, s)</span><br><span class="line">            if result:</span><br><span class="line">                return result.group(n)</span><br><span class="line">            else:</span><br><span class="line">                return &apos;&apos;</span><br><span class="line">    - .astype(int) # 转换pd类型</span><br><span class="line">    - help(pd.Series.value_counts) # 打印帮助文档</span><br></pre></td></tr></table></figure><h1 id="python绘图"><a href="#python绘图" class="headerlink" title="python绘图"></a>python绘图</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">1. pandas 绘图</span><br><span class="line">    - pd.date_range(&apos;2018/12/28&apos;, periods=10) # 产生日期格式, 以2018/12/28为起始产生以天为单位的日期时间list</span><br><span class="line">    - pandas绘图需要把横坐标作为index,之后再画图</span><br><span class="line">    - 折线图绘制需要注意各列幅度，否则数值不明显</span><br><span class="line">    - df.plot.bar() # barplot, stacked=True, 堆叠</span><br><span class="line">    - df.plot.barh() # 绘制水平的barplot</span><br><span class="line">    - df.plot.hist(bins = 20) # 绘制直方图,单维度</span><br><span class="line">    - df.plot.box() # 对每列去看一些分布outlier</span><br><span class="line">    - df.plot.area # 堆叠区域图</span><br><span class="line">    - df.plot.scatter(x=&apos;a&apos;, y=&apos;b&apos;) # 散点图</span><br><span class="line">    - df.plot.pie(subplots=True) # 绘制带图例的饼图</span><br><span class="line">    </span><br><span class="line">2. matplotlib 绘图</span><br><span class="line">    - plt.rcParams[&apos;figure.figsize&apos;] = (12,8) / plt.figure(figsize=(12,8)) # 设置画布大小</span><br><span class="line">    - ax = plt.plot(x,y,color=&apos;green&apos;, linewidth=&apos;-&apos;, marker=&apos;./*/x&apos;, label=r&apos;$y=cos&#123;x&#125;$&apos;/r&apos;$y=sin&#123;x&#125;$&apos;/r&apos;$y=\sqrt&#123;x&#125;$&apos;) # 绘图</span><br><span class="line">    - ax.spines[&apos;right&apos;].set_color(&apos;none&apos;) # 去掉右边的边框</span><br><span class="line">    - ax.xaxis.set_ticks_position(&apos;bottem&apos;) # ??????????????</span><br><span class="line">    - plt.xticks([2,4,6], [r&apos;a&apos;,r&apos;b&apos;,r&apos;c&apos;]) # 设置坐标轴刻度</span><br><span class="line">    - ax.spines[&apos;bottem&apos;].set_position(&apos;data&apos;, 0)  # 设置坐标轴从0开始</span><br><span class="line">    - plt.xlim(1,3) # 设置坐标位置</span><br><span class="line">    - plt.title() # 标题</span><br><span class="line">    - plt.xlabel(r&apos;xxx&apos;, fontsize=18, labelpad=12.5) # 绘制label, r值的是不转义的,$$值的是markdown格式</span><br><span class="line">    - plt.text(0.8, 0.9, r&apos;$$&apos;, color=&apos;k&apos;, fontsize=15) # 进行注解</span><br><span class="line">    - plt.scatter([8], [8], 50, color=&apos;m&apos;) # 在某个位置,点有多大,颜色是什么</span><br><span class="line">    - plt.annotate(r&apos;$xxx$&apos;, xy=(8,8), xytext=(8.2, 8.2), fontsize=16, color=&apos;m&apos;, arrowprops=dict(arrowstyle=&apos;-&gt;&apos;, connectionstyle=&apos;arc3, rad=0.1&apos;, color=&apos;m&apos;)) # 对某个点进行注解, 进行加箭头等等</span><br><span class="line">    - plt.grid(True) # 网格线 </span><br><span class="line">    - plt.plot(x, y) # xy应为np array,如果是pandas那么可以通过values进行取值转换</span><br><span class="line">3. matplotlib 绘图case</span><br><span class="line">    - 文件解压</span><br><span class="line">        x = zipfile.ZipFile(xxx, &apos;r&apos;) # 解压文件夹</span><br><span class="line">        x.extractall(&apos;xxxdir&apos;) # 解压到某个文件夹下</span><br><span class="line">        x.close() # 记得关闭</span><br><span class="line">    - matplotlib.rc(&apos;figure&apos;, figsize=(14,7)) # 设置一下图片尺寸</span><br><span class="line">    - matplotlib.rc(&apos;font&apos;, size=14) # 设置字体</span><br><span class="line">    - matplotlib.rc(&apos;axes.spines&apos;, top=False, right=False) # 设置边线</span><br><span class="line">    - matplotlib.rc(&apos;axes&apos;, grid=False) # 设置网格</span><br><span class="line">    - matplotlib.rc(&apos;axes&apos;, facecolor=&apos;white&apos;) # 设置颜色</span><br><span class="line">    - fig,ax含义</span><br><span class="line">        fig,ax = plt.subplots() # 创建绘图对象之后对ax进行操作，相当于先fig=plt.figure()再ax=fig.add_subplot(1,1,1)</span><br><span class="line">        https://blog.csdn.net/htuhxf/article/details/82986440</span><br><span class="line">    - ax.fill_between(x, low, upper, alpha=) # 对回归进行置信度绘制</span><br><span class="line">    - ax2 = ax1.twinx() # 共享同一个x轴</span><br><span class="line">    - ax2.spines[&apos;right&apos;].set_visible(True) # 对右侧坐标轴进行设置,得到相应的图</span><br><span class="line">    - 图的使用</span><br><span class="line">        关联分析:散点图,曲线图,置信区间曲线图,双坐标曲线图</span><br><span class="line">        分布分析:堆叠直方图, 密度图</span><br><span class="line">        组间分析:柱状图(带errorbar),boxplot,这个需要多看看,</span><br><span class="line">        </span><br><span class="line"> 4. seaborn 绘图</span><br><span class="line">    - 引入seaborn的同时也要引入matplotlib因为,是底层</span><br><span class="line">    - 颜色设置</span><br><span class="line">        sns.set(color_codes=True) # 一些集成的颜色</span><br><span class="line">        https://seaborn.pydata.org/tutorial/color_palettes.html</span><br><span class="line">    - sns.displot(x, kde=True, bins=20, rug=True, fit=stats.gamma) # histgram加密度线,样本分布情况, 拟合某些分布fit</span><br><span class="line">    - sns.kdeplot # 类似于上面的,kde是每个样本用正态分布画,如果样本多,高度就高,之后再做归一化</span><br><span class="line">    - sns.jointplot(x,y,data) # 绘制带有histgram以及散点图的图，两个变量</span><br><span class="line">    - sns.pairplot(df) # 直接绘制各个列之间的散点图以及对应的histgram，多个变量</span><br><span class="line">    - scatter plot的密度版</span><br><span class="line">        with sns.axes_style(&apos;ticks&apos;):</span><br><span class="line">            sns.jointplot(x,y,data, kind=&apos;hex&apos;/&apos;kde&apos;,color=&apos;m&apos;) #相当于对点很多的时候,六角箱图就能体现出点的多少,kde是等高线,密度联合分布</span><br><span class="line">    - 多图绘制1</span><br><span class="line">        g = sns.PairGrik(df) # 各个列混合,产出n*n个格子</span><br><span class="line">        g.map_diag(sns.kdeplot) # 对角线绘制</span><br><span class="line">        g.map_offdiag(sns.kdeplot, cmap=&apos;Blues_d&apos;, n_levels=20) # 绘制对角线是kde密度图其他为等高线的图</span><br><span class="line">    - 多图绘制2</span><br><span class="line">        g = FaceGrid(row=[..],aspect=1.5, data=)</span><br><span class="line">        g.map(sns.boxplot, x, y, hue, hue_order=[], ...)</span><br><span class="line">    - 多图绘制3</span><br><span class="line">        g = sns.PairGrid(data, x_vars=[], y_vars=[], aspect=0.5, size=3.5)</span><br><span class="line">        g.map(sns.violinplot, palette=&apos;bright&apos;) # x_vars数量*y_vars数量个子图，然后每个子图都绘制violinplot</span><br><span class="line">    - 关联分析 sns.lmplot</span><br><span class="line">        · sns.lmplot(x, y, data) # 散点图+线性回归,95%置信区间,适用于连续值</span><br><span class="line">        · sns.lmplot(x, y, data, x_jitter=0.08) # 左右抖动, 点如果离得近,会把点左右抖动开,适用于离散值</span><br><span class="line">        · sns.lmplot(x, y, data, x_estimator=np.mean, ci=95, scatter_kws=&#123;&apos;s&apos;:80&#125;, order=2, robust=True) # 对于离散值还可以这样操作,先求均值和95置信区间,之后再进行拟合, scatter_kws对点进行操作,order是说对数据点进行二次方的分布,而不是线性分布,robust打开的作用是踢除异常点,然后再进行绘制图</span><br><span class="line">        · sns.lmplot(x, y, data, x_estimator=np.mean, ci=95, scatter_kws=&#123;&apos;s&apos;:80&#125;, order=1, robust=True, logistic=True) # 相当于是说对二值化的数据进行logistic回归拟合,sigmoid拟合</span><br><span class="line">        · sns.lmplot(x, y, data, hue, col, row, col_wrap, aspect=0.5) # 散点图.线性回归,95%置信区间,适用于连续值,hue进行分组类似于pandas里面的groupby, hue变量一定是个离散变量, col也可以加一个变量,可以把图分成多列,row可以多行,如果row,col以及hue都指定,那么相当于在pandas里面groupby三个内容,col_wrap用于之指定每个col中的绘图数量</span><br><span class="line">    - sns.residplot() # 残差图</span><br><span class="line">    - sns.barplot(x,y,hue,ci=None)  # 是否打开置信区间</span><br><span class="line">    - sns.stripplot(x, y, data, jitter =True) # 基于x为离散数据的,类似于散点图的boxplot</span><br><span class="line">    - sns.swarmplot(x, y, data) #  蜂群图，类似于小提琴图的点版</span><br><span class="line">    - sns.boxplot()</span><br><span class="line">    - sns.violinplot(bw) # 属于kde以及boxplot的组合，既看了单变量分布，也看了各变量之间的差异</span><br><span class="line">    - sns.violinplot(split=True， hue， inner=&apos;stick&apos;) # split将hue为两个类型的进行拼接绘制小提琴图，stick，每个样本绘制竖线</span><br><span class="line">    - sns.countplot(x, data) # 绘制离散变量数量分布，类似于value_counts()，类似于barplot但是使用的统计量是数量</span><br><span class="line">    - sns.pointplot(x, y, hue) # 查看离散变量x以及hue在离散变量y上的差别，使用均值，画点</span><br><span class="line">    - sns.factorplot(x, y, hue, col, data, kind=&apos;swarm&apos;) # 是一种泛化的绘图函数</span><br><span class="line">    - a.savefig(&apos;xx&apos;) # 进行图片存储 plt函数</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mumpy </tag>
            
            <tag> pandas </tag>
            
            <tag> seaborn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python基础知识整理</title>
      <link href="/2019/07/20/python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/"/>
      <url>/2019/07/20/python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>#Canda环境安装以及包管理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">清华镜像下载https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/</span><br><span class="line">命令行启动jupyter notebook或点击快捷图标方式启动</span><br><span class="line">conda list <span class="comment"># 查看所在环境的安装的包</span></span><br><span class="line">conda upgrade --all <span class="comment"># 对包进行更新</span></span><br><span class="line">spyder <span class="comment"># 启动anaconda中的IDE</span></span><br><span class="line">conda install numpy pandas <span class="comment"># 在某个环境下能够安装某些Python包</span></span><br><span class="line">conda install numpy=<span class="number">1.10</span> <span class="comment"># 安装特定版本的包</span></span><br><span class="line">conda remove &lt; package_name &gt; <span class="comment"># 删除包</span></span><br><span class="line">conda env list <span class="comment"># 列出当前机器上创建的虚拟环境</span></span><br><span class="line">conda create -n env1 python=<span class="number">2.7</span> <span class="comment"># 创建一个名为env1的环境然后在其中安装python2.7</span></span><br><span class="line">conda create -n env1 numpy <span class="comment"># 创建一个名为env1的环境然后在其中安装numpy</span></span><br><span class="line">source activate env1 <span class="comment"># 进入env1虚拟环境，在window上不用加source</span></span><br><span class="line">source deactivate <span class="comment"># 离开环境</span></span><br><span class="line">conda install -n py27 ipykernel <span class="comment"># 在虚拟环境py27下安装ipykernel</span></span><br><span class="line">python -m ipykernel install --user --name py27 --display-name <span class="string">"python2"</span> <span class="comment"># 在py27环境内安装ipykernel并在菜单里命名为python2</span></span><br><span class="line">conda env remove -n py27 <span class="comment"># 移除py27的虚拟环境</span></span><br><span class="line">conda install jupyter notebook <span class="comment"># 在conda环境中安装jupyter notebook</span></span><br><span class="line">%matplotlib <span class="comment"># jupyter notebook中已交互式方式实现matplotlib的绘图</span></span><br><span class="line">%matplotlib inline <span class="comment"># 不跳出，直接内嵌在web中</span></span><br></pre></td></tr></table></figure><h1 id="jupyter-notebook常用配置"><a href="#jupyter-notebook常用配置" class="headerlink" title="jupyter notebook常用配置"></a>jupyter notebook常用配置</h1><h2 id="notebook中的magic开关"><a href="#notebook中的magic开关" class="headerlink" title="notebook中的magic开关"></a>notebook中的magic开关</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">为实现一些快捷操作，提升效率。notebook中提供magic开关，能极大的优化使用notebook的体验。</span><br><span class="line">magic开关分为两大类：%line magic &amp; %%cell magic</span><br></pre></td></tr></table></figure><p>​        </p><h2 id="magic开关总览"><a href="#magic开关总览" class="headerlink" title="magic开关总览"></a>magic开关总览</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">%quickref<span class="comment"># 所有magic命令 </span></span><br><span class="line">%lsmagic<span class="comment"># 打印所有magic命令</span></span><br><span class="line"></span><br><span class="line">%config ZMQInteractiveShell.ast_node_interactivity=<span class="string">'all'</span>/<span class="string">'last_expr'</span></span><br><span class="line">%pprint <span class="comment"># 打印所有结果,保证每次执行都输出,默认只输出最后一个内容</span></span><br><span class="line">%config ZMQInteractiveShell可以查看可选择的输出类型</span><br><span class="line">或者执行这个命令保证多输出</span><br><span class="line"><span class="keyword">from</span> IPython.core.interactiveshell <span class="keyword">import</span> InteractiveShell</span><br><span class="line">InteractiveShell.ast_node_interactivity = <span class="string">'all'</span>/<span class="string">'last_expr'</span></span><br><span class="line"></span><br><span class="line"> %%整个cell magic</span><br><span class="line"> %%writefile test.py <span class="comment"># 将cell中的命令写入文件test.py</span></span><br><span class="line"> %%timeit代码计时</span><br><span class="line"> %%bash <span class="comment"># 在cell内可以执行bash命令</span></span><br><span class="line"> %%writefile xx.py <span class="comment"># 把整个cell中的内容输入到xx.py中,如果新加内容可以%%writefile -a xx.py</span></span><br><span class="line"></span><br><span class="line"> %line magic命令</span><br><span class="line"> %matplotline inline <span class="comment"># 在jupyter内打印图片</span></span><br><span class="line"> %run utils.ipynb <span class="comment"># 执行本地的utils.ipynb文件,进行配置</span></span><br><span class="line"></span><br><span class="line"> line magic和cell magic区别就在于line magic只在一行有效,cell magic在多行都有效</span><br><span class="line"> 具体参考:https://gispark.readthedocs.io/zh_CN/latest/pystart/jupyter_magics.html</span><br></pre></td></tr></table></figure><h2 id="Jupyter-notebook扩展"><a href="#Jupyter-notebook扩展" class="headerlink" title="Jupyter notebook扩展"></a>Jupyter notebook扩展</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">jupyter_contrib_nbextensions</span><br><span class="line">直接安装官网conda命令安装即可</span><br><span class="line">conda install jupyter notebook</span><br><span class="line">conda install -c conda-forge jupyter_contrib_nbextensions </span><br><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple autopep8 </span><br><span class="line">pip安装加速镜像:https://www.cnblogs.com/microman/p/6107879.html</span><br><span class="line">jupyter 使用参考资料:https://zhuanlan.zhihu.com/p/33105153</span><br><span class="line">jupyter extension 参考资料:https://zhuanlan.zhihu.com/p/52890101</span><br></pre></td></tr></table></figure><h3 id="jupyter-使用linux命令"><a href="#jupyter-使用linux命令" class="headerlink" title="jupyter 使用linux命令"></a>jupyter 使用linux命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!head -n 5 xx.txt # 直接通过jupyter行使linux命令</span><br></pre></td></tr></table></figure><h1 id="python"><a href="#python" class="headerlink" title="python"></a>python</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">python语言是一种面向对象、动态数据类型的解释型语言</span><br><span class="line">1.运行方式</span><br><span class="line">    解释运行:直接py脚本运行</span><br><span class="line">    交互运行:jupyter输入一个输出一个</span><br><span class="line"></span><br><span class="line">2.命名规则:</span><br><span class="line">    常量大写，下划线隔开单词</span><br><span class="line">    类用驼峰命名</span><br><span class="line">     del xx 删除变量xx</span><br><span class="line"></span><br><span class="line">3.操作优先级：</span><br><span class="line">    函数调用，寻址，下标</span><br><span class="line">    幂运算</span><br><span class="line">    翻转运算符</span><br><span class="line">    正负号</span><br><span class="line">    * / %</span><br><span class="line">    - + </span><br><span class="line">4.赋值</span><br><span class="line">    多重赋值:a=b=10相当于a=10,b=10</span><br><span class="line">    多元赋值 a,b,c = 1,2,3</span><br><span class="line">    交换赋值 a,b = b,a # 指针</span><br><span class="line"></span><br><span class="line">5.解包(需要拓展 参考:https://zhuanlan.zhihu.com/p/33896402?utm_source=wechat_session&amp;utm_medium=social&amp;s_r=0)</span><br><span class="line">    l1 = [1,2,3,4,5,&apos;6&apos;]; a,b,*c,d = l1</span><br><span class="line">    l1=[1,2,3,4];b=&apos;sdaad&apos;;[*l1,*b]</span><br><span class="line">    b,=[[3,4,5]] # 逗号解包 </span><br><span class="line"></span><br><span class="line">6.python进制及基本类型</span><br><span class="line">    bin() 二进制</span><br><span class="line">    oct() 八进制</span><br><span class="line">    hex() 十六进制</span><br><span class="line"></span><br><span class="line">    float(&apos;inf&apos;) 正无穷</span><br></pre></td></tr></table></figure><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#行内注释</span><br><span class="line">&quot;&quot;&quot; &quot;&quot;&quot;多行注释</span><br><span class="line">？内省，显示对象的通用信息</span><br><span class="line">？？内省，显示出大部分函数的源代码</span><br><span class="line">help()显示一个对象的帮助文档</span><br><span class="line">%timeit 魔法命令，计算语句的平均执行时间</span><br><span class="line">type（x）查看变量x的数据类型</span><br><span class="line">in(x) 将变量x的数据类型转换为整型</span><br><span class="line">isinstance(x,float)检测变量x是否为浮点型，返回一个布尔型数值</span><br></pre></td></tr></table></figure><p>##字符串</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">s=u&quot; &quot;定义Unicode字符串</span><br><span class="line">s=r&quot; &quot;定义原始字符串，避免字符串中的字符串转义，在正则表达式中经常使用到</span><br><span class="line">len(s)返回s字符串的字数</span><br><span class="line">s.lower()字母全部转为小写</span><br><span class="line">s.upper()字母全部转为大写</span><br><span class="line">s.capitalize()将字符串s中的首个字符转换为大写，其余部分转换为小写</span><br><span class="line">s.replace(&apos;k&apos;,&apos;l&apos;)使用字符&quot;l&quot;替换掉s中所有的字符&quot;k&quot;,返回结果是cooldata</span><br><span class="line">s.strip()去掉s最前面和最后面的空格</span><br><span class="line">s.split(&quot;\t&quot;)使用制表符&quot;\t&quot;分割字符串</span><br><span class="line">&apos;%s is No.%d&apos;%(s,1)格式化</span><br><span class="line">&apos;&#123;&#125; is No.&#123;&#125;&apos;.format(s,1)格式化</span><br></pre></td></tr></table></figure><h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">list()空列表</span><br><span class="line">l[-1]返回列表的最后一个元素</span><br><span class="line">l[1:3]返回列表的第二个和第三个元素</span><br><span class="line">len(l)返回列表长度</span><br><span class="line">l[::-1]将列表进行逆序排列</span><br><span class="line">l.reverse()将列表进行逆序排列</span><br><span class="line">l.insert(1,&quot;b&quot;)在指定的索引位置插入&apos;b&apos;</span><br><span class="line">l.append()在列表末尾添加元素</span><br><span class="line">l.extend()等价于&quot;1+L&quot;，将列表L中的元素依次添加到1的末尾</span><br><span class="line">l.remove()删除列表中的某个元素</span><br><span class="line">l.pop()等价于del l[]，删除列表中对应索引位置的元素</span><br><span class="line">&quot; &quot;.join([&apos;&apos;c,&apos;o&apos;,&apos;o&apos;,&apos;k&apos;])将列表中的各个字符串元素用空格连接起来并转换为字符串，返回结果为cook</span><br></pre></td></tr></table></figure><h2 id="条件判断和循环语句"><a href="#条件判断和循环语句" class="headerlink" title="条件判断和循环语句"></a>条件判断和循环语句</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">if condition1:</span><br><span class="line">statement1</span><br><span class="line">elif condition2:</span><br><span class="line">statement2</span><br><span class="line">else:</span><br><span class="line">statement3</span><br><span class="line"></span><br><span class="line">for item in sequence:</span><br><span class="line">statement</span><br><span class="line"></span><br><span class="line">while condition:</span><br><span class="line">statement</span><br><span class="line"></span><br><span class="line">range(5)产生一个从0到5且间隔为1的整数列表[0，1，2，3，4]</span><br><span class="line">break从最内层for循环或while循环中跳出</span><br><span class="line">continue继续执行下一次循环</span><br><span class="line">pass占位符</span><br></pre></td></tr></table></figure><h2 id="enumerate（）and-zip（）"><a href="#enumerate（）and-zip（）" class="headerlink" title="enumerate（）and zip（）"></a>enumerate（）and zip（）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">for i,item in emumerate(l)在每一次循环时取出索引号和相应的值分别赋给和item</span><br><span class="line">s=&#123;item**2 for item in l&#125;集合推导式，对l中的每一个元素取平方得到的新集合</span><br><span class="line">D=&#123;key:value for key,value in zip(l,k)&#125;字典推导式，通过zip()函数将两个列表l和k中的元素组成键对并形成字典</span><br><span class="line">enumerate(list/set, start=0) # 遍历元素，start指定从哪个数字作为开始下标</span><br><span class="line">c = list(zip(a,b))</span><br><span class="line">c = set(zip(a,b))</span><br><span class="line">c = dict(zip(a,b))</span><br><span class="line">list(zip(*c)) # 解压</span><br></pre></td></tr></table></figure><h2 id="推导式"><a href="#推导式" class="headerlink" title="推导式"></a>推导式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">L=[item**2 for item in l]列表推导式，对l中每一个元素取平方得到新的列表</span><br><span class="line">S=&#123;item**2 for item in l&#125;集合推导式，对l中的每一个元素取平方得到新的集合</span><br><span class="line">D=&#123;key:value for key,value in zip(l,k)&#125;字典推导式，通过zip()函数将两个列表l和k中的元素组成键值对并形成字典</span><br><span class="line">[i for i in range(30) if 1%2==0] # 取0-29之间偶数</span><br><span class="line">[function(i) for i in range(30) if 1%2==0] # function可以自己定义</span><br><span class="line">[ x**2 if x%2 ==0 else x**3 for x in range(10)] # 两个条件</span><br></pre></td></tr></table></figure><h2 id="文件读写"><a href="#文件读写" class="headerlink" title="文件读写"></a>文件读写</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">读取文件</span><br><span class="line">f=open(filename,mode)返回一个文件对象f，读文件&quot;mode=r&quot;,写文件&quot;mode=w&quot;</span><br><span class="line">f.read(size)返回包含前size个字符的字符串</span><br><span class="line">f.readline()每次读取一行，返回该行字符串</span><br><span class="line">f.readlines()返回包含每个文件内容的列表，列表的元素为文件的每一行内容所构成的字符串</span><br><span class="line">f.close()关闭文件并释放它所占用的系统资源</span><br><span class="line">with open(&quot;aa.txt&quot;,&quot;r&quot;) as f:</span><br><span class="line">content = f.readlines()</span><br><span class="line">在with主体块语句执行完后，自动关闭文件并释放占用的系统资源</span><br><span class="line">import csv</span><br><span class="line">f=open(&quot;aa.csv&quot;,&quot;r&quot;)</span><br><span class="line">csvreader = csv.reader(f)</span><br><span class="line">content_list=list(csvreader)</span><br><span class="line">读取csv文件，并把数据存储为一个嵌套列表(列表的元素扔是一个对象)content_list</span><br><span class="line"></span><br><span class="line">写入文件</span><br><span class="line">f.write(s)</span><br><span class="line">print(s,file=f)</span><br><span class="line">两种等价的方式，将字符串s写入文件对象f中</span><br></pre></td></tr></table></figure><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def sum(a,b=1)</span><br><span class="line">return a+b</span><br><span class="line">def sum(*args,**kwargs)不定长参数，*args接收包含多个位置参数的元组，**kwargs接收包含多个关键字参数的字典。</span><br><span class="line">obj.methodname一个方法是一个&quot;属于&quot;对象并被命名为obj.methodname的函数</span><br></pre></td></tr></table></figure><h2 id="map-和lambda"><a href="#map-和lambda" class="headerlink" title="map()和lambda()"></a>map()和lambda()</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">map(func,sequence)将函数依次作用在序列的每个元素上，把结果作为一个新的序列返回。</span><br><span class="line">lambda a,b:a+b匿名函数，正常函数定义的语法糖</span><br><span class="line">lambda [参数列表]:表达式</span><br><span class="line">例如: sum = lambda x,y:x+y # 可以有多个参数,返回只能有一个式子</span><br><span class="line">可以作为一个函数的参数赋给另外一个参数</span><br><span class="line">当然普通函数也可以作为参数传入</span><br><span class="line">a = [&#123;&apos;name&apos;:&apos;ss&apos;,&apos;age&apos;:10&#125;,&#123;&apos;name&apos;:&apos;yy&apos;,&apos;age&apos;:7&#125;,&#123;&apos;name&apos;:&apos;zz&apos;,&apos;age&apos;:15&#125;] # 将匿名函数作为参数传入第三方函数的参数</span><br><span class="line">a.sort(key=lambda x:x[&apos;age&apos;]) # sort方法需要传入一个key，这个key可以作为排序依据，lambda可以提取每个元素，并对元素排列</span><br><span class="line">a.sort(key=lambda x:x[&apos;age&apos;]， reverse=True) # 降序</span><br></pre></td></tr></table></figure><h2 id="包模块"><a href="#包模块" class="headerlink" title="包模块"></a>包模块</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">包是一个文件夹</span><br><span class="line">模块是不同的python文件</span><br><span class="line">import package.module.func()</span><br><span class="line">import package1.module1, package2.module1 # 多个模块调用</span><br><span class="line">import package1.module1 as p1m1 # 对模块进行重命名使用</span><br><span class="line">from package.module import func1 # 调用某个包某个模块的某个函数</span><br><span class="line">import sys;sys.path # 搜索模块路径，包含当前文件夹</span><br><span class="line">package.module.__file__ # 可以确定当前的模块所在的路径</span><br><span class="line">__init__.py  #在包被加载的时候，会被执行。在一个包下面可以有也可以没有__init__.py</span><br><span class="line">from package import * # 引用包下面所有的模块都加载，自动搜索是不会发生的，</span><br><span class="line">    需要我们在__init__.py下进行定义才可以实现,定义的内容是__all__=[&quot;module1&quot;,&quot;module2&quot;],</span><br><span class="line">    将package下的module1和module2都加载进来</span><br><span class="line">    如果想直接加载某个函数，在__init__.py里面加入from .module1 import func1, __all__=[&quot;func1&quot;]</span><br><span class="line">    这样修改完之后，可以直接from package import *，然后直接调用func1即可，不用带package.module</span><br><span class="line">    restart kernel</span><br><span class="line">如果想要直接引用包,如：import package,这样的话，需要一定要有__init__.py，否则会在打印package.__file__的时候报错。</span><br><span class="line">注意import package和from package import *效果相同</span><br></pre></td></tr></table></figure><h2 id="Counter"><a href="#Counter" class="headerlink" title="Counter"></a>Counter</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">字典的继承类</span><br><span class="line">set dict list tuple 作为key</span><br><span class="line">from collection import Counter # 导入</span><br><span class="line">cnt = Counter()</span><br><span class="line">for i in [1,1,2,2,2,3]:</span><br><span class="line">    cnt[i] += 1</span><br><span class="line">print cnt</span><br><span class="line">如果用key的话会报错先做第一次初始化才行</span><br><span class="line">cnt2 = Counter(alist)  #可以统计每个元素出现的次数（字符串，set,list,）</span><br><span class="line">Counter(cat=4,dogs=8,abc=-1) # 初始化counter次数，或者用dictionary构建</span><br><span class="line">Counter(&#123;&apos;cat&apos;:4,&apos;dogs&apos;:8,&apos;abc&apos;:-1&#125;)</span><br><span class="line">Counter返回一个字典，如果缺失的话会返回0</span><br><span class="line">del cnt2[&apos;xx&apos;]</span><br><span class="line">.values()</span><br><span class="line">list(cnt),set(cnt),dict(cnt) # 前两个只返回key</span><br><span class="line">cnt.most_common()[0] # 对出现次数排序</span><br><span class="line">cnt.clear()</span><br><span class="line">cnt1+cnt2 # 对于key相同的value做加法，如果为0则不保留</span><br><span class="line">cnt1-cnt2 # 对于key相同的value做减法</span><br><span class="line">&amp; # 求key相同value的最小值</span><br><span class="line">| # 求key相同value的最大值</span><br></pre></td></tr></table></figure><h2 id="random"><a href="#random" class="headerlink" title="random"></a>random</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import random #引入</span><br><span class="line">random.random() # 0-1</span><br><span class="line">random.uniform(1,10) # 包含1，10的浮点数</span><br><span class="line">random.randint(1,10)  # 包含1，10的整数</span><br><span class="line">random.randrange(0,20,3) # 0-20能被3整除的数</span><br><span class="line">random.choice([1,2,3]) # 随机取元素</span><br><span class="line">random.choice(&quot;qwdwq&quot;) # 随机取元素</span><br><span class="line">random.shuffle([12,3,1,4,2,3]) # 混洗</span><br><span class="line">random.sample([1,2,3,4,5], 3) # 从前面的list中选3个</span><br></pre></td></tr></table></figure><h2 id="编码和解码"><a href="#编码和解码" class="headerlink" title="编码和解码"></a>编码和解码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import chardet</span><br><span class="line">chardet.detect(s)</span><br><span class="line">检测字符串的编码方式</span><br></pre></td></tr></table></figure><h2 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">try:</span><br><span class="line">statement</span><br><span class="line">except:</span><br><span class="line">pass</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">statement</span><br><span class="line">except Exception as e:</span><br><span class="line">print(e)</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">statement</span><br><span class="line">except (Exception1,Exception2) as e:</span><br><span class="line">statement1</span><br><span class="line">else:</span><br><span class="line">statement2</span><br><span class="line">finally:</span><br><span class="line">statement3#无论对错都运行</span><br><span class="line">抛出异常</span><br><span class="line">raise Exception(&apos;Oops！&apos;)</span><br><span class="line">assert statement,e#若继续运行代码、否则抛出e的错误提示信息</span><br></pre></td></tr></table></figure><h2 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">raw_s=r&apos;\d&#123;17&#125;[\d|x]|\d&#123;15&#125;&apos;</span><br><span class="line">pattern=re.compile(raw_s)</span><br><span class="line">re.search(pattern,s)</span><br><span class="line">用于匹配身份证号</span><br><span class="line">首先使用原始字符串定义正则表达式；然后编译原始字符为正则表达式Pattern对象；最后对整个字符串s进行模式搜索，如果模式匹配，则返回MatchObject的实例，如果该字符串没有模式匹配，则返回弄none</span><br><span class="line">re.search(r&apos;\d&#123;17&#125;[\d|x]|\d&#123;15&#125;&apos;,s)将Pattern编译过程与搜索过程合二为一</span><br><span class="line">re.match(pattern,s)从字符串s的起始位置匹配一个模式，如果匹配不成功返回None</span><br><span class="line">re.findall(pattern,s)返回一个包含所有满足条件的字串列表</span><br><span class="line">re.sub(pattern,repl,s)使用替换字符串repl替换匹配到的字符串</span><br><span class="line">re.split(pattern,s)利用满足匹配模式的字符串将字符串s分隔开，并返回一个列表</span><br><span class="line"></span><br><span class="line">1.正则表达式的match与search区别</span><br><span class="line">    https://segmentfault.com/a/1190000006736033</span><br><span class="line"></span><br><span class="line">2.贪婪匹配与非贪婪匹配的区别</span><br><span class="line">    https://segmentfault.com/a/1190000002640851</span><br><span class="line">    https://blog.csdn.net/lxcnn/article/details/4756030</span><br><span class="line"></span><br><span class="line">3. 练习网站</span><br><span class="line">    https://alf.nu/RegexGolf 一个正则表达式练习网站</span><br><span class="line">    https://regexr.com/ 验证网站</span><br><span class="line"></span><br><span class="line">4. 单字符匹配</span><br><span class="line">    . # 匹配出点换行符之外的任意字符</span><br><span class="line">    \. # 匹配单个.字符</span><br><span class="line">    [abd] # 匹配a/b/d单个字符</span><br><span class="line">    \d # 匹配数字, 相当于[1,2,3,4,5,6,7,8,9]</span><br><span class="line">    \D # 所有非字符</span><br><span class="line">    \s # 空白符,空格 tab等等</span><br><span class="line">    \S # 所有非空格</span><br><span class="line">    \w # a-z,A-Z,0-9,_</span><br><span class="line">    \W # 除了 a-z,A-Z,0-9</span><br><span class="line"></span><br><span class="line">5. 数量词用来多匹配</span><br><span class="line">    m&#123;2&#125; # 表示匹配两个m</span><br><span class="line">    m&#123;2,4&#125; # 表示匹配2/3/4个m,贪婪匹配</span><br><span class="line">    m* # 0个或者更多个,贪婪匹配</span><br><span class="line">    m+ # 1个或者更多个,贪婪匹配</span><br><span class="line">    m? # 0个或者1个</span><br><span class="line">    ^xx # 文本开头是xx进行匹配</span><br><span class="line">    xxx$ # 对结尾进行匹配</span><br><span class="line">    (re)su(lt) # group</span><br><span class="line"></span><br><span class="line">6. python中的正则表达式步骤</span><br><span class="line">    写一个文本pattern</span><br><span class="line">    进行匹配</span><br><span class="line">    对匹配的文本进行后续操作</span><br><span class="line">    例子:</span><br><span class="line">        import re</span><br><span class="line">        pattern = re.compile(r&apos;hello.*\!&apos;) # hello后面有若干个字符串直到有!</span><br><span class="line">        match = pattern.match(&apos;hello, xxx! how are you?&apos;) # 对文本进行匹配</span><br><span class="line">        if match: # 是否匹配</span><br><span class="line">            print match.group() # 如果匹配上了返回相应的匹配到的部分</span><br><span class="line"></span><br><span class="line">7. 使用实例</span><br><span class="line">    import re</span><br><span class="line">    re.compile(r&quot;&quot;&quot;</span><br><span class="line">    \d+ # 数字部分</span><br><span class="line">    \. # 小数点</span><br><span class="line">    \d # 小数部分</span><br><span class="line">    &quot;&quot;&quot;, re.X)</span><br><span class="line">    这种模式下可以写注解</span><br><span class="line">    re.compile(r&quot;\d+\.\d&quot;) # 与这个模式结果一样</span><br><span class="line"></span><br><span class="line">8.一些命令</span><br><span class="line">    match # 一次匹配结果,从头匹配,开头没有就匹配不上了</span><br><span class="line">    search # 所有匹配到的</span><br><span class="line">    findall # search返回第一个匹配的结果,findall会返回所有的结果</span><br><span class="line">    m=re.match()</span><br><span class="line">    m.string # 匹配的字符串</span><br><span class="line">    m.group(1,2) # 匹配1和2处字符串</span><br><span class="line"></span><br><span class="line">9. 替换和分割</span><br><span class="line">     split也可以使用正则表达式进行分割</span><br><span class="line">        p = re.compile(r&apos;\d+&apos;)</span><br><span class="line">        p.split(&apos;adwdwad1dawwd23dwadw&apos;) # 字符串复杂分割</span><br><span class="line">    sub # 用来替换</span><br><span class="line">        p = re.compile(r&apos;(\w+) (\w+)&apos;)</span><br><span class="line">        p.sub(r&apos;\2 \1&apos;, s) # 匹配字符串并且在匹配到的字符串处进行前后颠倒 </span><br><span class="line">    subn # 和sub类似,只不过除了返回替换的远足之外,还返回相应的替换次数,可以p.subn(afunc, s), afunc可以自己定义</span><br></pre></td></tr></table></figure><h2 id="日期处理"><a href="#日期处理" class="headerlink" title="日期处理"></a>日期处理</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from datetime import datetimes</span><br><span class="line">format=&quot;%Y-%m-%d %H:%M:%S&quot;指定日期格式</span><br><span class="line">date_s=datetime.striptime(s,format)</span><br><span class="line">date_s.year</span><br><span class="line">date_s.month</span><br><span class="line">date_s.now()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python正则表达式</title>
      <link href="/2019/07/08/python%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
      <url>/2019/07/08/python%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h2 id="Python正则表达式"><a href="#Python正则表达式" class="headerlink" title="Python正则表达式"></a>Python正则表达式</h2><p><strong>by 寒小阳(<a href="mailto:hanxiaoyang.ml@gmail.com" target="_blank" rel="noopener">hanxiaoyang.ml@gmail.com</a>)</strong></p><p>正则表达式是<strong>处理字符串</strong>的强大工具，拥有独特的语法和独立的处理引擎。</p><p>我们在大文本中匹配字符串时，有些情况用str自带的函数(比如find, in)可能可以完成，有些情况会稍稍复杂一些(比如说找出所有“像邮箱”的字符串，所有和julyedu相关的句子)，这个时候我们需要一个某种模式的工具，这个时候<strong>正则表达式</strong>就派上用场了。</p><p>说起来正则表达式效率上可能不如str自带的方法，但匹配功能实在强大太多。对啦，正则表达式不是Python独有的，如果已经在其他语言里使用过正则表达式，这里的说明只需要简单看一看就可以上手啦。</p><h2 id="1-语法"><a href="#1-语法" class="headerlink" title="1.语法"></a>1.语法</h2><p>废话少说，直接上技能</p><p>下面是一张有些同学比较熟的图，我们俗称python正则表达式小抄，把写正则表达式当做一个开卷考试，显然容易得多。</p><p>当你要匹配 <strong>一个/多个/任意个 数字/字母/非数字/非字母/某几个字符/任意字符</strong>，想要 <strong>贪婪/非贪婪</strong> 匹配，想要捕获匹配出来的 <strong>第一个/所有</strong> 内容的时候，记得这里有个小手册供你参考。</p><p><img src="http://life.chinaunix.net/bbsfile/forum/month_1012/101218124873e7f28d80d99801.jpg" alt="img"></p><h2 id="2-验证工具"><a href="#2-验证工具" class="headerlink" title="2.验证工具"></a>2.验证工具</h2><p>我们最喜爱的正则表达式在线验证工具之一是<a href="http://regexr.com/" target="_blank" rel="noopener">http://regexr.com/</a></p><p>谁用谁知道，用过一次以后欲罢不能。</p><p><img src="http://7xo0y8.com1.z0.glb.clouddn.com/regext.png" alt="img"></p><h2 id="3-挑战与提升"><a href="#3-挑战与提升" class="headerlink" title="3.挑战与提升"></a>3.挑战与提升</h2><p>长期做自然语言处理的同学正则表达式都非常熟，曾经有半年写了大量的正则表达式，以至于同事间开玩笑说，只要是符合某种规律或者模式的串，肯定分分钟能匹配出来。</p><p>对于想练习正则表达式，或者短期内快速get复杂技能，or想挑战更复杂的正则表达式的同学们。 请戳<a href="https://alf.nu/RegexGolf" target="_blank" rel="noopener">正则表达式进阶练习</a></p><p>so, 各位宝宝enjoy yourself</p><p><img src="http://7xo0y8.com1.z0.glb.clouddn.com/regext_2.png" alt="img"></p><h2 id="4-Python案例"><a href="#4-Python案例" class="headerlink" title="4.Python案例"></a>4.Python案例</h2><h3 id="re模块"><a href="#re模块" class="headerlink" title="re模块"></a>re模块</h3><p>Python通过re模块提供对正则表达式的支持。</p><p>使用re的一般步骤是</p><ul><li>1.将正则表达式的字符串形式编译为Pattern实例</li><li>2.使用Pattern实例处理文本并获得匹配结果（一个Match实例）</li><li>3.使用Match实例获得信息，进行其他的操作。</li></ul><p>In [13]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># encoding: UTF-8</span><br><span class="line">import re</span><br><span class="line"> </span><br><span class="line"># 将正则表达式编译成Pattern对象</span><br><span class="line">pattern = re.compile(r&apos;hello.*\!&apos;)</span><br><span class="line"> </span><br><span class="line"># 使用Pattern匹配文本，获得匹配结果，无法匹配时将返回None</span><br><span class="line">match = pattern.match(&apos;hello, hanxiaoyang! How are you?&apos;)</span><br><span class="line"> </span><br><span class="line">if match:</span><br><span class="line">    # 使用Match获得分组信息</span><br><span class="line">    print match.group()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hello, hanxiaoyang!</span><br></pre></td></tr></table></figure><h4 id="re-compile-strPattern-flag"><a href="#re-compile-strPattern-flag" class="headerlink" title="re.compile(strPattern[, flag]):"></a>re.compile(strPattern[, flag]):</h4><p>这个方法是Pattern类的工厂方法，用于将字符串形式的正则表达式编译为Pattern对象。</p><p>第二个参数flag是匹配模式，取值可以使用按位或运算符’|’表示同时生效，比如re.I | re.M。</p><p>当然，你也可以在regex字符串中指定模式，比如<strong>re.compile(‘pattern’, re.I | re.M)</strong>等价于<strong>re.compile(‘(?im)pattern’)</strong></p><p>flag可选值有：</p><ul><li>re.I(re.IGNORECASE): 忽略大小写（括号内是完整写法，下同）</li><li>re.M(MULTILINE): 多行模式，改变’^’和’$’的行为（参见上图）</li><li>re.S(DOTALL): 点任意匹配模式，改变’.’的行为</li><li>re.L(LOCALE): 使预定字符类 \w \W \b \B \s \S 取决于当前区域设定</li><li>re.U(UNICODE): 使预定字符类 \w \W \b \B \s \S \d \D 取决于unicode定义的字符属性</li><li>re.X(VERBOSE): 详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。以下两个正则表达式是等价的：</li></ul><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">regex_1 = re.compile(r&quot;&quot;&quot;\d +  # 数字部分</span><br><span class="line">                         \.    # 小数点部分</span><br><span class="line">                         \d *  # 小数的数字部分&quot;&quot;&quot;, re.X)</span><br><span class="line">regex_2 = re.compile(r&quot;\d+\.\d*&quot;)</span><br></pre></td></tr></table></figure><h3 id="Match"><a href="#Match" class="headerlink" title="Match"></a>Match</h3><p>Match对象是一次匹配的结果，包含了很多关于此次匹配的信息，可以使用Match提供的可读属性或方法来获取这些信息。</p><h4 id="match属性："><a href="#match属性：" class="headerlink" title="match属性："></a>match属性：</h4><ul><li>string: 匹配时使用的文本。</li><li>re: 匹配时使用的Pattern对象。</li><li>pos: 文本中正则表达式开始搜索的索引。值与Pattern.match()和Pattern.seach()方法的同名参数相同。</li><li>endpos: 文本中正则表达式结束搜索的索引。值与Pattern.match()和Pattern.seach()方法的同名参数相同。</li><li>lastindex: 最后一个被捕获的分组在文本中的索引。如果没有被捕获的分组，将为None。</li><li>lastgroup: 最后一个被捕获的分组的别名。如果这个分组没有别名或者没有被捕获的分组，将为None。</li></ul><h4 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h4><ul><li>group([group1, …]):<br>获得一个或多个分组截获的字符串；指定多个参数时将以元组形式返回。group1可以使用编号也可以使用别名；编号0代表整个匹配的子串；不填写参数时，返回group(0)；没有截获字符串的组返回None；截获了多次的组返回最后一次截获的子串。</li><li>groups([default]):<br>以元组形式返回全部分组截获的字符串。相当于调用group(1,2,…last)。default表示没有截获字符串的组以这个值替代，默认为None。</li><li>groupdict([default]):<br>返回以有别名的组的别名为键、以该组截获的子串为值的字典，没有别名的组不包含在内。default含义同上。</li><li>start([group]):<br>返回指定的组截获的子串在string中的起始索引（子串第一个字符的索引）。group默认值为0。</li><li>end([group]):<br>返回指定的组截获的子串在string中的结束索引（子串最后一个字符的索引+1）。group默认值为0。</li><li>span([group]):<br>返回(start(group), end(group))。</li><li>expand(template):<br>将匹配到的分组代入template中然后返回。template中可以使用\id或\g、\g引用分组，但不能使用编号0。\id与\g是等价的；但\10将被认为是第10个分组，如果你想表达\1之后是字符’0’，只能使用\g&lt;1&gt;0。</li></ul><p>In [14]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">m = re.match(r&apos;(\w+) (\w+)(?P&lt;sign&gt;.*)&apos;, &apos;hello hanxiaoyang!&apos;)</span><br><span class="line"> </span><br><span class="line">print &quot;m.string:&quot;, m.string</span><br><span class="line">print &quot;m.re:&quot;, m.re</span><br><span class="line">print &quot;m.pos:&quot;, m.pos</span><br><span class="line">print &quot;m.endpos:&quot;, m.endpos</span><br><span class="line">print &quot;m.lastindex:&quot;, m.lastindex</span><br><span class="line">print &quot;m.lastgroup:&quot;, m.lastgroup</span><br><span class="line"> </span><br><span class="line">print &quot;m.group(1,2):&quot;, m.group(1, 2)</span><br><span class="line">print &quot;m.groups():&quot;, m.groups()</span><br><span class="line">print &quot;m.groupdict():&quot;, m.groupdict()</span><br><span class="line">print &quot;m.start(2):&quot;, m.start(2)</span><br><span class="line">print &quot;m.end(2):&quot;, m.end(2)</span><br><span class="line">print &quot;m.span(2):&quot;, m.span(2)</span><br><span class="line">print r&quot;m.expand(r&apos;\2 \1\3&apos;):&quot;, m.expand(r&apos;\2 \1\3&apos;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">m.string: hello hanxiaoyang!</span><br><span class="line">m.re: &lt;_sre.SRE_Pattern object at 0x10b111be0&gt;</span><br><span class="line">m.pos: 0</span><br><span class="line">m.endpos: 18</span><br><span class="line">m.lastindex: 3</span><br><span class="line">m.lastgroup: sign</span><br><span class="line">m.group(1,2): (&apos;hello&apos;, &apos;hanxiaoyang&apos;)</span><br><span class="line">m.groups(): (&apos;hello&apos;, &apos;hanxiaoyang&apos;, &apos;!&apos;)</span><br><span class="line">m.groupdict(): &#123;&apos;sign&apos;: &apos;!&apos;&#125;</span><br><span class="line">m.start(2): 6</span><br><span class="line">m.end(2): 17</span><br><span class="line">m.span(2): (6, 17)</span><br><span class="line">m.expand(r&apos;\2 \1\3&apos;): hanxiaoyang hello!</span><br></pre></td></tr></table></figure><h3 id="Pattern"><a href="#Pattern" class="headerlink" title="Pattern"></a>Pattern</h3><p>Pattern对象是一个编译好的正则表达式，通过Pattern提供的一系列方法可以对文本进行匹配查找。</p><p>Pattern不能直接实例化，必须使用re.compile()进行构造。</p><p>Pattern提供了几个可读属性用于获取表达式的相关信息：</p><ul><li>pattern: 编译时用的表达式字符串。</li><li>flags: 编译时用的匹配模式。数字形式。</li><li>groups: 表达式中分组的数量。</li><li>groupindex: 以表达式中有别名的组的别名为键、以该组对应的编号为值的字典，没有别名的组不包含在内。</li></ul><p>In [15]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">p = re.compile(r&apos;(\w+) (\w+)(?P&lt;sign&gt;.*)&apos;, re.DOTALL)</span><br><span class="line"> </span><br><span class="line">print &quot;p.pattern:&quot;, p.pattern</span><br><span class="line">print &quot;p.flags:&quot;, p.flags</span><br><span class="line">print &quot;p.groups:&quot;, p.groups</span><br><span class="line">print &quot;p.groupindex:&quot;, p.groupindex</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">p.pattern: (\w+) (\w+)(?P&lt;sign&gt;.*)</span><br><span class="line">p.flags: 16</span><br><span class="line">p.groups: 3</span><br><span class="line">p.groupindex: &#123;&apos;sign&apos;: 3&#125;</span><br></pre></td></tr></table></figure><h3 id="使用pattern"><a href="#使用pattern" class="headerlink" title="使用pattern"></a>使用pattern</h3><ul><li><p>match(string[, pos[, endpos]]) | re.match(pattern, string[, flags])</p><p>:</p></li></ul><p>  这个方法将从string的pos下标处起尝试匹配pattern</p><p>  :</p><ul><li>如果pattern结束时仍可匹配，则返回一个Match对象</li><li>如果匹配过程中pattern无法匹配，或者匹配未结束就已到达endpos，则返回None。 </li><li>pos和endpos的默认值分别为0和len(string)。<br><strong>注意：这个方法并不是完全匹配。当pattern结束时若string还有剩余字符，仍然视为成功。想要完全匹配，可以在表达式末尾加上边界匹配符’$’。</strong> </li></ul><ul><li><p>search(string[, pos[, endpos]]) | re.search(pattern, string[, flags])</p><p>:</p></li></ul><p>  这个方法从string的pos下标处起尝试匹配pattern</p><ul><li>如果pattern结束时仍可匹配，则返回一个Match对象</li><li>若无法匹配，则将pos加1后重新尝试匹配，直到pos=endpos时仍无法匹配则返回None。 </li><li>pos和endpos的默认值分别为0和len(string))</li></ul><p>In [18]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># encoding: UTF-8 </span><br><span class="line">import re </span><br><span class="line"> </span><br><span class="line"># 将正则表达式编译成Pattern对象 </span><br><span class="line">pattern = re.compile(r&apos;H.*g&apos;) </span><br><span class="line"> </span><br><span class="line"># 使用search()查找匹配的子串，不存在能匹配的子串时将返回None </span><br><span class="line"># 这个例子中使用match()无法成功匹配 </span><br><span class="line">match = pattern.search(&apos;hello Hanxiaoyang!&apos;) </span><br><span class="line"> </span><br><span class="line">if match: </span><br><span class="line">    # 使用Match获得分组信息 </span><br><span class="line">    print match.group()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hanxiaoyang</span><br></pre></td></tr></table></figure><ul><li>split(string[, maxsplit]) | re.split(pattern, string[, maxsplit]):<ul><li>按照能够匹配的子串将string分割后返回列表。</li><li>maxsplit用于指定最大分割次数，不指定将全部分割。 </li></ul></li></ul><p>In [19]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"> </span><br><span class="line">p = re.compile(r&apos;\d+&apos;)</span><br><span class="line">print p.split(&apos;one1two2three3four4&apos;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&apos;one&apos;, &apos;two&apos;, &apos;three&apos;, &apos;four&apos;, &apos;&apos;]</span><br></pre></td></tr></table></figure><ul><li><p>findall(string[, pos[, endpos]]) | re.findall(pattern, string[, flags])</p><p>:</p></li></ul><ul><li>搜索string，以列表形式返回全部能匹配的子串。</li></ul><p>In [21]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"> </span><br><span class="line">p = re.compile(r&apos;\d+&apos;)</span><br><span class="line">print p.findall(&apos;one1two2three3four4&apos;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;]</span><br></pre></td></tr></table></figure><ul><li>finditer(string[, pos[, endpos]]) | re.finditer(pattern, string[, flags]): <ul><li>搜索string，返回一个顺序访问每一个匹配结果（Match对象）的迭代器。 </li></ul></li></ul><p>In [23]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"> </span><br><span class="line">p = re.compile(r&apos;\d+&apos;)</span><br><span class="line">for m in p.finditer(&apos;one1two2three3four4&apos;):</span><br><span class="line">    print m.group()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td></tr></table></figure><ul><li><p>sub(repl, string[, count]) | re.sub(pattern, repl, string[, count]): </p><ul><li>使用repl替换string中每一个匹配的子串后返回替换后的字符串。</li></ul></li></ul><pre><code>- 当repl是一个字符串时，可以使用\id或\g、\g引用分组，但不能使用编号0。 - 当repl是一个方法时，这个方法应当只接受一个参数（Match对象），并返回一个字符串用于替换（返回的字符串中不能再引用分组）。 count用于指定最多替换次数，不指定时全部替换。</code></pre><p>In [26]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"> </span><br><span class="line">p = re.compile(r&apos;(\w+) (\w+)&apos;)</span><br><span class="line">s = &apos;i say, hello hanxiaoyang!&apos;</span><br><span class="line"> </span><br><span class="line">print p.sub(r&apos;\2 \1&apos;, s)</span><br><span class="line"> </span><br><span class="line">def func(m):</span><br><span class="line">    return m.group(1).title() + &apos; &apos; + m.group(2).title()</span><br><span class="line"> </span><br><span class="line">print p.sub(func, s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">say i, hanxiaoyang hello!</span><br><span class="line">I Say, Hello Hanxiaoyang!</span><br></pre></td></tr></table></figure><ul><li>subn(repl, string[, count]) |re.sub(pattern, repl, string[, count]): <ul><li>返回 (sub(repl, string[, count]), 替换次数)。</li></ul></li></ul><p>In [28]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"> </span><br><span class="line">p = re.compile(r&apos;(\w+) (\w+)&apos;)</span><br><span class="line">s = &apos;i say, hello hanxiaoyang!&apos;</span><br><span class="line"> </span><br><span class="line">print p.subn(r&apos;\2 \1&apos;, s)</span><br><span class="line"> </span><br><span class="line">def func(m):</span><br><span class="line">    return m.group(1).title() + &apos; &apos; + m.group(2).title()</span><br><span class="line"> </span><br><span class="line">print p.subn(func, s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(&apos;say i, hanxiaoyang hello!&apos;, 2)</span><br><span class="line">(&apos;I Say, Hello Hanxiaoyang!&apos;, 2)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 正则表达式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jieba中文处理</title>
      <link href="/2019/06/08/jieba%E4%B8%AD%E6%96%87%E5%A4%84%E7%90%86/"/>
      <url>/2019/06/08/jieba%E4%B8%AD%E6%96%87%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h2 id="jieba中文处理"><a href="#jieba中文处理" class="headerlink" title="jieba中文处理"></a>jieba中文处理</h2><p>和拉丁语系不同，亚洲语言是不用空格分开每个有意义的词的。而当我们进行自然语言处理的时候，大部分情况下，词汇是我们对句子和文章理解的基础，因此需要一个工具去把完整的文本中分解成粒度更细的词。</p><p>jieba就是这样一个非常好用的中文工具，是以分词起家的，但是功能比分词要强大很多。</p><h3 id="1-基本分词函数与用法"><a href="#1-基本分词函数与用法" class="headerlink" title="1.基本分词函数与用法"></a>1.基本分词函数与用法</h3><p>jieba.cut 以及 jieba.cut_for_search 返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode)</p><p><strong>jieba.cut</strong> 方法接受三个输入参数:</p><ul><li>需要分词的字符串</li><li>cut_all 参数用来控制是否采用全模式</li><li>HMM 参数用来控制是否使用 HMM 模型</li></ul><p><strong>jieba.cut_for_search</strong> 方法接受两个参数</p><ul><li>需要分词的字符串</li><li>是否使用 HMM 模型。</li></ul><p>该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细</p><p>In [1]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut(<span class="string">"我在学习自然语言处理"</span>, cut_all=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">print</span> seg_list</span><br><span class="line">print(<span class="string">"Full Mode: "</span> + <span class="string">"/ "</span>.join(seg_list))  <span class="comment"># 全模式</span></span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut(<span class="string">"我在学习自然语言处理"</span>, cut_all=<span class="literal">False</span>)</span><br><span class="line">print(<span class="string">"Default Mode: "</span> + <span class="string">"/ "</span>.join(seg_list))  <span class="comment"># 精确模式</span></span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut(<span class="string">"他毕业于上海交通大学，在百度深度学习研究院进行研究"</span>)  <span class="comment"># 默认是精确模式</span></span><br><span class="line">print(<span class="string">", "</span>.join(seg_list))</span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut_for_search(<span class="string">"小明硕士毕业于中国科学院计算所，后在哈佛大学深造"</span>)  <span class="comment"># 搜索引擎模式</span></span><br><span class="line">print(<span class="string">", "</span>.join(seg_list))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Building prefix dict from the default dictionary ...</span><br><span class="line">Loading model from cache /var/folders/pn/xp31896922n9rqxgftrqk3l00000gn/T/jieba.cache</span><br><span class="line">Loading model cost 0.496 seconds.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;generator object cut at 0x10bbd91e0&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Prefix dict has been built succesfully.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Full Mode: 我/ 在/ 学习/ 自然/ 自然语言/ 语言/ 处理</span><br><span class="line">Default Mode: 我/ 在/ 学习/ 自然语言/ 处理</span><br><span class="line">他, 毕业, 于, 上海交通大学, ，, 在, 百度, 深度, 学习, 研究院, 进行, 研究</span><br><span class="line">小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 哈佛, 大学, 哈佛大学, 深造</span><br></pre></td></tr></table></figure><p><strong>jieba.lcut</strong>以及<strong>jieba.lcut_for_search</strong>直接返回 list</p><p>In [2]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">result_lcut = jieba.lcut(<span class="string">"小明硕士毕业于中国科学院计算所，后在哈佛大学深造"</span>)</span><br><span class="line"><span class="keyword">print</span> result_lcut</span><br><span class="line"><span class="keyword">print</span> <span class="string">" "</span>.join(result_lcut)</span><br><span class="line"><span class="keyword">print</span> <span class="string">" "</span>.join(jieba.lcut_for_search(<span class="string">"小明硕士毕业于中国科学院计算所，后在哈佛大学深造"</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">u'\u5c0f\u660e'</span>, <span class="string">u'\u7855\u58eb'</span>, <span class="string">u'\u6bd5\u4e1a'</span>, <span class="string">u'\u4e8e'</span>, <span class="string">u'\u4e2d\u56fd\u79d1\u5b66\u9662'</span>, <span class="string">u'\u8ba1\u7b97\u6240'</span>, <span class="string">u'\uff0c'</span>, <span class="string">u'\u540e'</span>, <span class="string">u'\u5728'</span>, <span class="string">u'\u54c8\u4f5b\u5927\u5b66'</span>, <span class="string">u'\u6df1\u9020'</span>]</span><br><span class="line">小明 硕士 毕业 于 中国科学院 计算所 ， 后 在 哈佛大学 深造</span><br><span class="line">小明 硕士 毕业 于 中国 科学 学院 科学院 中国科学院 计算 计算所 ， 后 在 哈佛 大学 哈佛大学 深造</span><br></pre></td></tr></table></figure><h4 id="添加用户自定义词典"><a href="#添加用户自定义词典" class="headerlink" title="添加用户自定义词典"></a>添加用户自定义词典</h4><p>很多时候我们需要针对自己的场景进行分词，会有一些领域内的专有词汇。</p><ul><li>1.可以用jieba.load_userdict(file_name)加载用户字典</li><li>2.少量的词汇可以自己用下面方法手动添加：<ul><li>用 add_word(word, freq=None, tag=None) 和 del_word(word) 在程序中动态修改词典</li><li>用 suggest_freq(segment, tune=True) 可调节单个词语的词频，使其能（或不能）被分出来。</li></ul></li></ul><p>In [3]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&apos;/&apos;.join(jieba.cut(&apos;如果放到旧字典中将出错。&apos;, HMM=False)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果/放到/旧/字典/中将/出错/。</span><br></pre></td></tr></table></figure><p>In [4]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jieba.suggest_freq((&apos;中&apos;, &apos;将&apos;), True)</span><br></pre></td></tr></table></figure><p>Out[4]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">494</span><br></pre></td></tr></table></figure><p>In [5]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&apos;/&apos;.join(jieba.cut(&apos;如果放到旧字典中将出错。&apos;, HMM=False)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果/放到/旧/字典/中/将/出错/。</span><br></pre></td></tr></table></figure><h3 id="关键词提取"><a href="#关键词提取" class="headerlink" title="关键词提取"></a>关键词提取</h3><h4 id="基于-TF-IDF-算法的关键词抽取"><a href="#基于-TF-IDF-算法的关键词抽取" class="headerlink" title="基于 TF-IDF 算法的关键词抽取"></a>基于 TF-IDF 算法的关键词抽取</h4><p>import jieba.analyse</p><ul><li>jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())<ul><li>sentence 为待提取的文本</li><li>topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20</li><li>withWeight 为是否一并返回关键词权重值，默认值为 False</li><li>allowPOS 仅包括指定词性的词，默认值为空，即不筛选</li></ul></li></ul><p>In [6]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import jieba.analyse as analyse</span><br><span class="line">lines = open(&apos;NBA.txt&apos;).read()</span><br><span class="line">print &quot;  &quot;.join(analyse.extract_tags(lines, topK=20, withWeight=False, allowPOS=()))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">韦少  杜兰特  全明星  全明星赛  MVP  威少  正赛  科尔  投篮  勇士  球员  斯布鲁克  更衣柜  张卫平  三连庄  NBA  西部  指导  雷霆  明星队</span><br></pre></td></tr></table></figure><p>In [7]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lines = open(u&apos;西游记.txt&apos;).read()</span><br><span class="line">print &quot;  &quot;.join(analyse.extract_tags(lines, topK=20, withWeight=False, allowPOS=()))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">行者  八戒  师父  三藏  唐僧  大圣  沙僧  妖精  菩萨  和尚  那怪  那里  长老  呆子  徒弟  怎么  不知  老孙  国王  一个</span><br></pre></td></tr></table></figure><h4 id="关于TF-IDF-算法的关键词抽取补充"><a href="#关于TF-IDF-算法的关键词抽取补充" class="headerlink" title="关于TF-IDF 算法的关键词抽取补充"></a>关于TF-IDF 算法的关键词抽取补充</h4><ul><li>关键词提取所使用逆向文件频率（IDF）文本语料库可以切换成自定义语料库的路径<ul><li>用法： jieba.analyse.set_idf_path(file_name) # file_name为自定义语料库的路径<ul><li>自定义语料库示例见<a href="https://github.com/fxsjy/jieba/blob/master/extra_dict/idf.txt.big" target="_blank" rel="noopener">这里</a></li><li>用法示例见<a href="https://github.com/fxsjy/jieba/blob/master/test/extract_tags_idfpath.py" target="_blank" rel="noopener">这里</a></li></ul></li><li>关键词提取所使用停止词（Stop Words）文本语料库可以切换成自定义语料库的路径<ul><li>用法： jieba.analyse.set_stop_words(file_name) # file_name为自定义语料库的路径</li><li>自定义语料库示例见<a href="https://github.com/fxsjy/jieba/blob/master/extra_dict/stop_words.txt" target="_blank" rel="noopener">这里</a></li><li>用法示例见<a href="https://github.com/fxsjy/jieba/blob/master/test/extract_tags_stop_words.py" target="_blank" rel="noopener">这里</a></li></ul></li></ul></li><li>关键词一并返回关键词权重值示例<ul><li>用法示例见<a href="https://github.com/fxsjy/jieba/blob/master/test/extract_tags_with_weight.py" target="_blank" rel="noopener">这里</a></li></ul></li></ul><h4 id="基于-TextRank-算法的关键词抽取"><a href="#基于-TextRank-算法的关键词抽取" class="headerlink" title="基于 TextRank 算法的关键词抽取"></a>基于 TextRank 算法的关键词抽取</h4><ul><li>jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(‘ns’, ‘n’, ‘vn’, ‘v’)) 直接使用，接口相同，注意默认过滤词性。</li><li>jieba.analyse.TextRank() 新建自定义 TextRank 实例</li></ul><p>算法论文： <a href="http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf" target="_blank" rel="noopener">TextRank: Bringing Order into Texts</a></p><p>基本思想:</p><ul><li>将待抽取关键词的文本进行分词</li><li>以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图</li><li>计算图中节点的PageRank，注意是无向带权图</li></ul><p>In [8]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba.analyse <span class="keyword">as</span> analyse</span><br><span class="line">lines = open(<span class="string">'NBA.txt'</span>).read()</span><br><span class="line"><span class="keyword">print</span> <span class="string">"  "</span>.join(analyse.textrank(lines, topK=<span class="number">20</span>, withWeight=<span class="literal">False</span>, allowPOS=(<span class="string">'ns'</span>, <span class="string">'n'</span>, <span class="string">'vn'</span>, <span class="string">'v'</span>)))</span><br><span class="line"><span class="keyword">print</span> <span class="string">"---------------------我是分割线----------------"</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"  "</span>.join(analyse.textrank(lines, topK=<span class="number">20</span>, withWeight=<span class="literal">False</span>, allowPOS=(<span class="string">'ns'</span>, <span class="string">'n'</span>)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">全明星赛  勇士  正赛  指导  对方  投篮  球员  没有  出现  时间  威少  认为  看来  结果  相隔  助攻  现场  三连庄  介绍  嘉宾</span><br><span class="line">---------------------我是分割线----------------</span><br><span class="line">勇士  正赛  全明星赛  指导  投篮  玩命  时间  对方  现场  结果  球员  嘉宾  时候  全队  主持人  特点  大伙  肥皂剧  全程  快船队</span><br></pre></td></tr></table></figure><p>In [9]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lines = open(u&apos;西游记.txt&apos;).read()</span><br><span class="line">print &quot;  &quot;.join(analyse.textrank(lines, topK=20, withWeight=False, allowPOS=(&apos;ns&apos;, &apos;n&apos;, &apos;vn&apos;, &apos;v&apos;)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">行者  师父  八戒  三藏  大圣  不知  菩萨  妖精  只见  长老  国王  却说  呆子  徒弟  小妖  出来  不得  不见  不能  师徒</span><br></pre></td></tr></table></figure><h3 id="词性标注"><a href="#词性标注" class="headerlink" title="词性标注"></a>词性标注</h3><ul><li>jieba.posseg.POSTokenizer(tokenizer=None) 新建自定义分词器，tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。</li><li>标注句子分词后每个词的词性，采用和 ictclas 兼容的标记法。</li><li>具体的词性对照表参见<a href="http://ictclas.nlpir.org/nlpir/html/readme.htm" target="_blank" rel="noopener">计算所汉语词性标记集</a></li></ul><p>In [10]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> pseg</span><br><span class="line">words = pseg.cut(<span class="string">"我爱自然语言处理"</span>)</span><br><span class="line"><span class="keyword">for</span> word, flag <span class="keyword">in</span> words:</span><br><span class="line">    print(<span class="string">'%s %s'</span> % (word, flag))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">我 r</span><br><span class="line">爱 v</span><br><span class="line">自然语言 l</span><br><span class="line">处理 v</span><br></pre></td></tr></table></figure><h3 id="并行分词"><a href="#并行分词" class="headerlink" title="并行分词"></a>并行分词</h3><p>原理：将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，然后归并结果，从而获得分词速度的可观提升 基于 python 自带的 multiprocessing 模块，目前暂不支持 Windows</p><p>用法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">jieba.enable_parallel(<span class="number">4</span>) <span class="comment"># 开启并行分词模式，参数为并行进程数</span></span><br><span class="line">jieba.disable_parallel() <span class="comment"># 关闭并行分词模式</span></span><br></pre></td></tr></table></figure><p>实验结果：在 4 核 3.4GHz Linux 机器上，对金庸全集进行精确分词，获得了 1MB/s 的速度，是单进程版的 3.3 倍。</p><p>注意：并行分词仅支持默认分词器 jieba.dt 和 jieba.posseg.dt。</p><p>In [11]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line">jieba.enable_parallel()</span><br><span class="line">content = open(<span class="string">u'西游记.txt'</span>,<span class="string">"r"</span>).read()</span><br><span class="line">t1 = time.time()</span><br><span class="line">words = <span class="string">"/ "</span>.join(jieba.cut(content))</span><br><span class="line">t2 = time.time()</span><br><span class="line">tm_cost = t2-t1</span><br><span class="line">print(<span class="string">'并行分词速度为 %s bytes/second'</span> % (len(content)/tm_cost))</span><br><span class="line"></span><br><span class="line">jieba.disable_parallel()</span><br><span class="line">content = open(<span class="string">u'西游记.txt'</span>,<span class="string">"r"</span>).read()</span><br><span class="line">t1 = time.time()</span><br><span class="line">words = <span class="string">"/ "</span>.join(jieba.cut(content))</span><br><span class="line">t2 = time.time()</span><br><span class="line">tm_cost = t2-t1</span><br><span class="line">print(<span class="string">'非并行分词速度为 %s bytes/second'</span> % (len(content)/tm_cost))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">并行分词速度为 830619.50933 bytes/second</span><br><span class="line">非并行分词速度为 259941.448353 bytes/second</span><br></pre></td></tr></table></figure><h3 id="Tokenize：返回词语在原文的起止位置"><a href="#Tokenize：返回词语在原文的起止位置" class="headerlink" title="Tokenize：返回词语在原文的起止位置"></a>Tokenize：返回词语在原文的起止位置</h3><p>注意，输入参数只接受 unicode</p><p>In [12]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> <span class="string">"这是默认模式的tokenize"</span></span><br><span class="line">result = jieba.tokenize(<span class="string">u'自然语言处理非常有用'</span>)</span><br><span class="line"><span class="keyword">for</span> tk <span class="keyword">in</span> result:</span><br><span class="line">    print(<span class="string">"%s\t\t start: %d \t\t end:%d"</span> % (tk[<span class="number">0</span>],tk[<span class="number">1</span>],tk[<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">"\n-----------我是神奇的分割线------------\n"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">"这是搜索模式的tokenize"</span></span><br><span class="line">result = jieba.tokenize(<span class="string">u'自然语言处理非常有用'</span>, mode=<span class="string">'search'</span>)</span><br><span class="line"><span class="keyword">for</span> tk <span class="keyword">in</span> result:</span><br><span class="line">    print(<span class="string">"%s\t\t start: %d \t\t end:%d"</span> % (tk[<span class="number">0</span>],tk[<span class="number">1</span>],tk[<span class="number">2</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">这是默认模式的tokenize</span><br><span class="line">自然语言 start: <span class="number">0</span>  end:<span class="number">4</span></span><br><span class="line">处理 start: <span class="number">4</span>  end:<span class="number">6</span></span><br><span class="line">非常 start: <span class="number">6</span>  end:<span class="number">8</span></span><br><span class="line">有用 start: <span class="number">8</span>  end:<span class="number">10</span></span><br><span class="line"></span><br><span class="line">-----------我是神奇的分割线------------</span><br><span class="line"></span><br><span class="line">这是搜索模式的tokenize</span><br><span class="line">自然 start: <span class="number">0</span>  end:<span class="number">2</span></span><br><span class="line">语言 start: <span class="number">2</span>  end:<span class="number">4</span></span><br><span class="line">自然语言 start: <span class="number">0</span>  end:<span class="number">4</span></span><br><span class="line">处理 start: <span class="number">4</span>  end:<span class="number">6</span></span><br><span class="line">非常 start: <span class="number">6</span>  end:<span class="number">8</span></span><br><span class="line">有用 start: <span class="number">8</span>  end:<span class="number">10</span></span><br></pre></td></tr></table></figure><h3 id="ChineseAnalyzer-for-Whoosh-搜索引擎"><a href="#ChineseAnalyzer-for-Whoosh-搜索引擎" class="headerlink" title="ChineseAnalyzer for Whoosh 搜索引擎"></a>ChineseAnalyzer for Whoosh 搜索引擎</h3><ul><li><code>from jieba.analyse import ChineseAnalyzer</code></li></ul><p>In [16]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals</span><br><span class="line"><span class="keyword">import</span> sys,os</span><br><span class="line">sys.path.append(<span class="string">"../"</span>)</span><br><span class="line"><span class="keyword">from</span> whoosh.index <span class="keyword">import</span> create_in,open_dir</span><br><span class="line"><span class="keyword">from</span> whoosh.fields <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> whoosh.qparser <span class="keyword">import</span> QueryParser</span><br><span class="line"></span><br><span class="line">analyzer = jieba.analyse.ChineseAnalyzer()</span><br><span class="line">schema = Schema(title=TEXT(stored=<span class="literal">True</span>), path=ID(stored=<span class="literal">True</span>), content=TEXT(stored=<span class="literal">True</span>, analyzer=analyzer))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">"tmp"</span>):</span><br><span class="line">    os.mkdir(<span class="string">"tmp"</span>)</span><br><span class="line"></span><br><span class="line">ix = create_in(<span class="string">"tmp"</span>, schema) <span class="comment"># for create new index</span></span><br><span class="line"><span class="comment">#ix = open_dir("tmp") # for read only</span></span><br><span class="line">writer = ix.writer()</span><br><span class="line"></span><br><span class="line">writer.add_document(</span><br><span class="line">    title=<span class="string">"document1"</span>,</span><br><span class="line">    path=<span class="string">"/a"</span>,</span><br><span class="line">    content=<span class="string">"This is the first document we’ve added!"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">writer.add_document(</span><br><span class="line">    title=<span class="string">"document2"</span>,</span><br><span class="line">    path=<span class="string">"/b"</span>,</span><br><span class="line">    content=<span class="string">"The second one 你 中文测试中文 is even more interesting! 吃水果"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">writer.add_document(</span><br><span class="line">    title=<span class="string">"document3"</span>,</span><br><span class="line">    path=<span class="string">"/c"</span>,</span><br><span class="line">    content=<span class="string">"买水果然后来世博园。"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">writer.add_document(</span><br><span class="line">    title=<span class="string">"document4"</span>,</span><br><span class="line">    path=<span class="string">"/c"</span>,</span><br><span class="line">    content=<span class="string">"工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">writer.add_document(</span><br><span class="line">    title=<span class="string">"document4"</span>,</span><br><span class="line">    path=<span class="string">"/c"</span>,</span><br><span class="line">    content=<span class="string">"咱俩交换一下吧。"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">writer.commit()</span><br><span class="line">searcher = ix.searcher()</span><br><span class="line">parser = QueryParser(<span class="string">"content"</span>, schema=ix.schema)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> keyword <span class="keyword">in</span> (<span class="string">"水果世博园"</span>,<span class="string">"你"</span>,<span class="string">"first"</span>,<span class="string">"中文"</span>,<span class="string">"交换机"</span>,<span class="string">"交换"</span>):</span><br><span class="line">    print(keyword+<span class="string">"的结果为如下："</span>)</span><br><span class="line">    q = parser.parse(keyword)</span><br><span class="line">    results = searcher.search(q)</span><br><span class="line">    <span class="keyword">for</span> hit <span class="keyword">in</span> results:</span><br><span class="line">        print(hit.highlights(<span class="string">"content"</span>))</span><br><span class="line">    print(<span class="string">"\n--------------我是神奇的分割线--------------\n"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> analyzer(<span class="string">"我的好朋友是李明;我爱北京天安门;IBM和Microsoft; I have a dream. this is intetesting and interested me a lot"</span>):</span><br><span class="line">    print(t.text)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">水果世博园的结果为如下：</span><br><span class="line">买&lt;b class="match term0"&gt;水果&lt;/b&gt;然后来&lt;b class="match term1"&gt;世博园&lt;/b&gt;</span><br><span class="line"></span><br><span class="line">--------------我是神奇的分割线--------------</span><br><span class="line"></span><br><span class="line">你的结果为如下：</span><br><span class="line">second one &lt;b class="match term0"&gt;你&lt;/b&gt; 中文测试中文 is even more interesting</span><br><span class="line"></span><br><span class="line">--------------我是神奇的分割线--------------</span><br><span class="line"></span><br><span class="line">first的结果为如下：</span><br><span class="line">&lt;b class="match term0"&gt;first&lt;/b&gt; document we’ve added</span><br><span class="line"></span><br><span class="line">--------------我是神奇的分割线--------------</span><br><span class="line"></span><br><span class="line">中文的结果为如下：</span><br><span class="line">second one 你 &lt;b class="match term0"&gt;中文&lt;/b&gt;测试&lt;b class="match term0"&gt;中文&lt;/b&gt; is even more interesting</span><br><span class="line"></span><br><span class="line">--------------我是神奇的分割线--------------</span><br><span class="line"></span><br><span class="line">交换机的结果为如下：</span><br><span class="line">干事每月经过下属科室都要亲口交代24口&lt;b class="match term0"&gt;交换机&lt;/b&gt;等技术性器件的安装工作</span><br><span class="line"></span><br><span class="line">--------------我是神奇的分割线--------------</span><br><span class="line"></span><br><span class="line">交换的结果为如下：</span><br><span class="line">咱俩&lt;b class="match term0"&gt;交换&lt;/b&gt;一下吧</span><br><span class="line">干事每月经过下属科室都要亲口交代24口&lt;b class="match term0"&gt;交换&lt;/b&gt;机等技术性器件的安装工作</span><br><span class="line"></span><br><span class="line">--------------我是神奇的分割线--------------</span><br><span class="line"></span><br><span class="line">我</span><br><span class="line">好</span><br><span class="line">朋友</span><br><span class="line">是</span><br><span class="line">李明</span><br><span class="line">我</span><br><span class="line">爱</span><br><span class="line">北京</span><br><span class="line">天安</span><br><span class="line">天安门</span><br><span class="line">ibm</span><br><span class="line">microsoft</span><br><span class="line">dream</span><br><span class="line">intetest</span><br><span class="line">interest</span><br><span class="line">me</span><br><span class="line">lot</span><br></pre></td></tr></table></figure><h3 id="命令行分词"><a href="#命令行分词" class="headerlink" title="命令行分词"></a>命令行分词</h3><p>使用示例：<code>python -m jieba news.txt &gt; cut_result.txt</code></p><p>命令行选项（翻译）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">使用: python -m jieba [options] filename</span><br><span class="line"></span><br><span class="line">结巴命令行界面。</span><br><span class="line"></span><br><span class="line">固定参数:</span><br><span class="line">  filename              输入文件</span><br><span class="line"></span><br><span class="line">可选参数:</span><br><span class="line">  -h, --help            显示此帮助信息并退出</span><br><span class="line">  -d [DELIM], --delimiter [DELIM]</span><br><span class="line">                        使用 DELIM 分隔词语，而不是用默认的<span class="string">' / '</span>。</span><br><span class="line">                        若不指定 DELIM，则使用一个空格分隔。</span><br><span class="line">  -p [DELIM], --pos [DELIM]</span><br><span class="line">                        启用词性标注；如果指定 DELIM，词语和词性之间</span><br><span class="line">                        用它分隔，否则用 _ 分隔</span><br><span class="line">  -D DICT, --dict DICT  使用 DICT 代替默认词典</span><br><span class="line">  -u USER_DICT, --user-dict USER_DICT</span><br><span class="line">                        使用 USER_DICT 作为附加词典，与默认词典或自定义词典配合使用</span><br><span class="line">  -a, --cut-all         全模式分词（不支持词性标注）</span><br><span class="line">  -n, --no-hmm          不使用隐含马尔可夫模型</span><br><span class="line">  -q, --quiet           不输出载入信息到 STDERR</span><br><span class="line">  -V, --version         显示版本信息并退出</span><br><span class="line"></span><br><span class="line">如果没有指定文件名，则使用标准输入。</span><br></pre></td></tr></table></figure><p><code>--help</code> 选项输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$&gt; python -m jieba --help</span><br><span class="line">Jieba command line interface.</span><br><span class="line"></span><br><span class="line">positional arguments:</span><br><span class="line">  filename              input file</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --help            show this help message <span class="keyword">and</span> exit</span><br><span class="line">  -d [DELIM], --delimiter [DELIM]</span><br><span class="line">                        use DELIM instead of <span class="string">' / '</span> <span class="keyword">for</span> word delimiter; <span class="keyword">or</span> a</span><br><span class="line">                        space <span class="keyword">if</span> it <span class="keyword">is</span> used without DELIM</span><br><span class="line">  -p [DELIM], --pos [DELIM]</span><br><span class="line">                        enable POS tagging; <span class="keyword">if</span> DELIM <span class="keyword">is</span> specified, use DELIM</span><br><span class="line">                        instead of <span class="string">'_'</span> <span class="keyword">for</span> POS delimiter</span><br><span class="line">  -D DICT, --dict DICT  use DICT <span class="keyword">as</span> dictionary</span><br><span class="line">  -u USER_DICT, --user-dict USER_DICT</span><br><span class="line">                        use USER_DICT together <span class="keyword">with</span> the default dictionary <span class="keyword">or</span></span><br><span class="line">                        DICT (<span class="keyword">if</span> specified)</span><br><span class="line">  -a, --cut-all         full pattern cutting (ignored <span class="keyword">with</span> POS tagging)</span><br><span class="line">  -n, --no-hmm          don<span class="string">'t use the Hidden Markov Model</span></span><br><span class="line"><span class="string">  -q, --quiet           don'</span>t <span class="keyword">print</span> loading messages to stderr</span><br><span class="line">  -V, --version         show program<span class="string">'s version number and exit</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">If no filename specified, use STDIN instead.</span></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 分词 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jieba </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
