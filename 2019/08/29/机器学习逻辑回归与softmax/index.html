<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no">
<meta name="author" content="MingmingYe">



<meta name="description" content="机器学习逻辑回归与softmax机器学习中的线性模型谈及线性模型，其实我们很早就已经与它打过交道，还记得高中数学必修3课本中那个顽皮的“最小二乘法”吗？这就是线性模型的经典算法之一：根据给定的（x，y）点对，求出一条与这些点拟合效果最好的直线y=ax+b，之前我们利用下面的公式便可以计算出拟合直线的系数a,b（3.1中给出了具体的计算过程），从而对于一个新的x，可以预测它所对应的y值。前面我们提到">
<meta name="keywords" content="机器学习,LR,softmax">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习逻辑回归与softmax">
<meta property="og:url" content="http://mmyblog.cn/2019/08/29/机器学习逻辑回归与softmax/index.html">
<meta property="og:site_name" content="Stay hungry, Stay foolish.">
<meta property="og:description" content="机器学习逻辑回归与softmax机器学习中的线性模型谈及线性模型，其实我们很早就已经与它打过交道，还记得高中数学必修3课本中那个顽皮的“最小二乘法”吗？这就是线性模型的经典算法之一：根据给定的（x，y）点对，求出一条与这些点拟合效果最好的直线y=ax+b，之前我们利用下面的公式便可以计算出拟合直线的系数a,b（3.1中给出了具体的计算过程），从而对于一个新的x，可以预测它所对应的y值。前面我们提到">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc722b068e48.png">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc722b0ccec4.png">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc72567b8bcd.png">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc722b0ad8f7.png">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc722b0af652.png">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc722b090543.png">
<meta property="og:image" content="http://mmyblog.cn/blog_picture/line04.jpg">
<meta property="og:image" content="http://mmyblog.cn/blog_picture/line05.jpg">
<meta property="og:image" content="http://mmyblog.cn/blog_picture/line06.jpg">
<meta property="og:image" content="http://mmyblog.cn/blog_picture/line07.jpg">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc722b0cde33.png">
<meta property="og:image" content="http://mmyblog.cn/blog_picture/line01.jpg">
<meta property="og:image" content="http://mmyblog.cn/blog_picture/line02.jpg">
<meta property="og:image" content="http://mmyblog.cn/blog_picture/line03.jpg">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc722b103cbf.png">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc722b0a2841.png">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc722b0c7748.png">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc722b0a655d.png">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc723b824f0c.png">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc723b817961.png">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc723b863ebb.png">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc723b85bfa9.png">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc723b8156e1.png">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc723b7e9db3.png">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc723b7e8a61.png">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc723b83d5e0.png">
<meta property="og:image" content="http://mmyblog.cn/blog_picture/line08.jpg">
<meta property="og:image" content="http://mmyblog.cn/blog_picture/line09.jpg">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc723b862bfb.png">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc723b8300d5.png">
<meta property="og:image" content="https://i.loli.net/2018/10/17/5bc726fe87ae2.png">
<meta property="og:image" content="http://mmyblog.cn/blog_picture/line10.jpg">
<meta property="og:image" content="http://mmyblog.cn/blog_picture/line11.jpg">
<meta property="og:image" content="http://mmyblog.cn/blog_picture/line12.jpg">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171104175530607-1392543504.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201710/1251096-20171021171313943-1199609768.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201710/1251096-20171021164616881-992414484.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201710/1251096-20171021165228865-866731732.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105100956795-1587348606.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102153701-629755133.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102416560-1451219507.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102622045-1005416234.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102706623-369597312.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105113747779-692061991.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171104162333248-539020480.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105114132232-517057992.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105110553560-1026190635.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105120226607-495282914.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105113747779-692061991.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105161912482-1607069737.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105162625888-1575402902.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105163457810-492161690.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105170233232-810575386.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105171748341-1281292385.png">
<meta property="og:image" content="http://ufldl.stanford.edu/wiki/images/math/c/e/0/ce027336c1cb3c0cd461406c81369ebf.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f%28x_i%2CW%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f%28x_i%3BW%29%3DWx_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+Li%3D-log%28%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L_i%3D-f_%7By_i%7D%2Blog%28%5Csum_je%5E%7Bf_j%7D%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f_j">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=R%28W%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=z">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=q">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=P%28y_i%7Cx_i%2CW%29%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=e%5E%7Bf_%7By_i%7D%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csum_j+e%5E%7Bf_j%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7BCe%5E%7Bf_%7By_i%7D%7D%7D%7BC%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%2BlogC%7D%7D%7B%5Csum_je%5E%7Bf_j%2BlogC%7D%7D">
<meta property="og:image" content="https://pic1.zhimg.com/80/a90ce9e0ff533f3efee4747305382064_hd.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5B1%2C-2%2C0%5D%5Cto%5Be%5E1%2Ce%5E%7B-2%7D%2Ce%5E0%5D%3D%5B2.71%2C0.14%2C1%5D%5Cto%5B0.7%2C0.04%2C0.26%5D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5B0.5%2C-1%2C0%5D%5Cto%5Be%5E%7B0.5%7D%2Ce%5E%7B-1%7D%2Ce%5E0%5D%3D%5B1.65%2C0.73%2C1%5D%5Cto%5B0.55%2C0.12%2C0.33%5D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5CDelta+%3D1">
<meta property="og:updated_time" content="2019-08-30T09:19:01.631Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习逻辑回归与softmax">
<meta name="twitter:description" content="机器学习逻辑回归与softmax机器学习中的线性模型谈及线性模型，其实我们很早就已经与它打过交道，还记得高中数学必修3课本中那个顽皮的“最小二乘法”吗？这就是线性模型的经典算法之一：根据给定的（x，y）点对，求出一条与这些点拟合效果最好的直线y=ax+b，之前我们利用下面的公式便可以计算出拟合直线的系数a,b（3.1中给出了具体的计算过程），从而对于一个新的x，可以预测它所对应的y值。前面我们提到">
<meta name="twitter:image" content="https://i.loli.net/2018/10/17/5bc722b068e48.png">

<link rel="apple-touch-icon" href="/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="Stay hungry, Stay foolish." type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">



<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>机器学习逻辑回归与softmax | Stay hungry, Stay foolish.</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: true
    }
</script>


    <script>
        yiliaConfig.jquery_ui = [true, "//cdn.bootcss.com/jqueryui/1.10.4/jquery-ui.min.js", "//cdn.bootcss.com/jqueryui/1.10.4/css/jquery-ui.min.css"];
    </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






</head></html>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/deep.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">MingmingYe</a></h1>
        </hgroup>

        
        <p class="header-subtitle">当你的才华撑不起你的野心时，只有静下心来好好学习！纵使命运注定是个打酱油的，也要打一瓶与别人不一样的酱油！</p>
        

        
            <form id="search-form">
            <input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false" />
            <i class="fa fa-times" onclick="resetSearch()"></i>
            </form>
            <div id="local-search-result"></div>
            <p class='no-result'>No results found <i class='fa fa-spinner fa-pulse'></i></p>
        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:878759487@qq.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AUC/">AUC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LR/">LR</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ROC/">ROC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/boosting/">boosting</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cbow/">cbow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hierarchical-softmax/">hierarchical softmax</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mumpy/">mumpy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/negative-sampling/">negative sampling</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pandas/">pandas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pyspark/">pyspark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/seaborn/">seaborn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/skip-gram/">skip-gram</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/softmax/">softmax</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/优化方法/">优化方法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/决策树/">决策树</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/微积分/">微积分</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/概率/">概率</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/模型调优/">模型调优</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习速查表/">深度学习速查表</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/特征工程/">特征工程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/线性代数/">线性代数</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/统计/">统计</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/聚类/">聚类</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/贝叶斯分类器/">贝叶斯分类器</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/降维/">降维</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/集成学习/">集成学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/面试/">面试</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://mmyblog.cn/">mmy</a>
                    
                    </div>
                </section>
                

                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">MingmingYe</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/deep.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">MingmingYe</a></h1>
            </hgroup>
            
            <p class="header-subtitle">当你的才华撑不起你的野心时，只有静下心来好好学习！纵使命运注定是个打酱油的，也要打一瓶与别人不一样的酱油！</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:878759487@qq.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap"><article id="post-机器学习逻辑回归与softmax" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/08/29/机器学习逻辑回归与softmax/" class="article-date">
      <time datetime="2019-08-29T08:39:02.000Z" itemprop="datePublished">2019-08-29</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      机器学习逻辑回归与softmax
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LR/">LR</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/softmax/">softmax</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="机器学习逻辑回归与softmax"><a href="#机器学习逻辑回归与softmax" class="headerlink" title="机器学习逻辑回归与softmax"></a>机器学习逻辑回归与softmax</h1><h2 id="机器学习中的线性模型"><a href="#机器学习中的线性模型" class="headerlink" title="机器学习中的线性模型"></a>机器学习中的线性模型</h2><p>谈及线性模型，其实我们很早就已经与它打过交道，还记得高中数学必修3课本中那个顽皮的“最小二乘法”吗？这就是线性模型的经典算法之一：根据给定的（x，y）点对，求出一条与这些点拟合效果最好的直线y=ax+b，之前我们利用下面的公式便可以计算出拟合直线的系数a,b（3.1中给出了具体的计算过程），从而对于一个新的x，可以预测它所对应的y值。前面我们提到：在机器学习的术语中，当预测值为连续值时，称为“回归问题”，离散值时为“分类问题”。本篇先从线性回归任务开始，接着讨论分类和多分类问题。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b068e48.png" alt="1.png"></p>
<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>线性回归问题就是试图学到一个线性模型尽可能准确地预测新样本的输出值，例如：通过历年的人口数据预测2017年人口数量。在这类问题中，往往我们会先得到一系列的有标记数据，例如：2000–&gt;13亿…2016–&gt;15亿，这时输入的属性只有一个，即年份；也有输入多属性的情形，假设我们预测一个人的收入，这时输入的属性值就不止一个了，例如：（学历，年龄，性别，颜值，身高，体重）–&gt;15k。</p>
<p>有时这些输入的属性值并不能直接被我们的学习模型所用，需要进行相应的处理，对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；对于离散值的属性，可作下面的处理：</p>
<ul>
<li><p>若属性值之间存在“序关系”，则可以将其转化为连续值，例如：身高属性分为“高”“中等”“矮”，可转化为数值：{1， 0.5， 0}。</p>
</li>
<li><p>若属性值之间不存在“序关系”，则通常将其转化为向量的形式，例如：性别属性分为“男”“女”，可转化为二维向量：{（1，0），（0，1）}。</p>
</li>
</ul>
<p>（1）当输入属性只有一个的时候，就是最简单的情形，也就是我们高中时最熟悉的“最小二乘法”（Euclidean distance），首先计算出每个样本预测值与真实值之间的误差并求和，通过最小化均方误差MSE，使用求偏导等于零的方法计算出拟合直线y=wx+b的两个参数w和b，计算过程如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0ccec4.png" alt="2.png"></p>
<p>（2）当输入属性有多个的时候，例如对于一个样本有d个属性{（x1,x2…xd）,y}，则y=wx+b需要写成：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72567b8bcd.png" alt="0.png"></p>
<p>通常对于多元问题，常常使用矩阵的形式来表示数据。在本问题中，将具有m个样本的数据集表示成矩阵X，将系数w与b合并成一个列向量，这样每个样本的预测值以及所有样本的均方误差最小化就可以写成下面的形式：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0ad8f7.png" alt="3.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0af652.png" alt="4.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b090543.png" alt="5.png"></p>
<p><img src="/blog_picture/line04.jpg" alt="avatar"></p>
<p><img src="/blog_picture/line05.jpg" alt="avatar"></p>
<p><img src="/blog_picture/line06.jpg" alt="avatar"></p>
<p><img src="/blog_picture/line07.jpg" alt="avatar"></p>
<h3 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h3><p>同样地，我们使用最小二乘法对w和b进行估计，令均方误差的求导等于0，需要注意的是，当一个矩阵的行列式不等于0时，我们才可能对其求逆，因此对于下式，我们需要考虑矩阵（X的转置*X）的行列式是否为0，若不为0，则可以求出其解，若为0，则需要使用其它的方法进行计算，书中提到了引入正则化，此处不进行深入。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0cde33.png" alt="6.png"></p>
<p>然而现实任务中当特征数量大于样本数时，XTX不满秩，此时θ有多个解；而且当数据量大时，求矩阵的逆非常耗时；对于不可逆矩阵（特征之间不相互独立），这种正规方程方法是不能用的。所以，还可以采用梯度下降法，利用迭代的方式求解θ。</p>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>梯度下降法是按下面的流程进行的：<br>1）首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。<br>2）改变θ的值，使得θ按梯度下降的方向进行减少。</p>
<p><img src="/blog_picture/line01.jpg" alt="avatar"></p>
<p>对于只有两维属性的样本，J(θ)即J(θ0,θ1)的等高线图</p>
<p><img src="/blog_picture/line02.jpg" alt="avatar"></p>
<p><img src="/blog_picture/line03.jpg" alt="avatar"></p>
<p>迭代更新的方式有多种</p>
<ul>
<li>批量梯度下降（batch gradient descent），也就是是梯度下降法最原始的形式，对全部的训练数据求得误差后再对θ<br>进行更新，优点是每步都趋向全局最优解；缺点是对于大量数据，由于每步要计算整体数据，训练过程慢；</li>
<li>随机梯度下降（stochastic gradient descent），每一步随机选择一个样本对θ<br>进行更新，优点是训练速度快；缺点是每次的前进方向不好确定，容易陷入局部最优；</li>
<li>微型批量梯度下降（mini-batch gradient descent），每步选择一小批数据进行批量梯度下降更新θ<br>，属于批量梯度下降和随机梯度下降的一种折中，非常适合并行处理。</li>
</ul>
<p>另一方面，有时像上面这种原始的线性回归可能并不能满足需求，例如：y值并不是线性变化，而是在指数尺度上变化。这时我们可以采用线性模型来逼近y的衍生物，例如lny，这时衍生的线性模型如下所示，实际上就是相当于将指数曲线投影在一条直线上，如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b103cbf.png" alt="7.png"></p>
<p>更一般地，考虑所有y的衍生物的情形，就得到了“广义的线性模型”（generalized linear model），其中，g（*）称为联系函数（link function）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0a2841.png" alt="8.png"></p>
<h3 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a>对数几率回归</h3><p>回归就是通过输入的属性值得到一个预测值，利用上述广义线性模型的特征，是否可以通过一个联系函数，将预测值转化为离散值从而进行分类呢？线性几率回归正是研究这样的问题。对数几率引入了一个对数几率函数（logistic function）,将预测值投影到0-1之间，从而将线性回归问题转化为二分类问题。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0c7748.png" alt="9.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0a655d.png" alt="10.png"></p>
<p>若将y看做样本为正例的概率，（1-y）看做样本为反例的概率，则上式实际上使用线性回归模型的预测结果器逼近真实标记的对数几率。因此这个模型称为“对数几率回归”（logistic regression），也有一些书籍称之为“逻辑回归”。下面使用最大似然估计的方法来计算出w和b两个参数的取值，下面只列出求解的思路，不列出具体的计算过程。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b824f0c.png" alt="11.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b817961.png" alt="12.png"></p>
<h3 id="线性判别分析"><a href="#线性判别分析" class="headerlink" title="线性判别分析"></a>线性判别分析</h3><p>线性判别分析（Linear Discriminant Analysis，简称LDA）,其基本思想是：将训练样本投影到一条直线上，使得同类的样例尽可能近，不同类的样例尽可能远。如图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b863ebb.png" alt="13.png"><img src="https://i.loli.net/2018/10/17/5bc723b85bfa9.png" alt="14.png"></p>
<p>想让同类样本点的投影点尽可能接近，不同类样本点投影之间尽可能远，即：让各类的协方差之和尽可能小，不用类之间中心的距离尽可能大。基于这样的考虑，LDA定义了两个散度矩阵。</p>
<ul>
<li>类内散度矩阵（within-class scatter matrix）</li>
</ul>
<p><img src="https://i.loli.net/2018/10/17/5bc723b8156e1.png" alt="15.png"></p>
<ul>
<li>类间散度矩阵(between-class scaltter matrix)</li>
</ul>
<p><img src="https://i.loli.net/2018/10/17/5bc723b7e9db3.png" alt="16.png"></p>
<p>因此得到了LDA的最大化目标：“广义瑞利商”（generalized Rayleigh quotient）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b7e8a61.png" alt="17.png"></p>
<p>从而分类问题转化为最优化求解w的问题，当求解出w后，对新的样本进行分类时，只需将该样本点投影到这条直线上，根据与各个类别的中心值进行比较，从而判定出新样本与哪个类别距离最近。求解w的方法如下所示，使用的方法为λ乘子。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b83d5e0.png" alt="18.png"></p>
<p>若将w看做一个投影矩阵，类似PCA的思想，则LDA可将样本投影到N-1维空间（N为类簇数），投影的过程使用了类别信息（标记信息），因此LDA也常被视为一种经典的监督降维技术。    </p>
<h3 id="回归与欠-过拟合"><a href="#回归与欠-过拟合" class="headerlink" title="回归与欠/过拟合"></a>回归与欠/过拟合</h3><p><img src="/blog_picture/line08.jpg" alt="avatar">    </p>
<h3 id="线性回归与正则化"><a href="#线性回归与正则化" class="headerlink" title="线性回归与正则化"></a>线性回归与正则化</h3><p><img src="/blog_picture/line09.jpg" alt="avatar">     </p>
<h3 id="多分类学习"><a href="#多分类学习" class="headerlink" title="多分类学习"></a>多分类学习</h3><p>现实中我们经常遇到不只两个类别的分类问题，即多分类问题，在这种情形下，我们常常运用“拆分”的策略，通过多个二分类学习器来解决多分类问题，即将多分类问题拆解为多个二分类问题，训练出多个二分类学习器，最后将多个分类结果进行集成得出结论。最为经典的拆分策略有三种：“一对一”（OvO）、“一对其余”（OvR）和“多对多”（MvM），核心思想与示意图如下所示。</p>
<ul>
<li><p>OvO：给定数据集D，假定其中有N个真实类别，将这N个类别进行两两配对（一个正类/一个反类），从而产生N（N-1）/2个二分类学习器，在测试阶段，将新样本放入所有的二分类学习器中测试，得出N（N-1）个结果，最终通过投票产生最终的分类结果。</p>
</li>
<li><p>OvM：给定数据集D，假定其中有N个真实类别，每次取出一个类作为正类，剩余的所有类别作为一个新的反类，从而产生N个二分类学习器，在测试阶段，得出N个结果，若仅有一个学习器预测为正类，则对应的类标作为最终分类结果。</p>
</li>
<li><p>MvM：给定数据集D，假定其中有N个真实类别，每次取若干个类作为正类，若干个类作为反类（通过ECOC码给出，编码），若进行了M次划分，则生成了M个二分类学习器，在测试阶段（解码），得出M个结果组成一个新的码，最终通过计算海明/欧式距离选择距离最小的类别作为最终分类结果。</p>
</li>
</ul>
<p><img src="https://i.loli.net/2018/10/17/5bc723b862bfb.png" alt="19.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b8300d5.png" alt="20.png"></p>
<h3 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h3><p>类别不平衡（class-imbanlance）就是指分类问题中不同类别的训练样本相差悬殊的情况，例如正例有900个，而反例只有100个，这个时候我们就需要进行相应的处理来平衡这个问题。常见的做法有三种：</p>
<ol>
<li>在训练样本较多的类别中进行“欠采样”（undersampling）,比如从正例中采出100个，常见的算法有：EasyEnsemble。</li>
<li>在训练样本较少的类别中进行“过采样”（oversampling）,例如通过对反例中的数据进行插值，来产生额外的反例，常见的算法有SMOTE。</li>
<li>直接基于原数据集进行学习，对预测值进行“再缩放”处理。其中再缩放也是代价敏感学习的基础。<img src="https://i.loli.net/2018/10/17/5bc726fe87ae2.png" alt="21.png"></li>
</ol>
<h3 id="LR应用经验"><a href="#LR应用经验" class="headerlink" title="LR应用经验"></a>LR应用经验</h3><p>LR实现简单高效易解释，计算速度快，易并行，在大规模数据情况下非常适用，更适合于应对数值型和标称型数据，主要适合解决线性可分的问题，但容易欠拟合，大多数情况下需要手动进行特征工程，构建组合特征，分类精度不高。</p>
<p>LR直接对分类可能性进行建模，无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题<br>LR能以概率的形式输出，而非知识0，1判定，对许多利用概率辅助决策的任务很有用<br>对率函数任意阶可导，具有很好的数学性质，许多现有的数值优化算法都可以用来求最优解，训练速度快<br>适用情景：LR是很多分类算法的基础组件，它的好处是输出值自然地落在0到1之间，并且有概率意义。因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。虽然效果一般，却胜在模型清晰，背后的概率学经得住推敲。它拟合出来的参数就代表了每一个特征(feature)对结果的影响。也是一个理解数据的好工具。</p>
<p>应用上： </p>
<ul>
<li>CTR预估，推荐系统的learning to rank，各种分类场景 </li>
<li>某搜索引擎厂的广告CTR预估基线版是LR </li>
<li>某电商搜索排序基线版是LR </li>
<li>某新闻app排序基线版是LR</li>
</ul>
<p>大规模工业实时数据，需要可解释性的金融数据，需要快速部署低耗时数据<br>LR就是简单，可解释，速度快，消耗资源少，分布式性能好</p>
<p>ADMM-LR:用ADMM求解LogisticRegression的优化方法称作ADMM_LR。ADMM算法是一种求解约束问题的最优化方法，它适用广泛。相比于SGD，ADMM在精度要求不高的情况下，在少数迭代轮数时就达到一个合理的精度，但是收敛到很精确的解则需要很多次迭代。</p>
<p><img src="/blog_picture/line10.jpg" alt="avatar">   </p>
<p><img src="/blog_picture/line11.jpg" alt="avatar">   </p>
<p><img src="/blog_picture/line12.jpg" alt="avatar">   </p>
<h2 id="LR多分类推广-Softmax回归"><a href="#LR多分类推广-Softmax回归" class="headerlink" title="LR多分类推广 - Softmax回归"></a>LR多分类推广 - Softmax回归</h2><p>LR是一个传统的二分类模型，它也可以用于多分类任务，其基本思想是：将多分类任务拆分成若干个二分类任务，然后对每个二分类任务训练一个模型，最后将多个模型的结果进行集成以获得最终的分类结果。一般来说，可以采取的拆分策略有：</p>
<h3 id="one-vs-one策略"><a href="#one-vs-one策略" class="headerlink" title="one vs one策略"></a>one vs one策略</h3><p>　　假设我们有N个类别，该策略基本思想就是不同类别两两之间训练一个分类器，这时我们一共会训练出<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171104175530607-1392543504.png" alt="img">种不同的分类器。在预测时，我们将样本提交给所有的分类器，一共会获得N(N-1)个结果，最终结果通过<strong>投票</strong>产生。</p>
<h3 id="one-vs-all策略"><a href="#one-vs-all策略" class="headerlink" title="one vs all策略"></a>one vs all策略</h3><p>　　该策略基本思想就是将第i种类型的所有样本作为正例，将剩下的所有样本作为负例，进行训练得到一个分类器。这样我们就一共可以得到N个分类器。在预测时，我们将样本提交给所有的分类器，一共会获得N个结果，我们<strong>选择其中概率值最大</strong>的那个作为最终分类结果。 <img src="https://images2017.cnblogs.com/blog/1251096/201710/1251096-20171021171313943-1199609768.png" alt="img"></p>
<h2 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h2><p>　　softmax是LR在多分类的推广。与LR一样，同属于广义线性模型。什么是Softmax函数？假设我们有一个数组A，<img src="https://images2017.cnblogs.com/blog/1251096/201710/1251096-20171021164616881-992414484.png" alt="img">表示的是数组A中的第i个元素，那么这个元素的Softmax值就是</p>
<p>　　　　　　　　　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201710/1251096-20171021165228865-866731732.png" alt="img"></p>
<p>也就是说，是该元素的指数，与所有元素指数和的比值。那么 softmax回归模型的假设函数又是怎么样的呢？</p>
<p>　　　　　　　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105100956795-1587348606.png" alt="img"></p>
<p>由上式很明显可以得出，假设函数的分母其实就是对概率分布进行了归一化，使得所有类别的概率之和为1；也可以看出LR其实就是K=2时的Softmax。在参数获得上，我们可以采用one vs all策略获得K个不同的训练数据集进行训练，进而针对每一类别都会得到一组参数向量<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102153701-629755133.png" alt="img">。当测试样本特征向量<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102416560-1451219507.png" alt="img">输入时，我们先用假设函数针对每一个类别<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102622045-1005416234.png" alt="img">估算出概率值<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102706623-369597312.png" alt="img">。因此我们的假设函数将要输出一个K维的向量（向量元素和为1）来表示K个类别的估计概率，我们选择其中得分最大的类别作为该输入的预测类别。Softmax看起来和one vs all 的LR很像，它们最大的不同在与Softmax得到的K个类别的得分和为1，而one vs all的LR并不是。</p>
<h3 id="softmax的代价函数"><a href="#softmax的代价函数" class="headerlink" title="softmax的代价函数"></a>softmax的代价函数</h3><p>　　类似于LR，其似然函数我们采用对数似然，故：</p>
<p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105113747779-692061991.png" alt="img"></p>
<p>加入<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171104162333248-539020480.png" alt="img">正则项的损失函数为：</p>
<p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105114132232-517057992.png" alt="img"></p>
<p>此处的<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105110553560-1026190635.png" alt="img">为符号函数。对于其参数的求解过程，我们依然采用梯度下降法。</p>
<h3 id="softmax的梯度的求解"><a href="#softmax的梯度的求解" class="headerlink" title="softmax的梯度的求解"></a>softmax的梯度的求解</h3><p>　　正则化项的求导很简单，就等于<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105120226607-495282914.png" alt="img">，下面我们主要讨论没有加正则项的损失函数的梯度求解，即</p>
<p>　　　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105113747779-692061991.png" alt="img"></p>
<p>的导数（梯度）。为了使得求解过程看起来简便、易于理解，我们仅仅只对于一个样本（x,y）情况（SGD）进行讨论，</p>
<p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105161912482-1607069737.png" alt="img"></p>
<p>此时，我们令</p>
<p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105162625888-1575402902.png" alt="img"></p>
<p>可以得到</p>
<p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105163457810-492161690.png" alt="img"></p>
<p>故：</p>
<p><img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105170233232-810575386.png" alt="img"></p>
<p>所以，正则化之后的损失函数的梯度为</p>
<p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105171748341-1281292385.png" alt="img"></p>
<p>然后通过梯度下降法最小化 <img src="http://ufldl.stanford.edu/wiki/images/math/c/e/0/ce027336c1cb3c0cd461406c81369ebf.png" alt="\textstyle J(\theta)">，我们就能实现一个可用的 softmax 回归模型了。</p>
<h3 id="多分类LR与Softmax回归"><a href="#多分类LR与Softmax回归" class="headerlink" title="多分类LR与Softmax回归"></a>多分类LR与Softmax回归</h3><p>　　有了多分类的处理方法，那么我们什么时候该用多分类LR？什么时候要用softmax呢？</p>
<p>总的来说，若待分类的<strong>类别互斥</strong>，我们就使用Softmax方法；若待分类的<strong>类别有相交</strong>，我们则要选用多分类LR，然后投票表决。</p>
<h2 id="Softmax分类器"><a href="#Softmax分类器" class="headerlink" title="Softmax分类器"></a>Softmax分类器</h2><p>SVM是最常用的两个分类器之一，而另一个就是<strong>Softmax分类器，</strong>它的损失函数与SVM的损失函数不同。对于学习过二元逻辑回归分类器的读者来说，Softmax分类器就可以理解为逻辑回归分类器面对多个分类的一般化归纳。SVM将输出<img src="https://www.zhihu.com/equation?tex=f%28x_i%2CW%29" alt="[公式]">作为每个分类的评分（因为无定标，所以难以直接解释）。与SVM不同，Softmax的输出（归一化的分类概率）更加直观，并且从概率上可以解释，这一点后文会讨论。在Softmax分类器中，函数映射<img src="https://www.zhihu.com/equation?tex=f%28x_i%3BW%29%3DWx_i" alt="[公式]">保持不变，但将这些评分值视为每个分类的未归一化的对数概率，并且将<em>折叶损失（hinge loss）</em>替换为<strong>交叉熵损失</strong>（<strong>cross-entropy loss）</strong>。公式如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+Li%3D-log%28%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%29" alt="[公式]"> 或等价的 <img src="https://www.zhihu.com/equation?tex=L_i%3D-f_%7By_i%7D%2Blog%28%5Csum_je%5E%7Bf_j%7D%29" alt="[公式]"></p>
<p>在上式中，使用<img src="https://www.zhihu.com/equation?tex=f_j" alt="[公式]">来表示分类评分向量<img src="https://www.zhihu.com/equation?tex=f" alt="[公式]">中的第j个元素。和之前一样，整个数据集的损失值是数据集中所有样本数据的损失值<img src="https://www.zhihu.com/equation?tex=L_i" alt="[公式]">的均值与正则化损失<img src="https://www.zhihu.com/equation?tex=R%28W%29" alt="[公式]">之和。其中函数<img src="https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D" alt="[公式]">被称作<strong>softmax 函数</strong>：其输入值是一个向量，向量中元素为任意实数的评分值（<img src="https://www.zhihu.com/equation?tex=z" alt="[公式]">中的），函数对其进行压缩，输出一个向量，其中每个元素值在0到1之间，且所有元素之和为1。所以，包含softmax函数的完整交叉熵损失看起唬人，实际上还是比较容易理解的。</p>
<p><strong>信息理论视角</strong>：在“真实”分布<img src="https://www.zhihu.com/equation?tex=p" alt="[公式]">和估计分布<img src="https://www.zhihu.com/equation?tex=q" alt="[公式]">之间的<em>交叉熵</em>定义如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29" alt="[公式]"></p>
<p><strong>*译者注</strong>：Kullback-Leibler差异（Kullback-Leibler Divergence）也叫做相对熵（Relative Entropy），它衡量的是相同事件空间里的两个概率分布的差异情况。*</p>
<p><strong>概率论解释</strong>：先看下面的公式：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28y_i%7Cx_i%2CW%29%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D" alt="[公式]"></p>
<p><strong>实操事项：数值稳定。</strong>编程实现softmax函数计算的时候，中间项<img src="https://www.zhihu.com/equation?tex=e%5E%7Bf_%7By_i%7D%7D" alt="[公式]">和<img src="https://www.zhihu.com/equation?tex=%5Csum_j+e%5E%7Bf_j%7D" alt="[公式]">因为存在指数函数，所以数值可能非常大。除以大数值可能导致数值计算的不稳定，所以学会使用归一化技巧非常重要。如果在分式的分子和分母都乘以一个常数<img src="https://www.zhihu.com/equation?tex=C" alt="[公式]">，并把它变换到求和之中，就能得到一个从数学上等价的公式：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7BCe%5E%7Bf_%7By_i%7D%7D%7D%7BC%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%2BlogC%7D%7D%7B%5Csum_je%5E%7Bf_j%2BlogC%7D%7D" alt="[公式]"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">f = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>]) <span class="comment"># 例子中有3个分类，每个评分的数值都很大</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 不妙：数值问题，可能导致数值爆炸</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 那么将f中的值平移到最大值为0：</span></span><br><span class="line">f -= np.max(f) <span class="comment"># f becomes [-666, -333, 0]</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 现在OK了，将给出正确结果</span></span><br></pre></td></tr></table></figure>

<p><strong>让人迷惑的命名规则</strong>：精确地说，SVM分类器使用的是<em>折叶损失（hinge loss）</em>，有时候又被称为<em>最大边界损失（max-margin loss）</em>。Softmax分类器使用的是<em>交叉熵损失（corss-entropy loss）</em>。Softmax分类器的命名是从<em>softmax函数</em>那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。注意从技术上说“softmax损失（softmax loss）”是没有意义的，因为softmax只是一个压缩数值的函数。但是在这个说法常常被用来做简称。</p>
<h2 id="SVM和Softmax的比较"><a href="#SVM和Softmax的比较" class="headerlink" title="SVM和Softmax的比较"></a>SVM和Softmax的比较</h2><p>下图有助于区分这 Softmax和SVM这两种分类器：</p>
<p>————————————————————————————————————————</p>
<p><img src="https://pic1.zhimg.com/80/a90ce9e0ff533f3efee4747305382064_hd.png" alt="img"></p>
<p>针对一个数据点，SVM和Softmax分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量<strong>f</strong>（本节中是通过矩阵乘来实现）。不同之处在于对<strong>f</strong>中分值的解释：SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别2）的分值比其他分类的分值高出至少一个边界值。Softmax分类器将这些数值看做是每个分类没有归一化的<strong>对数概率</strong>，鼓励正确分类的归一化的对数概率变高，其余的变低。SVM的最终的损失值是1.58，Softmax的最终的损失值是0.452，但要注意这两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。</p>
<p>————————————————————————————————————————</p>
<p><strong>Softmax分类器为每个分类提供了“可能性”</strong>：SVM的计算是无标定的，而且难以针对所有分类的评分值给出直观解释。Softmax分类器则不同，它允许我们计算出对于所有分类标签的可能性。举个例子，针对给出的图像，SVM分类器可能给你的是一个[12.5, 0.6, -23.0]对应分类“猫”，“狗”，“船”。而softmax分类器可以计算出这三个标签的”可能性“是[0.9, 0.09, 0.01]，这就让你能看出对于不同分类准确性的把握。为什么我们要在”可能性“上面打引号呢？这是因为可能性分布的集中或离散程度是由正则化参数λ直接决定的，λ是你能直接控制的一个输入参数。举个例子，假设3个分类的原始分数是[1, -2, 0]，那么softmax函数就会计算：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5B1%2C-2%2C0%5D%5Cto%5Be%5E1%2Ce%5E%7B-2%7D%2Ce%5E0%5D%3D%5B2.71%2C0.14%2C1%5D%5Cto%5B0.7%2C0.04%2C0.26%5D" alt="[公式]"></p>
<p>现在，如果正则化参数λ更大，那么权重W就会被惩罚的更多，然后他的权重数值就会更小。这样算出来的分数也会更小，假设小了一半吧[0.5, -1, 0]，那么softmax函数的计算就是：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5B0.5%2C-1%2C0%5D%5Cto%5Be%5E%7B0.5%7D%2Ce%5E%7B-1%7D%2Ce%5E0%5D%3D%5B1.65%2C0.73%2C1%5D%5Cto%5B0.55%2C0.12%2C0.33%5D" alt="[公式]"></p>
<p>现在看起来，概率的分布就更加分散了。还有，随着正则化参数λ不断增强，权重数值会越来越小，最后输出的概率会接近于均匀分布。这就是说，softmax分类器算出来的概率最好是看成一种对于分类正确性的自信。和SVM一样，数字间相互比较得出的大小顺序是可以解释的，但其绝对值则难以直观解释<strong>。</strong></p>
<p><strong>在实际使用中，SVM和Softmax经常是相似的</strong>：通常说来，两种分类器的表现差别很小，不同的人对于哪个分类器更好有不同的看法。相对于Softmax分类器，SVM更加“局部目标化（local objective）”，这既可以看做是一个特性，也可以看做是一个劣势。考虑一个评分是[10, -2, 3]的数据，其中第一个分类是正确的。那么一个SVM（<img src="https://www.zhihu.com/equation?tex=%5CDelta+%3D1" alt="[公式]">）会看到正确分类相较于不正确分类，已经得到了比边界值还要高的分数，它就会认为损失值是0。SVM对于数字个体的细节是不关心的：如果分数是[10, -100, -100]或者[10, 9, 9]，对于SVM来说没设么不同，只要满足超过边界值等于1，那么损失值就等于0。</p>
<p>对于softmax分类器，情况则不同。对于[10, 9, 9]来说，计算出的损失值就远远高于[10, -100, -100]的。换句话来说，softmax分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。但是，SVM只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。这可以被看做是SVM的一种特性。举例说来，一个汽车的分类器应该把他的大量精力放在如何分辨小轿车和大卡车上，而不应该纠结于如何与青蛙进行区分，因为区分青蛙得到的评分已经足够低了。</p>

      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>本文标题:</span><a href="/2019/08/29/机器学习逻辑回归与softmax/">机器学习逻辑回归与softmax</a></p>
        <p><span>文章作者:</span><a href="/" title="回到主页">MingmingYe</a></p>
        <p><span>发布时间:</span>2019-08-29, 16:39:02</p>
        <p><span>最后更新:</span>2019-08-30, 17:19:01</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/2019/08/29/机器学习逻辑回归与softmax/" title="机器学习逻辑回归与softmax">http://mmyblog.cn/2019/08/29/机器学习逻辑回归与softmax/</a>
            <span class="copy-path" data-clipboard-text="原文: http://mmyblog.cn/2019/08/29/机器学习逻辑回归与softmax/　　作者: MingmingYe" title="点击复制文章链接"><i class="fa fa-clipboard"></i></span>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
        </p>
    </div>



    <nav id="article-nav">
        
            <div id="article-nav-newer" class="article-nav-title">
                <a href="/2019/09/01/决策树与随机森林/">
                    决策树与随机森林
                </a>
            </div>
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/2019/08/23/机器学习基本概念/">
                    机器学习基本概念
                </a>
            </div>
        
    </nav>

  
  
    <! -- 添加捐赠图标 -->
<div class ="post-donate">
    <div id="donate_board" class="donate_bar center">
        <a id="btn_donate" class="btn_donate" href="javascript:;" title="打赏"></a>
        <span class="donate_txt">
           ↑<br>
           欣赏此文？求鼓励，求支持！
        </span>
        <br>
      </div>  
    <div id="donate_guide" class="donate_bar center hidden" >
        
            <!-- <img src="/img/Alipay.jpg" alt="支付宝打赏">
            <img src="/img/WeChatpay.jpg" alt="微信打赏"> -->
       
        <!-- 方式二；
            step1：在_config.yml中添加配置
                Alipay: /img/Alipay.jpg
                WeChatpay: /img/WeChatpay.jpg
            step2：此处两张图片的路径分别设置为如下
                <img src=""
                <img src=""
        -->
        <!-- 支付宝打赏图案 -->
        <img src="/img/Alipay.jpeg" alt="支付宝打赏">
        <!-- 微信打赏图案 -->
        <img src="/img/Wechat.png" alt="微信打赏">
    </div>
    <script type="text/javascript">
        document.getElementById('btn_donate').onclick = function(){
            $('#donate_board').addClass('hidden');
            $('#donate_guide').removeClass('hidden');
        }
    </script>
</div>
<! -- 添加捐赠图标 -->
  
</article>

    <div id="toc" class="toc-article">
        <strong class="toc-title">文章目录</strong>
        
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#机器学习逻辑回归与softmax"><span class="toc-number">1.</span> <span class="toc-text">机器学习逻辑回归与softmax</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#机器学习中的线性模型"><span class="toc-number">1.1.</span> <span class="toc-text">机器学习中的线性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#线性回归"><span class="toc-number">1.1.1.</span> <span class="toc-text">线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#最小二乘法"><span class="toc-number">1.1.2.</span> <span class="toc-text">最小二乘法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度下降法"><span class="toc-number">1.1.3.</span> <span class="toc-text">梯度下降法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#对数几率回归"><span class="toc-number">1.1.4.</span> <span class="toc-text">对数几率回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#线性判别分析"><span class="toc-number">1.1.5.</span> <span class="toc-text">线性判别分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#回归与欠-过拟合"><span class="toc-number">1.1.6.</span> <span class="toc-text">回归与欠/过拟合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#线性回归与正则化"><span class="toc-number">1.1.7.</span> <span class="toc-text">线性回归与正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#多分类学习"><span class="toc-number">1.1.8.</span> <span class="toc-text">多分类学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#类别不平衡问题"><span class="toc-number">1.1.9.</span> <span class="toc-text">类别不平衡问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LR应用经验"><span class="toc-number">1.1.10.</span> <span class="toc-text">LR应用经验</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LR多分类推广-Softmax回归"><span class="toc-number">1.2.</span> <span class="toc-text">LR多分类推广 - Softmax回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#one-vs-one策略"><span class="toc-number">1.2.1.</span> <span class="toc-text">one vs one策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#one-vs-all策略"><span class="toc-number">1.2.2.</span> <span class="toc-text">one vs all策略</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#softmax回归"><span class="toc-number">1.3.</span> <span class="toc-text">softmax回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#softmax的代价函数"><span class="toc-number">1.3.1.</span> <span class="toc-text">softmax的代价函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#softmax的梯度的求解"><span class="toc-number">1.3.2.</span> <span class="toc-text">softmax的梯度的求解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#多分类LR与Softmax回归"><span class="toc-number">1.3.3.</span> <span class="toc-text">多分类LR与Softmax回归</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Softmax分类器"><span class="toc-number">1.4.</span> <span class="toc-text">Softmax分类器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM和Softmax的比较"><span class="toc-number">1.5.</span> <span class="toc-text">SVM和Softmax的比较</span></a></li></ol></li></ol>
        
    </div>
    <style>
        .left-col .switch-btn,
        .left-col .switch-area {
            display: none;
        }
        .toc-level-5 i,
        .toc-level-5 ol {
            display: none !important;
        }
    </style>

    <input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">

    <script>
        yiliaConfig.toc = ["隐藏目录", "显示目录", !!"false"];
    </script>



    
<div class="share">
    
        <div class="bdsharebuttonbox">
            <a href="#" class="fa fa-twitter bds_twi" data-cmd="twi" title="分享到推特"></a>
            <a href="#" class="fa fa-weibo bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
            <a href="#" class="fa fa-qq bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a>
            <a href="#" class="fa fa-files-o bds_copy" data-cmd="copy" title="复制网址"></a>
            <a href="#" class="fa fa fa-envelope-o bds_mail" data-cmd="mail" title="通过邮件分享"></a>
            <a href="#" class="fa fa-weixin bds_weixin" data-cmd="weixin" title="生成文章二维码"></a>
            <a href="#" class="fa fa-share-alt bds_more" data-cmd="more"></i></a>
        </div>
        <script>
            window._bd_share_config={
                "common":{"bdSnsKey":{},"bdText":"机器学习逻辑回归与softmax　| Stay hungry, Stay foolish.　","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
        </script>
    

    
</div>







    




    <div class="scroll" id="post-nav-button">
        
            <a href="/2019/09/01/决策树与随机森林/" title="上一篇: 决策树与随机森林">
                <i class="fa fa-angle-left"></i>
            </a>
        

        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/2019/08/23/机器学习基本概念/" title="下一篇: 机器学习基本概念">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/04/24/word2vec/">word2vec</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/20/特征工程与模型调优/">特征工程与模型调优</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/20/SVM/">SVM</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/09/01/深度学习速查表/">深度学习速查表</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/09/01/模型调优/">模型调优</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/09/01/集成学习与boosting模型/">集成学习与boosting模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/09/01/聚类与降维/">聚类与降维</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/09/01/贝叶斯分类器/">贝叶斯分类器</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/09/01/决策树与随机森林/">决策树与随机森林</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/29/机器学习逻辑回归与softmax/">机器学习逻辑回归与softmax</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/23/机器学习基本概念/">机器学习基本概念</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/23/简洁版机器学习速查表/">简洁版机器学习速查表</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/23/CS229版机器学习速查表/">CS229版机器学习速查表</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/葫芦书学习笔记/">葫芦书学习笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/06/数学基础知识整理/">数学基础知识整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/24/大数据基础/">大数据基础</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/22/数据分析常用工具总结/">数据分析常用工具总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/20/python基础知识整理/">python基础知识整理</a></li></ul>




    <script>
        
    </script>

</div>
      <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2019-2020 MingmingYe
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>

    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>