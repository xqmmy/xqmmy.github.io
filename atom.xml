<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Stay hungry, Stay foolish.</title>
  
  <subtitle>å½“ä½ çš„æ‰åæ’‘ä¸èµ·ä½ çš„é‡å¿ƒæ—¶ï¼Œåªæœ‰é™ä¸‹å¿ƒæ¥å¥½å¥½å­¦ä¹ ï¼çºµä½¿å‘½è¿æ³¨å®šæ˜¯ä¸ªæ‰“é…±æ²¹çš„ï¼Œä¹Ÿè¦æ‰“ä¸€ç“¶ä¸åˆ«äººä¸ä¸€æ ·çš„é…±æ²¹ï¼</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://mmyblog.cn/"/>
  <updated>2020-06-09T01:01:12.300Z</updated>
  <id>http://mmyblog.cn/</id>
  
  <author>
    <name>MingmingYe</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>æ‰©å±•å†…å®¹</title>
    <link href="http://mmyblog.cn/2020/06/09/%E6%89%A9%E5%B1%95%E5%86%85%E5%AE%B9/"/>
    <id>http://mmyblog.cn/2020/06/09/æ‰©å±•å†…å®¹/</id>
    <published>2020-06-09T00:51:03.000Z</published>
    <updated>2020-06-09T01:01:12.300Z</updated>
    
    <content type="html"><![CDATA[<h3 id="ç»“æ„åŒ–é¢„æµ‹"><a href="#ç»“æ„åŒ–é¢„æµ‹" class="headerlink" title="ç»“æ„åŒ–é¢„æµ‹"></a><a href="https://shimo.im/docs/t9hjVGDx33vyTDjD" target="_blank" rel="noopener">ç»“æ„åŒ–é¢„æµ‹</a></h3><ul><li>éšé©¬å°”ç§‘å¤«æ¨¡å‹ <a href="http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf" target="_blank" rel="noopener">http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf</a></li><li>æœ€å¤§ç†µä¸è¯æ€§æ ‡æ³¨</li><li>æ¡ä»¶éšæœºåœº</li></ul><h3 id="ä¸­æ–‡åˆ†è¯-Chinese-Word-Segmentation"><a href="#ä¸­æ–‡åˆ†è¯-Chinese-Word-Segmentation" class="headerlink" title="ä¸­æ–‡åˆ†è¯ Chinese Word Segmentation"></a>ä¸­æ–‡åˆ†è¯ Chinese Word Segmentation</h3><ul><li><a href="https://www.aclweb.org/anthology/D18-1529.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D18-1529.pdf</a></li></ul><h3 id="Parsingä¸Recursive-Neural-Networks"><a href="#Parsingä¸Recursive-Neural-Networks" class="headerlink" title="Parsingä¸Recursive Neural Networks"></a>Parsingä¸Recursive Neural Networks</h3><ul><li><a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture18-TreeRNNs.pdf" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture18-TreeRNNs.pdf</a></li><li><a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture05-dep-parsing.pdf" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture05-dep-parsing.pdf</a></li></ul><h3 id="å›¾ç¥ç»ç½‘ç»œ"><a href="#å›¾ç¥ç»ç½‘ç»œ" class="headerlink" title="å›¾ç¥ç»ç½‘ç»œ"></a>å›¾ç¥ç»ç½‘ç»œ</h3><ul><li>GCN: <a href="https://arxiv.org/pdf/1609.02907.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1609.02907.pdf</a></li><li>GCN for relational graph: <a href="https://arxiv.org/pdf/1703.06103.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1703.06103.pdf</a></li></ul><h3 id="Data-to-Text-æ–‡æœ¬ç”Ÿæˆ"><a href="#Data-to-Text-æ–‡æœ¬ç”Ÿæˆ" class="headerlink" title="Data to Text æ–‡æœ¬ç”Ÿæˆ"></a>Data to Text æ–‡æœ¬ç”Ÿæˆ</h3><ul><li>GCNç”Ÿæˆæ–‡æœ¬ <a href="https://arxiv.org/pdf/1810.09995v1.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1810.09995v1.pdf</a></li></ul><h3 id="çŸ¥è¯†å›¾è°±ç›¸å…³é—®é¢˜"><a href="#çŸ¥è¯†å›¾è°±ç›¸å…³é—®é¢˜" class="headerlink" title="çŸ¥è¯†å›¾è°±ç›¸å…³é—®é¢˜"></a><a href="https://shimo.im/docs/9pwCHPwXxcGHRrxh" target="_blank" rel="noopener">çŸ¥è¯†å›¾è°±ç›¸å…³é—®é¢˜</a></h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;ç»“æ„åŒ–é¢„æµ‹&quot;&gt;&lt;a href=&quot;#ç»“æ„åŒ–é¢„æµ‹&quot; class=&quot;headerlink&quot; title=&quot;ç»“æ„åŒ–é¢„æµ‹&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://shimo.im/docs/t9hjVGDx33vyTDjD&quot; target=&quot;_blank&quot; rel=&quot;n
      
    
    </summary>
    
      <category term="NLP" scheme="http://mmyblog.cn/categories/NLP/"/>
    
    
      <category term="ä¸­æ–‡åˆ†è¯" scheme="http://mmyblog.cn/tags/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D/"/>
    
      <category term="å›¾ç¥ç»ç½‘ç»œ" scheme="http://mmyblog.cn/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="Recursive Neural Networks" scheme="http://mmyblog.cn/tags/Recursive-Neural-Networks/"/>
    
      <category term="Parsing" scheme="http://mmyblog.cn/tags/Parsing/"/>
    
  </entry>
  
  <entry>
    <title>XLNet</title>
    <link href="http://mmyblog.cn/2020/06/09/XLNet/"/>
    <id>http://mmyblog.cn/2020/06/09/XLNet/</id>
    <published>2020-06-09T00:33:08.000Z</published>
    <updated>2020-06-09T01:33:48.494Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context"><a href="#Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context" class="headerlink" title="Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"></a>Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</h2><p><a href="https://arxiv.org/pdf/1901.02860.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.02860.pdf</a></p><p>ç›¸è¾ƒäºä¼ ç»Ÿtransformer decoderï¼Œå¼•å…¥ä¸¤ä¸ªæ–°æ¨¡å—</p><ul><li>segment-level recurrence mechanism</li></ul><p><img src="https://uploader.shimo.im/f/DpNe30kuahkbOeW5.png!thumbnail" alt="img"></p><ul><li><p>a novel positional encoding scheme</p></li><li><p>è€ƒè™‘æˆ‘ä»¬åœ¨attentionæœºåˆ¶ä¸­å¦‚ä½•ä½¿ç”¨positional encoding</p></li></ul><p>(E_{x_i}^T+U_i^T)W_q^TW_kE_{x_j}U_j</p><p><img src="https://uploader.shimo.im/f/5zNU9yZQtQMClNiY.png!thumbnail" alt="img"></p><ul><li><p>Rä»–ä»¬é‡‡ç”¨çš„æ˜¯transformerå½“ä¸­çš„positional encoding</p></li><li><p>uå’Œvæ˜¯éœ€è¦è®­ç»ƒçš„æ¨¡å‹å‚æ•°</p></li></ul><p>æœ€ç»ˆTransformer XLæ¨¡å‹</p><p><img src="https://uploader.shimo.im/f/Nm1uk49MIjUys1aK.png!thumbnail" alt="img"></p><p>ä»£ç </p><p><a href="https://github.com/kimiyoung/transformer-xl" target="_blank" rel="noopener">https://github.com/kimiyoung/transformer-xl</a></p><h2 id="XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding"><a href="#XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding" class="headerlink" title="XLNet: Generalized Autoregressive Pretraining for Language Understanding"></a>XLNet: Generalized Autoregressive Pretraining for Language Understanding</h2><p><a href="https://arxiv.org/pdf/1906.08237.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1906.08237.pdf</a></p><p>èƒŒæ™¯çŸ¥è¯†</p><ul><li><p>è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼ˆAutoregressive Language Modelï¼‰ï¼šé‡‡ç”¨ä»å·¦å¾€å³æˆ–ä»å³å¾€å·¦çš„è¯­è¨€æ¨¡å‹ï¼Œæ ¹æ®ä¸Šæ–‡é¢„æµ‹ä¸‹æ–‡ã€‚</p></li><li><p>ç¼ºç‚¹ï¼šåªåˆ©ç”¨äº†é¢„æµ‹å•è¯å·¦è¾¹æˆ–å³è¾¹çš„ä¿¡æ¯ï¼Œæ— æ³•åŒæ—¶åˆ©ç”¨ä¸¤è¾¹çš„ä¿¡æ¯ã€‚ELMoåœ¨ä¸€å®šç¨‹åº¦ä¸Šè§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚</p></li><li><p><img src="https://uploader.shimo.im/f/cpfGbeRfzf8c1ga8.png!thumbnail" alt="img"></p></li><li><p>è‡ªç¼–ç æ¨¡å‹ï¼ˆDenoising Auto Encoder, DAEï¼‰ï¼šåœ¨è¾“å…¥ä¸­éšæœºmaskä¸€äº›å•è¯ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡æ¥é¢„æµ‹è¢«maskæ‰çš„å•è¯ã€‚BERTé‡‡ç”¨äº†è¿™ä¸€æ€è·¯ã€‚</p></li><li><p><img src="https://uploader.shimo.im/f/za1FnG3zHdsbm5gD.png!thumbnail" alt="img"></p></li></ul><p>ä¸¤ä¸ªæ¨¡å‹çš„é—®é¢˜</p><p><img src="https://uploader.shimo.im/f/A1rO6rAR1nAQqqvu.png!thumbnail" alt="img"></p><p>XLNetçš„ç›®æ ‡æ˜¯èåˆä»¥ä¸Šä¸¤ç§æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œè§£å†³å®ƒä»¬å„è‡ªå­˜åœ¨çš„é—®é¢˜ã€‚</p><p>XLNetæ¨¡å‹ï¼šPermutation Language Modeling</p><p><img src="https://uploader.shimo.im/f/LdaKeEgG8XwH3iNj.png!thumbnail" alt="img"></p><p>Two-Stream Self-Attention</p><p><img src="https://uploader.shimo.im/f/TdQVsxOeYMoakBW0.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/iLMqF1WinQI6wOsW.png!thumbnail" alt="img"></p><p>å‚è€ƒèµ„æ–™</p><p><a href="https://zhuanlan.zhihu.com/p/70257427" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/70257427</a></p><p>ä»£ç </p><p><a href="https://github.com/zihangdai/xlnet" target="_blank" rel="noopener">https://github.com/zihangdai/xlnet</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context&quot;&gt;&lt;a href=&quot;#Transformer-XL-Attentive-Language-Models-Beyond-a-
      
    
    </summary>
    
      <category term="NLP" scheme="http://mmyblog.cn/categories/NLP/"/>
    
    
      <category term="word-embedding" scheme="http://mmyblog.cn/tags/word-embedding/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch</title>
    <link href="http://mmyblog.cn/2020/06/08/PyTorch/"/>
    <id>http://mmyblog.cn/2020/06/08/PyTorch/</id>
    <published>2020-06-08T11:12:06.000Z</published>
    <updated>2020-06-09T01:28:17.998Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ä»€ä¹ˆæ˜¯PyTorch"><a href="#ä»€ä¹ˆæ˜¯PyTorch" class="headerlink" title="ä»€ä¹ˆæ˜¯PyTorch?"></a>ä»€ä¹ˆæ˜¯PyTorch?</h1><p>PyTorchæ˜¯ä¸€ä¸ªåŸºäºPythonçš„ç§‘å­¦è®¡ç®—åº“ï¼Œå®ƒæœ‰ä»¥ä¸‹ç‰¹ç‚¹:</p><ul><li>ç±»ä¼¼äºNumPyï¼Œä½†æ˜¯å®ƒå¯ä»¥ä½¿ç”¨GPU</li><li>å¯ä»¥ç”¨å®ƒå®šä¹‰æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¯ä»¥çµæ´»åœ°è¿›è¡Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒå’Œä½¿ç”¨</li></ul><h2 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h2><p>Tensorç±»ä¼¼ä¸NumPyçš„ndarrayï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯Tensorå¯ä»¥åœ¨GPUä¸ŠåŠ é€Ÿè¿ç®—ã€‚</p><p>In [2]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br></pre></td></tr></table></figure><p>æ„é€ ä¸€ä¸ªæœªåˆå§‹åŒ–çš„5x3çŸ©é˜µ:</p><p>In [4]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(5,3)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>Out[4]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.0000e+00, -8.5899e+09,  0.0000e+00],</span><br><span class="line">        [-8.5899e+09,         nan,  0.0000e+00],</span><br><span class="line">        [ 2.7002e-06,  1.8119e+02,  1.2141e+01],</span><br><span class="line">        [ 7.8503e+02,  6.7504e-07,  6.5200e-10],</span><br><span class="line">        [ 2.9537e-06,  1.7186e-04,         nan]])</span><br></pre></td></tr></table></figure><p>æ„å»ºä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„çŸ©é˜µ:</p><p>In [5]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(5,3)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>Out[5]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.4628, 0.7432, 0.9785],</span><br><span class="line">        [0.2068, 0.4441, 0.9176],</span><br><span class="line">        [0.1027, 0.5275, 0.3884],</span><br><span class="line">        [0.9380, 0.2113, 0.2839],</span><br><span class="line">        [0.0094, 0.4001, 0.6483]])</span><br></pre></td></tr></table></figure><p>æ„å»ºä¸€ä¸ªå…¨éƒ¨ä¸º0ï¼Œç±»å‹ä¸ºlongçš„çŸ©é˜µ:</p><p>In [8]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(5,3,dtype=torch.long)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>Out[8]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0, 0, 0],</span><br><span class="line">        [0, 0, 0],</span><br><span class="line">        [0, 0, 0],</span><br><span class="line">        [0, 0, 0],</span><br><span class="line">        [0, 0, 0]])</span><br></pre></td></tr></table></figure><p>In [11]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(5,3).long()</span><br><span class="line">x.dtype</span><br></pre></td></tr></table></figure><p>Out[11]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.int64</span><br></pre></td></tr></table></figure><p>ä»æ•°æ®ç›´æ¥ç›´æ¥æ„å»ºtensor:</p><p>In [12]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([5.5,3])</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>Out[12]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([5.5000, 3.0000])</span><br></pre></td></tr></table></figure><p>ä¹Ÿå¯ä»¥ä»ä¸€ä¸ªå·²æœ‰çš„tensoræ„å»ºä¸€ä¸ªtensorã€‚è¿™äº›æ–¹æ³•ä¼šé‡ç”¨åŸæ¥tensorçš„ç‰¹å¾ï¼Œä¾‹å¦‚ï¼Œæ•°æ®ç±»å‹ï¼Œé™¤éæä¾›æ–°çš„æ•°æ®ã€‚</p><p>In [16]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = x.new_ones(5,3, dtype=torch.double)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>Out[16]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p>In [17]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn_like(x, dtype=torch.float)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>Out[17]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.2411, -0.3961, -0.9206],</span><br><span class="line">        [-0.0508,  0.2653,  0.4685],</span><br><span class="line">        [ 0.5368, -0.3606, -0.0073],</span><br><span class="line">        [ 0.3383,  0.6826,  1.7368],</span><br><span class="line">        [-0.0811, -0.6957, -0.4566]])</span><br></pre></td></tr></table></figure><p>å¾—åˆ°tensorçš„å½¢çŠ¶:</p><p>In [20]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure><p>Out[20]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([5, 3])</span><br></pre></td></tr></table></figure><h4 id="æ³¨æ„"><a href="#æ³¨æ„" class="headerlink" title="æ³¨æ„"></a>æ³¨æ„</h4><p><code>torch.Size</code> è¿”å›çš„æ˜¯ä¸€ä¸ªtuple</p><p>Operations</p><p>æœ‰å¾ˆå¤šç§tensorè¿ç®—ã€‚æˆ‘ä»¬å…ˆä»‹ç»åŠ æ³•è¿ç®—ã€‚</p><p>In [21]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = torch.rand(5,3)</span><br><span class="line">y</span><br></pre></td></tr></table></figure><p>Out[21]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.9456, 0.3996, 0.1981],</span><br><span class="line">        [0.8728, 0.7097, 0.3721],</span><br><span class="line">        [0.7489, 0.9502, 0.6241],</span><br><span class="line">        [0.5176, 0.0200, 0.5130],</span><br><span class="line">        [0.3552, 0.2710, 0.7392]])</span><br></pre></td></tr></table></figure><p>In [23]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x + y</span><br></pre></td></tr></table></figure><p>Out[23]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.1866,  0.0035, -0.7225],</span><br><span class="line">        [ 0.8220,  0.9750,  0.8406],</span><br><span class="line">        [ 1.2857,  0.5896,  0.6168],</span><br><span class="line">        [ 0.8559,  0.7026,  2.2498],</span><br><span class="line">        [ 0.2741, -0.4248,  0.2826]])</span><br></pre></td></tr></table></figure><p>å¦ä¸€ç§ç€åŠ æ³•çš„å†™æ³•</p><p>In [24]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.add(x, y)</span><br></pre></td></tr></table></figure><p>Out[24]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.1866,  0.0035, -0.7225],</span><br><span class="line">        [ 0.8220,  0.9750,  0.8406],</span><br><span class="line">        [ 1.2857,  0.5896,  0.6168],</span><br><span class="line">        [ 0.8559,  0.7026,  2.2498],</span><br><span class="line">        [ 0.2741, -0.4248,  0.2826]])</span><br></pre></td></tr></table></figure><p>åŠ æ³•ï¼šæŠŠè¾“å‡ºä½œä¸ºä¸€ä¸ªå˜é‡</p><p>In [26]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">result = torch.empty(5,3)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line"># result = x + y</span><br><span class="line">result</span><br></pre></td></tr></table></figure><p>Out[26]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.1866,  0.0035, -0.7225],</span><br><span class="line">        [ 0.8220,  0.9750,  0.8406],</span><br><span class="line">        [ 1.2857,  0.5896,  0.6168],</span><br><span class="line">        [ 0.8559,  0.7026,  2.2498],</span><br><span class="line">        [ 0.2741, -0.4248,  0.2826]])</span><br></pre></td></tr></table></figure><p>in-placeåŠ æ³•</p><p>In [28]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.add_(x)</span><br><span class="line">y</span><br></pre></td></tr></table></figure><p>Out[28]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.1866,  0.0035, -0.7225],</span><br><span class="line">        [ 0.8220,  0.9750,  0.8406],</span><br><span class="line">        [ 1.2857,  0.5896,  0.6168],</span><br><span class="line">        [ 0.8559,  0.7026,  2.2498],</span><br><span class="line">        [ 0.2741, -0.4248,  0.2826]])</span><br></pre></td></tr></table></figure><h4 id="æ³¨æ„-1"><a href="#æ³¨æ„-1" class="headerlink" title="æ³¨æ„"></a>æ³¨æ„</h4><p>ä»»ä½•in-placeçš„è¿ç®—éƒ½ä¼šä»¥<code>_</code>ç»“å°¾ã€‚ ä¸¾ä¾‹æ¥è¯´ï¼š<code>x.copy_(y)</code>, <code>x.t_()</code>, ä¼šæ”¹å˜ <code>x</code>ã€‚</p><p>å„ç§ç±»ä¼¼NumPyçš„indexingéƒ½å¯ä»¥åœ¨PyTorch tensorä¸Šé¢ä½¿ç”¨ã€‚</p><p>In [31]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[1:, 1:]</span><br></pre></td></tr></table></figure><p>Out[31]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.2653,  0.4685],</span><br><span class="line">        [-0.3606, -0.0073],</span><br><span class="line">        [ 0.6826,  1.7368],</span><br><span class="line">        [-0.6957, -0.4566]])</span><br></pre></td></tr></table></figure><p>Resizing: å¦‚æœä½ å¸Œæœ›resize/reshapeä¸€ä¸ªtensorï¼Œå¯ä»¥ä½¿ç”¨<code>torch.view</code>ï¼š</p><p>In [39]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(4,4)</span><br><span class="line">y = x.view(16)</span><br><span class="line">z = x.view(-1,8)</span><br><span class="line">z</span><br></pre></td></tr></table></figure><p>Out[39]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.5683,  1.3885, -2.0829, -0.7613, -1.9115,  0.3732, -0.2055, -1.2300],</span><br><span class="line">        [-0.2612, -0.4682, -1.0596,  0.7447,  0.7603, -0.4281,  0.5495,  0.1025]])</span><br></pre></td></tr></table></figure><p>å¦‚æœä½ æœ‰ä¸€ä¸ªåªæœ‰ä¸€ä¸ªå…ƒç´ çš„tensorï¼Œä½¿ç”¨<code>.item()</code>æ–¹æ³•å¯ä»¥æŠŠé‡Œé¢çš„valueå˜æˆPythonæ•°å€¼ã€‚</p><p>In [40]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(1)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>Out[40]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([-1.1493])</span><br></pre></td></tr></table></figure><p>In [44]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.item()</span><br></pre></td></tr></table></figure><p>Out[44]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-1.1493233442306519</span><br></pre></td></tr></table></figure><p>In [48]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z.transpose(1,0)</span><br></pre></td></tr></table></figure><p>Out[48]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.5683, -0.2612],</span><br><span class="line">        [ 1.3885, -0.4682],</span><br><span class="line">        [-2.0829, -1.0596],</span><br><span class="line">        [-0.7613,  0.7447],</span><br><span class="line">        [-1.9115,  0.7603],</span><br><span class="line">        [ 0.3732, -0.4281],</span><br><span class="line">        [-0.2055,  0.5495],</span><br><span class="line">        [-1.2300,  0.1025]])</span><br></pre></td></tr></table></figure><p><strong>æ›´å¤šé˜…è¯»</strong></p><p>å„ç§Tensor operations, åŒ…æ‹¬transposing, indexing, slicing, mathematical operations, linear algebra, random numbersåœ¨<code>&lt;https://pytorch.org/docs/torch&gt;</code>.</p><h2 id="Numpyå’ŒTensorä¹‹é—´çš„è½¬åŒ–"><a href="#Numpyå’ŒTensorä¹‹é—´çš„è½¬åŒ–" class="headerlink" title="Numpyå’ŒTensorä¹‹é—´çš„è½¬åŒ–"></a>Numpyå’ŒTensorä¹‹é—´çš„è½¬åŒ–</h2><p>åœ¨Torch Tensorå’ŒNumPy arrayä¹‹é—´ç›¸äº’è½¬åŒ–éå¸¸å®¹æ˜“ã€‚</p><p>Torch Tensorå’ŒNumPy arrayä¼šå…±äº«å†…å­˜ï¼Œæ‰€ä»¥æ”¹å˜å…¶ä¸­ä¸€é¡¹ä¹Ÿä¼šæ”¹å˜å¦ä¸€é¡¹ã€‚</p><p>æŠŠTorch Tensorè½¬å˜æˆNumPy Array</p><p>In [49]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(5)</span><br><span class="line">a</span><br></pre></td></tr></table></figure><p>Out[49]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1., 1., 1., 1., 1.])</span><br></pre></td></tr></table></figure><p>In [50]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = a.numpy()</span><br><span class="line">b</span><br></pre></td></tr></table></figure><p>Out[50]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([1., 1., 1., 1., 1.], dtype=float32)</span><br></pre></td></tr></table></figure><p>æ”¹å˜numpy arrayé‡Œé¢çš„å€¼ã€‚</p><p>In [51]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b[1] = 2</span><br><span class="line">b</span><br></pre></td></tr></table></figure><p>Out[51]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([1., 2., 1., 1., 1.], dtype=float32)</span><br></pre></td></tr></table></figure><p>In [52]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a</span><br></pre></td></tr></table></figure><p>Out[52]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1., 2., 1., 1., 1.])</span><br></pre></td></tr></table></figure><p>æŠŠNumPy ndarrayè½¬æˆTorch Tensor</p><p>In [54]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br></pre></td></tr></table></figure><p>In [55]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = np.ones(5)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, 1, out=a)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2. 2. 2. 2. 2.]</span><br></pre></td></tr></table></figure><p>In [56]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b</span><br></pre></td></tr></table></figure><p>Out[56]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p>æ‰€æœ‰CPUä¸Šçš„Tensoréƒ½æ”¯æŒè½¬æˆnumpyæˆ–è€…ä»numpyè½¬æˆTensorã€‚</p><h2 id="CUDA-Tensors"><a href="#CUDA-Tensors" class="headerlink" title="CUDA Tensors"></a>CUDA Tensors</h2><p>ä½¿ç”¨<code>.to</code>æ–¹æ³•ï¼ŒTensorå¯ä»¥è¢«ç§»åŠ¨åˆ°åˆ«çš„deviceä¸Šã€‚</p><p>In [60]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line">    y = torch.ones_like(x, device=device)</span><br><span class="line">    x = x.to(device)</span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))</span><br></pre></td></tr></table></figure><p>Out[60]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">False</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.to(<span class="string">"cpu"</span>).data.numpy()</span><br><span class="line">y.cpu().data.numpy()</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = model.cuda()</span><br></pre></td></tr></table></figure><h2 id="çƒ­èº«-ç”¨numpyå®ç°ä¸¤å±‚ç¥ç»ç½‘ç»œ"><a href="#çƒ­èº«-ç”¨numpyå®ç°ä¸¤å±‚ç¥ç»ç½‘ç»œ" class="headerlink" title="çƒ­èº«: ç”¨numpyå®ç°ä¸¤å±‚ç¥ç»ç½‘ç»œ"></a>çƒ­èº«: ç”¨numpyå®ç°ä¸¤å±‚ç¥ç»ç½‘ç»œ</h2><p>ä¸€ä¸ªå…¨è¿æ¥ReLUç¥ç»ç½‘ç»œï¼Œä¸€ä¸ªéšè—å±‚ï¼Œæ²¡æœ‰biasã€‚ç”¨æ¥ä»xé¢„æµ‹yï¼Œä½¿ç”¨L2 Lossã€‚</p><ul><li>â„=ğ‘Š1ğ‘‹h=W1X</li><li>ğ‘=ğ‘šğ‘ğ‘¥(0,â„)a=max(0,h)</li><li>ğ‘¦â„ğ‘ğ‘¡=ğ‘Š2ğ‘yhat=W2a</li></ul><p>è¿™ä¸€å®ç°å®Œå…¨ä½¿ç”¨numpyæ¥è®¡ç®—å‰å‘ç¥ç»ç½‘ç»œï¼Œlossï¼Œå’Œåå‘ä¼ æ’­ã€‚</p><ul><li>forward pass</li><li>loss</li><li>backward pass</li></ul><p>numpy ndarrayæ˜¯ä¸€ä¸ªæ™®é€šçš„nç»´arrayã€‚å®ƒä¸çŸ¥é“ä»»ä½•å…³äºæ·±åº¦å­¦ä¹ æˆ–è€…æ¢¯åº¦(gradient)çš„çŸ¥è¯†ï¼Œä¹Ÿä¸çŸ¥é“è®¡ç®—å›¾(computation graph)ï¼Œåªæ˜¯ä¸€ç§ç”¨æ¥è®¡ç®—æ•°å­¦è¿ç®—çš„æ•°æ®ç»“æ„ã€‚</p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># éšæœºåˆ›å»ºä¸€äº›è®­ç»ƒæ•°æ®</span></span><br><span class="line">x = np.random.randn(N, D_in)</span><br><span class="line">y = np.random.randn(N, D_out)</span><br><span class="line"></span><br><span class="line">w1 = np.random.randn(D_in, H)</span><br><span class="line">w2 = np.random.randn(H, D_out)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    h = x.dot(w1) <span class="comment"># N * H</span></span><br><span class="line">    h_relu = np.maximum(h, <span class="number">0</span>) <span class="comment"># N * H</span></span><br><span class="line">    y_pred = h_relu.dot(w2) <span class="comment"># N * D_out</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute loss</span></span><br><span class="line">    loss = np.square(y_pred - y).sum()</span><br><span class="line">    print(it, loss)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    <span class="comment"># compute the gradient</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</span><br><span class="line">    grad_h = grad_h_relu.copy()</span><br><span class="line">    grad_h[h&lt;<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.T.dot(grad_h)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update weights of w1 and w2</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure><h2 id="PyTorch-Tensors"><a href="#PyTorch-Tensors" class="headerlink" title="PyTorch: Tensors"></a>PyTorch: Tensors</h2><p>è¿™æ¬¡æˆ‘ä»¬ä½¿ç”¨PyTorch tensorsæ¥åˆ›å»ºå‰å‘ç¥ç»ç½‘ç»œï¼Œè®¡ç®—æŸå¤±ï¼Œä»¥åŠåå‘ä¼ æ’­ã€‚</p><p>ä¸€ä¸ªPyTorch Tensorå¾ˆåƒä¸€ä¸ªnumpyçš„ndarrayã€‚ä½†æ˜¯å®ƒå’Œnumpy ndarrayæœ€å¤§çš„åŒºåˆ«æ˜¯ï¼ŒPyTorch Tensorå¯ä»¥åœ¨CPUæˆ–è€…GPUä¸Šè¿ç®—ã€‚å¦‚æœæƒ³è¦åœ¨GPUä¸Šè¿ç®—ï¼Œå°±éœ€è¦æŠŠTensoræ¢æˆcudaç±»å‹ã€‚</p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># éšæœºåˆ›å»ºä¸€äº›è®­ç»ƒæ•°æ®</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line">w1 = torch.randn(D_in, H)</span><br><span class="line">w2 = torch.randn(H, D_out)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    h = x.mm(w1) <span class="comment"># N * H</span></span><br><span class="line">    h_relu = h.clamp(min=<span class="number">0</span>) <span class="comment"># N * H</span></span><br><span class="line">    y_pred = h_relu.mm(w2) <span class="comment"># N * D_out</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum().item()</span><br><span class="line">    print(it, loss)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    <span class="comment"># compute the gradient</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.t().mm(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.mm(w2.t())</span><br><span class="line">    grad_h = grad_h_relu.clone()</span><br><span class="line">    grad_h[h&lt;<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.t().mm(grad_h)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update weights of w1 and w2</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure><p>ç®€å•çš„autograd</p><p>In [72]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">1.</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w = torch.tensor(<span class="number">2.</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = w*x + b <span class="comment"># y = 2*1+3</span></span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># dy / dw = x</span></span><br><span class="line">print(w.grad)</span><br><span class="line">print(x.grad)</span><br><span class="line">print(b.grad)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor(1.)</span><br><span class="line">tensor(2.)</span><br><span class="line">tensor(1.)</span><br></pre></td></tr></table></figure><h2 id="PyTorch-Tensorå’Œautograd"><a href="#PyTorch-Tensorå’Œautograd" class="headerlink" title="PyTorch: Tensorå’Œautograd"></a>PyTorch: Tensorå’Œautograd</h2><p>PyTorchçš„ä¸€ä¸ªé‡è¦åŠŸèƒ½å°±æ˜¯autogradï¼Œä¹Ÿå°±æ˜¯è¯´åªè¦å®šä¹‰äº†forward pass(å‰å‘ç¥ç»ç½‘ç»œ)ï¼Œè®¡ç®—äº†lossä¹‹åï¼ŒPyTorchå¯ä»¥è‡ªåŠ¨æ±‚å¯¼è®¡ç®—æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ã€‚</p><p>ä¸€ä¸ªPyTorchçš„Tensorè¡¨ç¤ºè®¡ç®—å›¾ä¸­çš„ä¸€ä¸ªèŠ‚ç‚¹ã€‚å¦‚æœ<code>x</code>æ˜¯ä¸€ä¸ªTensorå¹¶ä¸”<code>x.requires_grad=True</code>é‚£ä¹ˆ<code>x.grad</code>æ˜¯å¦ä¸€ä¸ªå‚¨å­˜ç€<code>x</code>å½“å‰æ¢¯åº¦(ç›¸å¯¹äºä¸€ä¸ªscalarï¼Œå¸¸å¸¸æ˜¯loss)çš„å‘é‡ã€‚</p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># éšæœºåˆ›å»ºä¸€äº›è®­ç»ƒæ•°æ®</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line">w1 = torch.randn(D_in, H, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn(H, D_out, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = x.mm(w1).clamp(min=<span class="number">0</span>).mm(w2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum() <span class="comment"># computation graph</span></span><br><span class="line">    print(it, loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update weights of w1 and w2</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        w1 -= learning_rate * w1.grad</span><br><span class="line">        w2 -= learning_rate * w2.grad</span><br><span class="line">        w1.grad.zero_()</span><br><span class="line">        w2.grad.zero_()</span><br></pre></td></tr></table></figure><h2 id="PyTorch-nn"><a href="#PyTorch-nn" class="headerlink" title="PyTorch: nn"></a>PyTorch: nn</h2><p>è¿™æ¬¡æˆ‘ä»¬ä½¿ç”¨PyTorchä¸­nnè¿™ä¸ªåº“æ¥æ„å»ºç½‘ç»œã€‚ ç”¨PyTorch autogradæ¥æ„å»ºè®¡ç®—å›¾å’Œè®¡ç®—gradientsï¼Œ ç„¶åPyTorchä¼šå¸®æˆ‘ä»¬è‡ªåŠ¨è®¡ç®—gradientã€‚</p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># éšæœºåˆ›å»ºä¸€äº›è®­ç»ƒæ•°æ®</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H, bias=<span class="literal">False</span>), <span class="comment"># w_1 * x + b_1</span></span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out, bias=<span class="literal">False</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">torch.nn.init.normal_(model[<span class="number">0</span>].weight)</span><br><span class="line">torch.nn.init.normal_(model[<span class="number">2</span>].weight)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model = model.cuda()</span></span><br><span class="line"></span><br><span class="line">loss_fn = nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(x) <span class="comment"># model.forward() </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y) <span class="comment"># computation graph</span></span><br><span class="line">    print(it, loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update weights of w1 and w2</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters(): <span class="comment"># param (tensor, grad)</span></span><br><span class="line">            param -= learning_rate * param.grad</span><br><span class="line">            </span><br><span class="line">    model.zero_grad()</span><br></pre></td></tr></table></figure><p>In [113]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model[0].weight</span><br></pre></td></tr></table></figure><p>Out[113]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[-0.0218,  0.0212,  0.0243,  ...,  0.0230,  0.0247,  0.0168],</span><br><span class="line">        [-0.0144,  0.0177, -0.0221,  ...,  0.0161,  0.0098, -0.0172],</span><br><span class="line">        [ 0.0086, -0.0122, -0.0298,  ..., -0.0236, -0.0187,  0.0295],</span><br><span class="line">        ...,</span><br><span class="line">        [ 0.0266, -0.0008, -0.0141,  ...,  0.0018,  0.0319, -0.0129],</span><br><span class="line">        [ 0.0296, -0.0005,  0.0115,  ...,  0.0141, -0.0088, -0.0106],</span><br><span class="line">        [ 0.0289, -0.0077,  0.0239,  ..., -0.0166, -0.0156, -0.0235]],</span><br><span class="line">       requires_grad=True)</span><br></pre></td></tr></table></figure><h2 id="PyTorch-optim"><a href="#PyTorch-optim" class="headerlink" title="PyTorch: optim"></a>PyTorch: optim</h2><p>è¿™ä¸€æ¬¡æˆ‘ä»¬ä¸å†æ‰‹åŠ¨æ›´æ–°æ¨¡å‹çš„weights,è€Œæ˜¯ä½¿ç”¨optimè¿™ä¸ªåŒ…æ¥å¸®åŠ©æˆ‘ä»¬æ›´æ–°å‚æ•°ã€‚ optimè¿™ä¸ªpackageæä¾›äº†å„ç§ä¸åŒçš„æ¨¡å‹ä¼˜åŒ–æ–¹æ³•ï¼ŒåŒ…æ‹¬SGD+momentum, RMSProp, Adamç­‰ç­‰ã€‚</p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># éšæœºåˆ›å»ºä¸€äº›è®­ç»ƒæ•°æ®</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H, bias=<span class="literal">False</span>), <span class="comment"># w_1 * x + b_1</span></span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out, bias=<span class="literal">False</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">torch.nn.init.normal_(model[<span class="number">0</span>].weight)</span><br><span class="line">torch.nn.init.normal_(model[<span class="number">2</span>].weight)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model = model.cuda()</span></span><br><span class="line"></span><br><span class="line">loss_fn = nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line"><span class="comment"># learning_rate = 1e-4</span></span><br><span class="line"><span class="comment"># optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span></span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(x) <span class="comment"># model.forward() </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y) <span class="comment"># computation graph</span></span><br><span class="line">    print(it, loss.item())</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update model parameters</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><h2 id="PyTorch-è‡ªå®šä¹‰-nn-Modules"><a href="#PyTorch-è‡ªå®šä¹‰-nn-Modules" class="headerlink" title="PyTorch: è‡ªå®šä¹‰ nn Modules"></a>PyTorch: è‡ªå®šä¹‰ nn Modules</h2><p>æˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªæ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ªnn.Moduleç±»ã€‚å¦‚æœéœ€è¦å®šä¹‰ä¸€ä¸ªæ¯”Sequentialæ¨¡å‹æ›´åŠ å¤æ‚çš„æ¨¡å‹ï¼Œå°±éœ€è¦å®šä¹‰nn.Moduleæ¨¡å‹ã€‚</p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># éšæœºåˆ›å»ºä¸€äº›è®­ç»ƒæ•°æ®</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></span><br><span class="line">        super(TwoLayerNet, self).__init__()</span><br><span class="line">        <span class="comment"># define the model architecture</span></span><br><span class="line">        self.linear1 = torch.nn.Linear(D_in, H, bias=<span class="literal">False</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(H, D_out, bias=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        y_pred = self.linear2(self.linear1(x).clamp(min=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line">model = TwoLayerNet(D_in, H, D_out)</span><br><span class="line">loss_fn = nn.MSELoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(x) <span class="comment"># model.forward() </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y) <span class="comment"># computation graph</span></span><br><span class="line">    print(it, loss.item())</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update model parameters</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;ä»€ä¹ˆæ˜¯PyTorch&quot;&gt;&lt;a href=&quot;#ä»€ä¹ˆæ˜¯PyTorch&quot; class=&quot;headerlink&quot; title=&quot;ä»€ä¹ˆæ˜¯PyTorch?&quot;&gt;&lt;/a&gt;ä»€ä¹ˆæ˜¯PyTorch?&lt;/h1&gt;&lt;p&gt;PyTorchæ˜¯ä¸€ä¸ªåŸºäºPythonçš„ç§‘å­¦è®¡ç®—åº“ï¼Œå®ƒæœ‰ä»¥ä¸‹ç‰¹ç‚¹:&lt;/p&gt;
      
    
    </summary>
    
      <category term="æœºå™¨å­¦ä¹ " scheme="http://mmyblog.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="PyTorch" scheme="http://mmyblog.cn/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>æœ´ç´ è´å¶æ–¯</title>
    <link href="http://mmyblog.cn/2020/06/08/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    <id>http://mmyblog.cn/2020/06/08/æœ´ç´ è´å¶æ–¯/</id>
    <published>2020-06-08T10:24:41.000Z</published>
    <updated>2020-06-08T10:27:10.275Z</updated>
    
    <content type="html"><![CDATA[<h1 id="æœ´ç´ è´å¶æ–¯"><a href="#æœ´ç´ è´å¶æ–¯" class="headerlink" title="æœ´ç´ è´å¶æ–¯"></a>æœ´ç´ è´å¶æ–¯</h1><h2 id="1-å¼•è¨€"><a href="#1-å¼•è¨€" class="headerlink" title="1. å¼•è¨€"></a>1. å¼•è¨€</h2><p>è´å¶æ–¯æ–¹æ³•æ˜¯ä¸€ä¸ªå†å²æ‚ ä¹…ï¼Œæœ‰ç€åšå®çš„ç†è®ºåŸºç¡€çš„æ–¹æ³•ï¼ŒåŒæ—¶å¤„ç†å¾ˆå¤šé—®é¢˜æ—¶ç›´æ¥è€Œåˆé«˜æ•ˆï¼Œå¾ˆå¤šé«˜çº§è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ä¹Ÿå¯ä»¥ä»å®ƒæ¼”åŒ–è€Œæ¥ã€‚å› æ­¤ï¼Œå­¦ä¹ è´å¶æ–¯æ–¹æ³•ï¼Œæ˜¯ç ”ç©¶è‡ªç„¶è¯­è¨€å¤„ç†é—®é¢˜çš„ä¸€ä¸ªéå¸¸å¥½çš„åˆ‡å…¥å£ã€‚</p><h2 id="2-è´å¶æ–¯å…¬å¼"><a href="#2-è´å¶æ–¯å…¬å¼" class="headerlink" title="2. è´å¶æ–¯å…¬å¼"></a>2. è´å¶æ–¯å…¬å¼</h2><p>è´å¶æ–¯å…¬å¼å°±ä¸€è¡Œï¼š</p><blockquote><p>$$<br>P(Y|X)=P(X|Y)P(Y)/P(X)<br>$$</p></blockquote><p>è€Œå®ƒå…¶å®æ˜¯ç”±ä»¥ä¸‹çš„è”åˆæ¦‚ç‡å…¬å¼æ¨å¯¼å‡ºæ¥ï¼š<br>$$<br>P(Y,X)=P(Y|X)P(X)=P(X|Y)P(Y)<br>$$<br>å…¶ä¸­P(Y)å«åšå…ˆéªŒæ¦‚ç‡ï¼ŒP(Y|X)å«åšåéªŒæ¦‚ç‡ï¼ŒP(Y,X)å«åšè”åˆæ¦‚ç‡ã€‚</p><p>æ²¡äº†ï¼Œè´å¶æ–¯æœ€æ ¸å¿ƒçš„å…¬å¼å°±è¿™ä¹ˆäº›ã€‚</p><h2 id="3-ç”¨æœºå™¨å­¦ä¹ çš„è§†è§’ç†è§£è´å¶æ–¯å…¬å¼"><a href="#3-ç”¨æœºå™¨å­¦ä¹ çš„è§†è§’ç†è§£è´å¶æ–¯å…¬å¼" class="headerlink" title="3. ç”¨æœºå™¨å­¦ä¹ çš„è§†è§’ç†è§£è´å¶æ–¯å…¬å¼"></a>3. ç”¨æœºå™¨å­¦ä¹ çš„è§†è§’ç†è§£è´å¶æ–¯å…¬å¼</h2><p>åœ¨æœºå™¨å­¦ä¹ çš„è§†è§’ä¸‹ï¼Œæˆ‘ä»¬æŠŠXç†è§£æˆ<strong>â€œå…·æœ‰æŸç‰¹å¾â€</strong>ï¼ŒæŠŠYç†è§£æˆ<strong>â€œç±»åˆ«æ ‡ç­¾â€</strong>(ä¸€èˆ¬æœºå™¨å­¦ä¹ ä¸ºé¢˜ä¸­éƒ½æ˜¯<code>X=&gt;ç‰¹å¾</code>, <code>Y=&gt;ç»“æœ</code>å¯¹å§)ã€‚åœ¨æœ€ç®€å•çš„äºŒåˆ†ç±»é—®é¢˜(<code>æ˜¯</code>ä¸<code>å¦</code>åˆ¤å®š)ä¸‹ï¼Œæˆ‘ä»¬å°†Yç†è§£æˆ<strong>â€œå±äºæŸç±»</strong>â€çš„æ ‡ç­¾ã€‚äºæ˜¯è´å¶æ–¯å…¬å¼å°±å˜å½¢æˆäº†ä¸‹é¢çš„æ ·å­:</p><blockquote><p>P(â€œå±äºæŸç±»â€|â€œå…·æœ‰æŸç‰¹å¾â€)=P(â€œå…·æœ‰æŸç‰¹å¾â€|â€œå±äºæŸç±»â€)P(â€œå±äºæŸç±»â€)P(â€œå…·æœ‰æŸç‰¹å¾â€)</p></blockquote><p>æˆ‘ä»¬ç®€åŒ–è§£é‡Šä¸€ä¸‹ä¸Šè¿°å…¬å¼ï¼š</p><blockquote><p>P(â€œå±äºæŸç±»â€|â€œå…·æœ‰æŸç‰¹å¾â€)=åœ¨å·²çŸ¥æŸæ ·æœ¬â€œå…·æœ‰æŸç‰¹å¾â€çš„æ¡ä»¶ä¸‹ï¼Œè¯¥æ ·æœ¬â€œå±äºæŸç±»â€çš„æ¦‚ç‡ã€‚æ‰€ä»¥å«åš<strong>ã€åéªŒæ¦‚ç‡ã€</strong>ã€‚<br>P(â€œå…·æœ‰æŸç‰¹å¾â€|â€œå±äºæŸç±»â€)=åœ¨å·²çŸ¥æŸæ ·æœ¬â€œå±äºæŸç±»â€çš„æ¡ä»¶ä¸‹ï¼Œè¯¥æ ·æœ¬â€œå…·æœ‰æŸç‰¹å¾â€çš„æ¦‚ç‡ã€‚<br>P(â€œå±äºæŸç±»â€)=ï¼ˆåœ¨æœªçŸ¥æŸæ ·æœ¬å…·æœ‰è¯¥â€œå…·æœ‰æŸç‰¹å¾â€çš„æ¡ä»¶ä¸‹ï¼Œï¼‰è¯¥æ ·æœ¬â€œå±äºæŸç±»â€çš„æ¦‚ç‡ã€‚æ‰€ä»¥å«åš<strong>ã€å…ˆéªŒæ¦‚ç‡ã€</strong>ã€‚<br>P(â€œå…·æœ‰æŸç‰¹å¾â€)=(åœ¨æœªçŸ¥æŸæ ·æœ¬â€œå±äºæŸç±»â€çš„æ¡ä»¶ä¸‹ï¼Œ)è¯¥æ ·æœ¬â€œå…·æœ‰æŸç‰¹å¾â€çš„æ¦‚ç‡ã€‚</p></blockquote><p>è€Œæˆ‘ä»¬äºŒåˆ†ç±»é—®é¢˜çš„æœ€ç»ˆç›®çš„å°±æ˜¯è¦<strong>åˆ¤æ–­P(â€œå±äºæŸç±»â€|â€œå…·æœ‰æŸç‰¹å¾â€)æ˜¯å¦å¤§äº1/2</strong>å°±å¤Ÿäº†ã€‚è´å¶æ–¯æ–¹æ³•æŠŠè®¡ç®—<strong>â€œå…·æœ‰æŸç‰¹å¾çš„æ¡ä»¶ä¸‹å±äºæŸç±»â€</strong>çš„æ¦‚ç‡è½¬æ¢æˆéœ€è¦è®¡ç®—<strong>â€œå±äºæŸç±»çš„æ¡ä»¶ä¸‹å…·æœ‰æŸç‰¹å¾â€</strong>çš„æ¦‚ç‡ï¼Œè€Œåè€…è·å–æ–¹æ³•å°±ç®€å•å¤šäº†ï¼Œæˆ‘ä»¬åªéœ€è¦æ‰¾åˆ°ä¸€äº›åŒ…å«å·²çŸ¥ç‰¹å¾æ ‡ç­¾çš„æ ·æœ¬ï¼Œå³å¯è¿›è¡Œè®­ç»ƒã€‚è€Œæ ·æœ¬çš„ç±»åˆ«æ ‡ç­¾éƒ½æ˜¯æ˜ç¡®çš„ï¼Œæ‰€ä»¥è´å¶æ–¯æ–¹æ³•åœ¨æœºå™¨å­¦ä¹ é‡Œå±äºæœ‰ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚</p><p>è¿™é‡Œå†è¡¥å……ä¸€ä¸‹ï¼Œä¸€èˆ¬<strong>ã€å…ˆéªŒæ¦‚ç‡ã€ã€ã€åéªŒæ¦‚ç‡ã€æ˜¯ç›¸å¯¹</strong>å‡ºç°çš„ï¼Œæ¯”å¦‚P(Y)ä¸P(Y|X)æ˜¯å…³äºYçš„å…ˆéªŒæ¦‚ç‡ä¸åéªŒæ¦‚ç‡ï¼ŒP(X)ä¸P(X|Y)æ˜¯å…³äºXçš„å…ˆéªŒæ¦‚ç‡ä¸åéªŒæ¦‚ç‡ã€‚</p><h2 id="4-åƒåœ¾é‚®ä»¶è¯†åˆ«"><a href="#4-åƒåœ¾é‚®ä»¶è¯†åˆ«" class="headerlink" title="4. åƒåœ¾é‚®ä»¶è¯†åˆ«"></a>4. åƒåœ¾é‚®ä»¶è¯†åˆ«</h2><p>ä¸¾ä¸ªä¾‹å­å¥½å•¦ï¼Œæˆ‘ä»¬ç°åœ¨è¦å¯¹é‚®ä»¶è¿›è¡Œåˆ†ç±»ï¼Œè¯†åˆ«åƒåœ¾é‚®ä»¶å’Œæ™®é€šé‚®ä»¶ï¼Œå¦‚æœæˆ‘ä»¬é€‰æ‹©ä½¿ç”¨æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ï¼Œé‚£ç›®æ ‡å°±æ˜¯<strong>åˆ¤æ–­P(â€œåƒåœ¾é‚®ä»¶â€|â€œå…·æœ‰æŸç‰¹å¾â€)æ˜¯å¦å¤§äº1/2</strong>ã€‚ç°åœ¨å‡è®¾æˆ‘ä»¬æœ‰åƒåœ¾é‚®ä»¶å’Œæ­£å¸¸é‚®ä»¶å„1ä¸‡å°ä½œä¸ºè®­ç»ƒé›†ã€‚éœ€è¦åˆ¤æ–­ä»¥ä¸‹è¿™ä¸ªé‚®ä»¶æ˜¯å¦å±äºåƒåœ¾é‚®ä»¶ï¼š</p><blockquote><p>â€œæˆ‘å¸å¯åŠç†æ­£è§„å‘ç¥¨ï¼ˆä¿çœŸï¼‰17%å¢å€¼ç¨å‘ç¥¨ç‚¹æ•°ä¼˜æƒ ï¼â€</p></blockquote><p>ä¹Ÿå°±æ˜¯<strong>åˆ¤æ–­æ¦‚ç‡P(â€œåƒåœ¾é‚®ä»¶â€|â€œæˆ‘å¸å¯åŠç†æ­£è§„å‘ç¥¨ï¼ˆä¿çœŸï¼‰17%å¢å€¼ç¨å‘ç¥¨ç‚¹æ•°ä¼˜æƒ ï¼â€)æ˜¯å¦å¤§äº1/2</strong>ã€‚</p><p>å’³å’³ï¼Œæœ‰æœ¨æœ‰å‘ç°ï¼Œè½¬æ¢æˆçš„è¿™ä¸ªæ¦‚ç‡ï¼Œè®¡ç®—çš„æ–¹æ³•ï¼šå°±æ˜¯å†™ä¸ªè®¡æ•°å™¨ï¼Œç„¶å+1 +1 +1ç»Ÿè®¡å‡ºæ‰€æœ‰åƒåœ¾é‚®ä»¶å’Œæ­£å¸¸é‚®ä»¶ä¸­å‡ºç°è¿™å¥è¯çš„æ¬¡æ•°å•Šï¼ï¼ï¼å¥½ï¼Œå…·ä½“ç‚¹è¯´ï¼š</p><blockquote><p>P(â€œåƒåœ¾é‚®ä»¶â€|â€œæˆ‘å¸å¯åŠç†æ­£è§„å‘ç¥¨ï¼ˆä¿çœŸï¼‰17%å¢å€¼ç¨å‘ç¥¨ç‚¹æ•°ä¼˜æƒ ï¼â€) =åƒåœ¾é‚®ä»¶ä¸­å‡ºç°è¿™å¥è¯çš„æ¬¡æ•°åƒåœ¾é‚®ä»¶ä¸­å‡ºç°è¿™å¥è¯çš„æ¬¡æ•°+æ­£å¸¸é‚®ä»¶ä¸­å‡ºç°è¿™å¥è¯çš„æ¬¡æ•°</p></blockquote><h2 id="5-åˆ†è¯"><a href="#5-åˆ†è¯" class="headerlink" title="5. åˆ†è¯"></a>5. åˆ†è¯</h2><p>ä¸€ä¸ªå¾ˆæ‚²å“€ä½†æ˜¯å¾ˆç°å®çš„ç»“è®ºï¼š <strong>è®­ç»ƒé›†æ˜¯æœ‰é™çš„ï¼Œè€Œå¥å­çš„å¯èƒ½æ€§åˆ™æ˜¯æ— é™çš„ã€‚æ‰€ä»¥è¦†ç›–æ‰€æœ‰å¥å­å¯èƒ½æ€§çš„è®­ç»ƒé›†æ˜¯ä¸å­˜åœ¨çš„ã€‚</strong></p><p>æ‰€ä»¥è§£å†³æ–¹æ³•æ˜¯ï¼Ÿ <strong>å¥å­çš„å¯èƒ½æ€§æ— é™ï¼Œä½†æ˜¯è¯è¯­å°±é‚£ä¹ˆäº›ï¼ï¼</strong>æ±‰è¯­å¸¸ç”¨å­—2500ä¸ªï¼Œå¸¸ç”¨è¯è¯­ä¹Ÿå°±56000ä¸ª(ä½ ç»ˆäºæ˜ç™½å°å­¦è¯­æ–‡è€å¸ˆçš„ç”¨å¿ƒè‰¯è‹¦äº†)ã€‚æŒ‰äººä»¬çš„ç»éªŒç†è§£ï¼Œä¸¤å¥è¯æ„æ€ç›¸è¿‘å¹¶ä¸å¼ºæ±‚éå¾—æ¯ä¸ªå­—ã€è¯è¯­éƒ½ä¸€æ ·ã€‚æ¯”å¦‚<strong>â€œæˆ‘å¸å¯åŠç†æ­£è§„å‘ç¥¨ï¼Œ17%å¢å€¼ç¨å‘ç¥¨ç‚¹æ•°ä¼˜æƒ ï¼â€</strong>ï¼Œè¿™å¥è¯å°±æ¯”ä¹‹å‰é‚£å¥è¯å°‘äº†<strong>â€œï¼ˆä¿çœŸï¼‰â€</strong>è¿™ä¸ªè¯ï¼Œä½†æ˜¯æ„æ€åŸºæœ¬ä¸€æ ·ã€‚å¦‚æœæŠŠè¿™äº›æƒ…å†µä¹Ÿè€ƒè™‘è¿›æ¥ï¼Œé‚£æ ·æœ¬æ•°é‡å°±ä¼šå¢åŠ ï¼Œè¿™å°±æ–¹ä¾¿æˆ‘ä»¬è®¡ç®—äº†ã€‚</p><p>äºæ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥ä¸æ‹¿å¥å­ä½œä¸ºç‰¹å¾ï¼Œè€Œæ˜¯æ‹¿å¥å­é‡Œé¢çš„è¯è¯­ï¼ˆç»„åˆï¼‰ä½œä¸ºç‰¹å¾å»è€ƒè™‘ã€‚æ¯”å¦‚<strong>â€œæ­£è§„å‘ç¥¨â€</strong>å¯ä»¥ä½œä¸ºä¸€ä¸ªå•ç‹¬çš„è¯è¯­ï¼Œ<strong>â€œå¢å€¼ç¨â€</strong>ä¹Ÿå¯ä»¥ä½œä¸ºä¸€ä¸ªå•ç‹¬çš„è¯è¯­ç­‰ç­‰ã€‚</p><blockquote><p>å¥å­<strong>â€œæˆ‘å¸å¯åŠç†æ­£è§„å‘ç¥¨ï¼Œ17%å¢å€¼ç¨å‘ç¥¨ç‚¹æ•°ä¼˜æƒ ï¼â€å°±å¯ä»¥å˜æˆï¼ˆâ€œæˆ‘â€,â€œå¸â€,â€œå¯â€,â€œåŠç†â€,â€œæ­£è§„å‘ç¥¨â€,â€œä¿çœŸâ€,â€œå¢å€¼ç¨â€,â€œå‘ç¥¨â€,â€œç‚¹æ•°â€,â€œä¼˜æƒ â€)ï¼‰</strong>ã€‚</p></blockquote><p>äºæ˜¯ä½ æ¥è§¦åˆ°äº†ä¸­æ–‡NLPä¸­ï¼Œæœ€æœ€æœ€é‡è¦çš„æŠ€æœ¯ä¹‹ä¸€ï¼š<strong>åˆ†è¯</strong>ï¼ï¼ï¼ä¹Ÿå°±æ˜¯<strong>æŠŠä¸€æ•´å¥è¯æ‹†åˆ†æˆæ›´ç»†ç²’åº¦çš„è¯è¯­æ¥è¿›è¡Œè¡¨ç¤º</strong>ã€‚å¦å¤–ï¼Œåˆ†è¯ä¹‹å<strong>å»é™¤æ ‡ç‚¹ç¬¦å·ã€æ•°å­—ç”šè‡³æ— å…³æˆåˆ†(åœç”¨è¯)æ˜¯ç‰¹å¾é¢„å¤„ç†ä¸­çš„ä¸€é¡¹æŠ€æœ¯</strong>ã€‚</p><p><strong>ä¸­æ–‡åˆ†è¯æ˜¯ä¸€ä¸ªä¸“é—¨çš„æŠ€æœ¯é¢†åŸŸ(æˆ‘ä¸ä¼šå‘Šè¯‰ä½ æŸæœç´¢å¼•æ“å‚ç ç –å·¥æœ‰ä¸“é—¨åšåˆ†è¯çš„ï¼ï¼ï¼)ï¼Œä¸Šè¿‡ä¹‹å‰è¯¾ç¨‹çš„åŒå­¦éƒ½çŸ¥é“pythonæœ‰ä¸€ä¸ªéå¸¸æ–¹ä¾¿çš„åˆ†è¯å·¥å…·jiebaï¼Œå‡å®šæˆ‘ä»¬å·²ç»å®Œæˆåˆ†è¯å·¥ä½œï¼š</strong></p><p>æˆ‘ä»¬è§‚å¯Ÿï¼ˆâ€œæˆ‘â€,â€œå¸â€,â€œå¯â€,â€œåŠç†â€,â€œæ­£è§„å‘ç¥¨â€,â€œä¿çœŸâ€,â€œå¢å€¼ç¨â€,â€œå‘ç¥¨â€,â€œç‚¹æ•°â€,â€œä¼˜æƒ â€)ï¼Œ<strong>è¿™å¯ä»¥ç†è§£æˆä¸€ä¸ªå‘é‡ï¼šå‘é‡çš„æ¯ä¸€ç»´åº¦éƒ½è¡¨ç¤ºç€è¯¥ç‰¹å¾è¯åœ¨æ–‡æœ¬ä¸­çš„ç‰¹å®šä½ç½®å­˜åœ¨ã€‚è¿™ç§å°†ç‰¹å¾æ‹†åˆ†æˆæ›´å°çš„å•å…ƒï¼Œä¾æ®è¿™äº›æ›´çµæ´»ã€æ›´ç»†ç²’åº¦çš„ç‰¹å¾è¿›è¡Œåˆ¤æ–­çš„æ€ç»´æ–¹å¼ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸æœºå™¨å­¦ä¹ ä¸­éƒ½æ˜¯éå¸¸å¸¸è§åˆæœ‰æ•ˆçš„ã€‚</strong></p><p>å› æ­¤è´å¶æ–¯å…¬å¼å°±å˜æˆäº†ï¼š</p><blockquote><p>P(â€œåƒåœ¾é‚®ä»¶â€|ï¼ˆâ€œæˆ‘â€,â€œå¸â€,â€œå¯â€,â€œåŠç†â€,â€œæ­£è§„å‘ç¥¨â€,â€œä¿çœŸâ€,â€œå¢å€¼ç¨â€,â€œå‘ç¥¨â€,â€œç‚¹æ•°â€,â€œä¼˜æƒ â€)ï¼‰ =P(ï¼ˆâ€œæˆ‘â€,â€œå¸â€,â€œå¯â€,â€œåŠç†â€,â€œæ­£è§„å‘ç¥¨â€,â€œä¿çœŸâ€,â€œå¢å€¼ç¨â€,â€œå‘ç¥¨â€,â€œç‚¹æ•°â€,â€œä¼˜æƒ â€)|â€åƒåœ¾é‚®ä»¶â€ï¼‰P(â€œåƒåœ¾é‚®ä»¶â€)P(ï¼ˆâ€œæˆ‘â€,â€œå¸â€,â€œå¯â€,â€œåŠç†â€,â€œæ­£è§„å‘ç¥¨â€,â€œä¿çœŸâ€,â€œå¢å€¼ç¨â€,â€œå‘ç¥¨â€,â€œç‚¹æ•°â€,â€œä¼˜æƒ â€))</p><p>P(â€œæ­£å¸¸é‚®ä»¶â€|ï¼ˆâ€œæˆ‘â€,â€œå¸â€,â€œå¯â€,â€œåŠç†â€,â€œæ­£è§„å‘ç¥¨â€,â€œä¿çœŸâ€,â€œå¢å€¼ç¨â€,â€œå‘ç¥¨â€,â€œç‚¹æ•°â€,â€œä¼˜æƒ â€)ï¼‰ =P(ï¼ˆâ€œæˆ‘â€,â€œå¸â€,â€œå¯â€,â€œåŠç†â€,â€œæ­£è§„å‘ç¥¨â€,â€œä¿çœŸâ€,â€œå¢å€¼ç¨â€,â€œå‘ç¥¨â€,â€œç‚¹æ•°â€,â€œä¼˜æƒ â€)|â€æ­£å¸¸é‚®ä»¶â€ï¼‰P(â€œæ­£å¸¸é‚®ä»¶â€)P(ï¼ˆâ€œæˆ‘â€,â€œå¸â€,â€œå¯â€,â€œåŠç†â€,â€œæ­£è§„å‘ç¥¨â€,â€œä¿çœŸâ€,â€œå¢å€¼ç¨â€,â€œå‘ç¥¨â€,â€œç‚¹æ•°â€,â€œä¼˜æƒ â€))</p></blockquote><h2 id="6-æ¡ä»¶ç‹¬ç«‹å‡è®¾"><a href="#6-æ¡ä»¶ç‹¬ç«‹å‡è®¾" class="headerlink" title="6. æ¡ä»¶ç‹¬ç«‹å‡è®¾"></a>6. æ¡ä»¶ç‹¬ç«‹å‡è®¾</h2><p>ä¸‹é¢æˆ‘ä»¬é©¬ä¸Šä¼šçœ‹åˆ°ä¸€ä¸ªéå¸¸ç®€å•ç²—æš´çš„å‡è®¾ã€‚</p><p>æ¦‚ç‡P(ï¼ˆâ€œæˆ‘â€,â€œå¸â€,â€œå¯â€,â€œåŠç†â€,â€œæ­£è§„å‘ç¥¨â€,â€œä¿çœŸâ€,â€œå¢å€¼ç¨â€,â€œå‘ç¥¨â€,â€œç‚¹æ•°â€,â€œä¼˜æƒ â€)|â€åƒåœ¾é‚®ä»¶â€ï¼‰ä¾æ—§ä¸å¤Ÿå¥½æ±‚ï¼Œæˆ‘ä»¬å¼•è¿›ä¸€ä¸ª<strong>å¾ˆæœ´ç´ çš„è¿‘ä¼¼</strong>ã€‚ä¸ºäº†è®©å…¬å¼æ˜¾å¾—æ›´åŠ ç´§å‡‘ï¼Œæˆ‘ä»¬ä»¤å­—æ¯Sè¡¨ç¤ºâ€œåƒåœ¾é‚®ä»¶â€,ä»¤å­—æ¯Hè¡¨ç¤ºâ€œæ­£å¸¸é‚®ä»¶â€ã€‚è¿‘ä¼¼å…¬å¼å¦‚ä¸‹ï¼š</p><blockquote><p>P(ï¼ˆâ€œæˆ‘â€,â€œå¸â€,â€œå¯â€,â€œåŠç†â€,â€œæ­£è§„å‘ç¥¨â€,â€œä¿çœŸâ€,â€œå¢å€¼ç¨â€,â€œå‘ç¥¨â€,â€œç‚¹æ•°â€,â€œä¼˜æƒ â€)|Sï¼‰<br>=P(â€œæˆ‘â€|Sï¼‰Ã—P(â€œå¸â€|Sï¼‰Ã—P(â€œå¯â€|Sï¼‰Ã—P(â€œåŠç†â€|Sï¼‰Ã—P(â€œæ­£è§„å‘ç¥¨â€|Sï¼‰ Ã—P(â€œä¿çœŸâ€|Sï¼‰Ã—P(â€œå¢å€¼ç¨â€|Sï¼‰Ã—P(â€œå‘ç¥¨â€|Sï¼‰Ã—P(â€œç‚¹æ•°â€|Sï¼‰Ã—P(â€œä¼˜æƒ â€|S)</p></blockquote><p>è¿™å°±æ˜¯ä¼ è¯´ä¸­çš„<strong>æ¡ä»¶ç‹¬ç«‹å‡è®¾</strong>ã€‚åŸºäºâ€œæ­£å¸¸é‚®ä»¶â€çš„æ¡ä»¶ç‹¬ç«‹å‡è®¾çš„å¼å­ä¸ä¸Šå¼ç±»ä¼¼ï¼Œæ­¤å¤„çœå»ã€‚æ¥ç€ï¼Œå°†æ¡ä»¶ç‹¬ç«‹å‡è®¾ä»£å…¥ä¸Šé¢ä¸¤ä¸ªç›¸åäº‹ä»¶çš„è´å¶æ–¯å…¬å¼ã€‚</p><p>äºæ˜¯æˆ‘ä»¬å°±åªéœ€è¦æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªå¼å­çš„å¤§å°ï¼š</p><blockquote><p>C=P(â€œæˆ‘â€|S)P(â€œå¸â€|S)P(â€œå¯â€|S)P(â€œåŠç†â€|S)P(â€œæ­£è§„å‘ç¥¨â€|S) Ã—P(â€œä¿çœŸâ€|S)P(â€œå¢å€¼ç¨â€|S)P(â€œå‘ç¥¨â€|S)P(â€œç‚¹æ•°â€|S)P(â€œä¼˜æƒ â€|S)P(â€œåƒåœ¾é‚®ä»¶â€) Câ¯â¯â¯â¯=P(â€œæˆ‘â€|H)P(â€œå¸â€|H)P(â€œå¯â€|H)P(â€œåŠç†â€|H)P(â€œæ­£è§„å‘ç¥¨â€|H) Ã—P(â€œä¿çœŸâ€|H)P(â€œå¢å€¼ç¨â€|H)P(â€œå‘ç¥¨â€|H)P(â€œç‚¹æ•°â€|H)P(â€œä¼˜æƒ â€|H)P(â€œæ­£å¸¸é‚®ä»¶â€)</p></blockquote><p>å‰(wo)å®³(cao)ï¼é…±ç´«å¤„ç†å<strong>å¼å­ä¸­çš„æ¯ä¸€é¡¹éƒ½ç‰¹åˆ«å¥½æ±‚</strong>ï¼åªéœ€è¦<strong>åˆ†åˆ«ç»Ÿè®¡å„ç±»é‚®ä»¶ä¸­è¯¥å…³é”®è¯å‡ºç°çš„æ¦‚ç‡</strong>å°±å¯ä»¥äº†ï¼ï¼ï¼æ¯”å¦‚ï¼š</p><blockquote><p>P(â€œå‘ç¥¨â€|Sï¼‰=åƒåœ¾é‚®ä»¶ä¸­æ‰€æœ‰â€œå‘ç¥¨â€çš„æ¬¡æ•°åƒåœ¾é‚®ä»¶ä¸­æ‰€æœ‰è¯è¯­çš„æ¬¡æ•°</p></blockquote><p>ç»Ÿè®¡æ¬¡æ•°éå¸¸æ–¹ä¾¿ï¼Œè€Œä¸”æ ·æœ¬æ•°é‡è¶³å¤Ÿå¤§ï¼Œç®—å‡ºæ¥çš„æ¦‚ç‡æ¯”è¾ƒæ¥è¿‘çœŸå®ã€‚äºæ˜¯åƒåœ¾é‚®ä»¶è¯†åˆ«çš„é—®é¢˜å°±å¯è§£äº†ã€‚</p><h2 id="7-æœ´ç´ è´å¶æ–¯-Naive-Bayes-ï¼Œâ€œNaiveâ€åœ¨ä½•å¤„ï¼Ÿ"><a href="#7-æœ´ç´ è´å¶æ–¯-Naive-Bayes-ï¼Œâ€œNaiveâ€åœ¨ä½•å¤„ï¼Ÿ" class="headerlink" title="7. æœ´ç´ è´å¶æ–¯(Naive Bayes)ï¼Œâ€œNaiveâ€åœ¨ä½•å¤„ï¼Ÿ"></a>7. æœ´ç´ è´å¶æ–¯(Naive Bayes)ï¼Œâ€œNaiveâ€åœ¨ä½•å¤„ï¼Ÿ</h2><p><strong>åŠ ä¸Šæ¡ä»¶ç‹¬ç«‹å‡è®¾çš„è´å¶æ–¯æ–¹æ³•å°±æ˜¯æœ´ç´ è´å¶æ–¯æ–¹æ³•ï¼ˆNaive Bayesï¼‰ã€‚</strong> Naiveçš„å‘éŸ³æ˜¯â€œä¹ƒä¸€æ±¡â€ï¼Œæ„æ€æ˜¯â€œæœ´ç´ çš„â€ã€â€œå¹¼ç¨šçš„â€ã€<strong>â€œè ¢è ¢çš„â€</strong>ã€‚å’³å’³ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¤§ç¥ä»¬å–åè¯´è¯¥æ–¹æ³•æ˜¯ä¸€ç§æ¯”è¾ƒèŒè ¢çš„æ–¹æ³•ï¼Œä¸ºå•¥ï¼Ÿ</p><p>å°†å¥å­ï¼ˆâ€œæˆ‘â€,â€œå¸â€,â€œå¯â€,â€œåŠç†â€,â€œæ­£è§„å‘ç¥¨â€) ä¸­çš„ ï¼ˆâ€œæˆ‘â€,â€œå¸â€ï¼‰ä¸ï¼ˆâ€œæ­£è§„å‘ç¥¨â€ï¼‰è°ƒæ¢ä¸€ä¸‹é¡ºåºï¼Œå°±å˜æˆäº†ä¸€ä¸ªæ–°çš„å¥å­ï¼ˆâ€œæ­£è§„å‘ç¥¨â€,â€œå¯â€,â€œåŠç†â€, â€œæˆ‘â€, â€œå¸â€)ã€‚æ–°å¥å­ä¸æ—§å¥å­çš„æ„æ€å®Œå…¨ä¸åŒã€‚<strong>ä½†ç”±äºä¹˜æ³•äº¤æ¢å¾‹ï¼Œæœ´ç´ è´å¶æ–¯æ–¹æ³•ä¸­ç®—å‡ºæ¥äºŒè€…çš„æ¡ä»¶æ¦‚ç‡å®Œå…¨ä¸€æ ·ï¼</strong>è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š</p><blockquote><p>P(ï¼ˆâ€œæˆ‘â€,â€œå¸â€,â€œå¯â€,â€œåŠç†â€,â€œæ­£è§„å‘ç¥¨â€)|S) =P(â€œæˆ‘â€|S)P(â€œå¸â€|S)P(â€œå¯â€|S)P(â€œåŠç†â€|S)P(â€œæ­£è§„å‘ç¥¨â€|S) =P(â€œæ­£è§„å‘ç¥¨â€|S)P(â€œå¯â€|S)P(â€œåŠç†â€|S)P(â€œæˆ‘â€|S)P(â€œå¸â€|Sï¼‰ =P(ï¼ˆâ€œæ­£è§„å‘ç¥¨â€,â€œå¯â€,â€œåŠç†â€,â€œæˆ‘â€,â€œå¸â€)|S)</p></blockquote><p><strong>ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨æœ´ç´ è´å¶æ–¯çœ¼é‡Œï¼Œâ€œæˆ‘å¸å¯åŠç†æ­£è§„å‘ç¥¨â€ä¸â€œæ­£è§„å‘ç¥¨å¯åŠç†æˆ‘å¸â€å®Œå…¨ç›¸åŒã€‚æœ´ç´ è´å¶æ–¯å¤±å»äº†è¯è¯­ä¹‹é—´çš„é¡ºåºä¿¡æ¯ã€‚</strong>è¿™å°±ç›¸å½“äºæŠŠæ‰€æœ‰çš„è¯æ±‡æ‰”è¿›åˆ°ä¸€ä¸ªè¢‹å­é‡Œéšä¾¿æ…å’Œï¼Œè´å¶æ–¯éƒ½è®¤ä¸ºå®ƒä»¬ä¸€æ ·ã€‚å› æ­¤è¿™ç§æƒ…å†µä¹Ÿç§°ä½œ<strong>è¯è¢‹å­æ¨¡å‹(bag of words)</strong>ã€‚</p><p><img src="blob:file:///f3e451e8-1f8f-4c4c-b15f-25793dff88ca" alt="è¯è¢‹å­é…å›¾"></p><p>è¯è¢‹å­æ¨¡å‹ä¸äººä»¬çš„æ—¥å¸¸ç»éªŒå®Œå…¨ä¸åŒã€‚æ¯”å¦‚ï¼Œåœ¨æ¡ä»¶ç‹¬ç«‹å‡è®¾çš„æƒ…å†µä¸‹ï¼Œ<strong>â€œæ­¦æ¾æ‰“æ­»äº†è€è™â€ä¸â€œè€è™æ‰“æ­»äº†æ­¦æ¾â€è¢«å®ƒè®¤ä½œä¸€ä¸ªæ„æ€äº†ã€‚</strong>æ©ï¼Œæœ´ç´ è´å¶æ–¯å°±æ˜¯è¿™ä¹ˆå•çº¯å’Œç›´æ¥ï¼Œå¯¹æ¯”äºå…¶ä»–åˆ†ç±»å™¨ï¼Œå¥½åƒæ˜¯æ˜¾å¾—æœ‰é‚£ä¹ˆç‚¹èŒè ¢ã€‚</p><h2 id="8-ç®€å•é«˜æ•ˆï¼ŒåŠä¸é€†è¢­"><a href="#8-ç®€å•é«˜æ•ˆï¼ŒåŠä¸é€†è¢­" class="headerlink" title="8. ç®€å•é«˜æ•ˆï¼ŒåŠä¸é€†è¢­"></a>8. ç®€å•é«˜æ•ˆï¼ŒåŠä¸é€†è¢­</h2><p>è™½ç„¶è¯´æœ´ç´ è´å¶æ–¯æ–¹æ³•èŒè ¢èŒè ¢çš„ï¼Œä½†å®è·µè¯æ˜åœ¨åƒåœ¾é‚®ä»¶è¯†åˆ«çš„åº”ç”¨è¿˜<strong>ä»¤äººè¯§å¼‚åœ°å¥½</strong>ã€‚Paul Grahamå…ˆç”Ÿè‡ªå·±ç®€å•åšäº†ä¸€ä¸ªæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ï¼Œ<strong>â€œ1000å°åƒåœ¾é‚®ä»¶èƒ½å¤Ÿè¢«è¿‡æ»¤æ‰995å°ï¼Œå¹¶ä¸”æ²¡æœ‰ä¸€ä¸ªè¯¯åˆ¤â€ã€‚</strong>ï¼ˆPaul Grahamã€Šé»‘å®¢ä¸ç”»å®¶ã€‹ï¼‰</p><p>é‚£ä¸ªâ€¦æ•ˆæœä¸ºå•¥å¥½å‘¢ï¼Ÿ</p><p>â€œæœ‰äººå¯¹æ­¤æå‡ºäº†ä¸€ä¸ªç†è®ºè§£é‡Šï¼Œå¹¶ä¸”å»ºç«‹äº†ä»€ä¹ˆæ—¶å€™æœ´ç´ è´å¶æ–¯çš„æ•ˆæœèƒ½å¤Ÿç­‰ä»·äºéæœ´ç´ è´å¶æ–¯çš„å……è¦æ¡ä»¶ï¼Œè¿™ä¸ªè§£é‡Šçš„æ ¸å¿ƒå°±æ˜¯ï¼šæœ‰äº›ç‹¬ç«‹å‡è®¾åœ¨å„ä¸ªåˆ†ç±»ä¹‹é—´çš„åˆ†å¸ƒéƒ½æ˜¯å‡åŒ€çš„æ‰€ä»¥å¯¹äºä¼¼ç„¶çš„ç›¸å¯¹å¤§å°ä¸äº§ç”Ÿå½±å“ï¼›å³ä¾¿ä¸æ˜¯å¦‚æ­¤ï¼Œä¹Ÿæœ‰å¾ˆå¤§çš„å¯èƒ½æ€§<strong>å„ä¸ªç‹¬ç«‹å‡è®¾æ‰€äº§ç”Ÿçš„æ¶ˆæå½±å“æˆ–ç§¯æå½±å“äº’ç›¸æŠµæ¶ˆï¼Œæœ€ç»ˆå¯¼è‡´ç»“æœå—åˆ°çš„å½±å“ä¸å¤§</strong>ã€‚å…·ä½“çš„æ•°å­¦å…¬å¼è¯·å‚è€ƒ<a href="http://www.cs.unb.ca/profs/hzhang/publications/FLAIRS04ZhangH.pdf" target="_blank" rel="noopener">è¿™ç¯‡ paper</a>ã€‚â€ï¼ˆåˆ˜æœªé¹ã€Šï¼šå¹³å‡¡è€Œåˆç¥å¥‡çš„è´å¶æ–¯æ–¹æ³•ã€‹ï¼‰</p><p>æ©ï¼Œè¿™ä¸ªåˆ†ç±»å™¨ä¸­æœ€ç®€å•ç›´æ¥çœ‹ä¼¼èŒè ¢çš„å°ç›†å‹ã€æœ´ç´ è´å¶æ–¯ã€ï¼Œå®é™…ä¸Šå´æ˜¯<strong>ç®€å•ã€å®ç”¨ã€ä¸”å¼ºå¤§</strong>çš„ã€‚</p><h2 id="9-å¤„ç†é‡å¤è¯è¯­çš„ä¸‰ç§æ–¹å¼"><a href="#9-å¤„ç†é‡å¤è¯è¯­çš„ä¸‰ç§æ–¹å¼" class="headerlink" title="9. å¤„ç†é‡å¤è¯è¯­çš„ä¸‰ç§æ–¹å¼"></a>9. å¤„ç†é‡å¤è¯è¯­çš„ä¸‰ç§æ–¹å¼</h2><p>æˆ‘ä»¬<strong>ä¹‹å‰çš„åƒåœ¾é‚®ä»¶å‘é‡ï¼ˆâ€œæˆ‘â€,â€œå¸â€,â€œå¯â€,â€œåŠç†â€,â€œæ­£è§„å‘ç¥¨â€,â€œä¿çœŸâ€,â€œå¢å€¼ç¨â€,â€œå‘ç¥¨â€,â€œç‚¹æ•°â€,â€œä¼˜æƒ â€)ï¼Œå…¶ä¸­æ¯ä¸ªè¯éƒ½ä¸é‡å¤ã€‚</strong>è€Œè¿™åœ¨ç°å®ä¸­å…¶å®å¾ˆå°‘è§ã€‚å› ä¸ºå¦‚æœæ–‡æœ¬é•¿åº¦å¢åŠ ï¼Œæˆ–è€…åˆ†è¯æ–¹æ³•æ”¹å˜ï¼Œ<strong>å¿…ç„¶ä¼šæœ‰è®¸å¤šè¯é‡å¤å‡ºç°</strong>ï¼Œå› æ­¤éœ€è¦å¯¹è¿™ç§æƒ…å†µè¿›è¡Œè¿›ä¸€æ­¥æ¢è®¨ã€‚æ¯”å¦‚ä»¥ä¸‹è¿™æ®µé‚®ä»¶ï¼š</p><blockquote><p>â€œä»£å¼€å‘ç¥¨ã€‚å¢å€¼ç¨å‘ç¥¨ï¼Œæ­£è§„å‘ç¥¨ã€‚â€ åˆ†è¯åä¸ºå‘é‡ï¼š ï¼ˆâ€œä»£å¼€â€,â€œå‘ç¥¨â€,â€œå¢å€¼ç¨â€,â€œå‘ç¥¨â€,â€œæ­£è§„â€,â€œå‘ç¥¨â€ï¼‰</p></blockquote><p>å…¶ä¸­â€œå‘ç¥¨â€é‡å¤äº†ä¸‰æ¬¡ã€‚</p><h3 id="9-1-å¤šé¡¹å¼æ¨¡å‹ï¼š"><a href="#9-1-å¤šé¡¹å¼æ¨¡å‹ï¼š" class="headerlink" title="9.1 å¤šé¡¹å¼æ¨¡å‹ï¼š"></a>9.1 å¤šé¡¹å¼æ¨¡å‹ï¼š</h3><p>å¦‚æœæˆ‘ä»¬è€ƒè™‘é‡å¤è¯è¯­çš„æƒ…å†µï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œ<strong>é‡å¤çš„è¯è¯­æˆ‘ä»¬è§†ä¸ºå…¶å‡ºç°å¤šæ¬¡</strong>ï¼Œç›´æ¥æŒ‰æ¡ä»¶ç‹¬ç«‹å‡è®¾çš„æ–¹å¼æ¨å¯¼ï¼Œåˆ™æœ‰</p><blockquote><p>P(ï¼ˆâ€œä»£å¼€â€,â€œå‘ç¥¨â€,â€œå¢å€¼ç¨â€,â€œå‘ç¥¨â€,â€œæ­£è§„â€,â€œå‘ç¥¨â€)|Sï¼‰ =P(â€œä»£å¼€â€â€|S)P(â€œå‘ç¥¨â€|S)P(â€œå¢å€¼ç¨â€|S)P(â€œå‘ç¥¨â€|S)P(â€œæ­£è§„â€|S)P(â€œå‘ç¥¨â€|Sï¼‰=P(â€œä»£å¼€â€â€|S)P3(â€œå‘ç¥¨â€|S)P(â€œå¢å€¼ç¨â€|S)P(â€œæ­£è§„â€|S) <strong>æ³¨æ„è¿™ä¸€é¡¹</strong>:P3(â€œå‘ç¥¨â€|Sï¼‰ã€‚</p></blockquote><p>åœ¨ç»Ÿè®¡è®¡ç®—P(â€œå‘ç¥¨â€|Sï¼‰æ—¶ï¼Œæ¯ä¸ªè¢«ç»Ÿè®¡çš„åƒåœ¾é‚®ä»¶æ ·æœ¬ä¸­é‡å¤çš„è¯è¯­ä¹Ÿç»Ÿè®¡å¤šæ¬¡ã€‚</p><blockquote><p>P(â€œå‘ç¥¨â€|Sï¼‰=æ¯å°åƒåœ¾é‚®ä»¶ä¸­å‡ºç°â€œå‘ç¥¨â€çš„æ¬¡æ•°çš„æ€»å’Œæ¯å°åƒåœ¾é‚®ä»¶ä¸­æ‰€æœ‰è¯å‡ºç°æ¬¡æ•°ï¼ˆè®¡ç®—é‡å¤æ¬¡æ•°ï¼‰çš„æ€»å’Œ</p></blockquote><p>ä½ çœ‹è¿™ä¸ªå¤šæ¬¡å‡ºç°çš„ç»“æœï¼Œå‡ºç°åœ¨æ¦‚ç‡çš„æŒ‡æ•°/æ¬¡æ–¹ä¸Šï¼Œå› æ­¤è¿™æ ·çš„æ¨¡å‹å«ä½œ<strong>å¤šé¡¹å¼æ¨¡å‹</strong>ã€‚</p><h3 id="9-2-ä¼¯åŠªåˆ©æ¨¡å‹"><a href="#9-2-ä¼¯åŠªåˆ©æ¨¡å‹" class="headerlink" title="9.2 ä¼¯åŠªåˆ©æ¨¡å‹"></a>9.2 ä¼¯åŠªåˆ©æ¨¡å‹</h3><p>å¦ä¸€ç§æ›´åŠ ç®€åŒ–çš„æ–¹æ³•æ˜¯<strong>å°†é‡å¤çš„è¯è¯­éƒ½è§†ä¸ºå…¶åªå‡ºç°1æ¬¡</strong>ï¼Œ</p><blockquote><p>P(ï¼ˆâ€œä»£å¼€â€,â€œå‘ç¥¨â€,â€œå¢å€¼ç¨â€,â€œå‘ç¥¨â€,â€œæ­£è§„â€,â€œå‘ç¥¨â€)|Sï¼‰ =P(â€œå‘ç¥¨â€|S)P(â€œä»£å¼€â€â€|S)P(â€œå¢å€¼ç¨â€|S)P(â€œæ­£è§„â€|Sï¼‰</p></blockquote><p>ç»Ÿè®¡è®¡ç®—P(â€œè¯è¯­â€|Sï¼‰æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p><blockquote><p>P(â€œå‘ç¥¨â€|Sï¼‰=å‡ºç°â€œå‘ç¥¨â€çš„åƒåœ¾é‚®ä»¶çš„å°æ•°æ¯å°åƒåœ¾é‚®ä»¶ä¸­æ‰€æœ‰è¯å‡ºç°æ¬¡æ•°ï¼ˆå‡ºç°äº†åªè®¡ç®—ä¸€æ¬¡ï¼‰çš„æ€»å’Œ</p></blockquote><p>è¿™æ ·çš„æ¨¡å‹å«ä½œ<strong>ä¼¯åŠªåˆ©æ¨¡å‹</strong>ï¼ˆåˆç§°ä¸º<strong>äºŒé¡¹ç‹¬ç«‹æ¨¡å‹</strong>ï¼‰ã€‚è¿™ç§æ–¹å¼æ›´åŠ ç®€åŒ–ä¸æ–¹ä¾¿ã€‚å½“ç„¶å®ƒä¸¢å¤±äº†è¯é¢‘çš„ä¿¡æ¯ï¼Œå› æ­¤æ•ˆæœå¯èƒ½ä¼šå·®ä¸€äº›ã€‚</p><h3 id="9-3-æ··åˆæ¨¡å‹"><a href="#9-3-æ··åˆæ¨¡å‹" class="headerlink" title="9.3 æ··åˆæ¨¡å‹"></a>9.3 æ··åˆæ¨¡å‹</h3><p>ç¬¬ä¸‰ç§æ–¹å¼æ˜¯åœ¨è®¡ç®—å¥å­æ¦‚ç‡æ—¶ï¼Œä¸è€ƒè™‘é‡å¤è¯è¯­å‡ºç°çš„æ¬¡æ•°ï¼Œä½†æ˜¯åœ¨ç»Ÿè®¡è®¡ç®—è¯è¯­çš„æ¦‚ç‡P(â€œè¯è¯­â€|Sï¼‰æ—¶ï¼Œå´è€ƒè™‘é‡å¤è¯è¯­çš„å‡ºç°æ¬¡æ•°ï¼Œè¿™æ ·çš„æ¨¡å‹å¯ä»¥å«ä½œ<strong>æ··åˆæ¨¡å‹</strong>ã€‚</p><p>æˆ‘ä»¬é€šè¿‡ä¸‹å›¾å±•ç¤ºä¸‰ç§æ¨¡å‹çš„å…³ç³»ã€‚</p><p><img src="blob:file:///157f8870-f4d9-4b18-9922-0c1e7a18074b" alt="ä¸‰ç§å½¢æ€"></p><p>å…·ä½“å®è·µä¸­é‡‡ç”¨é‚£ç§æ¨¡å‹ï¼Œå…³é”®çœ‹å…·ä½“çš„ä¸šåŠ¡åœºæ™¯ï¼Œä¸€ä¸ªç®€å•ç»éªŒæ˜¯ï¼Œ<strong>å¯¹äºåƒåœ¾é‚®ä»¶è¯†åˆ«ï¼Œæ··åˆæ¨¡å‹æ›´å¥½äº›</strong>ã€‚</p><h2 id="10-å»é™¤åœç”¨è¯ä¸é€‰æ‹©å…³é”®è¯"><a href="#10-å»é™¤åœç”¨è¯ä¸é€‰æ‹©å…³é”®è¯" class="headerlink" title="10. å»é™¤åœç”¨è¯ä¸é€‰æ‹©å…³é”®è¯"></a>10. å»é™¤åœç”¨è¯ä¸é€‰æ‹©å…³é”®è¯</h2><p>æˆ‘ä»¬ç»§ç»­è§‚å¯Ÿ<strong>ï¼ˆâ€œæˆ‘â€,â€œå¸â€,â€œå¯â€,â€œåŠç†â€,â€œæ­£è§„å‘ç¥¨â€,â€œä¿çœŸâ€,â€œå¢å€¼ç¨â€,â€œå‘ç¥¨â€,â€œç‚¹æ•°â€,â€œä¼˜æƒ â€)</strong> è¿™å¥è¯ã€‚å…¶å®ï¼Œåƒ<strong>â€œæˆ‘â€ã€â€œå¯â€</strong>ä¹‹ç±»è¯å…¶å®éå¸¸ä¸­æ€§ï¼Œæ— è®ºå…¶æ˜¯å¦å‡ºç°åœ¨åƒåœ¾é‚®ä»¶ä¸­éƒ½æ— æ³•å¸®åŠ©åˆ¤æ–­çš„æœ‰ç”¨ä¿¡æ¯ã€‚æ‰€ä»¥å¯ä»¥ç›´æ¥ä¸è€ƒè™‘è¿™äº›å…¸å‹çš„è¯ã€‚è¿™äº›æ— åŠ©äºæˆ‘ä»¬åˆ†ç±»çš„è¯è¯­å«ä½œ<strong>â€œåœç”¨è¯â€ï¼ˆStop Wordsï¼‰</strong>ã€‚è¿™æ ·å¯ä»¥<strong>å‡å°‘æˆ‘ä»¬è®­ç»ƒæ¨¡å‹ã€åˆ¤æ–­åˆ†ç±»çš„æ—¶é—´</strong>ã€‚ äºæ˜¯ä¹‹å‰çš„å¥å­å°±å˜æˆäº†<strong>ï¼ˆâ€œå¸â€,â€œåŠç†â€,â€œæ­£è§„å‘ç¥¨â€,â€œä¿çœŸâ€,â€œå¢å€¼ç¨â€,â€œå‘ç¥¨â€,â€œç‚¹æ•°â€,â€œä¼˜æƒ â€)</strong> ã€‚</p><p>æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æã€‚ä»¥äººç±»çš„ç»éªŒï¼Œå…¶å®<strong>â€œæ­£è§„å‘ç¥¨â€ã€â€œå‘ç¥¨â€</strong>è¿™ç±»çš„è¯å¦‚æœå‡ºç°çš„è¯ï¼Œé‚®ä»¶ä½œä¸ºåƒåœ¾é‚®ä»¶çš„æ¦‚ç‡éå¸¸å¤§ï¼Œå¯ä»¥ä½œä¸ºæˆ‘ä»¬åŒºåˆ†åƒåœ¾é‚®ä»¶çš„<strong>â€œå…³é”®è¯â€</strong>ã€‚è€Œåƒ<strong>â€œå¸â€ã€â€œåŠç†â€ã€â€œä¼˜æƒ â€</strong>è¿™ç±»çš„è¯åˆ™æœ‰ç‚¹é¸¡è‚‹ï¼Œå¯èƒ½æœ‰åŠ©äºåˆ†ç±»ï¼Œä½†åˆä¸é‚£ä¹ˆå¼ºçƒˆã€‚å¦‚æœæƒ³çœäº‹åšä¸ªç®€å•çš„åˆ†ç±»å™¨çš„è¯ï¼Œåˆ™å¯ä»¥ç›´æ¥é‡‡ç”¨â€œå…³é”®è¯â€è¿›è¡Œç»Ÿè®¡ä¸åˆ¤æ–­ï¼Œå‰©ä¸‹çš„è¯å°±å¯ä»¥å…ˆä¸ç®¡äº†ã€‚äºæ˜¯ä¹‹å‰çš„åƒåœ¾é‚®ä»¶å¥å­å°±å˜æˆäº†<strong>ï¼ˆâ€œæ­£è§„å‘ç¥¨â€,â€œå‘ç¥¨â€)</strong> ã€‚è¿™æ ·å°±æ›´åŠ å‡å°‘äº†æˆ‘ä»¬è®­ç»ƒæ¨¡å‹ã€åˆ¤æ–­åˆ†ç±»çš„æ—¶é—´ï¼Œé€Ÿåº¦éå¸¸å¿«ã€‚</p><p><strong>â€œåœç”¨è¯â€å’Œâ€œå…³é”®è¯â€ä¸€èˆ¬éƒ½å¯ä»¥æå‰é äººå·¥ç»éªŒæŒ‡å®š</strong>ã€‚ä¸åŒçš„â€œåœç”¨è¯â€å’Œâ€œå…³é”®è¯â€è®­ç»ƒå‡ºæ¥çš„åˆ†ç±»å™¨çš„æ•ˆæœä¹Ÿä¼šæœ‰äº›å·®å¼‚ã€‚</p><h2 id="11-æµ…è°ˆå¹³æ»‘æŠ€æœ¯"><a href="#11-æµ…è°ˆå¹³æ»‘æŠ€æœ¯" class="headerlink" title="11. æµ…è°ˆå¹³æ»‘æŠ€æœ¯"></a>11. æµ…è°ˆå¹³æ»‘æŠ€æœ¯</h2><p>æˆ‘ä»¬æ¥è¯´ä¸ªé—®é¢˜(ä¸­æ–‡NLPé‡Œé—®é¢˜è¶…çº§å¤šï¼Œå“­çT_T)ï¼Œæ¯”å¦‚åœ¨è®¡ç®—ä»¥ä¸‹ç‹¬ç«‹æ¡ä»¶å‡è®¾çš„æ¦‚ç‡ï¼š</p><blockquote><p>P(ï¼ˆâ€œæˆ‘â€,â€œå¸â€,â€œå¯â€,â€œåŠç†â€,â€œæ­£è§„å‘ç¥¨â€)|S) =P(â€œæˆ‘â€|S)P(â€œå¸â€|S)P(â€œå¯â€|S)P(â€œåŠç†â€|S)P(â€œæ­£è§„å‘ç¥¨â€|Sï¼‰</p></blockquote><p>æˆ‘ä»¬æ‰«æä¸€ä¸‹è®­ç»ƒé›†ï¼Œå‘ç°<strong>â€œæ­£è§„å‘ç¥¨â€è¿™ä¸ªè¯ä»å‡ºç°è¿‡ï¼ï¼ï¼*ï¼Œäºæ˜¯P(â€œæ­£è§„å‘ç¥¨â€|Sï¼‰=0â€¦é—®é¢˜ä¸¥é‡äº†ï¼Œæ•´ä¸ªæ¦‚ç‡éƒ½å˜æˆ0äº†ï¼ï¼ï¼æœ´ç´ è´å¶æ–¯æ–¹æ³•é¢å¯¹ä¸€å †0ï¼Œå¾ˆå‡„æƒ¨åœ°å¤±æ•ˆäº†â€¦æ›´æ®‹é…·çš„æ˜¯</strong>è¿™ç§æƒ…å†µå…¶å®å¾ˆå¸¸è§<strong>ï¼Œå› ä¸ºå“ªæ€•è®­ç»ƒé›†å†å¤§ï¼Œä¹Ÿå¯èƒ½æœ‰è¦†ç›–ä¸åˆ°çš„è¯è¯­ã€‚æœ¬è´¨ä¸Šè¿˜æ˜¯</strong>æ ·æœ¬æ•°é‡å¤ªå°‘ï¼Œä¸æ»¡è¶³å¤§æ•°å®šå¾‹ï¼Œè®¡ç®—å‡ºæ¥çš„æ¦‚ç‡å¤±çœŸ**ã€‚ä¸ºäº†è§£å†³è¿™æ ·çš„é—®é¢˜ï¼Œä¸€ç§åˆ†ææ€è·¯å°±æ˜¯ç›´æ¥ä¸è€ƒè™‘è¿™æ ·çš„è¯è¯­ï¼Œä½†è¿™ç§æ–¹æ³•å°±ç›¸å½“äºé»˜è®¤ç»™P(â€œæ­£è§„å‘ç¥¨â€|Sï¼‰èµ‹å€¼ä¸º1ã€‚å…¶å®æ•ˆæœä¸å¤ªå¥½ï¼Œå¤§é‡çš„ç»Ÿè®¡ä¿¡æ¯ç»™æµªè´¹æ‰äº†ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æï¼Œæ—¢ç„¶å¯ä»¥é»˜è®¤èµ‹å€¼ä¸º1ï¼Œä¸ºä»€ä¹ˆä¸èƒ½é»˜è®¤èµ‹å€¼ä¸ºä¸€ä¸ªå¾ˆå°çš„æ•°ï¼Ÿè¿™å°±æ˜¯å¹³æ»‘æŠ€æœ¯çš„åŸºæœ¬æ€è·¯ï¼Œä¾æ—§ä¿æŒç€ä¸€è´¯çš„ä½œé£ï¼Œ<code>æœ´å®/åœŸ</code>ä½†æ˜¯<code>ç›´æ¥è€Œæœ‰æ•ˆ</code>ã€‚</p><p>å¯¹äºä¼¯åŠªåˆ©æ¨¡å‹ï¼ŒP(â€œæ­£è§„å‘ç¥¨â€|Sï¼‰çš„ä¸€ç§å¹³æ»‘ç®—æ³•æ˜¯ï¼š</p><blockquote><p>P(â€œæ­£è§„å‘ç¥¨â€|Sï¼‰=å‡ºç°â€œæ­£è§„å‘ç¥¨â€çš„åƒåœ¾é‚®ä»¶çš„å°æ•°+1æ¯å°åƒåœ¾é‚®ä»¶ä¸­æ‰€æœ‰è¯å‡ºç°æ¬¡æ•°ï¼ˆå‡ºç°äº†åªè®¡ç®—ä¸€æ¬¡ï¼‰çš„æ€»å’Œ+2</p></blockquote><p>å¯¹äºå¤šé¡¹å¼æ¨¡å‹ï¼ŒP(â€œæ­£è§„å‘ç¥¨â€| Sï¼‰çš„ä¸€ç§å¹³æ»‘ç®—æ³•æ˜¯ï¼š</p><blockquote><p>P(â€œå‘ç¥¨â€|Sï¼‰=æ¯å°åƒåœ¾é‚®ä»¶ä¸­å‡ºç°â€œå‘ç¥¨â€çš„æ¬¡æ•°çš„æ€»å’Œ+1æ¯å°åƒåœ¾é‚®ä»¶ä¸­æ‰€æœ‰è¯å‡ºç°æ¬¡æ•°ï¼ˆè®¡ç®—é‡å¤æ¬¡æ•°ï¼‰çš„æ€»å’Œ+è¢«ç»Ÿè®¡çš„è¯è¡¨çš„è¯è¯­æ•°é‡</p></blockquote><p>è¯´èµ·æ¥ï¼Œå¹³æ»‘æŠ€æœ¯çš„ç§ç±»å…¶å®éå¸¸å¤šï¼Œæœ‰å…´è¶£çš„è¯å›å¤´æˆ‘ä»¬ä¸“é—¨æ‹‰ä¸ªä¸“é¢˜è®²è®²å¥½äº†ã€‚è¿™é‡Œåªæä¸€ç‚¹ï¼Œå°±æ˜¯æ‰€æœ‰çš„<strong>å¹³æ»‘æŠ€æœ¯éƒ½æ˜¯ç»™æœªå‡ºç°åœ¨è®­ç»ƒé›†ä¸­çš„è¯è¯­ä¸€ä¸ªä¼°è®¡çš„æ¦‚ç‡ï¼Œè€Œç›¸åº”åœ°è°ƒä½å…¶ä»–å·²ç»å‡ºç°çš„è¯è¯­çš„æ¦‚ç‡</strong>ã€‚</p><p>å¹³æ»‘æŠ€æœ¯æ˜¯å› ä¸ºæ•°æ®é›†å¤ªå°è€Œäº§ç”Ÿçš„ç°å®éœ€æ±‚ã€‚<strong>å¦‚æœæ•°æ®é›†è¶³å¤Ÿå¤§ï¼Œå¹³æ»‘æŠ€æœ¯å¯¹ç»“æœçš„å½±å“å°†ä¼šå˜å°ã€‚</strong></p><h2 id="12-å†…å®¹å°ç»“"><a href="#12-å†…å®¹å°ç»“" class="headerlink" title="12. å†…å®¹å°ç»“"></a>12. å†…å®¹å°ç»“</h2><p>æˆ‘ä»¬æ‰¾äº†ä¸ªæœ€ç®€å•å¸¸è§çš„ä¾‹å­ï¼šåƒåœ¾é‚®ä»¶è¯†åˆ«ï¼Œè¯´æ˜äº†ä¸€ä¸‹æœ´ç´ è´å¶æ–¯è¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„æ€è·¯è¿‡ç¨‹ã€‚åŸºæœ¬æ€è·¯æ˜¯å…ˆåŒºåˆ†å¥½è®­ç»ƒé›†ä¸æµ‹è¯•é›†ï¼Œå¯¹æ–‡æœ¬é›†åˆè¿›è¡Œåˆ†è¯ã€å»é™¤æ ‡ç‚¹ç¬¦å·ç­‰ç‰¹å¾é¢„å¤„ç†çš„æ“ä½œï¼Œç„¶åä½¿ç”¨æ¡ä»¶ç‹¬ç«‹å‡è®¾ï¼Œå°†åŸæ¦‚ç‡è½¬æ¢æˆè¯æ¦‚ç‡ä¹˜ç§¯ï¼Œå†è¿›è¡Œåç»­çš„å¤„ç†ã€‚</p><blockquote><p>è´å¶æ–¯å…¬å¼ + æ¡ä»¶ç‹¬ç«‹å‡è®¾ = æœ´ç´ è´å¶æ–¯æ–¹æ³•</p></blockquote><p>åŸºäºå¯¹é‡å¤è¯è¯­åœ¨è®­ç»ƒé˜¶æ®µä¸åˆ¤æ–­ï¼ˆæµ‹è¯•ï¼‰é˜¶æ®µçš„ä¸‰ç§ä¸åŒå¤„ç†æ–¹å¼ï¼Œæˆ‘ä»¬ç›¸åº”çš„æœ‰ä¼¯åŠªåˆ©æ¨¡å‹ã€å¤šé¡¹å¼æ¨¡å‹å’Œæ··åˆæ¨¡å‹ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œå¦‚æœæ ·æœ¬é›†åˆå¤ªå°å¯¼è‡´æŸäº›è¯è¯­å¹¶æœªå‡ºç°ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨å¹³æ»‘æŠ€æœ¯å¯¹å…¶æ¦‚ç‡ç»™ä¸€ä¸ªä¼°è®¡å€¼ã€‚è€Œä¸”å¹¶ä¸æ˜¯æ‰€æœ‰çš„è¯è¯­éƒ½éœ€è¦ç»Ÿè®¡ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ç›¸åº”çš„â€œåœç”¨è¯â€å’Œâ€œå…³é”®è¯â€å¯¹æ¨¡å‹è¿›è¡Œè¿›ä¸€æ­¥ç®€åŒ–ï¼Œæé«˜è®­ç»ƒå’Œåˆ¤æ–­é€Ÿåº¦ã€‚</p><h2 id="13-ä¸ºä»€ä¹ˆä¸ç›´æ¥åŒ¹é…å…³é”®è¯æ¥è¯†åˆ«åƒåœ¾é‚®ä»¶ï¼Ÿ"><a href="#13-ä¸ºä»€ä¹ˆä¸ç›´æ¥åŒ¹é…å…³é”®è¯æ¥è¯†åˆ«åƒåœ¾é‚®ä»¶ï¼Ÿ" class="headerlink" title="13. ä¸ºä»€ä¹ˆä¸ç›´æ¥åŒ¹é…å…³é”®è¯æ¥è¯†åˆ«åƒåœ¾é‚®ä»¶ï¼Ÿ"></a>13. ä¸ºä»€ä¹ˆä¸ç›´æ¥åŒ¹é…å…³é”®è¯æ¥è¯†åˆ«åƒåœ¾é‚®ä»¶ï¼Ÿ</h2><p>æœ‰åŒå­¦å¯èƒ½ä¼šé—®ï¼šâ€œä½•å¿…è´¹è¿™ä¹ˆå¤§åŠ²ç®—é‚£ä¹ˆå¤šè¯çš„æ¦‚ç‡ï¼Ÿç›´æ¥çœ‹é‚®ä»¶ä¸­æœ‰æ²¡æœ‰â€˜ä»£å¼€å‘ç¥¨â€™ã€â€˜è½¬å”®å‘ç¥¨â€™ä¹‹ç±»çš„å…³é”®è¯ä¸å°±å¾—äº†ï¼Ÿå¦‚æœå…³é”®è¯æ¯”è¾ƒå¤šå°±è®¤ä¸ºæ˜¯åƒåœ¾é‚®ä»¶å‘—ã€‚â€</p><p>å…¶å®å…³é”®è¯åŒ¹é…çš„æ–¹æ³•å¦‚æœæœ‰æ•ˆçš„è¯çœŸä¸å¿…ç”¨æœ´ç´ è´å¶æ–¯ã€‚æ¯•ç«Ÿè¿™ç§æ–¹æ³•ç®€å•å˜›ï¼Œ<strong>å°±æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åŒ¹é…</strong>ã€‚ä»å†å²æ¥çœ‹ï¼Œä¹‹å‰æ²¡æœ‰è´å¶æ–¯æ–¹æ³•çš„æ—¶å€™ä¸»è¦ä¹Ÿæ˜¯ç”¨å…³é”®è¯åŒ¹é…ã€‚<strong>ä½†æ˜¯è¿™ç§æ–¹æ³•å‡†ç¡®ç‡å¤ªä½</strong>ã€‚æˆ‘ä»¬åœ¨å·¥ä½œé¡¹ç›®ä¸­ä¹Ÿå°è¯•è¿‡ç”¨å…³é”®è¯åŒ¹é…çš„æ–¹æ³•å»è¿›è¡Œæ–‡æœ¬åˆ†ç±»ï¼Œå‘ç°å¤§é‡è¯¯æŠ¥ã€‚æ„Ÿè§‰å°±åƒæ‰”åˆ°åƒåœ¾ç®±çš„é‚®ä»¶99%éƒ½æ˜¯æ­£å¸¸çš„ï¼è¿™æ ·çš„æ•ˆæœä¸å¿ç›´è§†ã€‚è€ŒåŠ ä¸€ä¸ªæœ´ç´ è´å¶æ–¯æ–¹æ³•å°±å¯èƒ½æŠŠè¯¯æŠ¥ç‡æ‹‰ä½è¿‘ä¸€ä¸ªæ•°é‡çº§ï¼Œä½“éªŒå¥½å¾—ä¸è¦ä¸è¦çš„ã€‚</p><p><strong>å¦ä¸€ä¸ªåŸå› æ˜¯è¯è¯­ä¼šéšç€æ—¶é—´ä¸æ–­å˜åŒ–</strong>ã€‚å‘åƒåœ¾é‚®ä»¶çš„äººä¹Ÿä¸å‚»ï¼Œå½“ä»–ä»¬å‘ç°è‡ªå·±çš„é‚®ä»¶è¢«å¤§é‡å±è”½ä¹‹åï¼Œä¹Ÿä¼šè€ƒè™‘é‡‡ç”¨æ–°çš„æ–¹å¼ï¼Œ<strong>å¦‚å˜æ¢æ–‡å­—ã€è¯è¯­ã€å¥å¼ã€é¢œè‰²ç­‰æ–¹å¼æ¥ç»•è¿‡ååƒåœ¾é‚®ä»¶ç³»ç»Ÿ</strong>ã€‚æ¯”å¦‚å¯¹äºåƒåœ¾é‚®ä»¶â€œæˆ‘å¸å¯åŠç†æ­£è§„å‘ç¥¨ï¼Œ17%å¢å€¼ç¨å‘ç¥¨ç‚¹æ•°ä¼˜æƒ â€,ä»–ä»¬é‡‡ç”¨ç«æ˜Ÿæ–‡ï¼š<strong>â€œæ¶å¸å²¢åŠç†ãŠ£è¦é«®ç¥¨ï¼Œ17%å¢å€¤ç¨…é«®ç¥¨åš¸æ•¸å„ªè•™â€</strong>ï¼Œé‚£ä¹ˆå­—ç¬¦ä¸²åŒ¹é…çš„æ–¹æ³•åˆè¦é‡æ–°æ‰¾å‡ºè¿™äº›ç«æ˜Ÿæ–‡ï¼Œä¸€ä¸ªä¸€ä¸ªæ‰¾å‡ºå…³é”®è¯ï¼Œé‡æ–°å†™ä¸€äº›åŒ¹é…è§„åˆ™ã€‚æ›´å¯æ€•çš„æ˜¯ï¼Œè¿™äº›è§„åˆ™å¯èƒ½ç›¸äº’ä¹‹é—´çš„è€¦åˆå…³ç³»å¼‚å¸¸å¤æ‚ï¼Œè¦æŠŠå®ƒä»¬æ¢³ç†æ¸…æ¥šåˆæ˜¯å¤§ä¸€ä¸ªæ•°é‡çº§çš„å·¥ä½œé‡ã€‚ç­‰è¿™äº›è§„åˆ™å¤±æ•ˆäº†åˆè¦æ‰‹åŠ¨æ›´æ–°æ–°çš„è§„åˆ™â€¦â€¦<strong>æ— ç©·æ— å°½çŒ«é¼ æ¸¸æˆæœ€ç»ˆä¼šæŠŠçŒ«ç»™ç´¯æ­»</strong>ã€‚</p><p>è€Œæœ´ç´ è´å¶æ–¯æ–¹æ³•å´æ˜¾ç¤ºå‡ºæ— æ¯”çš„ä¼˜åŠ¿ã€‚å› ä¸ºå®ƒæ˜¯<strong>åŸºäºç»Ÿè®¡æ–¹æ³•</strong>çš„ï¼Œåªè¦è®­ç»ƒæ ·æœ¬ä¸­æœ‰æ›´æ–°çš„åƒåœ¾é‚®ä»¶çš„æ–°è¯è¯­ï¼Œå“ªæ€•å®ƒä»¬æ˜¯ç«æ˜Ÿæ–‡ï¼Œ<strong>éƒ½èƒ½è‡ªåŠ¨åœ°æŠŠå“ªäº›æ›´æ•æ„Ÿçš„è¯è¯­ï¼ˆå¦‚â€œé«®â€ã€â€œãŠ£â€ç­‰ï¼‰ç»™å‡¸æ˜¾å‡ºæ¥ï¼Œå¹¶æ ¹æ®ç»Ÿè®¡æ„ä¹‰ä¸Šçš„æ•æ„Ÿæ€§ç»™ä»–ä»¬åˆ†é…é€‚å½“çš„æƒé‡</strong> ï¼Œè¿™æ ·å°±ä¸éœ€è¦ä»€ä¹ˆäººå·¥äº†ï¼Œéå¸¸çœäº‹ã€‚<strong>ä½ åªéœ€è¦æ—¶ä¸æ—¶åœ°æ‹¿ä¸€äº›æœ€æ–°çš„æ ·æœ¬æ‰”åˆ°è®­ç»ƒé›†ä¸­ï¼Œé‡æ–°è®­ç»ƒä¸€æ¬¡å³å¯</strong>ã€‚</p><p>å°è¡¥å……ä¸€ä¸‹ï¼Œå¯¹äºç«æ˜Ÿæ–‡ã€åŒéŸ³å­—ç­‰æ›¿ä»£è¯­è¨€ï¼Œä¸€èˆ¬çš„åˆ†è¯æŠ€æœ¯å¯èƒ½ä¼šåˆ†å¾—ä¸å‡†ï¼Œæœ€ç»ˆå¯èƒ½åªæŠŠä¸€ä¸ªä¸€ä¸ªå­—ç»™åˆ†å‡ºæ¥ï¼Œæˆä¸ºâ€œåˆ†å­—â€ã€‚æ•ˆæœå¯èƒ½ä¸ä¼šå¤ªå¥½ã€‚ä¹Ÿå¯ä»¥ç”¨è¿‡n-gramä¹‹ç±»çš„è¯­è¨€æ¨¡å‹ï¼Œæ‹¿åˆ°æœ€å¸¸è§çŸ­è¯­ã€‚å½“ç„¶ï¼Œå¯¹äºè‹±æ–‡ç­‰å¤©ç”Ÿè‡ªå¸¦ç©ºæ ¼æ¥é—´éš”å•è¯çš„è¯­è¨€ï¼Œåˆ†è¯åˆ™ä¸æ˜¯ä»€ä¹ˆé—®é¢˜ï¼Œä½¿ç”¨æœ´ç´ è´å¶æ–¯æ–¹æ³•å°†ä¼šæ›´åŠ é¡ºç•…ã€‚</p><h2 id="14-å®é™…å·¥ç¨‹çš„tricks"><a href="#14-å®é™…å·¥ç¨‹çš„tricks" class="headerlink" title="14.å®é™…å·¥ç¨‹çš„tricks"></a>14.å®é™…å·¥ç¨‹çš„tricks</h2><p>åº”ç”¨æœ´ç´ è´å¶æ–¯æ–¹æ³•çš„è¿‡ç¨‹ä¸­ï¼Œä¸€äº›tricksèƒ½æ˜¾è‘—å¸®åŠ©å·¥ç¨‹è§£å†³é—®é¢˜ã€‚æˆ‘ä»¬æ¯•ç«Ÿç»éªŒæœ‰é™ï¼Œæ— æ³•å°†å®ƒä»¬å…¨éƒ½ç½—åˆ—å‡ºæ¥ï¼Œåªèƒ½å°±æ‰€çŸ¥çš„ä¸€ç‚¹ç‚¹ç»éªŒä¸å¤§å®¶åˆ†äº«ï¼Œæ¬¢è¿æ‰¹è¯„æŒ‡æ­£ã€‚</p><h3 id="14-1-trick1ï¼šå–å¯¹æ•°"><a href="#14-1-trick1ï¼šå–å¯¹æ•°" class="headerlink" title="14.1 trick1ï¼šå–å¯¹æ•°"></a>14.1 trick1ï¼šå–å¯¹æ•°</h3><p>æˆ‘ä»¬æåˆ°ç”¨æ¥è¯†åˆ«åƒåœ¾é‚®ä»¶çš„æ–¹æ³•æ˜¯æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªæ¦‚ç‡çš„å¤§å°ï¼ˆå­—æ¯Sè¡¨ç¤ºâ€œåƒåœ¾é‚®ä»¶â€,å­—æ¯Hè¡¨ç¤ºâ€œæ­£å¸¸é‚®ä»¶â€ï¼‰ï¼š</p><blockquote><p>C=P(â€œæˆ‘â€|S)P(â€œå¸â€|S)P(â€œå¯â€|S)P(â€œåŠç†â€|S)P(â€œæ­£è§„å‘ç¥¨â€|S)</p><p>Ã—P(â€œä¿çœŸâ€|S)P(â€œå¢å€¼ç¨â€|S)P(â€œå‘ç¥¨â€|S)P(â€œç‚¹æ•°â€|S)P(â€œä¼˜æƒ â€|S)P(â€œåƒåœ¾é‚®ä»¶â€)</p><p>Câ¯â¯â¯â¯=P(â€œæˆ‘â€|H)P(â€œå¸â€|H)P(â€œå¯â€|H)P(â€œåŠç†â€|H)P(â€œæ­£è§„å‘ç¥¨â€|H)</p><p>Ã—P(â€œä¿çœŸâ€|H)P(â€œå¢å€¼ç¨â€|H)P(â€œå‘ç¥¨â€|H)P(â€œç‚¹æ•°â€|H)P(â€œä¼˜æƒ â€|H)P(â€œæ­£å¸¸é‚®ä»¶â€)</p></blockquote><p>ä½†è¿™é‡Œè¿›è¡Œäº†<strong>å¾ˆå¤šä¹˜æ³•è¿ç®—ï¼Œè®¡ç®—çš„æ—¶é—´å¼€é”€æ¯”è¾ƒå¤§</strong>ã€‚å°¤å…¶æ˜¯å¯¹äºç¯‡å¹…æ¯”è¾ƒé•¿çš„é‚®ä»¶ï¼Œå‡ ä¸‡ä¸ªæ•°ç›¸ä¹˜èµ·æ¥è¿˜æ˜¯éå¸¸èŠ±æ—¶é—´çš„ã€‚å¦‚æœèƒ½<strong>æŠŠè¿™äº›ä¹˜æ³•å˜æˆåŠ æ³•åˆ™æ–¹ä¾¿å¾—å¤š</strong>ã€‚åˆšå¥½æ•°å­¦ä¸­çš„å¯¹æ•°å‡½æ•°logå°±å¯ä»¥å®ç°è¿™æ ·çš„åŠŸèƒ½ã€‚ä¸¤è¾¹åŒæ—¶å–å¯¹æ•°ï¼ˆæœ¬æ–‡ç»Ÿä¸€å–åº•æ•°ä¸º2ï¼‰ï¼Œåˆ™ä¸Šé¢çš„å…¬å¼å˜ä¸ºï¼š</p><blockquote><p>logC=logP(â€œæˆ‘â€|S)+logP(â€œå¸â€|S)+logP(â€œå¯â€|S)+logP(â€œåŠç†â€|S)+logP(â€œæ­£è§„å‘ç¥¨â€|S)</p><p>+logP(â€œä¿çœŸâ€|S)+logP(â€œå¢å€¼ç¨â€|S)+logP(â€œå‘ç¥¨â€|S)+logP(â€œç‚¹æ•°â€|S)+logP(â€œä¼˜æƒ â€|S)+logP(â€œåƒåœ¾é‚®ä»¶â€)</p><p>logCâ¯â¯â¯â¯=logP(â€œæˆ‘â€|H)+logP(â€œå¸â€|H)+logP(â€œå¯â€|H)+logP(â€œåŠç†â€|H)+logP(â€œæ­£è§„å‘ç¥¨â€|H)</p><p>+logP(â€œä¿çœŸâ€|H)+logP(â€œå¢å€¼ç¨â€|H)+logP(â€œå‘ç¥¨â€|H)+logP(â€œç‚¹æ•°â€|H)+logP(â€œä¼˜æƒ â€|H)+logP(â€œæ­£å¸¸é‚®ä»¶â€)</p></blockquote><p>æœ‰åŒå­¦å¯èƒ½è¦å«äº†ï¼šâ€œåšå¯¹æ•°è¿ç®—å²‚ä¸ä¼šä¹Ÿå¾ˆèŠ±æ—¶é—´ï¼Ÿâ€çš„ç¡®å¦‚æ­¤ï¼Œä½†æ˜¯å¯ä»¥åœ¨è®­ç»ƒé˜¶æ®µç›´æ¥è®¡ç®— logP ï¼Œç„¶åæŠŠä»–ä»¬å­˜åœ¨ä¸€å¼ å¤§çš„hashè¡¨é‡Œã€‚<strong>åœ¨åˆ¤æ–­çš„æ—¶å€™ç›´æ¥æå–hashè¡¨ä¸­å·²ç»è®¡ç®—å¥½çš„å¯¹æ•°æ¦‚ç‡ï¼Œç„¶åç›¸åŠ å³å¯ã€‚è¿™æ ·ä½¿å¾—åˆ¤æ–­æ‰€éœ€è¦çš„è®¡ç®—æ—¶é—´è¢«è½¬ç§»åˆ°äº†è®­ç»ƒé˜¶æ®µ</strong>ï¼Œå®æ—¶è¿è¡Œçš„æ—¶å€™é€Ÿåº¦å°±æ¯”ä¹‹å‰å¿«å¾—å¤šï¼Œè¿™å¯ä¸æ­¢å‡ ä¸ªæ•°é‡çº§çš„æå‡ã€‚</p><h3 id="14-2-trick2ï¼šè½¬æ¢ä¸ºæƒé‡"><a href="#14-2-trick2ï¼šè½¬æ¢ä¸ºæƒé‡" class="headerlink" title="14.2 trick2ï¼šè½¬æ¢ä¸ºæƒé‡"></a>14.2 trick2ï¼šè½¬æ¢ä¸ºæƒé‡</h3><p>å¯¹äºäºŒåˆ†ç±»ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ç»§ç»­æé«˜åˆ¤æ–­çš„é€Ÿåº¦ã€‚æ—¢ç„¶è¦æ¯”è¾ƒlogC å’ŒlogCâ¯â¯â¯â¯ çš„å¤§å°ï¼Œé‚£å°±å¯ä»¥ç›´æ¥å°†ä¸Šä¸‹ä¸¤å¼ç›¸å‡ï¼Œå¹¶ç»§ç»­åŒ–ç®€ï¼š</p><blockquote><p>logCCâ¯â¯â¯â¯â¯=logP(â€œæˆ‘â€|S)P(â€œæˆ‘â€|H)+logP(â€œå¸â€|S)P(â€œå¸â€|H)+logP(â€œå¯â€|S)P(â€œå¯â€|H)+logP(â€œåŠç†â€|S)P(â€œåŠç†â€|H)+logP(â€œæ­£è§„å‘ç¥¨â€|S)P(â€œæ­£è§„å‘ç¥¨â€|H)</p><p>+logP(â€œä¿çœŸâ€|S)P(â€œä¿çœŸâ€|H)+logP(â€œå¢å€¼ç¨â€|S)P(â€œå¢å€¼ç¨â€|H)+logP(â€œå‘ç¥¨â€|S)P(â€œå‘ç¥¨â€|H)+logP(â€œç‚¹æ•°â€|S)P(â€œç‚¹æ•°â€|H)+logP(â€œä¼˜æƒ â€|S)P(â€œä¼˜æƒ â€|H)+logP(â€œæ­£å¸¸é‚®ä»¶â€|S)P(â€œæ­£å¸¸é‚®ä»¶â€)</p></blockquote><p><strong>logCCâ¯â¯â¯â¯â¯ å¦‚æœå¤§äº0åˆ™å±äºåƒåœ¾é‚®ä»¶ã€‚æˆ‘ä»¬å¯ä»¥æŠŠå…¶ä¸­æ¯ä¸€é¡¹ä½œä¸ºå…¶å¯¹åº”è¯è¯­çš„æƒé‡</strong>ï¼Œæ¯”å¦‚logP(â€œå‘ç¥¨â€|S)P(â€œå‘ç¥¨â€|H) å°±å¯ä»¥ä½œä¸ºè¯è¯­â€œå‘ç¥¨â€çš„æƒé‡ï¼Œæƒé‡è¶Šå¤§å°±è¶Šè¯´æ˜â€œå‘ç¥¨â€æ›´å¯èƒ½æ˜¯ä¸â€œåƒåœ¾é‚®ä»¶â€ç›¸å…³çš„ç‰¹å¾ã€‚<strong>è¿™æ ·å¯ä»¥æ ¹æ®æƒé‡çš„å¤§å°æ¥è¯„ä¼°å’Œç­›é€‰æ˜¾è‘—çš„ç‰¹å¾ï¼Œæ¯”å¦‚å…³é”®è¯ã€‚è€Œè¿™äº›æƒé‡å€¼å¯ä»¥ç›´æ¥æå‰è®¡ç®—å¥½è€Œå­˜åœ¨hashè¡¨ä¸­</strong> ã€‚åˆ¤æ–­çš„æ—¶å€™ç›´æ¥å°†æƒé‡æ±‚å’Œå³å¯ã€‚</p><p>å…³é”®è¯hashè¡¨çš„æ ·å­å¦‚ä¸‹ï¼Œå·¦åˆ—æ˜¯æƒé‡ï¼Œå³åˆ—æ˜¯å…¶å¯¹åº”çš„è¯è¯­ï¼Œæƒé‡è¶Šé«˜çš„è¯´æ˜è¶Šâ€œå…³é”®â€ï¼š</p><p><img src="blob:file:///6f29d16a-1075-45cb-9c7e-206aefcf4e4a" alt="hash"></p><h3 id="14-3-trick3ï¼šé€‰å–topkçš„å…³é”®è¯"><a href="#14-3-trick3ï¼šé€‰å–topkçš„å…³é”®è¯" class="headerlink" title="14.3 trick3ï¼šé€‰å–topkçš„å…³é”®è¯"></a>14.3 trick3ï¼šé€‰å–topkçš„å…³é”®è¯</h3><p>å‰æ–‡è¯´è¿‡å¯ä»¥é€šè¿‡æå‰é€‰å–å…³é”®è¯æ¥æé«˜åˆ¤æ–­çš„é€Ÿåº¦ã€‚æœ‰ä¸€ç§æ–¹æ³•å¯ä»¥çœç•¥æå‰é€‰å–å…³é”®è¯çš„æ­¥éª¤ï¼Œ<strong>å°±æ˜¯ç›´æ¥é€‰å–ä¸€æ®µæ–‡æœ¬ä¸­æƒé‡æœ€é«˜çš„Kä¸ªè¯è¯­ï¼Œå°†å…¶æƒé‡è¿›è¡ŒåŠ å’Œ</strong>ã€‚æ¯”å¦‚Paul Graham åœ¨ã€Šé»‘å®¢ä¸ç”»å®¶ã€‹ä¸­æ˜¯é€‰å–é‚®ä»¶ä¸­æƒé‡æœ€é«˜çš„15ä¸ªè¯è¯­è®¡ç®—çš„ã€‚</p><p>é€šè¿‡æƒé‡hashè¡¨å¯çŸ¥ï¼Œå¦‚æœæ˜¯æ‰€æœ‰è¯è¯­çš„æƒé‡ï¼Œåˆ™æƒé‡æœ‰æ­£æœ‰è´Ÿã€‚å¦‚æœåªé€‰æ‹©æƒé‡æœ€é«˜çš„Kä¸ªè¯è¯­ï¼Œåˆ™å®ƒä»¬çš„æƒé‡åŸºæœ¬éƒ½æ˜¯æ­£çš„ã€‚æ‰€ä»¥å°±ä¸èƒ½åƒä¹‹å‰é‚£æ ·åˆ¤æ–­logCCâ¯â¯â¯â¯â¯ æ˜¯å¦å¤§äº0æ¥åŒºåˆ†é‚®ä»¶äº†ã€‚è€Œè¿™<strong>éœ€è¦ä¾é ç»éªŒé€‰å®šä¸€ä¸ªæ­£æ•°çš„é˜ˆå€¼ï¼ˆé—¨æ§›å€¼ï¼‰</strong> ï¼Œä¾æ®logCCâ¯â¯â¯â¯â¯ ä¸è¯¥é—¨æ§›å€¼çš„å¤§å°æ¥è¯†åˆ«åƒåœ¾é‚®ä»¶ã€‚</p><p>å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè“è‰²ç‚¹ä»£è¡¨åƒåœ¾é‚®ä»¶ï¼Œç»¿è‰²ç‚¹ä»£è¡¨æ­£å¸¸é‚®ä»¶ï¼Œæ¨ªåæ ‡ä¸ºè®¡ç®—å‡ºæ¥çš„logCCâ¯â¯â¯â¯â¯ å€¼ï¼Œä¸­é—´çš„çº¢çº¿ä»£è¡¨é˜ˆå€¼ã€‚</p><p><img src="blob:file:///fa290902-0e88-4f3b-8a65-79c442c05c05" alt="æƒé‡"></p><h3 id="14-4-trick4ï¼šåˆ†å‰²æ ·æœ¬"><a href="#14-4-trick4ï¼šåˆ†å‰²æ ·æœ¬" class="headerlink" title="14.4 trick4ï¼šåˆ†å‰²æ ·æœ¬"></a>14.4 trick4ï¼šåˆ†å‰²æ ·æœ¬</h3><p>é€‰å–topkä¸ªè¯è¯­çš„æ–¹æ³•å¯¹äºç¯‡å¹…å˜åŠ¨ä¸å¤§çš„é‚®ä»¶æ ·æœ¬æ¯”è¾ƒæœ‰æ•ˆã€‚ä½†æ˜¯å¯¹ç¯‡å¹…è¿‡å¤§æˆ–è€…è¿‡å°çš„é‚®ä»¶åˆ™ä¼šæœ‰åˆ¤æ–­è¯¯å·®ã€‚</p><p>æ¯”å¦‚è¿™ä¸ªåƒåœ¾é‚®ä»¶çš„ä¾‹å­ï¼šï¼ˆâ€œæˆ‘â€,â€œå¸â€,â€œå¯â€,â€œåŠç†â€,â€œæ­£è§„å‘ç¥¨â€,â€œä¿çœŸâ€,â€œå¢å€¼ç¨â€,â€œå‘ç¥¨â€,â€œç‚¹æ•°â€,â€œä¼˜æƒ â€)ã€‚åˆ†è¯å‡ºäº†10ä¸ªè¯è¯­ï¼Œå…¶ä¸­æœ‰â€œæ­£è§„å‘ç¥¨â€ã€â€œå‘ç¥¨â€2ä¸ªå…³é”®è¯ã€‚å…³é”®è¯çš„å¯†åº¦è¿˜æ˜¯è›®å¤§çš„ï¼Œåº”è¯¥ç®—æ˜¯æ•æ„Ÿé‚®ä»¶ã€‚ä½†å› ä¸ºé‡‡ç”¨æœ€é«˜15ä¸ªè¯è¯­çš„æƒé‡æ±‚å’Œï¼Œå¹¶ä¸”ç›¸åº”çš„é˜ˆå€¼æ˜¯åŸºäº15ä¸ªè¯çš„æƒ…å†µæœ‰æ•ˆï¼Œå¯èƒ½ç®—å‡ºæ¥çš„ç»“æœè¿˜å°äºä¹‹å‰çš„é˜ˆå€¼ï¼Œè¿™å°±é€ æˆæ¼åˆ¤äº†ã€‚</p><p>ç±»ä¼¼çš„ï¼Œå¦‚æœä¸€å°ç¨åŠ¡ä¸»é¢˜çš„é‚®ä»¶æœ‰1000ä¸ªè¯è¯­ï¼Œå…¶ä¸­åªæœ‰â€œæ­£è§„å‘ç¥¨â€ã€â€œå‘ç¥¨â€ã€â€œé¿ç¨æ–¹æ³•â€3ä¸ªæƒé‡æ¯”è¾ƒå¤§çš„è¯è¯­ï¼Œå®ƒä»¬åªæ˜¯åœ¨æ­£æ–‡è¡¨è¿°ä¸­é¡ºå¸¦æåˆ°çš„å†…å®¹ã€‚å…³é”®è¯çš„å¯†åº¦è¢«è¾ƒé•¿çš„ç¯‡å¹…ç¨€é‡Šäº†ï¼Œåº”è¯¥ç®—æ˜¯æ­£å¸¸é‚®ä»¶ã€‚ä½†æ˜¯å´è¢«é˜ˆå€¼åˆ¤æ–­æˆæ•æ„Ÿé‚®ä»¶ï¼Œé€ æˆè¯¯åˆ¤äº†ã€‚</p><p><strong>è¿™ä¸¤ç§æƒ…å†µéƒ½è¯´æ˜topkå…³é”®è¯çš„æ–¹æ³•éœ€è¦è€ƒè™‘ç¯‡å¹…çš„å½±å“</strong>ã€‚è¿™é‡Œæœ‰è®¸å¤šç§å¤„ç†æ–¹å¼ï¼Œ<strong>å®ƒä»¬çš„åŸºæœ¬æ€æƒ³éƒ½æ˜¯é€‰å–è¯è¯­çš„ä¸ªæ•°åŠå¯¹åº”çš„é˜ˆå€¼è¦ä¸ç¯‡å¹…çš„å¤§å°æˆæ­£æ¯”</strong>ï¼Œæœ¬æ–‡åªä»‹ç»å…¶ä¸­ä¸€ç§æ–¹æ–¹æ³•ï¼š</p><ul><li>å¯¹äºé•¿ç¯‡å¹…é‚®ä»¶ï¼ŒæŒ‰ä¸€å®šçš„å¤§å°ï¼Œæ¯”å¦‚æ¯500å­—ï¼Œå°†å…¶åˆ†å‰²æˆå°çš„æ–‡æœ¬æ®µè½ï¼Œå†å¯¹å°æ–‡æœ¬æ®µè½é‡‡ç”¨topkå…³é”®è¯çš„æ–¹æ³•ã€‚åªè¦å…¶ä¸­æœ‰ä¸€ä¸ªå°æ–‡æœ¬æ®µè½è¶…è¿‡é˜ˆå€¼å°±åˆ¤æ–­æ•´å°é‚®ä»¶æ˜¯åƒåœ¾é‚®ä»¶ã€‚</li><li>å¯¹äºè¶…çŸ­ç¯‡å¹…é‚®ä»¶ï¼Œæ¯”å¦‚50å­—ï¼Œå¯ä»¥æŒ‰ç¯‡å¹…ä¸æ ‡å‡†æ¯”è¾ƒç¯‡å¹…çš„æ¯”ä¾‹æ¥é€‰å–topkï¼Œä»¥ç¡®å®šåº”è¯¥åŒ¹é…å…³é”®è¯è¯­çš„ä¸ªæ•°ã€‚æ¯”å¦‚é€‰å– 50500Ã—15â‰ˆ2 ä¸ªè¯è¯­è¿›è¡ŒåŒ¹é…ï¼Œç›¸åº”çš„é˜ˆå€¼å¯ä»¥æ˜¯ä¹‹å‰é˜ˆå€¼çš„ 215 ã€‚ä»¥æ­¤æ¥åˆ¤æ–­åˆ™æ›´åˆç†ã€‚</li></ul><h3 id="14-5-trick5ï¼šä½ç½®æƒé‡"><a href="#14-5-trick5ï¼šä½ç½®æƒé‡" class="headerlink" title="14.5 trick5ï¼šä½ç½®æƒé‡"></a>14.5 trick5ï¼šä½ç½®æƒé‡</h3><p>åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å¯¹è¯è¯­æƒé‡æ±‚å’Œçš„è¿‡ç¨‹éƒ½æ²¡æœ‰è€ƒè™‘é‚®ä»¶ç¯‡ç« ç»“æ„çš„å› ç´ ã€‚æ¯”å¦‚â€œæ­£è§„å‘ç¥¨â€å¦‚æœå‡ºç°åœ¨æ ‡é¢˜ä¸­åº”è¯¥æ¯”å®ƒå‡ºç°åœ¨æ­£æ–‡ä¸­å¯¹åˆ¤æ–­æ•´ä¸ªé‚®ä»¶çš„å½±å“æ›´å¤§ï¼›è€Œå‡ºç°åœ¨æ®µé¦–å¥ä¸­åˆæ¯”å…¶å‡ºç°åœ¨æ®µè½æ­£æ–‡ä¸­å¯¹åˆ¤æ–­æ•´ä¸ªé‚®ä»¶çš„å½±å“æ›´å¤§ã€‚<strong>æ‰€ä»¥å¯ä»¥æ ¹æ®è¯è¯­å‡ºç°çš„ä½ç½®ï¼Œå¯¹å…¶æƒé‡å†ä¹˜ä»¥ä¸€ä¸ªæ”¾å¤§ç³»æ•°ï¼Œä»¥æ‰©å¤§å…¶å¯¹æ•´å°é‚®ä»¶çš„å½±å“ï¼Œæé«˜è¯†åˆ«å‡†ç¡®åº¦</strong>ã€‚</p><p>æ¯”å¦‚ä¸€å°é‚®ä»¶å…¶æ ‡é¢˜æ˜¯â€œæ­£è§„å‘ç¥¨â€ï¼ˆå‡è®¾æ ‡é¢˜çš„æ”¾å¤§ç³»æ•°ä¸º2ï¼‰ï¼Œæ®µé¦–å¥æ˜¯â€œå‘ç¥¨â€,â€œç‚¹æ•°â€,â€œä¼˜æƒ â€ï¼ˆå‡è®¾æ®µé¦–çš„æ”¾å¤§ç³»æ•°ä¸º1.5ï¼‰ï¼Œå‰©ä¸‹çš„å¥å­æ˜¯ï¼ˆâ€œæˆ‘â€,â€œå¸â€,â€œå¯â€,â€œåŠç†â€,â€œä¿çœŸâ€ï¼‰ã€‚åˆ™è®¡ç®—logCCâ¯â¯â¯â¯â¯ æ—¶çš„å…¬å¼å°±å¯ä»¥è°ƒæ•´ä¸ºï¼š</p><blockquote><p>logCCâ¯â¯â¯â¯â¯=2Ã—logP(â€œæ­£è§„å‘ç¥¨â€|S)P(â€œæ­£è§„å‘ç¥¨â€|H)+1.5Ã—logP(â€œå‘ç¥¨â€|S)P(â€œå‘ç¥¨â€|H)+1.5Ã—logP(â€œç‚¹æ•°â€|S)P(â€œç‚¹æ•°â€|H)+1.5Ã—logP(â€œä¼˜æƒ â€|S)P(â€œä¼˜æƒ â€|H)</p><p>+logP(â€œæˆ‘â€|S)P(â€œæˆ‘â€|H)+logP(â€œå¸â€|S)P(â€œå¸â€|H)+logP(â€œå¯â€|S)P(â€œå¯â€|H)+logP(â€œåŠç†â€|S)P(â€œåŠç†â€|H)+logP(â€œä¿çœŸâ€|S)P(â€œä¿çœŸâ€|H)+logP(â€œæ­£å¸¸é‚®ä»¶â€|S)P(â€œæ­£å¸¸é‚®ä»¶â€)</p></blockquote><h3 id="14-6-trick6ï¼šèœœç½"><a href="#14-6-trick6ï¼šèœœç½" class="headerlink" title="14.6 trick6ï¼šèœœç½"></a>14.6 trick6ï¼šèœœç½</h3><p>æˆ‘ä»¬é€šè¿‡è¾›è¾›è‹¦è‹¦çš„ç»Ÿè®¡ä¸è®¡ç®—ï¼Œå¥½ä¸å®¹æ˜“å¾—åˆ°äº†ä¸åŒè¯è¯­çš„æƒé‡ã€‚ç„¶è€Œè¿™å¹¶ä¸æ˜¯ä¸€åŠ³æ°¸é€¸çš„ã€‚æˆ‘ä»¬æˆ‘ä»¬ä¹‹å‰äº¤ä»£è¿‡ï¼Œ<strong>è¯è¯­åŠå…¶æƒé‡ä¼šéšç€æ—¶é—´ä¸æ–­å˜åŒ–ï¼Œéœ€è¦æ—¶ä¸æ—¶åœ°ç”¨æœ€æ–°çš„æ ·æœ¬æ¥è®­ç»ƒä»¥æ›´æ–°è¯è¯­åŠå…¶æƒé‡</strong>ã€‚</p><p>è€Œæœé›†æœ€æ–°åƒåœ¾é‚®ä»¶æœ‰ä¸€ä¸ªæŠ€å·§ï¼Œå°±æ˜¯éšä¾¿æ³¨å†Œä¸€äº›é‚®ç®±ï¼Œç„¶åå°†å®ƒä»¬å…¬å¸ƒåœ¨å„å¤§è®ºå›ä¸Šã€‚æ¥ä¸‹æ¥å°±åç­‰ä¸€ä¸ªæœˆï¼Œåˆ°æ—¶å€™æ”¶åˆ°çš„é‚®ä»¶å°±ç»å¤§éƒ¨åˆ†éƒ½æ˜¯åƒåœ¾é‚®ä»¶äº†ï¼ˆå¥½å¥¸è¯ˆï¼‰ã€‚å†æ‰¾ä¸€äº›æ­£å¸¸çš„é‚®ä»¶ï¼ŒåŸºæœ¬å°±èƒ½å¤Ÿè®­ç»ƒäº†ã€‚è¿™äº›ç”¨äºè‡ªåŠ¨æœé›†åƒåœ¾é‚®ä»¶çš„é‚®ç®±å«åšâ€œèœœç½â€ã€‚<strong>â€œèœœç½â€æ˜¯ç½‘ç»œå®‰å…¨é¢†åŸŸå¸¸ç”¨çš„æ‰‹æ®µï¼Œå› å…¶åŸç†ç±»ä¼¼è¯±æ•æ˜†è™«çš„è£…æœ‰èœœçš„ç½å­è€Œå¾—å</strong>ã€‚æ¯”å¦‚æ€æ¯’è½¯ä»¶å…¬å¸ä¼šåˆ©ç”¨èœœç½æ¥ç›‘è§†æˆ–è·å¾—è®¡ç®—æœºç½‘ç»œä¸­çš„ç—…æ¯’æ ·æœ¬ã€æ”»å‡»è¡Œä¸ºç­‰ã€‚</p><h2 id="15-è´å¶æ–¯æ–¹æ³•çš„æ€ç»´æ–¹å¼"><a href="#15-è´å¶æ–¯æ–¹æ³•çš„æ€ç»´æ–¹å¼" class="headerlink" title="15. è´å¶æ–¯æ–¹æ³•çš„æ€ç»´æ–¹å¼"></a>15. è´å¶æ–¯æ–¹æ³•çš„æ€ç»´æ–¹å¼</h2><p>è®²äº†è¿™ä¹ˆå¤štricksï¼Œä½†è¿™äº›æ‰‹æ®µéƒ½æ˜¯å»ºç«‹åœ¨è´å¶æ–¯æ–¹æ³•åŸºç¡€ä¹‹ä¸Šçš„ã€‚å› æ­¤æœ‰å¿…è¦æ¢è®¨ä¸€ä¸‹è´å¶æ–¯æ–¹æ³•çš„æ€ç»´æ–¹å¼ï¼Œä»¥ä¾¿æ›´å¥½åœ°åº”ç”¨è¿™ç§æ–¹æ³•è§£å†³å®é™…é—®é¢˜ã€‚</p><h3 id="15-1-é€†æ¦‚é—®é¢˜"><a href="#15-1-é€†æ¦‚é—®é¢˜" class="headerlink" title="15.1 é€†æ¦‚é—®é¢˜"></a>15.1 é€†æ¦‚é—®é¢˜</h3><p>æˆ‘ä»¬é‡æ–°çœ‹ä¸€çœ¼è´å¶æ–¯å…¬å¼ï¼š</p><blockquote><p>P(Y|X)=P(X|Y)P(Y)P(X)</p></blockquote><p>å…ˆä¸è€ƒè™‘å…ˆéªŒæ¦‚ç‡P(Y)ä¸P(X)ï¼Œè§‚å¯Ÿä¸¤ä¸ªåéªŒæ¦‚ç‡P(Y|X)ä¸P(X|Y)ï¼Œå¯è§è´å¶æ–¯å…¬å¼èƒ½å¤Ÿæ­ç¤º<strong>ä¸¤ä¸ªç›¸åæ–¹å‘çš„æ¡ä»¶æ¦‚ç‡ä¹‹é—´çš„è½¬æ¢å…³ç³»</strong>ã€‚</p><p>ä»è´å¶æ–¯å…¬å¼çš„å‘ç°å†å²æ¥çœ‹ï¼Œå…¶å°±æ˜¯ä¸ºäº†å¤„ç†æ‰€è°“â€œé€†æ¦‚â€é—®é¢˜è€Œè¯ç”Ÿçš„ã€‚æ¯”å¦‚P(Y|X) ä¸èƒ½é€šè¿‡ç›´æ¥è§‚æµ‹æ¥å¾—åˆ°ç»“æœï¼Œè€ŒP(X|Y) å´å®¹æ˜“é€šè¿‡ç›´æ¥è§‚æµ‹å¾—åˆ°ç»“æœï¼Œå°±å¯ä»¥é€šè¿‡è´å¶æ–¯å…¬å¼<strong>ä»é—´æ¥åœ°è§‚æµ‹å¯¹è±¡å»æ¨æ–­ä¸å¯ç›´æ¥è§‚æµ‹çš„å¯¹è±¡çš„æƒ…å†µ</strong>ã€‚</p><p>å¥½å§ï¼Œæˆ‘ä»¬è¯´äººè¯ã€‚åŸºäºé‚®ä»¶çš„æ–‡æœ¬å†…å®¹åˆ¤æ–­å…¶å±äºåƒåœ¾é‚®ä»¶çš„æ¦‚ç‡ä¸å¥½æ±‚ï¼ˆä¸å¯é€šè¿‡ç›´æ¥è§‚æµ‹ã€ç»Ÿè®¡å¾—åˆ°ï¼‰ï¼Œä½†æ˜¯åŸºäºå·²ç»æœé›†å¥½çš„åƒåœ¾é‚®ä»¶æ ·æœ¬ï¼Œå»ç»Ÿè®¡ï¼ˆç›´æ¥è§‚æµ‹ï¼‰å…¶æ–‡æœ¬å†…éƒ¨å„ä¸ªè¯è¯­çš„æ¦‚ç‡å´éå¸¸æ–¹ä¾¿ã€‚è¿™å°±å¯ä»¥ç”¨è´å¶æ–¯æ–¹æ³•ã€‚</p><p>å¼•ç”³ä¸€æ­¥ï¼ŒåŸºäºæ ·æœ¬ç‰¹å¾å»åˆ¤æ–­å…¶æ‰€å±æ ‡ç­¾çš„æ¦‚ç‡ä¸å¥½æ±‚ï¼Œä½†æ˜¯åŸºäºå·²ç»æœé›†å¥½çš„æ‰“ä¸Šæ ‡ç­¾çš„æ ·æœ¬ï¼ˆæœ‰ç›‘ç£ï¼‰ï¼Œå´å¯ä»¥ç›´æ¥ç»Ÿè®¡å±äºåŒä¸€æ ‡ç­¾çš„æ ·æœ¬å†…éƒ¨å„ä¸ªç‰¹å¾çš„æ¦‚ç‡åˆ†å¸ƒã€‚å› æ­¤è´å¶æ–¯æ–¹æ³•çš„ç†è®ºè§†è§’é€‚ç”¨äºä¸€åˆ‡åˆ†ç±»é—®é¢˜çš„æ±‚è§£ã€‚</p><h3 id="15-2-å¤„ç†å¤šåˆ†ç±»é—®é¢˜"><a href="#15-2-å¤„ç†å¤šåˆ†ç±»é—®é¢˜" class="headerlink" title="15.2 å¤„ç†å¤šåˆ†ç±»é—®é¢˜"></a>15.2 å¤„ç†å¤šåˆ†ç±»é—®é¢˜</h3><p>å‰é¢æˆ‘ä»¬ä¸€ç›´åœ¨æ¢è®¨äºŒåˆ†ç±»ï¼ˆåˆ¤æ–­é¢˜ï¼‰é—®é¢˜ï¼Œç°åœ¨å¯ä»¥å¼•ç”³åˆ°å¤šåˆ†ç±»ï¼ˆå•é€‰é¢˜ï¼‰é—®é¢˜äº†ã€‚</p><p>è¿˜æ˜¯ç”¨é‚®ä»¶åˆ†ç±»çš„ä¾‹å­ï¼Œè¿™æ˜¯ç°åœ¨ä¸åªè¦åˆ¤æ–­åƒåœ¾é‚®ä»¶ï¼Œè¿˜è¦å°†æ­£å¸¸é‚®ä»¶ç»†åˆ†ä¸ºç§äººé‚®ä»¶ã€å·¥ä½œé‚®ä»¶ã€‚ç°åœ¨æœ‰è¿™3ç±»é‚®ä»¶å„1ä¸‡å°ä½œä¸ºæ ·æœ¬ã€‚éœ€è¦è®­ç»ƒå‡ºä¸€ä¸ªè´å¶æ–¯åˆ†ç±»å™¨ã€‚è¿™é‡Œä¾æ¬¡ç”¨Y1,Y2,Y3è¡¨ç¤ºè¿™ä¸‰ç±»é‚®ä»¶ï¼Œç”¨Xè¡¨ç¤ºè¢«åˆ¤æ–­çš„é‚®ä»¶ã€‚å¥—ç”¨è´å¶æ–¯å…¬å¼æœ‰ï¼š</p><blockquote><p>P(Y1|X)=P(X|Y1)P(Y1)P(X)</p><p>P(Y2|X)=P(X|Y2)P(Y2)P(X)</p><p>P(Y3|X)=P(X|Y3)P(Y3)P(X)</p></blockquote><p>é€šè¿‡æ¯”è¾ƒ3ä¸ªæ¦‚ç‡å€¼çš„å¤§å°å³å¯å¾—åˆ°Xæ‰€å±çš„åˆ†ç±»ã€‚å‘ç°ä¸‰ä¸ªå¼å­çš„åˆ†æ¯P(X) ä¸€æ ·ï¼Œæ¯”è¾ƒå¤§å°æ—¶å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œäºæ˜¯å°±å¯ä»¥ç”¨ä¸‹é¢è¿™ä¸€ä¸ªå¼å­è¡¨è¾¾ä¸Šé¢3å¼ï¼š</p><blockquote><p>P(Yi|X)âˆP(X|Yi)P(Yi)ï¼›i=1,2,3</p></blockquote><p>å…¶ä¸­ âˆ è¡¨ç¤ºâ€œæ­£æ¯”äºâ€ã€‚è€ŒP(X|Yi) åˆ™æœ‰ä¸ªç‰¹åˆ«é«˜é€¼æ ¼çš„åå­—å«åšâ€œ<strong>ä¼¼ç„¶å‡½æ•°</strong>â€ã€‚æˆ‘ä»¬ä¸Šå¤§å­¦çš„æ—¶å€™ä¹Ÿè¢«è¿™ä¸ªåå­—æå¾—æ™•æ™•ä¹ä¹çš„ï¼Œå…¶å®å®ƒä¹Ÿæ˜¯ä¸ªæ¦‚ç‡ï¼Œç›´æ¥ç†è§£æˆ<strong>â€œP(Yi|X) çš„é€†åæ¡ä»¶æ¦‚ç‡â€</strong> å°±æ–¹ä¾¿äº†ã€‚</p><p>è¿™é‡Œåªæ˜¯ä»¥åƒåœ¾é‚®ä»¶3åˆ†ç±»é—®é¢˜ä¸¾äº†ä¸ªä¾‹å­ï¼Œ<strong>å¯¹äºä»»æ„å¤šåˆ†ç±»çš„é—®é¢˜éƒ½å¯ä»¥ç”¨è¿™æ ·çš„æ€è·¯å»ç†è§£ã€‚æ¯”å¦‚æ–°é—»åˆ†ç±»ã€æƒ…æ„Ÿå–œæ€’å“€ä¹åˆ†ç±»ç­‰ç­‰</strong>ã€‚</p><h3 id="15-3-å…ˆéªŒæ¦‚ç‡çš„é—®é¢˜"><a href="#15-3-å…ˆéªŒæ¦‚ç‡çš„é—®é¢˜" class="headerlink" title="15.3 å…ˆéªŒæ¦‚ç‡çš„é—®é¢˜"></a>15.3 å…ˆéªŒæ¦‚ç‡çš„é—®é¢˜</h3><p>åœ¨åƒåœ¾é‚®ä»¶çš„ä¾‹å­ä¸­ï¼Œå…ˆéªŒæ¦‚ç‡éƒ½ç›¸ç­‰ï¼ŒP(Y1)=P(Y2)=P(Y3)=10000/30000=1/3ï¼Œæ‰€ä»¥ä¸Šé¢æ˜¯å¼å­åˆå¯ä»¥è¿›ä¸€æ­¥åŒ–ç®€ï¼š</p><blockquote><p>P(Yi|X)âˆP(X|Yi)ï¼›i=1,2,3</p></blockquote><p>åªéœ€æ¯”è¾ƒå³è¾¹å¼å­ï¼ˆä¹Ÿå°±æ˜¯â€œä¼¼ç„¶å‡½æ•°â€ï¼‰çš„å¤§å°å°±å¯ä»¥äº†ã€‚è¿™ç§æ–¹æ³•å°±æ˜¯ä¼ è¯´ä¸­çš„<strong>æœ€å¤§ä¼¼ç„¶æ³•</strong>:ä¸è€ƒè™‘å…ˆéªŒæ¦‚ç‡è€Œç›´æ¥æ¯”è¾ƒä¼¼ç„¶å‡½æ•°ã€‚</p><p>å…³äºé€‰å‡ºæœ€ä½³åˆ†ç±»Yiæ˜¯å¦è¦è€ƒè™‘å…ˆéªŒæ¦‚ç‡P(Yi)çš„é—®é¢˜ï¼Œæ›¾ç»åœ¨é¢‘ç‡å­¦æ´¾å’Œè´å¶æ–¯å­¦æ´¾ä¹‹é—´äº§ç”Ÿäº†æ¿€çƒˆçš„æ•™æ´¾å†²çªã€‚ç»Ÿè®¡å­¦å®¶ï¼ˆé¢‘ç‡å­¦æ´¾ï¼‰è¯´ï¼šæˆ‘ä»¬è®©æ•°æ®è‡ªå·±è¯´è¯ã€‚è¨€ä¸‹ä¹‹æ„å°±æ˜¯è¦æ‘’å¼ƒå…ˆéªŒæ¦‚ç‡ã€‚è€Œè´å¶æ–¯å­¦æ´¾æ”¯æŒè€…åˆ™è¯´ï¼šæ•°æ®ä¼šæœ‰å„ç§å„æ ·çš„åå·®ï¼Œè€Œä¸€ä¸ª<strong>é è°±çš„å…ˆéªŒæ¦‚ç‡</strong>åˆ™å¯ä»¥å¯¹è¿™äº›éšæœºå™ªéŸ³åšåˆ°å¥å£®ã€‚å¯¹æ­¤æœ‰å…´è¶£çš„åŒå­¦å¯ä»¥æ‰¾æ›´å¤šèµ„æ–™è¿›è¡Œäº†è§£ï¼Œæœ¬æ–‡åœ¨æ­¤ä¸åšæ›´å¤šçš„å¼•ç”³ï¼ŒåªåŸºäºåƒåœ¾é‚®ä»¶è¯†åˆ«çš„ä¾‹å­è¿›è¡Œæ¢è®¨ã€‚</p><p>æ¯”å¦‚æˆ‘ä»¬åœ¨é‡‡é›†åƒåœ¾é‚®ä»¶æ ·æœ¬çš„æ—¶å€™ï¼Œä¸å°å¿ƒdeleteæ‰äº†ä¸€åŠçš„æ•°æ®ï¼Œå°±å‰©ä¸‹5000å°é‚®ä»¶ã€‚åˆ™è®¡ç®—å‡ºæ¥çš„å…ˆéªŒæ¦‚ç‡ä¸º:</p><blockquote><p>P(Y1)=5000/25000=1/5ï¼Œ</p><p>P(Y2)=P(Y3)=10000/25000=2/5</p></blockquote><p>å¦‚æœè¿˜ç”¨è´å¶æ–¯æ–¹æ³•,å°±è¦åœ¨ä¼¼ç„¶å‡½æ•°åé¢ä¹˜ä¸Šå…ˆéªŒæ¦‚ç‡ã€‚æ¯”å¦‚ä¹‹å‰ç”¨æœ€å¤§ä¼¼ç„¶æ³•ç®—å‡ºY1 åƒåœ¾é‚®ä»¶çš„æ¦‚ç‡å¤§ï¼Œä½†æ˜¯å› ä¸ºP(Y1)ç‰¹åˆ«å°ï¼Œç”¨è´å¶æ–¯æ–¹æ³•å¾—å‡ºçš„ç»“æœæ˜¯Y2 ç§äººé‚®ä»¶çš„æ¦‚ç‡å¤§ã€‚é‚£ç›¸ä¿¡å“ªä¸ªå‘¢ï¼Ÿå…¶å®ï¼Œæˆ‘ä»¬åˆ æ‰äº†éƒ¨åˆ†å¸¦æ ‡ç­¾çš„æ ·æœ¬ï¼Œä»è®¡ç®—ç»“æœçœ‹P(Y1)ï¼ŒP(Y2)ï¼ŒP(Y3)çš„æ¦‚ç‡åˆ†å¸ƒå˜åŒ–äº†ï¼Œä½†å®é™…ä¸Š<strong>è¿™ä¸‰ä¸ªç±»åˆ«çš„çœŸå®åˆ†å¸ƒåº”è¯¥æ˜¯ä¸€ä¸ªå®¢è§‚çš„çŠ¶æ€ï¼Œä¸åº”è¯¥å› ä¸ºæˆ‘ä»¬çš„è®¡ç®—æ–¹æ³•è€Œå‘ç”Ÿå˜åŒ–</strong>ã€‚æ‰€ä»¥æ˜¯æˆ‘ä»¬è®¡ç®—å‡ºæ¥çš„å…ˆéªŒæ¦‚ç‡å¤±çœŸï¼Œåº”è¯¥æ”¾å¼ƒè¿™æ ·è®¡ç®—å‡ºæ¥çš„å…ˆéªŒæ¦‚ç‡ï¼Œè€Œç”¨æœ€å¤§ä¼¼ç„¶æ³•ã€‚ä½†å³ä¾¿æˆ‘ä»¬ä¸åˆ æ‰ä¸€åŠåƒåœ¾é‚®ä»¶ï¼Œè¿™ä¸‰ç±»é‚®ä»¶çš„åˆ†å¸ƒå°±çœŸçš„æ˜¯1:1:1é‚£æ ·å¹³å‡å—ï¼Ÿé‚£ä¹Ÿæœªå¿…ã€‚<strong>æˆ‘ä»¬åªæ˜¯æŒ‰1:1:1è¿™æ ·çš„æ–¹å¼è¿›è¡Œäº†æŠ½æ ·è€Œå·²ï¼ŒçœŸæ­£åœ¨é‚®ç®±é‡Œæ”¶åˆ°çš„è¿™ä¸‰ç±»é‚®ä»¶çš„åˆ†å¸ƒå¯èƒ½å¹¶ä¸æ˜¯è¿™æ ·</strong>ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œ<strong>åœ¨æˆ‘ä»¬å¯¹äºå…ˆéªŒæ¦‚ç‡ä¸€æ— æ‰€çŸ¥æ—¶ï¼Œåªèƒ½å‡è®¾æ¯ç§çŒœæµ‹çš„å…ˆéªŒæ¦‚ç‡æ˜¯å‡ç­‰çš„ï¼ˆå…¶å®è¿™ä¹Ÿæ˜¯äººç±»ç»éªŒçš„ç»“æœï¼‰ï¼Œè¿™ä¸ªæ—¶å€™å°±åªæœ‰ç”¨æœ€å¤§ä¼¼ç„¶äº†</strong>ã€‚åœ¨ç°å®è¿ç”¨è¿‡ç¨‹ä¸­å¦‚æœå‘ç°æœ€å¤§ä¼¼ç„¶æ³•æœ‰åå·®ï¼Œå¯ä»¥è€ƒè™‘å¯¹ä¸åŒçš„ä¼¼ç„¶å‡½æ•°è®¾å®šä¸€äº›ç³»æ•°æˆ–è€…é˜ˆå€¼ï¼Œä½¿å…¶æ¥è¿‘çœŸå®æƒ…å†µã€‚</p><p>ä½†æ˜¯ï¼Œ<strong>å¦‚æœæˆ‘ä»¬æœ‰è¶³å¤Ÿçš„è‡ªä¿¡ï¼Œè®­ç»ƒé›†ä¸­è¿™ä¸‰ç±»çš„æ ·æœ¬åˆ†å¸ƒçš„ç¡®å¾ˆæ¥è¿‘çœŸå®çš„æƒ…å†µï¼Œè¿™æ—¶å°±åº”è¯¥ç”¨è´å¶æ–¯æ–¹æ³•</strong>ã€‚éš¾æ€ªå‰é¢çš„è´å¶æ–¯å­¦æ´¾å¼ºè°ƒçš„æ˜¯â€œé è°±çš„å…ˆéªŒæ¦‚ç‡â€ã€‚æ‰€ä»¥è¯´<strong>è´å¶æ–¯å­¦æ´¾çš„é€‚ç”¨èŒƒå›´æ›´å¹¿ï¼Œå…³é”®è¦å…ˆéªŒæ¦‚ç‡é è°±ï¼Œè€Œé¢‘ç‡å­¦æ´¾æœ‰æ•ˆçš„å‰æä¹Ÿæ˜¯ä»–ä»¬çš„å…ˆéªŒæ¦‚ç‡åŒæ ·æ˜¯ç»éªŒç»Ÿè®¡çš„ç»“æœ</strong>ã€‚</p><h2 id="16-æœ´ç´ -è´å¶æ–¯æ–¹æ³•çš„å¸¸è§åº”ç”¨"><a href="#16-æœ´ç´ -è´å¶æ–¯æ–¹æ³•çš„å¸¸è§åº”ç”¨" class="headerlink" title="16. (æœ´ç´ )è´å¶æ–¯æ–¹æ³•çš„å¸¸è§åº”ç”¨"></a>16. (æœ´ç´ )è´å¶æ–¯æ–¹æ³•çš„å¸¸è§åº”ç”¨</h2><p>è¯´äº†è¿™ä¹ˆå¤šç†è®ºçš„é—®é¢˜ï¼Œå’±ä»¬å°±å¯ä»¥æ¢è®¨ä¸€ä¸‹(æœ´ç´ )è´å¶æ–¯æ–¹æ³•åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€äº›å¸¸è§åº”ç”¨äº†ã€‚ä»¥ä¸‹åªæ˜¯ä»åŸç†ä¸Šè¿›è¡Œæ¢è®¨ï¼Œå¯¹äºå…·ä½“çš„æŠ€æœ¯ç»†èŠ‚é¡¾åŠä¸å¤šã€‚</p><h3 id="16-1-è¤’è´¬åˆ†æ"><a href="#16-1-è¤’è´¬åˆ†æ" class="headerlink" title="16.1 è¤’è´¬åˆ†æ"></a>16.1 è¤’è´¬åˆ†æ</h3><p>ä¸€ä¸ªæ¯”è¾ƒå¸¸è§çš„åº”ç”¨åœºæ™¯æ˜¯æƒ…æ„Ÿè¤’è´¬åˆ†æã€‚æ¯”å¦‚ä½ è¦ç»Ÿè®¡å¾®åšä¸Šäººä»¬å¯¹ä¸€ä¸ªæ–°ä¸Šæ˜ ç”µå½±çš„è¤’è´¬ç¨‹åº¦è¯„ä»·ï¼šå¥½ç‰‡è¿˜æ˜¯çƒ‚ç‰‡ã€‚ä½†æ˜¯ä¸€æ¡ä¸€æ¡åœ°çœ‹å¾®åšæ˜¯æ ¹æœ¬çœ‹ä¸è¿‡æ¥ï¼Œåªèƒ½ç”¨è‡ªåŠ¨åŒ–çš„æ–¹æ³•ã€‚æˆ‘ä»¬å¯ä»¥æœ‰ä¸€ä¸ªå¾ˆç²—ç•¥çš„æ€è·¯ï¼š</p><ul><li>é¦–å…ˆæ˜¯ç”¨çˆ¬è™«å°†å¾®åšä¸Šæåˆ°è¿™ä¸ªç”µå½±åå­—çš„å¾®åšå…¨éƒ½æŠ“å–ä¸‹æ¥ï¼Œæ¯”å¦‚æœ‰10ä¸‡æ¡ã€‚</li><li>ç„¶åç”¨è®­ç»ƒå¥½çš„æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨åˆ†åˆ«åˆ¤æ–­è¿™äº›å¾®åšå¯¹ç”µå½±æ˜¯å¥½è¯„è¿˜æ˜¯å·®è¯„ã€‚</li><li>æœ€åç»Ÿè®¡å‡ºè¿™äº›å¥½è¯„çš„å½±è¯„å æ‰€æœ‰æ ·æœ¬ä¸­çš„æ¯”ä¾‹ï¼Œå°±èƒ½å½¢æˆå¾®åšç½‘å‹å¯¹è¿™ä¸ªç”µå½±ç»¼åˆè¯„ä»·çš„å¤§è‡´ä¼°è®¡ã€‚</li></ul><p>æ¥ä¸‹æ¥çš„æ ¸å¿ƒé—®é¢˜å°±æ˜¯è®­ç»ƒå‡ºä¸€ä¸ªé è°±çš„åˆ†ç±»å™¨ã€‚é¦–å…ˆéœ€è¦æœ‰æ‰“å¥½æ ‡ç­¾çš„æ–‡æœ¬ã€‚è¿™ä¸ªå¥½æ‰¾ï¼Œè±†ç“£å½±è¯„ä¸Šå°±æœ‰å¤§é‡ç½‘å‹å¯¹ä¹‹å‰ç”µå½±çš„è¯„ä»·ï¼Œå¹¶ä¸”å¯¹ç”µå½±è¿›è¡Œ1æ˜Ÿåˆ°5æ˜Ÿçš„è¯„ä»·ã€‚æˆ‘ä»¬å¯ä»¥è®¤ä¸º3æ˜Ÿä»¥ä¸Šçš„è¯„è®ºéƒ½æ˜¯å¥½è¯„ï¼Œ3æ˜Ÿä»¥ä¸‹çš„è¯„è®ºéƒ½æ˜¯å·®è¯„ã€‚è¿™æ ·å°±åˆ†åˆ«å¾—åˆ°äº†å¥½è¯„å·®è¯„ä¸¤ç±»çš„è¯­æ–™æ ·æœ¬ã€‚å‰©ä¸‹å°±å¯ä»¥ç”¨æœ´ç´ è´å¶æ–¯æ–¹æ³•è¿›è¡Œè®­ç»ƒäº†ã€‚åŸºæœ¬æ€è·¯å¦‚ä¸‹ï¼š</p><ul><li>è®­ç»ƒä¸æµ‹è¯•æ ·æœ¬ï¼šè±†ç“£å½±è¯„çš„ç½‘å‹è¯„è®ºï¼Œç”¨çˆ¬è™«æŠ“å–ä¸‹100ä¸‡æ¡ã€‚</li><li>æ ‡ç­¾ï¼š3æ˜Ÿä»¥ä¸Šçš„æ˜¯å¥½è¯„ï¼Œ3æ˜Ÿä»¥ä¸‹çš„æ˜¯å·®è¯„ã€‚</li><li>ç‰¹å¾ï¼šè±†ç“£è¯„è®ºåˆ†è¯åçš„è¯è¯­ã€‚ä¸€ä¸ªç®€å•çš„æ–¹æ³•æ˜¯åªé€‰æ‹©å…¶ä¸­çš„å½¢å®¹è¯ï¼Œç½‘ä¸Šæœ‰å¤§é‡çš„æƒ…ç»ªè¯åº“å¯ä»¥ä¸ºæˆ‘ä»¬æ‰€ç”¨ã€‚</li><li>ç„¶åå†ç”¨å¸¸è§„çš„æœ´ç´ è´å¶æ–¯æ–¹æ³•è¿›è¡Œè®­ç»ƒã€‚</li></ul><p>ä½†æ˜¯ç”±äºè‡ªç„¶è¯­è¨€çš„ç‰¹ç‚¹ï¼Œåœ¨æå–ç‰¹å¾çš„è¿‡ç¨‹å½“ä¸­ï¼Œæœ‰ä¸€äº›trickséœ€è¦æ³¨æ„ï¼š</p><ul><li><strong>å¯¹å¦å®šå¥è¿›è¡Œç‰¹åˆ«çš„å¤„ç†</strong>ã€‚æ¯”å¦‚è¿™å¥è¯â€œæˆ‘ä¸æ˜¯å¾ˆå–œæ¬¢éƒ¨ç”µå½±ï¼Œå› ä¸ºå®ƒè®©æˆ‘å¼€å¿ƒä¸èµ·æ¥ã€‚â€å…¶ä¸­ä¸¤ä¸ªå½¢å®¹è¯â€œå–œæ¬¢â€ã€â€œå¼€å¿ƒâ€éƒ½æ˜¯è¤’ä¹‰è¯ï¼Œä½†æ˜¯å› ä¸ºå¥å­çš„å¦å®šå¥ï¼Œæ‰€ä»¥æ•´ä½“æ˜¯è´¬ä¹‰çš„ã€‚æœ‰ä¸€ç§æ¯”è¾ƒç®€å•ç²—æš´çš„å¤„ç†æ–¹å¼ï¼Œå°±æ˜¯<strong>â€œå¯¹å¦å®šè¯ï¼ˆâ€œä¸â€ã€â€œéâ€ã€â€œæ²¡â€ç­‰ï¼‰ä¸å¥å°¾æ ‡ç‚¹ä¹‹é—´çš„æ‰€æœ‰å½¢å®¹è¯éƒ½é‡‡ç”¨å…¶å¦å®šå½¢å¼â€</strong> ã€‚åˆ™è¿™å¥è¯ä¸­æå–å‡ºæ¥çš„å½¢å®¹è¯å°±åº”è¯¥æ˜¯â€œä¸å–œæ¬¢â€å’Œâ€œä¸å¼€å¿ƒâ€ã€‚</li><li>ä¸€èˆ¬è¯´æ¥ï¼Œæœ€ç›¸å…³çš„æƒ…æ„Ÿè¯åœ¨ä¸€äº›æ–‡æœ¬ç‰‡æ®µä¸­ä»…ä»…å‡ºç°ä¸€æ¬¡ï¼Œè¯é¢‘æ¨¡å‹èµ·å¾—ä½œç”¨æœ‰é™ï¼Œç”šè‡³æ˜¯è´Ÿä½œç”¨ï¼Œ<strong>åˆ™ä½¿ç”¨ä¼¯åŠªåˆ©æ¨¡å‹ä»£æ›¿å¤šé¡¹å¼æ¨¡å‹</strong>ã€‚è¿™ç§æƒ…å†µåœ¨å¾®åšè¿™æ ·çš„å°ç¯‡å¹…æ–‡æœ¬ä¸­ä¼¼ä¹ä¸å¤ªæ˜æ˜¾ï¼Œä½†æ˜¯åœ¨åšå®¢ã€ç©ºé—´ã€è®ºå›ä¹‹ç±»å…è®¸é•¿ç¯‡å¹…æ–‡æœ¬å‡ºç°çš„å¹³å°ä¸­éœ€è¦æ³¨æ„ã€‚</li><li>å…¶å®ï¼Œå‰¯è¯å¯¹æƒ…æ„Ÿçš„è¯„ä»·æœ‰ä¸€å®šå½±å“ã€‚â€œä¸å¾ˆå–œæ¬¢â€ä¸â€œå¾ˆä¸å–œæ¬¢â€çš„ç¨‹åº¦å°±æœ‰å¾ˆå¤§å·®å¼‚ã€‚ä½†å¦‚æœæ˜¯æœ´ç´ è´å¶æ–¯æ–¹æ³•çš„è¯æ¯”è¾ƒéš¾å¤„ç†è¿™æ ·çš„æƒ…å†µã€‚æˆ‘ä»¬å¯ä»¥è€ƒè™‘ç”¨è¯­è¨€æ¨¡å‹æˆ–è€…åŠ å…¥è¯æ€§æ ‡æ³¨çš„ä¿¡æ¯è¿›è¡Œç»¼åˆåˆ¤æ–­ã€‚è¿™äº›å†…å®¹æˆ‘ä»¬å°†åœ¨ä¹‹åçš„æ–‡ç« è¿›è¡Œæ¢è®¨ã€‚</li></ul><p>å½“ç„¶ç»è¿‡ä»¥ä¸Šçš„å¤„ç†ï¼Œæƒ…æ„Ÿåˆ†æè¿˜æ˜¯ä¼šæœ‰ä¸€éƒ¨åˆ†è¯¯åˆ¤ã€‚è¿™é‡Œæ¶‰åŠåˆ°è®¸å¤šé—®é¢˜ï¼Œéƒ½æ˜¯æƒ…æ„Ÿåˆ†æçš„éš¾ç‚¹ï¼š</p><ul><li><strong>æƒ…ç»ªè¡¨è¾¾çš„å«è“„å¾®å¦™</strong>ï¼šâ€œå¯¼æ¼”ä½ å‡ºæ¥ï¼Œæˆ‘ä¿è¯ä¸æ‰“æ­»ä½ ã€‚â€ä½ è®©æœºå™¨æ€ä¹ˆåˆ¤æ–­æ˜¯è¤’è¿˜æ˜¯è´¬ï¼Ÿ</li><li><strong>è½¬æŠ˜æ€§è¡¨è¾¾</strong>ï¼šâ€œæˆ‘éå¸¸å–œæ¬¢è¿™äº›å¤§ç‰Œæ¼”å‘˜ï¼Œéå¸¸å´‡æ‹œè¿™ä¸ªå¯¼æ¼”ï¼Œéå¸¸èµèµè¿™ä¸ªå‰§æœ¬ï¼Œéå¸¸æ¬£èµä»–ä»¬çš„é¢„å‘Šç‰‡ï¼Œæˆ‘ç”šè‡³ä¸ºäº†è¿™éƒ¨å½±ç‰‡æ•´æ•´æœŸå¾…äº†ä¸€å¹´ï¼Œæœ€åè¿›äº†ç”µå½±é™¢å‘ç°è¿™æ˜¯ä¸ªå™©æ¢¦ã€‚â€ äº”ä¸ªè¤’ä¹‰çš„å½¢å®¹è¯ã€å‰¯è¯å¯¹ä¸€ä¸ªä¸é‚£ä¹ˆè´¬ä¹‰çš„è¯ã€‚æœºå™¨è‡ªç„¶åˆ¤æ–­æˆè¤’ä¹‰ï¼Œä½†è¿™å¥è¯æ˜¯å¦¥å¦¥çš„è´¬ä¹‰ã€‚</li></ul><h3 id="16-2-æ‹¼å†™çº é”™"><a href="#16-2-æ‹¼å†™çº é”™" class="headerlink" title="16.2 æ‹¼å†™çº é”™"></a>16.2 æ‹¼å†™çº é”™</h3><p>æ‹¼å†™çº é”™æœ¬è´¨ä¸Šä¹Ÿæ˜¯ä¸€ä¸ªåˆ†ç±»é—®é¢˜ã€‚ä½†æŒ‰ç…§é”™è¯¯ç±»å‹ä¸åŒï¼Œåˆåˆ†ä¸ºä¸¤ç§æƒ…å†µï¼š</p><ul><li>éè¯é”™è¯¯ï¼ˆNon-word Errorsï¼‰ï¼šæŒ‡é‚£äº›æ‹¼å†™é”™è¯¯åçš„è¯æœ¬èº«å°±ä¸åˆæ³•ï¼Œå¦‚å°†â€œwifiâ€å†™æˆâ€œwifyâ€ï¼›</li><li>çœŸè¯é”™è¯¯ï¼ˆReal-word Errorsï¼‰ï¼šæŒ‡é‚£äº›æ‹¼å†™é”™è¯¯åçš„è¯ä»ç„¶æ˜¯åˆæ³•çš„æƒ…å†µï¼Œå¦‚å°†â€œwifiâ€å†™æˆâ€œwifeâ€ã€‚</li></ul><p>çœŸè¯é”™è¯¯å¤æ‚ä¸€äº›ï¼Œæˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„æ–‡ç« ä¸­è¿›è¡Œæ¢è®¨ã€‚è€Œå¯¹äºéè¯é”™è¯¯ï¼Œå°±å¯ä»¥ç›´æ¥é‡‡ç”¨è´å¶æ–¯æ–¹æ³•ï¼Œå…¶åŸºæœ¬æ€è·¯å¦‚ä¸‹ï¼š</p><ul><li>æ ‡ç­¾ï¼šé€šè¿‡è®¡ç®—é”™è¯¯è¯è¯­çš„æœ€å°ç¼–è¾‘è·ç¦»ï¼ˆä¹‹å‰å’±ä»¬æåˆ°è¿‡çš„ï¼‰ï¼Œè·å–æœ€ç›¸ä¼¼çš„å€™é€‰è¯ï¼Œæ¯ä¸ªå€™é€‰è¯ä½œä¸ºä¸€ä¸ªåˆ†ç±»ã€‚</li><li>ç‰¹å¾ï¼šæ‹¼å†™é”™è¯¯çš„è¯æœ¬èº«ã€‚å› ä¸ºå®ƒå°±ä¸€ä¸ªç‰¹å¾ï¼Œæ‰€ä»¥æ²¡æœ‰ä»€ä¹ˆæ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾ã€æœ´ç´ è´å¶æ–¯å•¥çš„ã€‚å®ƒå°±æ˜¯çº¯è€Œåˆçº¯çš„è´å¶æ–¯æ–¹æ³•ã€‚</li><li>åˆ¤åˆ«å…¬å¼:</li></ul><blockquote><p>P(å€™é€‰è¯i|é”™è¯¯è¯)âˆP(é”™è¯¯è¯|å€™é€‰è¯i)P(å€™é€‰è¯i)ï¼›i=1,2,3,â€¦</p></blockquote><ul><li>è®­ç»ƒæ ·æœ¬1ï¼šè¯¥åœºæ™¯ä¸‹çš„æ­£å¸¸ç”¨è¯è¯­æ–™åº“ï¼Œç”¨äºè®¡ç®—P(å€™é€‰è¯i)ã€‚</li></ul><blockquote><p>P(å€™é€‰è¯i)=å€™é€‰è¯å‡ºç°çš„æ¬¡æ•°æ‰€æœ‰è¯å‡ºç°çš„æ¬¡æ•°</p></blockquote><ul><li>è®­ç»ƒæ ·æœ¬2ï¼šè¯¥åœºæ™¯ä¸‹é”™è¯¯è¯ä¸æ­£ç¡®è¯å¯¹åº”å…³ç³»çš„è¯­æ–™åº“ï¼Œç”¨äºè®¡ç®—P(é”™è¯¯è¯|å€™é€‰è¯i)</li></ul><blockquote><p>P(é”™è¯¯è¯|å€™é€‰è¯i)=å€™é€‰è¯è¢«æ‹¼å†™æˆè¯¥â€œé”™è¯¯è¯â€çš„æ¬¡æ•°å€™é€‰è¯å‡ºç°çš„æ¬¡æ•°</p></blockquote><p>ç”±äºè‡ªç„¶è¯­è¨€çš„ç‰¹ç‚¹ï¼Œæœ‰ä¸€äº›trickséœ€è¦æ³¨æ„ï¼š</p><ul><li>æ®ç»Ÿè®¡ï¼Œ80%çš„æ‹¼å†™é”™è¯¯ç¼–è¾‘è·ç¦»ä¸º1ï¼Œå‡ ä¹æ‰€æœ‰çš„æ‹¼å†™é”™è¯¯ç¼–è¾‘è·ç¦»å°äºç­‰äº2ã€‚æˆ‘ä»¬<strong>åªé€‰æ‹©ç¼–è¾‘è·ç¦»ä¸º1æˆ–2çš„è¯ä½œä¸ºå€™é€‰è¯ï¼Œè¿™æ ·å°±å¯ä»¥å‡å°‘å¤§é‡ä¸å¿…è¦çš„è®¡ç®—</strong>ã€‚</li><li>ç”±äºæˆ‘ä»¬åªé€‰æ‹©ç¼–è¾‘è·ç¦»ä¸º1æˆ–2çš„è¯ï¼Œå…¶å·®åˆ«åªæ˜¯ä¸€ä¸¤ä¸ªå­—æ¯çº§åˆ«å·®åˆ«ã€‚å› æ­¤è®¡ç®—ä¼¼ç„¶å‡½æ•°çš„æ—¶å€™ï¼Œ<strong>å¯ä»¥åªç»Ÿè®¡å­—æ¯å±‚é¢çš„ç¼–è¾‘é”™è¯¯ï¼Œè¿™æ ·æœé›†çš„æ ·æœ¬æ›´å¤šï¼Œæ›´æ»¡è¶³å¤§æ•°å®šå¾‹ï¼Œä¹Ÿæ›´ç®€å•</strong>ã€‚å¯¹äºç¼–è¾‘è·ç¦»ä¸º1çš„ä¼¼ç„¶å‡½æ•°è®¡ç®—å…¬å¼å¯ä»¥è¿›åŒ–ä¸ºï¼š</li></ul><blockquote><p>P(é”™è¯¯è¯|å€™é€‰è¯i)=â§â©â¨âªâªâªâªâªâªå­—æ¯â€œxyâ€è¢«æ‹¼å†™æˆâ€œyâ€çš„æ¬¡æ•°å­—æ¯â€œxyâ€å‡ºç°çš„æ¬¡æ•°,å­—æ¯â€œxâ€è¢«æ‹¼å†™æˆâ€œxyâ€çš„æ¬¡æ•°å­—æ¯â€œxâ€å‡ºç°çš„æ¬¡æ•°,å­—æ¯â€œxâ€è¢«æ‹¼å†™æˆâ€œyâ€çš„æ¬¡æ•°å­—æ¯â€œxâ€å‡ºç°çš„æ¬¡æ•°,å­—æ¯â€œxyâ€è¢«æ‹¼å†™æˆâ€œyxçš„æ¬¡æ•°å­—æ¯â€œxyâ€å‡ºç°çš„æ¬¡æ•°,</p></blockquote><ul><li><strong>é”®ç›˜ä¸Šä¸´è¿‘çš„æŒ‰é”®æ›´å®¹æ˜“æ‹¼å†™é”™è¯¯ï¼Œæ®æ­¤å¯ä»¥å¯¹ä¸Šé¢è¿™ä¸ªæ¡ä»¶æ¦‚ç‡è¿›è¡ŒåŠ æƒ</strong>ã€‚</li></ul><p><img src="blob:file:///3ecb40bd-0f8b-4742-ac50-e0912d5c6cc0" alt="\[é”®ç›˜\]"></p><h2 id="17-å†…å®¹å°ç»“"><a href="#17-å†…å®¹å°ç»“" class="headerlink" title="17. å†…å®¹å°ç»“"></a>17. å†…å®¹å°ç»“</h2><p>ä»å‰é¢å¤§å®¶åŸºæœ¬å¯ä»¥çœ‹å‡ºï¼Œå·¥ç¨‹åº”ç”¨ä¸åŒäºå­¦æœ¯ç†è®ºï¼Œæœ‰è®¸å¤štrickséœ€è¦è€ƒè™‘ï¼Œè€Œç†è®ºæœ¬è´¨å°±æ˜¯ç¿»æ¥å€’å»æŠ˜è…¾è´å¶æ–¯å…¬å¼ï¼Œéƒ½å¿«ç©å‡ºèŠ±æ¥äº†ã€‚</p><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;æœ´ç´ è´å¶æ–¯&quot;&gt;&lt;a href=&quot;#æœ´ç´ è´å¶æ–¯&quot; class=&quot;headerlink&quot; title=&quot;æœ´ç´ è´å¶æ–¯&quot;&gt;&lt;/a&gt;æœ´ç´ è´å¶æ–¯&lt;/h1&gt;&lt;h2 id=&quot;1-å¼•è¨€&quot;&gt;&lt;a href=&quot;#1-å¼•è¨€&quot; class=&quot;headerlink&quot; title=&quot;1. å¼•è¨€
      
    
    </summary>
    
      <category term="æœºå™¨å­¦ä¹ " scheme="http://mmyblog.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="æœ´ç´ è´å¶æ–¯" scheme="http://mmyblog.cn/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
  </entry>
  
  <entry>
    <title>GPTæ¨¡å‹</title>
    <link href="http://mmyblog.cn/2020/05/30/GPT%E6%A8%A1%E5%9E%8B/"/>
    <id>http://mmyblog.cn/2020/05/30/GPTæ¨¡å‹/</id>
    <published>2020-05-30T00:30:42.000Z</published>
    <updated>2020-06-09T01:26:50.690Z</updated>
    
    <content type="html"><![CDATA[<p>æ–‡æœ¬åˆ†ç±»ï¼š</p><p>æ•°æ®é›†</p><p>THUCNews æ•°æ®é›†å­é›†,é“¾æ¥: <a href="https://pan.baidu.com/s/1hugrfRu" target="_blank" rel="noopener">https://pan.baidu.com/s/1hugrfRu</a> å¯†ç : qfud</p><p>Language Understanding</p><ul><li><p>intent classification</p></li><li><p>dialogue state tracking</p></li><li><p>sentiment classification</p></li></ul><p>Language Generation</p><ul><li>information, structured, sentiment â€“&gt; language</li></ul><h1 id="å¿…è¯»è®ºæ–‡"><a href="#å¿…è¯»è®ºæ–‡" class="headerlink" title="å¿…è¯»è®ºæ–‡"></a>å¿…è¯»è®ºæ–‡</h1><h1 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h1><p>Radford et. al., Improving Language Understanding by Generative Pre-Training</p><p><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</a></p><p>è¿™ç¯‡æ–‡ç« æ¨å‡ºäº†generative pre-training + discriminative fine-tuningçš„æ–¹æ³•ï¼Œåæ¥ä¹Ÿè¢«BERTæ²¿ç”¨ã€‚task-aware input transformationä¹Ÿæ˜¯BERTå€Ÿç”¨çš„ä¸€ä¸ªç‚¹ã€‚å½“å¹´è¿™ç¯‡æ–‡ç« åˆšå‡ºæ¥çš„æ—¶å€™åˆ·æ¦œä¸€æ³¢ï¼Œä¸è¿‡ç¦»BERTå¤ªè¿‘ï¼Œå¯¼è‡´åæ¥å¤§å®¶éƒ½ä¸æ€ä¹ˆå…³å¿ƒè¿™ç¯‡æ–‡ç« äº†ã€‚</p><p><s>  a b c d e f </s></p><p>|        |  |  |  |  |  |</p><p>a       b c d e f  </p><p>1 0 0 0 0 0 0</p><p>1 1 0 0 0 0 0</p><p>1 1 1 0 0 0 0</p><p>1 1 1 1 0 0 0</p><p>1 1 1 1 1 0 0</p><p>1 1 1 1 1 1 0</p><h2 id="é¢„è®­ç»ƒ"><a href="#é¢„è®­ç»ƒ" class="headerlink" title="é¢„è®­ç»ƒ"></a>é¢„è®­ç»ƒ</h2><p>è¯­è¨€æ¨¡å‹objective</p><p><img src="https://uploader.shimo.im/f/jUXNKYwxjuUim5hM.png!thumbnail" alt="img"></p><p>Transformer Decoder</p><p><img src="https://uploader.shimo.im/f/2VVdkCN84SM8h2NA.png!thumbnail" alt="img"></p><p>è®­ç»ƒä½¿ç”¨BooksCorpusæ•°æ®é›†ï¼Œ7000æœ¬ä¹¦ã€‚</p><p>æ¨¡å‹å‚æ•°ï¼š</p><ul><li><p>12å±‚transformer decoder</p></li><li><p>768 hidden states, 12 attention heads</p></li><li><p>FFNå±‚æœ‰3072ç»´åº¦inner states</p></li></ul><h2 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine tuning"></a>Fine tuning</h2><p>ä½¿ç”¨æœ€åä¸€å±‚æœ€åä¸€ä¸ªtokençš„representationæ¥åštask specificçš„æ¨¡å‹fine tuning</p><p><img src="https://uploader.shimo.im/f/sMzH5VF5BcQsk8pY.png!thumbnail" alt="img"></p><p>ä¾ç„¶ä½¿ç”¨Log loss</p><p><img src="https://uploader.shimo.im/f/odJ6a19y3swDAYcI.png!thumbnail" alt="img"></p><p>ä½œè€…å‘ç°åœ¨fine tuningçš„æ—¶å€™ç»§ç»­ä½¿ç”¨è¯­è¨€æ¨¡å‹çš„lossä¹Ÿæœ‰å¥½å¤„</p><p><img src="https://uploader.shimo.im/f/z2GX6ss3yL8CrvaN.png!thumbnail" alt="img"></p><h2 id="Task-specific-Input-Transformations"><a href="#Task-specific-Input-Transformations" class="headerlink" title="Task-specific Input Transformations"></a>Task-specific Input Transformations</h2><p>å››ç§é—®é¢˜æœ‰å››ç§ä¸åŒçš„æ–‡æœ¬è¡¨ç¤ºæ–¹æ³•</p><p><img src="https://uploader.shimo.im/f/e3Ep9QOmlzI2YmM8.png!thumbnail" alt="img"></p><p>Natural Language Inference</p><ul><li><p>åˆ¤æ–­ä¸¤å¥è¯çš„å…³ç³»ï¼Œentailment æ‰¿æ¥å…³ç³»ï¼Œcontradiction çŸ›ç›¾å…³ç³»ï¼Œneutral ä¸­ç«‹å…³ç³»</p></li><li><p>åœ¨å‡ ä¸ªNLIä»»åŠ¡ä¸Šéƒ½æœ‰ä¸å°çš„æå‡</p></li></ul><p>Question Answering and Common Sense Reasoning</p><p>Semantic Similarity è¯­ä¹‰ç›¸ä¼¼åº¦</p><ul><li><p>Microsoft Paraphrase Corpus</p></li><li><p>Quora Question Pairs</p></li></ul><p>åˆ†ç±»é—®é¢˜ </p><ul><li><p>Corpus of Lingustic Acceptabilityï¼Œåˆ¤æ–­ä¸€å¥è¯çš„è¯­æ³•æ˜¯ä¸æ˜¯æ­£ç¡®ã€‚</p></li><li><p>Stanford Sentiment Treebank æƒ…æ„Ÿåˆ†ç±»</p></li></ul><h1 id="GPT2"><a href="#GPT2" class="headerlink" title="GPT2"></a>GPT2</h1><p>Radford et. al., <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener">Language Models are Unsupervised Multitask Learners</a></p><p><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener">https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</a></p><p>æ¯”GPTæ›´å¤§çš„è®­ç»ƒæ•°æ®é›†</p><ul><li>Common Crawlæ¥è‡ªç½‘é¡µçˆ¬å–ï¼Œåˆ é™¤äº†Wikipediaï¼Œæ€»å…±40GBæ•°æ®ã€‚</li></ul><p>evaluationä»»åŠ¡</p><ul><li><p>The Winograd Schema Challenge</p></li><li><p>LAMBADA dataset <a href="https://arxiv.org/pdf/1606.06031.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1606.06031.pdf</a></p></li></ul><p>å…³äºæ–‡æœ¬ç”Ÿæˆ</p><p><a href="https://arxiv.org/pdf/1904.09751.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.09751.pdf</a></p><h2 id="ä»£ç è§£è¯»"><a href="#ä»£ç è§£è¯»" class="headerlink" title="ä»£ç è§£è¯»"></a>ä»£ç è§£è¯»</h2><p>æˆ‘å¯¹ä»£ç æ·»åŠ äº†ä¸€äº›æ³¨é‡Š</p><p><a href="https://github.com/ZeweiChu/gpt-2/blob/master/src/model.py" target="_blank" rel="noopener">https://github.com/ZeweiChu/gpt-2/blob/master/src/model.py</a></p><p>huggingfaceä»£ç </p><p><a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_gpt2.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_gpt2.py</a></p><p>è‡ªåŠ¨æ£€æµ‹</p><p>def attention_mask(nd, ns, *, dtype):</p><p>â€‹    â€œâ€â€1â€™s in the lower triangle, counting from the lower right corner.</p><p>â€‹    å·¦ä¸‹è§’çš„ä¸‰è§’å½¢éƒ½æ˜¯1ï¼Œå…¶ä½™æ˜¯0ï¼Œç”¨äºç”Ÿæˆmaskã€‚</p><p>â€‹    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesnâ€™t produce garbage on TPUs.</p><p>â€‹    â€œâ€â€</p><p>â€‹    i = tf.range(nd)[:,None]</p><p>â€‹    j = tf.range(ns)</p><p>â€‹    m = i &gt;= j - ns + nd</p><p>â€‹    return tf.cast(m, dtype)</p><p>0 0 0 0 0</p><p>1 1 1 1 1</p><p>2 2 2 2 2</p><p>3 3 3 3 3</p><p>4 4 4 4 4</p><p>0 1 2 3 4 </p><p>1 0 0 0 0</p><p>1 1 0 0 0</p><p>1 1 1 0 0</p><p>1 1 1 1 0</p><p>1 1 1 1 1</p><p>w00 w01-inf w02-inf w03-inf w04-inf</p><p>w10 w11 w12-inf w13-inf w14-inf</p><p>w20 w21 w22 w23-inf w24-inf</p><p>w30 w31 w32 w33 w34-inf</p><p>w40 w41 w42 w43 w44</p><p>é˜…è¯»GPT2ä»£ç ï¼š<a href="https://github.com/ZeweiChu/gpt-2/blob/master/src/model.py" target="_blank" rel="noopener">https://github.com/ZeweiChu/gpt-2/blob/master/src/model.py</a></p><h1 id="Google-T5-Transformeré¢„è®­ç»ƒæ¨¡å‹å¤§æ€»ç»“"><a href="#Google-T5-Transformeré¢„è®­ç»ƒæ¨¡å‹å¤§æ€»ç»“" class="headerlink" title="Google T5: Transformeré¢„è®­ç»ƒæ¨¡å‹å¤§æ€»ç»“"></a>Google T5: Transformeré¢„è®­ç»ƒæ¨¡å‹å¤§æ€»ç»“</h1><p>è®ºæ–‡åœ°å€ï¼š<a href="https://arxiv.org/pdf/1910.10683.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1910.10683.pdf</a></p><p>ä»£ç +é¢„è®­ç»ƒæ¨¡å‹ï¼š<a href="https://github.com/google-research/text-to-text-transfer-transformer" target="_blank" rel="noopener">https://github.com/google-research/text-to-text-transfer-transformer</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;æ–‡æœ¬åˆ†ç±»ï¼š&lt;/p&gt;
&lt;p&gt;æ•°æ®é›†&lt;/p&gt;
&lt;p&gt;THUCNews æ•°æ®é›†å­é›†,é“¾æ¥: &lt;a href=&quot;https://pan.baidu.com/s/1hugrfRu&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://pan.baidu.co
      
    
    </summary>
    
      <category term="NLP" scheme="http://mmyblog.cn/categories/NLP/"/>
    
    
      <category term="GPT" scheme="http://mmyblog.cn/tags/GPT/"/>
    
  </entry>
  
  <entry>
    <title>BERTç³»åˆ—é¢„è®­ç»ƒæ¨¡å‹</title>
    <link href="http://mmyblog.cn/2020/05/30/BERT%E7%B3%BB%E5%88%97%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    <id>http://mmyblog.cn/2020/05/30/BERTç³»åˆ—é¢„è®­ç»ƒæ¨¡å‹/</id>
    <published>2020-05-30T00:29:27.000Z</published>
    <updated>2020-06-09T01:26:08.758Z</updated>
    
    <content type="html"><![CDATA[<p>BERTï¼šMasked Language Modelingé¢„è®­ç»ƒæ¨¡å‹</p><p>è®ºæ–‡åœ°å€ï¼š<a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1810.04805.pdf</a></p><p>ä¸­æ–‡ç¿»è¯‘ï¼š<a href="https://zhuanlan.zhihu.com/p/59775981" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/59775981</a></p><p> Language modelingé¢„è®­ç»ƒä»»åŠ¡</p><h2 id="Masked-Language-Model"><a href="#Masked-Language-Model" class="headerlink" title="Masked Language Model"></a>Masked Language Model</h2><p>å®Œå½¢å¡«ç©º</p><p>I study at Julyedu . </p><p>80% I study at [MASK] . </p><p>10% I study at Apple . </p><p>10% I study at Julyedu . </p><p>[CLS] I study at [MASK] .  [SEP] I love [MASK] language processing . [SEP]</p><p>â€“&gt; transformer encoder</p><p>o1, o2, o3, o4, o5, â€¦., o_n</p><p>o5 â€“&gt; Julyedu  cross entropy</p><p>o10 â€“&gt; natural cross entropy</p><p>o1 â€“&gt; True cross entropy</p><p>BERTè¯´ï¼šâ€œæˆ‘è¦ç”¨ transformer çš„ encodersâ€</p><p>Ernieä¸å±‘é“ï¼šâ€œå‘µå‘µï¼Œä½ ä¸èƒ½åƒBi-Lstmä¸€æ ·è€ƒè™‘æ–‡ç« â€</p><p>BERTè‡ªä¿¡å›ç­”é“ï¼šâ€œæˆ‘ä»¬ä¼šç”¨masksâ€</p><blockquote><p>è§£é‡Šä¸€ä¸‹Maskï¼š</p></blockquote><blockquote></blockquote><blockquote><p>è¯­è¨€æ¨¡å‹ä¼šæ ¹æ®å‰é¢å•è¯æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œä½†æ˜¯self-attentionçš„æ³¨æ„åŠ›åªä¼šæ”¾åœ¨è‡ªå·±èº«ä¸Šï¼Œé‚£ä¹ˆè¿™æ ·100%é¢„æµ‹åˆ°è‡ªå·±ï¼Œæ¯«æ— æ„ä¹‰ï¼Œæ‰€ä»¥ç”¨Maskï¼ŒæŠŠéœ€è¦é¢„æµ‹çš„è¯ç»™æŒ¡ä½ã€‚</p></blockquote><p>å¦‚ä¸‹å›¾ï¼š</p><p><img src="https://uploader.shimo.im/f/jvcJ8SPeBEwszR8M.png!thumbnail" alt="img"></p><h2 id="Two-sentence-Tasks"><a href="#Two-sentence-Tasks" class="headerlink" title="Two-sentence Tasks"></a>Two-sentence Tasks</h2><p>æˆ‘ä»¬å›é¡¾ä¸€ä¸‹OpenAI transformerå¤„ç†ä¸åŒä»»åŠ¡çš„è¾“å…¥è½¬æ¢ï¼Œä½ ä¼šå‘ç°åœ¨æŸäº›ä»»åŠ¡ä¸Šæˆ‘ä»¬éœ€è¦2ä¸ªå¥å­ä½œä¸ºè¾“å…¥ï¼Œå¹¶åšä¸€äº›æ›´ä¸ºæ™ºèƒ½çš„åˆ¤æ–­ï¼Œæ¯”å¦‚æ˜¯å¦ç›¸ä¼¼ï¼Œæ¯”å¦‚ ç»™å‡ºä¸€ä¸ªç»´åŸºç™¾ç§‘çš„å†…å®¹ä½œä¸ºè¾“å…¥ï¼ŒåŒæ—¶åœ¨æ”¾å…¥ä¸€æ¡é’ˆå¯¹è¯¥æ¡ç›®çš„é—®é¢˜ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„ç®—æ³•æ¨¡å‹èƒ½å¤Ÿå¤„ç†è¿™ä¸ªé—®é¢˜å—ï¼Ÿ</p><p>ä¸ºäº†ä½¿BERTæ›´å¥½çš„å¤„ç†2ä¸ªå¥å­ä¹‹é—´çš„å…³ç³»ï¼Œé¢„è®­ç»ƒçš„è¿‡ç¨‹è¿˜æœ‰ä¸€ä¸ªé¢å¤–çš„ä»»åŠ¡ï¼šç»™å®š2ä¸ªå¥å­ï¼ˆAå’ŒBï¼‰,Aä¸Bæ˜¯å¦ç›¸ä¼¼ï¼Ÿï¼ˆ0æˆ–è€…1ï¼‰</p><h2 id="ç‰¹æ®ŠNLPä»»åŠ¡"><a href="#ç‰¹æ®ŠNLPä»»åŠ¡" class="headerlink" title="ç‰¹æ®ŠNLPä»»åŠ¡"></a>ç‰¹æ®ŠNLPä»»åŠ¡</h2><p>BERTçš„è®ºæ–‡ä¸ºæˆ‘ä»¬ä»‹ç»äº†å‡ ç§BERTå¯ä»¥å¤„ç†çš„NLPä»»åŠ¡ï¼š</p><ol><li><p>çŸ­æ–‡æœ¬ç›¸ä¼¼ </p></li><li><p>æ–‡æœ¬åˆ†ç±»</p></li><li><p>QAæœºå™¨äºº</p></li><li><p>è¯­ä¹‰æ ‡æ³¨</p></li></ol><p><img src="https://uploader.shimo.im/f/yKFxOevBvMQXvjnv.png!thumbnail" alt="img"></p><h2 id="BERTç”¨åšç‰¹å¾æå–"><a href="#BERTç”¨åšç‰¹å¾æå–" class="headerlink" title="BERTç”¨åšç‰¹å¾æå–"></a>BERTç”¨åšç‰¹å¾æå–</h2><p>å¾®è°ƒæ–¹æ³•å¹¶ä¸æ˜¯ä½¿ç”¨BERTçš„å”¯ä¸€æ–¹æ³•ï¼Œå°±åƒELMoä¸€æ ·ï¼Œä½ å¯ä»¥ä½¿ç”¨é¢„é€‰è®­ç»ƒå¥½çš„BERTæ¥åˆ›å»ºè¯­å¢ƒåŒ–è¯åµŒå…¥ã€‚ç„¶åä½ å¯ä»¥å°†è¿™äº›åµŒå…¥æä¾›ç»™ç°æœ‰çš„æ¨¡å‹ã€‚</p><p><img src="https://uploader.shimo.im/f/uKUkG73gELQGry4L.png!thumbnail" alt="img"></p><p>å“ªä¸ªå‘é‡æœ€é€‚åˆä½œä¸ºä¸Šä¸‹æ–‡åµŒå…¥ï¼Ÿ æˆ‘è®¤ä¸ºè¿™å–å†³äºä»»åŠ¡ã€‚ æœ¬æ–‡è€ƒå¯Ÿäº†å…­ç§é€‰æ‹©ï¼ˆä¸å¾®è°ƒæ¨¡å‹ç›¸æ¯”ï¼Œå¾—åˆ†ä¸º96.4ï¼‰ï¼š</p><p><img src="https://uploader.shimo.im/f/bfpUyWE9YCEP9IU2.png!thumbnail" alt="img"></p><ul><li><p>Feature Extractionï¼šç‰¹å¾æå–</p></li><li><p>Finetuneï¼šå¾®è°ƒ</p></li></ul><h1 id="å¦‚ä½•ä½¿ç”¨BERT"><a href="#å¦‚ä½•ä½¿ç”¨BERT" class="headerlink" title="å¦‚ä½•ä½¿ç”¨BERT"></a>å¦‚ä½•ä½¿ç”¨BERT</h1><h2 id="BERTæºç "><a href="#BERTæºç " class="headerlink" title="BERTæºç "></a>BERTæºç </h2><p>æŸ¥çœ‹<a href="https://github.com/google-research/bert" target="_blank" rel="noopener">BERTä»“åº“</a>ä¸­çš„ä»£ç ï¼š</p><ol><li><p>è¯¥æ¨¡å‹åœ¨modeling.pyï¼ˆBertModelç±»ï¼‰ä¸­æ„å»ºï¼Œä¸vanilla Transformerç¼–ç å™¨å®Œå…¨ç›¸åŒã€‚</p></li><li><p>run_classifier.pyæ˜¯å¾®è°ƒè¿‡ç¨‹çš„ä¸€ä¸ªç¤ºä¾‹ã€‚å®ƒè¿˜æ„å»ºäº†ç›‘ç£æ¨¡å‹çš„åˆ†ç±»å±‚ã€‚å¦‚æœè¦æ„å»ºè‡ªå·±çš„åˆ†ç±»å™¨ï¼Œè¯·æŸ¥çœ‹è¯¥æ–‡ä»¶ä¸­çš„create_model()æ–¹æ³•ã€‚</p></li><li><p>å¯ä»¥ä¸‹è½½å‡ ç§é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹ã€‚æ¶µç›–102ç§è¯­è¨€çš„å¤šè¯­è¨€æ¨¡å‹ï¼Œè¿™äº›è¯­è¨€éƒ½æ˜¯åœ¨ç»´åŸºç™¾ç§‘çš„æ•°æ®åŸºç¡€ä¸Šè®­ç»ƒè€Œæˆçš„ã€‚</p></li><li><p>BERTä¸ä¼šå°†å•è¯è§†ä¸ºtokensã€‚ç›¸åï¼Œå®ƒæ³¨é‡WordPiecesã€‚ tokenization.pyæ˜¯å°†ä½ çš„å•è¯è½¬æ¢ä¸ºé€‚åˆBERTçš„wordPiecesçš„tokensizerã€‚</p></li></ol><p>å¯ä»¥æŸ¥çœ‹BERTçš„PyTorchå®ç° (<a href="https://github.com/huggingface/transformers)ã€‚" target="_blank" rel="noopener">https://github.com/huggingface/transformers)ã€‚</a> </p><ul><li><p>modeling: <a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py</a></p></li><li><p>BertEmbedding: wordpiece embedding + position embedding + token type embedding</p></li><li><p>BertSelfAttnetion: query, key, valueçš„å˜æ¢</p></li><li><p>BertSelfOutput: </p></li><li><p>BertIntermediate</p></li><li><p>BertOutput</p></li><li><p>BertForSequenceClassification</p></li><li><p>configuration: <a href="https://github.com/huggingface/transformers/blob/master/transformers/configuration_bert.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/transformers/configuration_bert.py</a></p></li><li><p>tokenization: <a href="https://github.com/huggingface/transformers/blob/master/transformers/tokenization_bert.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/transformers/tokenization_bert.py</a></p></li><li><p>DataProcessor: <a href="https://github.com/huggingface/transformers/blob/master/transformers/data/processors/glue.py#L194" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/transformers/data/processors/glue.py#L194</a></p></li></ul><h2 id="BERTæ¨¡å‹çš„ä½¿ç”¨"><a href="#BERTæ¨¡å‹çš„ä½¿ç”¨" class="headerlink" title="BERTæ¨¡å‹çš„ä½¿ç”¨"></a>BERTæ¨¡å‹çš„ä½¿ç”¨</h2><ul><li>æ–‡æœ¬åˆ†ç±»ï¼š<a href="https://github.com/huggingface/transformers/blob/master/examples/run_glue.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/examples/run_glue.py</a></li></ul><h1 id="BERTå‡çº§ç‰ˆ"><a href="#BERTå‡çº§ç‰ˆ" class="headerlink" title="BERTå‡çº§ç‰ˆ"></a>BERTå‡çº§ç‰ˆ</h1><h2 id="RoBERTaï¼šæ›´å¼ºå¤§çš„BERT"><a href="#RoBERTaï¼šæ›´å¼ºå¤§çš„BERT" class="headerlink" title="RoBERTaï¼šæ›´å¼ºå¤§çš„BERT"></a>RoBERTaï¼šæ›´å¼ºå¤§çš„BERT</h2><p>è®ºæ–‡åœ°å€ï¼š<a href="https://arxiv.org/pdf/1907.11692.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1907.11692.pdf</a></p><ul><li><p>åŠ å¤§è®­ç»ƒæ•°æ® 16GB -&gt; 160GBï¼Œæ›´å¤§çš„batch sizeï¼Œè®­ç»ƒæ—¶é—´åŠ é•¿</p></li><li><p>ä¸éœ€è¦NSP Loss: natural inference </p></li><li><p>ä½¿ç”¨æ›´é•¿çš„è®­ç»ƒ Sequence</p></li><li><p>Static vs. Dynamic Masking </p></li><li><p>æ¨¡å‹è®­ç»ƒæˆæœ¬åœ¨6ä¸‡ç¾é‡‘ä»¥ä¸Šï¼ˆä¼°ç®—ï¼‰</p></li></ul><h2 id="ALBERTï¼šå‚æ•°æ›´å°‘çš„BERT"><a href="#ALBERTï¼šå‚æ•°æ›´å°‘çš„BERT" class="headerlink" title="ALBERTï¼šå‚æ•°æ›´å°‘çš„BERT"></a>ALBERTï¼šå‚æ•°æ›´å°‘çš„BERT</h2><p>è®ºæ–‡åœ°å€ï¼š<a href="https://arxiv.org/pdf/1909.11942.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1909.11942.pdf</a></p><ul><li><p>ä¸€ä¸ªè½»é‡çº§çš„BERTæ¨¡å‹</p></li><li><p>æ ¸å¿ƒæ€æƒ³ï¼š</p></li><li><p>å…±äº«å±‚ä¸å±‚ä¹‹é—´çš„å‚æ•° ï¼ˆå‡å°‘æ¨¡å‹å‚æ•°ï¼‰</p></li><li><p>å¢åŠ å•å±‚å‘é‡ç»´åº¦</p></li></ul><h2 id="DistilBERTï¼šè½»é‡ç‰ˆBERT"><a href="#DistilBERTï¼šè½»é‡ç‰ˆBERT" class="headerlink" title="DistilBERTï¼šè½»é‡ç‰ˆBERT"></a>DistilBERTï¼šè½»é‡ç‰ˆBERT</h2><p>MNIST</p><p>0, 1, 2, 3, â€¦, 9</p><p>logits: [0.1, 0.6, â€¦, 0.01] q</p><p><strong>label: 2 [0, 0, 1, â€¦, 0] p</strong></p><p>loss: cross entropy loss -\sum_{i=1}^10 p_i*log q_i</p><p>loss: -log q_{label}</p><p>è®­ç»ƒä¸€ä¸ªStudent networkï¼Œmimic the behavior of the teacher network</p><p>teacher network: [0.1, 0.6, â€¦, 0.01] t</p><p><strong>student network</strong>: [s_1, s_2, .., s_10]</p><p>cross entropy loss: -sum_{i=1}^10 t_i * log s_i</p><p>4, 7</p><p><a href="https://arxiv.org/pdf/1910.01108.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1910.01108.pdf</a></p><ul><li><p>MLM, NSP</p></li><li><p>MLM: cross entropy loss: -\sum_{i=1}^k p_i log (q_i) = - log (q_{label})</p></li><li><p>teacher (MLM) = distribution</p></li><li><p>student: å­¦ä¹ distribution: -\sum_{i=1}^k p_teacher_i log (q_student_i)</p></li></ul><p>Patient Distillation</p><p><a href="https://arxiv.org/abs/1908.09355" target="_blank" rel="noopener">https://arxiv.org/abs/1908.09355</a></p><p><img src="https://uploader.shimo.im/f/FtKDArmN5UoEwpsF.png!thumbnail" alt="img"></p><h1 id="å‚è€ƒé˜…è¯»èµ„æ–™"><a href="#å‚è€ƒé˜…è¯»èµ„æ–™" class="headerlink" title="å‚è€ƒé˜…è¯»èµ„æ–™"></a>å‚è€ƒé˜…è¯»èµ„æ–™</h1><h3 id="BERT-Distillation"><a href="#BERT-Distillation" class="headerlink" title="BERT Distillation"></a>BERT Distillation</h3><p>å¯¹äºBERTæ¨¡å‹å‹ç¼©æ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥å‚è€ƒä»¥ä¸‹èµ„æ–™</p><ul><li>Patient Knowledge Distillation for BERT Model Compression  <a href="https://www.aclweb.org/anthology/D19-1441.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D19-1441.pdf</a></li></ul><p>å…³äºBERTæ¨¡å‹å‹ç¼©çš„ä¸€å¥—æ–¹æ³•</p><h3 id="ELECTRA"><a href="#ELECTRA" class="headerlink" title="ELECTRA"></a>ELECTRA</h3><p><a href="https://openreview.net/pdf?id=r1xMH1BtvB" target="_blank" rel="noopener">https://openreview.net/pdf?id=r1xMH1BtvB</a></p><p>ä½¿ç”¨GANè®­ç»ƒBERTæ¨¡å‹</p><p><img src="https://uploader.shimo.im/f/PJ9RGb3HpgIA4WYN.png!thumbnail" alt="img"></p><ul><li><p>Generatoré’ˆå¯¹[MASK]ä½ç½®ç”Ÿæˆå•è¯ï¼ŒDiscriminatoråˆ¤æ–­è¿™äº›å•è¯æ˜¯ç”±Generator (ä»[MASK]) ç”Ÿæˆçš„è¿˜æ˜¯åŸæœ¬å°±å­˜åœ¨çš„ã€‚</p></li><li><p>Discriminatorè¢«ç”¨äºdownstream task finetuningã€‚</p></li></ul><h3 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h3><p>æˆ‘åœ¨ä¸Šä¸€æœŸNLPå°±ä¸šç­ä¸­ä»‹ç»äº†XLNetï¼Œä¸è¿‡ç”±äºè¿‘äº›æ—¥å­BERTçš„å„ç§åŠ å¼ºç‰ˆå±‚å‡ºä¸ç©·ï¼ŒXLNetæ˜¾å¾—å¹¶ä¸ç‰¹åˆ«çªå‡ºã€‚æ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥å‚è€ƒä¸Šä¸€æœŸçš„è¯¾ä»¶ï¼š<a href="https://shimo.im/docs/PHqcpWtYjJjW3yH3" target="_blank" rel="noopener">https://shimo.im/docs/PHqcpWtYjJjW3yH3</a></p><p>XLNetçš„ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ä¹Ÿå¯ä»¥åœ¨Huggingfaceçš„ç‰ˆæœ¬ä¸­æ‰¾åˆ°ã€‚</p><h3 id="NLPé¢„è®­ç»ƒæ¨¡å‹ä¸²è®²"><a href="#NLPé¢„è®­ç»ƒæ¨¡å‹ä¸²è®²" class="headerlink" title="NLPé¢„è®­ç»ƒæ¨¡å‹ä¸²è®²"></a>NLPé¢„è®­ç»ƒæ¨¡å‹ä¸²è®²</h3><p>æˆ‘ä¹‹å‰åœ¨ä¸ƒæœˆåœ¨çº¿çš„å…¬å¼€è¯¾ä¸­ä½¿ç”¨çš„PPT</p><p>NLPé¢„è®­ç»ƒæ¨¡å‹.pdf1.9MB</p><h3 id="å‚è€ƒé˜…è¯»ï¼šThe-Illustrated-BERT-ELMo-and-co"><a href="#å‚è€ƒé˜…è¯»ï¼šThe-Illustrated-BERT-ELMo-and-co" class="headerlink" title="å‚è€ƒé˜…è¯»ï¼šThe Illustrated BERT, ELMo, and co."></a>å‚è€ƒé˜…è¯»ï¼šThe Illustrated BERT, ELMo, and co.</h3><p><a href="https://shimo.im/docs/Y6q3gX8yGGjpWqXx" target="_blank" rel="noopener">https://shimo.im/docs/Y6q3gX8yGGjpWqXx</a></p><ul><li><p>é˜…è¯»BertSelfAttentionä»£ç  <a href="https://github.com/huggingface/transformers/blob/master/transformers/modeling_bert.py#L190" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/transformers/modeling_bert.py#L190</a></p></li><li><p>é˜…è¯»run_glue.py <a href="https://github.com/huggingface/transformers/blob/master/examples/run_glue.py#L152" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/examples/run_glue.py#L152</a></p></li><li><p>é˜…è¯»BertForSequenceClassification <a href="https://github.com/huggingface/transformers/blob/master/transformers/modeling_bert.py#L970" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/transformers/modeling_bert.py#L970</a></p></li><li><p>é˜…è¯»glue.py <a href="https://github.com/huggingface/transformers/blob/master/transformers/data/processors/glue.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/transformers/data/processors/glue.py</a> ç”¨æ¥åšæ–‡æœ¬é¢„å¤„ç†</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;BERTï¼šMasked Language Modelingé¢„è®­ç»ƒæ¨¡å‹&lt;/p&gt;
&lt;p&gt;è®ºæ–‡åœ°å€ï¼š&lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxi
      
    
    </summary>
    
      <category term="NLP" scheme="http://mmyblog.cn/categories/NLP/"/>
    
    
      <category term="BERT" scheme="http://mmyblog.cn/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>é˜…è¯»ç†è§£</title>
    <link href="http://mmyblog.cn/2020/05/19/%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3/"/>
    <id>http://mmyblog.cn/2020/05/19/é˜…è¯»ç†è§£/</id>
    <published>2020-05-19T00:45:58.000Z</published>
    <updated>2020-06-09T01:25:31.415Z</updated>
    
    <content type="html"><![CDATA[<p>NLPå½“ä¸­çš„é˜…è¯»ç†è§£(Reading Comprehension, Question Answering)ä»»åŠ¡ä¸»è¦æ˜¯ä»¥ä¸‹å½¢å¼ï¼šç»™å®šä¸€äº›èƒŒæ™¯çŸ¥è¯†ï¼Œä¸»è¦æ˜¯ä¸€ç¯‡æ–‡ç« ï¼Œæœ‰æ—¶å€™ä¹Ÿå¯èƒ½æ˜¯ä¸€äº›ç»“æ„åŒ–çš„çŸ¥è¯†å›¾è°±ï¼Œç„¶åå›ç­”ä¸è¯¥èƒŒæ™¯çŸ¥è¯†çš„ç›¸å…³é—®é¢˜ã€‚</p><p>å¸¸è§çš„é—®é¢˜å’Œç­”æ¡ˆå½¢å¼æœ‰ï¼š</p><ul><li><p>å®Œå½¢å¡«ç©ºï¼šåœ¨æ–‡ç« ä¸­ç»™å®šä¸€ä¸ªç©ºä½å’Œä¸€äº›å€™é€‰ç­”æ¡ˆï¼Œæˆ‘ä»¬éœ€è¦æŠŠä¸€ä¸ªå€™é€‰ç­”æ¡ˆå¡«å……è¿›å»ã€‚</p></li><li><p>ç®€ç­”é¢˜ï¼šç»™å®šä¸€ç¯‡æ–‡ç« å’Œä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦ä»æ–‡ç« ä¸­å»æ‰¾ç­”æ¡ˆï¼Œä¸”è¿™ä¸ªç­”æ¡ˆä¸€å®šåœ¨æ–‡ç« ä¸­å‡ºç°è¿‡ã€‚SQuAD</p></li><li><p>é€‰æ‹©é¢˜ï¼šç»™å®šä¸€ç¯‡æ–‡ç« ï¼Œä¸€ä¸ªé—®é¢˜å’Œä¸€äº›å€™é€‰ç­”æ¡ˆï¼Œé€‰æ‹©ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆã€‚</p></li></ul><p>è¿˜æœ‰ä¸€äº›åœ¨ä¸Šè¿°é—®ç­”ä»»åŠ¡åŸºç¡€ä¸Šçš„æ‹“å±•æƒ…å†µï¼Œä¾‹å¦‚æœ‰çš„ä»»åŠ¡éœ€è¦åœ¨å¤šç¯‡æ–‡ç« çš„åŸºç¡€ä¸Šä½œç­”ï¼Œæœ‰çš„QAä»»åŠ¡éœ€è¦æˆ‘ä»¬è‡ªå·±æ¥æ¨ç†å’Œæ’°å†™ç­”æ¡ˆ (open domain)ï¼Œæ— æ³•ç›´æ¥ä»æ–‡ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚</p><p>æ•´ä¸ªQAé¢†åŸŸçš„å‘å±•ä¸»è¦éƒ½æ˜¯ä¾é ä¸€äº›æ•°æ®é›†çš„æå‡ºå’Œè§£å†³æ¥æ¨åŠ¨çš„ã€‚å¾€å¾€æ˜¯æœ‰äººåˆ¶ä½œäº†ä¸€ä¸ªæ•°æ®é›†å’Œä¸€ä¸ªQAä»»åŠ¡ï¼Œç„¶åå¤§å®¶å¼€å§‹æ¯”èµ›è°èƒ½æ›´å¥½åœ°è§£å†³å®ƒã€‚</p><p>æˆ‘è®¤ä¸ºæœ€å¥½çš„å­¦ä¹ æ–¹æ³•æ˜¯å»äº†è§£è¿™äº›QAæ•°æ®é›†ï¼ˆ<a href="http://nlpprogress.com/english/question_answering.htmlï¼‰ï¼Œé’ˆå¯¹è‡ªå·±æ„Ÿå…´è¶£çš„æ•°æ®é›†å»å¯»æ‰¾ç›¸åº”çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­äº†è§£QAçš„è§£å†³æ–¹æ³•ã€‚å°†æ¥å¦‚æœåœ¨å®é™…çš„åº”ç”¨åœºæ™¯ä¸­é‡åˆ°ç±»ä¼¼çš„QAä»»åŠ¡ï¼ˆä¾‹å¦‚ä¸€äº›å®¢æœæœºå™¨äººç­‰ï¼‰ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¯»æ‰¾åˆ°æ¯”è¾ƒç›¸å…³çš„QAæ•°æ®é›†ï¼Œä½¿ç”¨åœ¨è¿™äº›æ•°æ®é›†ä¸Šæœ€å¥½çš„è§£å†³æ–¹æ¡ˆæ¥è§£å†³è‡ªå·±çš„ä»»åŠ¡ã€‚" target="_blank" rel="noopener">http://nlpprogress.com/english/question_answering.htmlï¼‰ï¼Œé’ˆå¯¹è‡ªå·±æ„Ÿå…´è¶£çš„æ•°æ®é›†å»å¯»æ‰¾ç›¸åº”çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­äº†è§£QAçš„è§£å†³æ–¹æ³•ã€‚å°†æ¥å¦‚æœåœ¨å®é™…çš„åº”ç”¨åœºæ™¯ä¸­é‡åˆ°ç±»ä¼¼çš„QAä»»åŠ¡ï¼ˆä¾‹å¦‚ä¸€äº›å®¢æœæœºå™¨äººç­‰ï¼‰ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¯»æ‰¾åˆ°æ¯”è¾ƒç›¸å…³çš„QAæ•°æ®é›†ï¼Œä½¿ç”¨åœ¨è¿™äº›æ•°æ®é›†ä¸Šæœ€å¥½çš„è§£å†³æ–¹æ¡ˆæ¥è§£å†³è‡ªå·±çš„ä»»åŠ¡ã€‚</a></p><h2 id><a href="#" class="headerlink" title=" "></a> </h2><h1 id="ä¸€äº›æœ‰åçš„é˜…è¯»ç†è§£æ•°æ®é›†å’Œæ¨¡å‹"><a href="#ä¸€äº›æœ‰åçš„é˜…è¯»ç†è§£æ•°æ®é›†å’Œæ¨¡å‹" class="headerlink" title="ä¸€äº›æœ‰åçš„é˜…è¯»ç†è§£æ•°æ®é›†å’Œæ¨¡å‹"></a>ä¸€äº›æœ‰åçš„é˜…è¯»ç†è§£æ•°æ®é›†å’Œæ¨¡å‹</h1><h2 id="SQuAD-1-0-2-0"><a href="#SQuAD-1-0-2-0" class="headerlink" title="SQuAD 1.0/2.0"></a>SQuAD 1.0/2.0</h2><p>ä»æ–‡ç« ä¸­æ‰¾ç­”æ¡ˆ</p><p><a href="https://aclweb.org/anthology/D16-1264" target="_blank" rel="noopener">https://aclweb.org/anthology/D16-1264</a></p><p><img src="https://uploader.shimo.im/f/yqtsScSwls8OkJWs.png!thumbnail" alt="img"></p><h3 id="BiDAFæ¨¡å‹"><a href="#BiDAFæ¨¡å‹" class="headerlink" title="BiDAFæ¨¡å‹"></a>BiDAFæ¨¡å‹</h3><p>Bi-Directional Attention Fflow for Machine Comprehension</p><p><a href="https://arxiv.org/pdf/1611.01603.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1611.01603.pdf</a></p><p>2017å¹´çš„æ¨¡å‹ï¼Œç”¨äºè§£å†³SQuADä¹‹ç±»çš„é—®é¢˜ã€‚åæ¥çš„å¾ˆå¤šæ¨¡å‹éƒ½å‚è€ƒäº†è¯¥æ¨¡å‹çš„è®¾è®¡æ€æƒ³</p><p><img src="https://uploader.shimo.im/f/hL8lQitMAxMtYJ5w.png!thumbnail" alt="img"></p><p>é¢„æµ‹ï¼š start_pos, end_pos</p><p>å…¶ä»–ç›¸å…³æ¨¡å‹</p><p>Document Reader (single model)</p><p>r-net (single model)</p><p>QANet (single)</p><p><a href="https://github.com/allenai/bi-att-flow" target="_blank" rel="noopener">https://github.com/allenai/bi-att-flow</a></p><p>MCTest</p><p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/MCTest_EMNLP2013.pdf" target="_blank" rel="noopener">https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/MCTest_EMNLP2013.pdf</a></p><h3 id="BERTæ¨¡å‹"><a href="#BERTæ¨¡å‹" class="headerlink" title="BERTæ¨¡å‹"></a>BERTæ¨¡å‹</h3><p>æ¨¡å‹ <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1810.04805.pdf</a></p><p>ä»£ç  <a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1402" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1402</a></p><p><a href="https://github.com/huggingface/transformers/blob/master/examples/run_squad.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/examples/run_squad.py</a></p><h2 id="CNN-Daily-Mail"><a href="#CNN-Daily-Mail" class="headerlink" title="CNN/Daily Mail"></a>CNN/Daily Mail</h2><p>å®Œå½¢å¡«ç©ºç±»é—®é¢˜</p><p>Teaching Machines to Read and Comprehend</p><p><a href="https://arxiv.org/pdf/1506.03340.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.03340.pdf</a></p><p><img src="https://uploader.shimo.im/f/ppAcqx7DjtM3486H.png!thumbnail" alt="img"></p><h3 id="Attention-Sumæ¨¡å‹"><a href="#Attention-Sumæ¨¡å‹" class="headerlink" title="Attention Sumæ¨¡å‹"></a>Attention Sumæ¨¡å‹</h3><p><a href="https://arxiv.org/pdf/1603.01547.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.01547.pdf</a></p><p>Gated Attention Sumæ¨¡å‹æ˜¯è¯¥æ¨¡å‹çš„ä¸€ä¸ªæ‹“å±•å½¢å¼</p><p><a href="https://arxiv.org/abs/1606.01549" target="_blank" rel="noopener">https://arxiv.org/abs/1606.01549</a></p><h3 id="é™ˆä¸¹ç¦åœ¨CNN-Daily-Mailä¸Šçš„å·¥ä½œ"><a href="#é™ˆä¸¹ç¦åœ¨CNN-Daily-Mailä¸Šçš„å·¥ä½œ" class="headerlink" title="é™ˆä¸¹ç¦åœ¨CNN/Daily Mailä¸Šçš„å·¥ä½œ"></a>é™ˆä¸¹ç¦åœ¨CNN/Daily Mailä¸Šçš„å·¥ä½œ</h3><p>A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task</p><p><a href="https://www.aclweb.org/anthology/P16-1223" target="_blank" rel="noopener">https://www.aclweb.org/anthology/P16-1223</a></p><p><img src="https://uploader.shimo.im/f/BKipbLYDzic4bWbc.png!thumbnail" alt="img"></p><p>é¡ºä¾¿ä»‹ç»ä¸€ä¸‹ï¼Œ<a href="https://www.cs.princeton.edu/~danqic/" target="_blank" rel="noopener">é™ˆä¸¹ç¦</a>åœ¨QAé¢†åŸŸåšäº†å¾ˆå¤šå·¥ä½œï¼Œå¯¹QAæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥å‚è€ƒå¥¹çš„åšå£«è®ºæ–‡ã€‚</p><p><a href="https://www.cs.princeton.edu/~danqic/" target="_blank" rel="noopener">https://www.cs.princeton.edu/~danqic/</a></p><p><a href="https://www.cs.princeton.edu/~danqic/papers/thesis.pdf" target="_blank" rel="noopener">https://www.cs.princeton.edu/~danqic/papers/thesis.pdf</a></p><p><a href="https://arxiv.org/pdf/1506.03340.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.03340.pdf</a></p><p><a href="https://arxiv.org/pdf/1603.01547.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.01547.pdf</a></p><p><a href="http://www.cs.cmu.edu/~bdhingra/papers/ga_reader.pdf" target="_blank" rel="noopener">http://www.cs.cmu.edu/~bdhingra/papers/ga_reader.pdf</a></p><p><a href="https://arxiv.org/pdf/1611.07954.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1611.07954.pdf</a></p><h2 id="RACEæ•°æ®é›†"><a href="#RACEæ•°æ®é›†" class="headerlink" title="RACEæ•°æ®é›†"></a>RACEæ•°æ®é›†</h2><p>RACE: Large-scale ReAding Comprehension Dataset From Examinations</p><p><a href="https://arxiv.org/pdf/1704.04683.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1704.04683.pdf</a></p><p>RACEæ•°æ®é›†æ¥è‡ªä¸­å›½çš„ä¸­é«˜è€ƒè‹±è¯­é˜…è¯»ç†è§£é¢˜ã€‚</p><p>ä»£ç ï¼š</p><p><a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1204" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1204</a></p><p><a href="https://github.com/huggingface/transformers/blob/master/examples/utils_multiple_choice.py#L36" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/examples/utils_multiple_choice.py#L36</a></p><p><a href="https://github.com/huggingface/transformers/blob/master/examples/run_multiple_choice.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/master/examples/run_multiple_choice.py</a></p><p>SWAG</p><p>åæ¥å‡ºç°äº†å¾ˆå¤šçš„ä¸åŒæ–¹å‘çš„QAé—®é¢˜</p><ul><li><p>åŸºäºå¤šæ–‡æœ¬çš„ã€é•¿æ–‡ç« çš„é—®ç­”</p></li><li><p>narrative qa <a href="https://arxiv.org/pdf/1712.07040.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1712.07040.pdf</a></p></li><li><p>åŸºäºç»´åŸºç™¾ç§‘ï¼Œç»“åˆæ–‡æœ¬æœç´¢ç³»ç»Ÿçš„é—®ç­”</p></li><li><p>Dr QA <a href="https://arxiv.org/pdf/1704.00051.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1704.00051.pdf</a></p></li><li><p>åŸºäºèŠå¤©è®°å½•çš„é—®ç­” </p></li><li><p>QuAC : Question Answering in Context <a href="https://arxiv.org/pdf/1808.07036.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1808.07036.pdf</a></p></li></ul><p>å‚è€ƒä»¥ä¸‹é“¾æ¥</p><ul><li><p><a href="https://github.com/karthikncode/nlp-datasets#question-answering" target="_blank" rel="noopener">https://github.com/karthikncode/nlp-datasets#question-answering</a></p></li><li><p><a href="http://nlpprogress.com/english/question_answering.html" target="_blank" rel="noopener">http://nlpprogress.com/english/question_answering.html</a></p></li></ul><h2 id="åŸºäºå¤šæ–‡æœ¬çš„QAä»»åŠ¡"><a href="#åŸºäºå¤šæ–‡æœ¬çš„QAä»»åŠ¡" class="headerlink" title="åŸºäºå¤šæ–‡æœ¬çš„QAä»»åŠ¡"></a>åŸºäºå¤šæ–‡æœ¬çš„QAä»»åŠ¡</h2><p>HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering</p><p><a href="https://www.aclweb.org/anthology/D18-1259" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D18-1259</a></p><p>HotpotQAçš„ä¸»è¦ç‰¹ç‚¹æ˜¯ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šæ–‡æœ¬çš„QAä»»åŠ¡ã€‚ç»™å®šä¸€ç³»åˆ—çš„æ–‡ç« å’Œä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦ç»™å‡ºè¯¥é—®é¢˜çš„ç­”æ¡ˆï¼Œå¹¶ä¸”å›ç­”æˆ‘ä»¬æ˜¯ä»å“ªäº›ç›¸å…³çš„å¥å­ä¸­å¾—åˆ°é—®é¢˜çš„ç­”æ¡ˆçš„ã€‚</p><p>å·²ç»å…¬å¼€çš„å¯å‚è€ƒè®ºæ–‡</p><p>Hierarchical Graph Network for Multi-hop Question Answering <a href="https://arxiv.org/pdf/1911.00484.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1911.00484.pdf</a></p><p>Select, Answer and Explain: Interpretable Multi-hop Reading Comprehension over Multiple Documents <a href="https://arxiv.org/pdf/1911.03631.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1911.03631.pdf</a></p><h2 id="CoQA-åŸºäºå¯¹è¯çš„é—®ç­”æ•°æ®é›†"><a href="#CoQA-åŸºäºå¯¹è¯çš„é—®ç­”æ•°æ®é›†" class="headerlink" title="CoQA åŸºäºå¯¹è¯çš„é—®ç­”æ•°æ®é›†"></a>CoQA åŸºäºå¯¹è¯çš„é—®ç­”æ•°æ®é›†</h2><p>leaderboard <a href="https://stanfordnlp.github.io/coqa/" target="_blank" rel="noopener">https://stanfordnlp.github.io/coqa/</a></p><p>è®ºæ–‡ <a href="https://arxiv.org/abs/1808.07042" target="_blank" rel="noopener">https://arxiv.org/abs/1808.07042</a></p><p>æœç‹—æœ‰ä¸€ä¸ªBERTæ¨¡å‹çš„å®ç°</p><p><a href="https://github.com/sogou/SMRCToolkit" target="_blank" rel="noopener">https://github.com/sogou/SMRCToolkit</a></p><p>è¿™ä½åŒå­¦ä¹Ÿå®ç°äº†ä¸€äº›æ¨¡å‹</p><p><a href="https://github.com/jayelm/dialog-qa" target="_blank" rel="noopener">https://github.com/jayelm/dialog-qa</a></p><h2 id="ä¸­æ–‡æ•°æ®é›†"><a href="#ä¸­æ–‡æ•°æ®é›†" class="headerlink" title="ä¸­æ–‡æ•°æ®é›†"></a>ä¸­æ–‡æ•°æ®é›†</h2><h3 id="æ³•ç ”æ¯-é˜…è¯»ç†è§£æ•°æ®é›†"><a href="#æ³•ç ”æ¯-é˜…è¯»ç†è§£æ•°æ®é›†" class="headerlink" title="æ³•ç ”æ¯ é˜…è¯»ç†è§£æ•°æ®é›†"></a>æ³•ç ”æ¯ é˜…è¯»ç†è§£æ•°æ®é›†</h3><p><a href="http://cail.cipsc.org.cn/" target="_blank" rel="noopener">http://cail.cipsc.org.cn/</a></p><h3 id="è®¯é£æ¯-ä¸­æ–‡é˜…è¯»ç†è§£è¯„æµ‹"><a href="#è®¯é£æ¯-ä¸­æ–‡é˜…è¯»ç†è§£è¯„æµ‹" class="headerlink" title="è®¯é£æ¯ ä¸­æ–‡é˜…è¯»ç†è§£è¯„æµ‹"></a>è®¯é£æ¯ ä¸­æ–‡é˜…è¯»ç†è§£è¯„æµ‹</h3><p><a href="https://hfl-rc.github.io/cmrc2017/" target="_blank" rel="noopener">https://hfl-rc.github.io/cmrc2017/</a></p><p><a href="https://hfl-rc.github.io/cmrc2018/" target="_blank" rel="noopener">https://hfl-rc.github.io/cmrc2018/</a></p><p><a href="https://hfl-rc.github.io/cmrc2019/" target="_blank" rel="noopener">https://hfl-rc.github.io/cmrc2019/</a></p><p>åŒå­¦ä»¬å¯ä»¥æ‰¾åˆ°è¿™ä¸‰æ¬¡æ¯”èµ›çš„æ•°æ®é›†å’Œç›¸åº”çš„è¡¨ç°æœ€å¥½çš„æ¨¡å‹ä»£ç è¿›è¡Œå­¦ä¹ ã€‚</p><p>KBQA</p><p><a href="http://tcci.ccf.org.cn/conference/2018/papers/EV51.pdf" target="_blank" rel="noopener">http://tcci.ccf.org.cn/conference/2018/papers/EV51.pdf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;NLPå½“ä¸­çš„é˜…è¯»ç†è§£(Reading Comprehension, Question Answering)ä»»åŠ¡ä¸»è¦æ˜¯ä»¥ä¸‹å½¢å¼ï¼šç»™å®šä¸€äº›èƒŒæ™¯çŸ¥è¯†ï¼Œä¸»è¦æ˜¯ä¸€ç¯‡æ–‡ç« ï¼Œæœ‰æ—¶å€™ä¹Ÿå¯èƒ½æ˜¯ä¸€äº›ç»“æ„åŒ–çš„çŸ¥è¯†å›¾è°±ï¼Œç„¶åå›ç­”ä¸è¯¥èƒŒæ™¯çŸ¥è¯†çš„ç›¸å…³é—®é¢˜ã€‚&lt;/p&gt;
&lt;p&gt;å¸¸è§çš„é—®é¢˜å’Œç­”æ¡ˆå½¢å¼æœ‰ï¼š&lt;/p
      
    
    </summary>
    
      <category term="NLP" scheme="http://mmyblog.cn/categories/NLP/"/>
    
    
      <category term="QA" scheme="http://mmyblog.cn/tags/QA/"/>
    
  </entry>
  
  <entry>
    <title>Transformeræ¨¡å‹è§£è¯»</title>
    <link href="http://mmyblog.cn/2020/05/18/Transformer%E6%A8%A1%E5%9E%8B%E8%A7%A3%E8%AF%BB/"/>
    <id>http://mmyblog.cn/2020/05/18/Transformeræ¨¡å‹è§£è¯»/</id>
    <published>2020-05-18T00:28:15.000Z</published>
    <updated>2020-06-09T01:32:41.815Z</updated>
    
    <content type="html"><![CDATA[<p>contextualized word vectors</p><p>RNN, LSTM</p><p>RNN(I study at Julyedu.) â€“&gt; RNN(I)-&gt;h1, RNN(study, h1)-&gt;h2, RNN(at, h2)-&gt;h3. </p><p>Encoder. æˆ‘å¯ä»¥åŒæ—¶è§‚çœ‹å…¨å±€ä¿¡æ¯ã€‚</p><p>query, keys, values</p><p>q1, q2, .., q5</p><p>k1, k2, k3, k4, k5</p><p>score(q, k1), score(q, k2), â€¦, score(q, k5)</p><p>v1, v2, v3, v4, v5</p><p>\sum_{i=1}^5 func(score_i) v_i</p><p>dot(a, b)</p><p>mean</p><p>var(dot(a, b))</p><p>dot(a, b) = a1<em>b1 + a2</em>b2. â€¦. </p><p>E(dot(a, b)) = n * E(ai*bi)</p><p>var(dot(a, b)) = E(dot(a, b)^2) - E(dot(a, b))^2</p><p>affine transformation</p><p>WX+b</p><p>Attention(Q, K, V ) = softmax(QKT âˆš dk )V</p><p>Q : seq_len, hid_size</p><p>K^T:  hid_size, seq_len</p><p>V: seq_len, hid_size</p><p>QK^T : seq_len, seq_len</p><p>QK^T V: seq_len, hid_size</p><p>[emb_w(x), emb_p(i)]W â€“&gt; </p><p>è¿‘ä¸¤å¹´æ¥ï¼ŒNLPé¢†åŸŸçš„æ¨¡å‹ç ”ç©¶å·²ç»è¢«transformeræ¨¡å‹ä»¥åŠå®ƒçš„å„ç§å˜ç§ç»™å é¢†äº†ã€‚Transformeræ¨¡å‹çš„ç«çˆ†æœ‰å¾ˆå¤šåŸå› ï¼Œä¾‹å¦‚ï¼š</p><ul><li><p>æ¨¡å‹ç®€å•æ˜“æ‡‚ï¼Œencoderå’Œdecoderæ¨¡å—é«˜åº¦ç›¸ä¼¼ä¸”é€šç”¨</p></li><li><p>ï¼ˆencoderï¼‰å®¹æ˜“å¹¶è¡Œï¼Œæ¨¡å‹è®­ç»ƒé€Ÿåº¦å¿«</p></li><li><p>æ•ˆæœæ‹”ç¾¤ï¼Œåœ¨NMTç­‰é¢†åŸŸéƒ½å–å¾—äº†state-of-the-artçš„æ•ˆæœ</p></li></ul><p>è®ºæ–‡åœ°å€</p><ul><li><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a> </li></ul><p>ä¸‹é¢çš„æ–‡ç« ç¿»è¯‘è‡ª</p><ul><li><p><a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">The Illustrated Transformer</a></p></li><li><p><a href="https://blog.csdn.net/yujianmin1990/article/details/85221271" target="_blank" rel="noopener">ä¸­æ–‡ç¿»è¯‘</a></p></li></ul><p>é«˜å±‹å»ºç“´åœ°è¯´ï¼ŒTransformeræ¨¡å‹æ‹¿åˆ°ä¸€ä¸ªåºåˆ—ï¼Œç”¨æ¥ç”Ÿæˆå¦ä¸€ä¸ªåºåˆ—ã€‚</p><p><img src="https://uploader.shimo.im/f/vkvOEopS6TMPw0SL.png!thumbnail" alt="img"></p><p>æ‰“å¼€è¿™ä¸ªé»‘ç®±ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°å…¶ä¸­åŒ…å«äº†ä¸¤ä¸ªéƒ¨åˆ†ï¼Œencoderså’Œdecodersã€‚</p><p><img src="https://uploader.shimo.im/f/hraPVC4iek06oDwt.png!thumbnail" alt="img"></p><p>å…¶ä¸­encoderså’Œdecoderséƒ½æ˜¯ä¸¤ä¸ªå †å æ¶æ„ã€‚ä¸€å±‚ä¸€å±‚åŒè´¨çš„ç»“æ„å †å åˆ°ä¸€èµ·ï¼Œç»„æˆäº†ç¼–ç å™¨å’Œè§£ç å™¨ã€‚</p><p><img src="https://uploader.shimo.im/f/WFbnFyb8peoeJuXW.png!thumbnail" alt="img"></p><p>é¦–å…ˆæˆ‘ä»¬æ‰“å¼€æ¯ä¸ªencoderæ¥å‚è§‚ä¸€ä¸‹å…¶ä¸­åŒ…å«çš„å†…å®¹ï¼š</p><p><img src="https://uploader.shimo.im/f/c7oNzYNSIoceXYFZ.png!thumbnail" alt="img"></p><p>æ¯ä¸€ä¸ªencoderéƒ½åŒ…å«äº†ä¸€ä¸ªè‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰å±‚å’Œä¸€ä¸ªFeed Forward Neural Networkã€‚</p><p>encoderçš„è¾“å…¥é¦–å…ˆä¼šç»è¿‡ä¸€ä¸ªself-attentionå±‚ã€‚self-attentionçš„ä½œç”¨æ˜¯è®©æ¯ä¸ªå•è¯å¯ä»¥çœ‹åˆ°è‡ªå·±å’Œå…¶ä»–å•è¯çš„å…³ç³»ï¼Œå¹¶ä¸”å°†è‡ªå·±è½¬æ¢æˆä¸€ä¸ªä¸æ‰€æœ‰å•è¯ç›¸å…³çš„ï¼Œ<strong>focusåœ¨è‡ªå·±èº«ä¸Šçš„è¯å‘é‡(?)</strong>ã€‚</p><p>self-attentionä¹‹åçš„è¾“å‡ºä¼šå†ç»è¿‡ä¸€å±‚feed-forwardç¥ç»ç½‘ç»œã€‚æ¯ä¸ªä½ç½®çš„è¾“å‡ºè¢«åŒæ ·çš„feed-forward networkå¤„ç†ã€‚</p><p>decoderä¹Ÿæœ‰åŒæ ·çš„self-attentionå’Œfeed-forwardç»“æ„ï¼Œä½†æ˜¯åœ¨è¿™ä¸¤å±‚ä¹‹é—´è¿˜æœ‰ä¸€å±‚encoder-decoder attentionå±‚ï¼Œå¸®åŠ©decoderå…³æ³¨åˆ°æŸä¸€äº›ç‰¹åˆ«éœ€è¦å…³æ³¨çš„encoderä½ç½®ã€‚</p><h2 id="Tensorçš„å˜åŒ–"><a href="#Tensorçš„å˜åŒ–" class="headerlink" title="Tensorçš„å˜åŒ–"></a>Tensorçš„å˜åŒ–</h2><p><img src="https://uploader.shimo.im/f/Hmbb5V4mEJkBYpFS.png!thumbnail" alt="img"></p><h2 id="ç¼–ç å™¨"><a href="#ç¼–ç å™¨" class="headerlink" title="ç¼–ç å™¨"></a>ç¼–ç å™¨</h2><p>ä¸‹é¢æˆ‘ä»¬æ¥è¯¦ç»†è§£è¯»ä¸€ä¸‹ç¼–ç å™¨çš„å·¥ä½œã€‚</p><p><img src="https://uploader.shimo.im/f/MzJmdqVJiSUz4DT9.png!thumbnail" alt="img"></p><h3 id="Self-Attentionæœºåˆ¶"><a href="#Self-Attentionæœºåˆ¶" class="headerlink" title="Self-Attentionæœºåˆ¶"></a>Self-Attentionæœºåˆ¶</h3><p>æˆ‘ä»¬è€ƒè™‘ç”¨Transformeræ¨¡å‹ç¿»è¯‘ä¸‹é¢è¿™ä¸€å¥è¯ï¼š</p><p>â€œThe animal didnâ€™t cross the street because it was too tiredâ€ã€‚</p><p>å½“æˆ‘ä»¬ç¿»è¯‘åˆ° it çš„æ—¶å€™ï¼Œæˆ‘ä»¬çŸ¥é“ it æŒ‡ä»£çš„æ˜¯ animal è€Œä¸æ˜¯ streetã€‚æ‰€ä»¥ï¼Œå¦‚æœæœ‰åŠæ³•å¯ä»¥è®© it å¯¹åº”ä½ç½®çš„ embedding é€‚å½“åŒ…å« animal çš„ä¿¡æ¯ï¼Œå°±ä¼šéå¸¸æœ‰ç”¨ã€‚self-attentionçš„å‡ºç°å°±æ˜¯ä¸ºäº†å®Œæˆè¿™ä¸€ä»»åŠ¡ã€‚</p><p>å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œself attnetionä¼šè®©å•è¯ it å’Œ æŸäº›å•è¯å‘ç”Ÿæ¯”è¾ƒå¼ºçš„è”ç³»ï¼Œå¾—åˆ°æ¯”è¾ƒæçš„attentionåˆ†æ•°ã€‚</p><p><img src="https://uploader.shimo.im/f/UsNXjO1OpN0usuAg.png!thumbnail" alt="img"></p><p>weight(The) = softmax(v(it) * v(The) / \sqrt(d))</p><p>weight(The) = softmx(Query(It) * Key(The) / \sqrt(d))</p><p>\sum_{word} weight(word) * Value(word)</p><h3 id="Self-attentionçš„ç»†èŠ‚"><a href="#Self-attentionçš„ç»†èŠ‚" class="headerlink" title="Self-attentionçš„ç»†èŠ‚"></a>Self-attentionçš„ç»†èŠ‚</h3><p>ä¸ºäº†å®ç° self-attentionï¼Œæ¯ä¸ªè¾“å…¥çš„ä½ç½®éœ€è¦äº§ç”Ÿä¸‰ä¸ªå‘é‡ï¼Œåˆ†åˆ«æ˜¯ <strong>Query å‘é‡ï¼ŒKey å‘é‡å’Œ Value å‘é‡</strong>ã€‚è¿™äº›å‘é‡éƒ½æ˜¯ç”±è¾“å…¥ embedding é€šè¿‡ä¸‰ä¸ª matrices ï¼ˆä¹Ÿå°±æ˜¯çº¿æ€§å˜åŒ–ï¼‰äº§ç”Ÿçš„ã€‚</p><p>æ³¨æ„åˆ°åœ¨Transformeræ¶æ„ä¸­ï¼Œè¿™äº›æ–°çš„å‘é‡æ¯”åŸæ¥çš„è¾“å…¥å‘é‡è¦å°ï¼ŒåŸæ¥çš„å‘é‡æ˜¯512ç»´ï¼Œè½¬å˜åçš„ä¸‰ä¸ªå‘é‡éƒ½æ˜¯64ç»´ã€‚</p><p><img src="https://uploader.shimo.im/f/MAqlj67rbPYBI7Ad.png!thumbnail" alt="img"></p><p>ç¬¬äºŒæ­¥æ˜¯<strong>è®¡ç®—åˆ†æ•°</strong>ã€‚å½“æˆ‘ä»¬åœ¨ç”¨self-attention encodeæŸä¸ªä½ç½®ä¸Šçš„æŸä¸ªå•è¯çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¸Œæœ›çŸ¥é“è¿™ä¸ªå•è¯å¯¹åº”çš„å¥å­ä¸Šå…¶ä»–å•è¯çš„åˆ†æ•°ã€‚å…¶ä»–å•è¯æ‰€å¾—åˆ°çš„åˆ†æ•°è¡¨ç¤ºäº†å½“æˆ‘ä»¬encodeå½“å‰å•è¯çš„æ—¶å€™ï¼Œåº”è¯¥æ”¾å¤šå°‘çš„å…³æ³¨åº¦åœ¨å…¶ä½™çš„æ¯ä¸ªå•è¯ä¸Šã€‚åˆæˆ–è€…è¯´ï¼Œå…¶ä»–å•è¯å’Œæˆ‘å½“å‰çš„å•è¯æœ‰å¤šå¤§çš„ç›¸å…³æ€§æˆ–è€…ç›¸ä¼¼æ€§ã€‚</p><p>åœ¨transformeræ¨¡å‹ä¸­ï¼Œè¿™ä¸ªåˆ†æ•°æ˜¯ç”±query vectorå’Œkey vectoråšç‚¹ç§¯ï¼ˆdot productï¼‰æ‰€å¾—çš„ç»“æœã€‚æ‰€ä»¥è¯´ï¼Œå½“æˆ‘ä»¬åœ¨å¯¹ç¬¬ä¸€ä¸ªå•è¯åšself-attentionå¤„ç†çš„æ—¶å€™ï¼Œç¬¬ä¸€ä¸ªå•è¯çš„åˆ†æ•°æ˜¯q_1å’Œk_1çš„ç‚¹ç§¯ï¼Œç¬¬äºŒä¸ªåˆ†æ•°æ˜¯q_1å’Œk_2çš„åˆ†æ•°ã€‚</p><p><img src="https://uploader.shimo.im/f/kW9cJM4TjTc9xtMV.png!thumbnail" alt="img"></p><p>ç¬¬ä¸‰æ­¥å’Œç¬¬å››æ­¥æ˜¯å°†è¿™äº›åˆ†æ•°é™¤ä»¥8ã€‚8è¿™ä¸ªæ•°å­—æ˜¯64çš„å¼€æ–¹ï¼Œä¹Ÿå°±æ˜¯key vectorçš„ç»´åº¦çš„å¼€æ–¹ã€‚æ®è¯´è¿™ä¹ˆåšå¯ä»¥ç¨³å®šæ¨¡å‹çš„gradientã€‚ç„¶åæˆ‘ä»¬å°†è¿™äº›åˆ†æ•°ä¼ å…¥softmaxå±‚äº§ç”Ÿä¸€äº›ç¬¦åˆæ¦‚ç‡åˆ†å¸ƒçš„probability scoresã€‚</p><p><img src="https://uploader.shimo.im/f/6kTtVymp0XgZCDh0.png!thumbnail" alt="img"></p><p>softmax = exp(x_i) / sum exp(x_i)</p><p>è¿™äº›åˆ†æ•°å°±è¡¨ç¤ºäº†åœ¨å¤„ç†å½“å‰å•è¯çš„æ—¶å€™æˆ‘ä»¬åº”è¯¥åˆ†é…å¤šå°‘çš„å…³æ³¨åº¦ç»™å…¶ä»–å•è¯ã€‚</p><p>ç¬¬äº”æ­¥æ˜¯å°†æ¯ä¸ªvalue vectorä¹˜ä»¥å®ƒä»¬å„è‡ªçš„attention scoreã€‚ç¬¬å…­æ­¥æ˜¯æŠŠè¿™äº›weighted value vectorsç›¸åŠ ï¼Œæˆä¸ºå½“å‰å•è¯çš„vectorè¡¨ç¤ºã€‚</p><p><img src="https://uploader.shimo.im/f/FrqMNrQrlo0tLBgV.png!thumbnail" alt="img"></p><p>å¾—åˆ°äº†self-attentionç”Ÿæˆçš„è¯å‘é‡ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†å®ƒä»¬ä¼ å…¥feed-forward networkäº†ã€‚</p><h3 id="Self-Attentionä¸­çš„çŸ©é˜µè¿ç®—"><a href="#Self-Attentionä¸­çš„çŸ©é˜µè¿ç®—" class="headerlink" title="Self-Attentionä¸­çš„çŸ©é˜µè¿ç®—"></a>Self-Attentionä¸­çš„çŸ©é˜µè¿ç®—</h3><p>é¦–å…ˆï¼Œæˆ‘ä»¬è¦å¯¹æ¯ä¸€ä¸ªè¯å‘é‡è®¡ç®—Query, Keyå’ŒValueçŸ©é˜µã€‚æˆ‘ä»¬æŠŠå¥å­ä¸­çš„æ¯ä¸ªè¯å‘é‡æ‹¼æ¥åˆ°ä¸€èµ·å˜æˆä¸€ä¸ªçŸ©é˜µXï¼Œç„¶åä¹˜ä»¥ä¸åŒçš„çŸ©é˜µåšçº¿æ€§å˜æ¢ï¼ˆWQ, WK, WVï¼‰ã€‚</p><p><img src="https://uploader.shimo.im/f/xRsGTXMRHTQsNPiL.png!thumbnail" alt="img"></p><p>ç„¶åæˆ‘ä»¬å°±ç”¨çŸ©é˜µä¹˜æ³•å®ç°ä¸Šé¢ä»‹ç»è¿‡çš„Self-Attentionæœºåˆ¶äº†ã€‚</p><p><img src="https://uploader.shimo.im/f/S1IEPFyGeMUTWMBk.png!thumbnail" alt="img"></p><h3 id="Multi-headed-attention"><a href="#Multi-headed-attention" class="headerlink" title="Multi-headed attention"></a>Multi-headed attention</h3><p>åœ¨è®ºæ–‡å½“ä¸­ï¼Œæ¯ä¸ªembedding vectorå¹¶ä¸æ­¢äº§ç”Ÿä¸€ä¸ªkey, value, query vectorsï¼Œè€Œæ˜¯äº§ç”Ÿè‹¥å¹²ç»„è¿™æ ·çš„vectorsï¼Œç§°ä¹‹ä¸ºâ€multi-headedâ€ attentionã€‚è¿™ä¹ˆåšæœ‰å‡ ä¸ªå¥½å¤„ï¼š</p><ul><li><p>k: key, q: query, v: value</p></li><li><p>æ¨¡å‹æœ‰æ›´å¼ºçš„èƒ½åŠ›äº§ç”Ÿä¸åŒçš„attentionæœºåˆ¶ï¼Œfocusåœ¨ä¸åŒçš„å•è¯ä¸Šã€‚</p></li><li><p>attention layeræœ‰å¤šä¸ªä¸åŒçš„â€representation spaceâ€ã€‚</p></li></ul><p><img src="https://uploader.shimo.im/f/vQX0sIYIoqUNYO4J.png!thumbnail" alt="img"></p><p>æ¯ä¸ªattention headæœ€ç»ˆéƒ½äº§ç”Ÿäº†ä¸€ä¸ªmatrixè¡¨ç¤ºè¿™ä¸ªå¥å­ä¸­çš„æ‰€æœ‰è¯å‘é‡ã€‚åœ¨transformeræ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬äº§ç”Ÿäº†å…«ä¸ªmatricesã€‚æˆ‘ä»¬çŸ¥é“self attentionä¹‹åå°±æ˜¯ä¸€ä¸ªfeed-forward networkã€‚é‚£ä¹ˆæˆ‘ä»¬æ˜¯å¦éœ€è¦åš8æ¬¡feed-forward networkè¿ç®—å‘¢ï¼Ÿäº‹å®ä¸Šæ˜¯ä¸ç”¨çš„ã€‚æˆ‘ä»¬åªéœ€è¦å°†è¿™8ä¸ªmatricesæ‹¼æ¥åˆ°ä¸€èµ·ï¼Œç„¶ååšä¸€æ¬¡å‰å‘ç¥ç»ç½‘ç»œçš„è¿ç®—å°±å¯ä»¥äº†ã€‚</p><p><img src="https://uploader.shimo.im/f/E4AxOnUs2JgGJ0bW.png!thumbnail" alt="img"></p><p>ç»¼åˆèµ·æ¥ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸‹é¢ä¸€å¼ å›¾è¡¨ç¤ºSelf-Attentionæ¨¡å—æ‰€åšçš„äº‹æƒ…ã€‚</p><p><img src="https://uploader.shimo.im/f/YmfWxTGsc48tTbfi.png!thumbnail" alt="img"></p><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>thinking machine</p><p>w_1, w_2</p><p>p_1, p_2</p><p>positional_embedding = nn.Embedding(512, 300)</p><p>w_1 + p_1, w_2 + p_2, w_3 + p_3, â€¦, w_n + p_n</p><p>åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®Œå…¨æ²¡æœ‰è€ƒè™‘å•è¯çš„é¡ºåºã€‚å³ä½¿æˆ‘ä»¬å°†å¥å­ä¸­å•è¯çš„é¡ºåºå®Œå…¨æ‰“ä¹±ï¼Œå¯¹äºtransformerè¿™ä¸ªæ¨¡å‹æ¥è¯´ï¼Œå¹¶æ²¡æœ‰ä»€ä¹ˆåŒºåˆ«ã€‚ä¸ºäº†åŠ å…¥å¥å­ä¸­å•è¯çš„é¡ºåºä¿¡æ¯ï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ä¸ªæ¦‚å¿µå«åšpositional encodingã€‚</p><p><img src="https://uploader.shimo.im/f/1F6bv1ngvE4hEp99.png!thumbnail" alt="img"></p><p>å¦‚æœæˆ‘ä»¬å‡è®¾è¾“å…¥çš„embeddingæ˜¯4ä¸ªç»´åº¦çš„ï¼Œé‚£ä¹ˆä»–ä»¬çš„position encodingså¤§æ¦‚é•¿ä¸‹é¢è¿™æ ·ã€‚</p><p><img src="https://uploader.shimo.im/f/1p4K2IclsvwWGw0Z.png!thumbnail" alt="img"></p><p>ä¸‹é¢è¿™å¼ å›¾çš„æ¯ä¸€è¡Œè¡¨ç¤ºä¸€ä¸ªpositional encoding vectorã€‚ç¬¬ä¸€è¡Œè¡¨ç¤ºç¬¬ä¸€ä¸ªå•è¯çš„positional encodingï¼Œä»¥æ­¤ç±»æ¨ã€‚æ¯ä¸€è¡Œéƒ½æœ‰512ä¸ª-1åˆ°1ä¹‹é—´çš„æ•°å­—ã€‚æˆ‘ä»¬ç”¨é¢œè‰²æ ‡è®°äº†è¿™äº›vectorsã€‚</p><p><img src="https://uploader.shimo.im/f/HMQy3lipFooyu8rO.png!thumbnail" alt="img"></p><h3 id="Residuals"><a href="#Residuals" class="headerlink" title="Residuals"></a>Residuals</h3><p>å¦å¤–ä¸€ä¸ªç»†èŠ‚æ˜¯ï¼Œencoderä¸­çš„æ¯ä¸€å±‚éƒ½åŒ…å«äº†ä¸€ä¸ªresidual connectionå’Œlayer-normalizationã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚</p><p><img src="https://uploader.shimo.im/f/1qIGhKLQLYkHahSn.png!thumbnail" alt="img"></p><p>ä¸‹é¢è¿™å¼ å›¾æ˜¯æ›´è¯¦ç»†çš„vectorè¡¨ç¤ºã€‚</p><p><img src="https://uploader.shimo.im/f/ivgMtxCc8CsI7lAF.png!thumbnail" alt="img"></p><p>decoderä¹Ÿæ˜¯åŒæ ·çš„æ¶æ„ã€‚å¦‚æœæˆ‘ä»¬æŠŠencoderå’Œdecoderæ”¾åˆ°ä¸€èµ·ï¼Œä»–ä»¬å°±é•¿è¿™æ ·ã€‚</p><p><img src="https://uploader.shimo.im/f/TumXWzLQ6XMjJneZ.png!thumbnail" alt="img"></p><h2 id="è§£ç å™¨"><a href="#è§£ç å™¨" class="headerlink" title="è§£ç å™¨"></a>è§£ç å™¨</h2><p>encoderæœ€åä¸€å±‚ä¼šè¾“å‡ºattention vectors Kå’ŒVã€‚Kå’ŒVä¼šè¢«decoderç”¨ä½œè§£ç çš„åŸææ–™ã€‚</p><p><img src="https://uploader.shimo.im/f/3AgIt6lqzgADLwuf.png!thumbnail" alt="img"></p><p>åœ¨è§£ç çš„è¿‡ç¨‹ä¸­ï¼Œè§£ç å™¨æ¯ä¸€æ­¥ä¼šè¾“å‡ºä¸€ä¸ªtokenã€‚ä¸€ç›´å¾ªç¯å¾€å¤ï¼Œç›´åˆ°å®ƒè¾“å‡ºäº†ä¸€ä¸ªç‰¹æ®Šçš„end of sequence tokenï¼Œè¡¨ç¤ºè§£ç ç»“æŸäº†ã€‚</p><p><img src="https://uploader.shimo.im/f/ai444UV6eQ4E0f6O.png!thumbnail" alt="img"></p><p>decoderçš„self attentionæœºåˆ¶ä¸encoderç¨æœ‰ä¸åŒã€‚åœ¨decoderå½“ä¸­ï¼Œself attentionå±‚åªèƒ½çœ‹åˆ°ä¹‹å‰å·²ç»è§£ç çš„æ–‡å­—ã€‚æˆ‘ä»¬åªéœ€è¦æŠŠå½“å‰è¾“å‡ºä½ç½®ä¹‹åçš„å•è¯å…¨éƒ½maskæ‰ï¼ˆsoftmaxå±‚ä¹‹å‰å…¨éƒ½è®¾ç½®æˆ-infï¼‰å³å¯ã€‚</p><p>softmax(Q matmul K^T / sqrt(d)) matmul V</p><p>weights = Q matmul K^T: [seq_len, seq_len]</p><p>Masked Self Attention</p><p>q, k (<strong>100, 24</strong>, 35 - inf, 88 - inf, -55 - inf) â€“&gt; softmax â€“&gt; (0.9, 0.1, 0, 0, 0)</p><p>attention_mask</p><p>0, -inf, -inf, -inf</p><p>0, 0, -inf, -inf</p><p>0, 0, 0, -inf </p><p>0, 0, 0, 0</p><p>softmax(weights - attention_mask, -1)</p><p>è®­ç»ƒ</p><p>QKV, å¹¶è¡Œè®­ç»ƒ</p><p>é¢„æµ‹</p><p>ä¸€ä¸ªå•è¯ä¸€ä¸ªå•è¯è§£ç </p><p>Encoder-Decoder Attentionå±‚å’Œæ™®é€šçš„multiheaded self-attentionä¸€æ ·ï¼Œé™¤äº†å®ƒçš„Querieså®Œå…¨æ¥è‡ªä¸‹é¢çš„decoderå±‚ï¼Œç„¶åKeyå’ŒValueæ¥è‡ªencoderçš„è¾“å‡ºå‘é‡ã€‚</p><p>batch_size * seq_length * hidden_size </p><p>padding_mask</p><p>tgt_mask</p><h3 id="æœ€åçš„çº¿æ€§å±‚å’Œsoftmaxå±‚"><a href="#æœ€åçš„çº¿æ€§å±‚å’Œsoftmaxå±‚" class="headerlink" title="æœ€åçš„çº¿æ€§å±‚å’Œsoftmaxå±‚"></a>æœ€åçš„çº¿æ€§å±‚å’Œsoftmaxå±‚</h3><p>è§£ç å™¨æœ€åè¾“å‡ºæµ®ç‚¹å‘é‡ï¼Œå¦‚ä½•å°†å®ƒè½¬æˆè¯ï¼Ÿè¿™æ˜¯æœ€åçš„çº¿æ€§å±‚å’Œsoftmaxå±‚çš„ä¸»è¦å·¥ä½œã€‚</p><p>çº¿æ€§å±‚æ˜¯ä¸ªç®€å•çš„å…¨è¿æ¥å±‚ï¼Œå°†è§£ç å™¨çš„æœ€åè¾“å‡ºæ˜ å°„åˆ°ä¸€ä¸ªéå¸¸å¤§çš„logitså‘é‡ä¸Šã€‚å‡è®¾æ¨¡å‹å·²çŸ¥æœ‰1ä¸‡ä¸ªå•è¯ï¼ˆè¾“å‡ºçš„è¯è¡¨ï¼‰ä»è®­ç»ƒé›†ä¸­å­¦ä¹ å¾—åˆ°ã€‚é‚£ä¹ˆï¼Œlogitså‘é‡å°±æœ‰1ä¸‡ç»´ï¼Œæ¯ä¸ªå€¼è¡¨ç¤ºæ˜¯æŸä¸ªè¯çš„å¯èƒ½å€¾å‘å€¼ã€‚</p><p>softmaxå±‚å°†è¿™äº›åˆ†æ•°è½¬æ¢æˆæ¦‚ç‡å€¼ï¼ˆéƒ½æ˜¯æ­£å€¼ï¼Œä¸”åŠ å’Œä¸º1ï¼‰ï¼Œæœ€é«˜å€¼å¯¹åº”çš„ç»´ä¸Šçš„è¯å°±æ˜¯è¿™ä¸€æ­¥çš„è¾“å‡ºå•è¯ã€‚</p><p><img src="https://uploader.shimo.im/f/7ffWFIfMqOgtsK22.png!thumbnail" alt="img"></p><h2 id="æ¨¡å‹çš„è®­ç»ƒ"><a href="#æ¨¡å‹çš„è®­ç»ƒ" class="headerlink" title="æ¨¡å‹çš„è®­ç»ƒ"></a>æ¨¡å‹çš„è®­ç»ƒ</h2><p>ç°åœ¨æˆ‘ä»¬å·²ç»äº†è§£äº†ä¸€ä¸ªè®­ç»ƒå®Œæ¯•çš„Transformerçš„å‰å‘è¿‡ç¨‹ï¼Œé¡ºé“çœ‹ä¸‹è®­ç»ƒçš„æ¦‚å¿µä¹Ÿæ˜¯éå¸¸æœ‰ç”¨çš„ã€‚åœ¨è®­ç»ƒæ—¶ï¼Œæ¨¡å‹å°†ç»å†ä¸Šè¿°çš„å‰å‘è¿‡ç¨‹ï¼Œå½“æˆ‘ä»¬åœ¨æ ‡è®°è®­ç»ƒé›†ä¸Šè®­ç»ƒæ—¶ï¼Œå¯ä»¥å¯¹æ¯”é¢„æµ‹è¾“å‡ºä¸å®é™…è¾“å‡ºã€‚ä¸ºäº†å¯è§†åŒ–ï¼Œå‡è®¾è¾“å‡ºä¸€å…±åªæœ‰6ä¸ªå•è¯ï¼ˆâ€œaâ€, â€œamâ€, â€œiâ€, â€œthanksâ€, â€œstudentâ€, â€œâ€ï¼‰</p><p><img src="https://uploader.shimo.im/f/FNgjBBm5gbUGYgbs.png!thumbnail" alt="img"></p><p>æ¨¡å‹çš„è¯è¡¨æ˜¯åœ¨è®­ç»ƒä¹‹å‰çš„é¢„å¤„ç†ä¸­ç”Ÿæˆçš„</p><p>ä¸€æ—¦å®šä¹‰äº†è¯è¡¨ï¼Œæˆ‘ä»¬å°±èƒ½å¤Ÿæ„é€ ä¸€ä¸ªåŒç»´åº¦çš„å‘é‡æ¥è¡¨ç¤ºæ¯ä¸ªå•è¯ï¼Œæ¯”å¦‚one-hotç¼–ç ï¼Œä¸‹é¢ä¸¾ä¾‹ç¼–ç â€œamâ€ã€‚</p><p><img src="https://uploader.shimo.im/f/feV2TQAHPF0z3Rr2.png!thumbnail" alt="img"></p><p>ä¸¾ä¾‹é‡‡ç”¨one-hotç¼–ç è¾“å‡ºè¯è¡¨</p><p>ä¸‹é¢è®©æˆ‘ä»¬è®¨è®ºä¸‹æ¨¡å‹çš„lossæŸå¤±ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”¨æ¥ä¼˜åŒ–çš„æŒ‡æ ‡ï¼ŒæŒ‡å¯¼å­¦ä¹ å¾—åˆ°ä¸€ä¸ªéå¸¸å‡†ç¡®çš„æ¨¡å‹ã€‚</p><h3 id="æŸå¤±å‡½æ•°"><a href="#æŸå¤±å‡½æ•°" class="headerlink" title="æŸå¤±å‡½æ•°"></a>æŸå¤±å‡½æ•°</h3><p>æˆ‘ä»¬ç”¨ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥ç¤ºèŒƒè®­ç»ƒï¼Œæ¯”å¦‚ç¿»è¯‘â€œmerciâ€ä¸ºâ€œthanksâ€ã€‚é‚£æ„å‘³ç€è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒæŒ‡å‘å•è¯â€œthanksâ€ï¼Œä½†æ˜¯ç”±äºæ¨¡å‹æœªè®­ç»ƒæ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œä¸å¤ªå¯èƒ½å°±æ˜¯æœŸæœ›çš„è¾“å‡ºã€‚</p><p><img src="https://uploader.shimo.im/f/aWNgQPklQh8odGQP.png!thumbnail" alt="img"></p><p>ç”±äºæ¨¡å‹å‚æ•°æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œæœªè®­ç»ƒçš„æ¨¡å‹è¾“å‡ºéšæœºå€¼ã€‚æˆ‘ä»¬å¯ä»¥å¯¹æ¯”çœŸå®è¾“å‡ºï¼Œç„¶ååˆ©ç”¨è¯¯å·®åä¼ è°ƒæ•´æ¨¡å‹æƒé‡ï¼Œä½¿å¾—è¾“å‡ºæ›´æ¥è¿‘ä¸çœŸå®è¾“å‡ºã€‚å¦‚ä½•å¯¹æ¯”ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒå‘¢ï¼Ÿç®€å•é‡‡ç”¨ <a href="https://colah.github.io/posts/2015-09-Visual-Information/" target="_blank" rel="noopener">cross-entropy</a>æˆ–è€…<a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained" target="_blank" rel="noopener">Kullback-Leibler divergence</a>ä¸­çš„ä¸€ç§ã€‚é‰´äºè¿™æ˜¯ä¸ªæå…¶ç®€å•çš„ä¾‹å­ï¼Œæ›´çœŸå®çš„æƒ…å†µæ˜¯ï¼Œä½¿ç”¨ä¸€ä¸ªå¥å­ä½œä¸ºè¾“å…¥ã€‚æ¯”å¦‚ï¼Œè¾“å…¥æ˜¯â€œje suis Ã©tudiantâ€ï¼ŒæœŸæœ›è¾“å‡ºæ˜¯â€œi am a studentâ€ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸‹ï¼Œæˆ‘ä»¬æœŸæœ›æ¨¡å‹è¾“å‡ºè¿ç»­çš„æ¦‚ç‡åˆ†å¸ƒæ»¡è¶³å¦‚ä¸‹æ¡ä»¶ï¼š</p><ol><li><p>æ¯ä¸ªæ¦‚ç‡åˆ†å¸ƒéƒ½ä¸è¯è¡¨åŒç»´åº¦</p></li><li><p>ç¬¬ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒå¯¹â€œiâ€å…·æœ‰æœ€é«˜çš„é¢„æµ‹æ¦‚ç‡å€¼ã€‚</p></li><li><p>ç¬¬äºŒä¸ªæ¦‚ç‡åˆ†å¸ƒå¯¹â€œamâ€å…·æœ‰æœ€é«˜çš„é¢„æµ‹æ¦‚ç‡å€¼ã€‚</p></li><li><p>ä¸€ç›´åˆ°ç¬¬äº”ä¸ªè¾“å‡ºæŒ‡å‘â€â€æ ‡è®°ã€‚</p></li></ol><p><img src="https://uploader.shimo.im/f/rAnz8qY0eHgt2OAe.png!thumbnail" alt="img"></p><p>å¯¹ä¸€ä¸ªå¥å­è€Œè¨€ï¼Œè®­ç»ƒæ¨¡å‹çš„ç›®æ ‡æ¦‚ç‡åˆ†å¸ƒ</p><p>åœ¨è¶³å¤Ÿå¤§çš„è®­ç»ƒé›†ä¸Šè®­ç»ƒè¶³å¤Ÿæ—¶é—´ä¹‹åï¼Œæˆ‘ä»¬æœŸæœ›äº§ç”Ÿçš„æ¦‚ç‡åˆ†å¸ƒå¦‚ä¸‹æ‰€ç¤ºï¼š</p><p><img src="https://uploader.shimo.im/f/IyKk2fNcC3k4tgBt.png!thumbnail" alt="img"></p><p>è®­ç»ƒå¥½ä¹‹åï¼Œæ¨¡å‹çš„è¾“å‡ºæ˜¯æˆ‘ä»¬æœŸæœ›çš„ç¿»è¯‘ã€‚å½“ç„¶ï¼Œè¿™å¹¶ä¸æ„å‘³ç€è¿™ä¸€è¿‡ç¨‹æ˜¯æ¥è‡ªè®­ç»ƒé›†ã€‚æ³¨æ„ï¼Œæ¯ä¸ªä½ç½®éƒ½èƒ½æœ‰å€¼ï¼Œå³ä¾¿ä¸è¾“å‡ºè¿‘ä¹æ— å…³ï¼Œè¿™ä¹Ÿæ˜¯softmaxå¯¹è®­ç»ƒæœ‰å¸®åŠ©çš„åœ°æ–¹ã€‚ç°åœ¨ï¼Œå› ä¸ºæ¨¡å‹æ¯æ­¥åªäº§ç”Ÿä¸€ç»„è¾“å‡ºï¼Œå‡è®¾æ¨¡å‹é€‰æ‹©æœ€é«˜æ¦‚ç‡ï¼Œæ‰”æ‰å…¶ä»–çš„éƒ¨åˆ†ï¼Œè¿™æ˜¯ç§äº§ç”Ÿé¢„æµ‹ç»“æœçš„æ–¹æ³•ï¼Œå«åšgreedy è§£ç ã€‚å¦å¤–ä¸€ç§æ–¹æ³•æ˜¯beam searchï¼Œæ¯ä¸€æ­¥ä»…ä¿ç•™æœ€å¤´éƒ¨é«˜æ¦‚ç‡çš„ä¸¤ä¸ªè¾“å‡ºï¼Œæ ¹æ®è¿™ä¿©è¾“å‡ºå†é¢„æµ‹ä¸‹ä¸€æ­¥ï¼Œå†ä¿ç•™å¤´éƒ¨é«˜æ¦‚ç‡çš„ä¸¤ä¸ªè¾“å‡ºï¼Œé‡å¤ç›´åˆ°é¢„æµ‹ç»“æŸ</p><h2 id="æ›´å¤šèµ„æ–™"><a href="#æ›´å¤šèµ„æ–™" class="headerlink" title="æ›´å¤šèµ„æ–™"></a>æ›´å¤šèµ„æ–™</h2><ul><li><p><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a> </p></li><li><p>Transformeråšå®¢æ–‡ç«  <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank" rel="noopener">Transformer: A Novel Neural Network Architecture for Language Understanding</a></p></li><li><p><a href="https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html" target="_blank" rel="noopener">Tensor2Tensor announcement</a>.</p></li><li><p><a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb" target="_blank" rel="noopener">Jupyter Notebook provided as part of the Tensor2Tensor repo</a></p></li><li><p><a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener">Tensor2Tensor repo</a>.</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;contextualized word vectors&lt;/p&gt;
&lt;p&gt;RNN, LSTM&lt;/p&gt;
&lt;p&gt;RNN(I study at Julyedu.) â€“&amp;gt; RNN(I)-&amp;gt;h1, RNN(study, h1)-&amp;gt;h2, RNN(at, h2)-&amp;gt;
      
    
    </summary>
    
      <category term="NLP" scheme="http://mmyblog.cn/categories/NLP/"/>
    
    
      <category term="Transformer" scheme="http://mmyblog.cn/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Transformer-XL</title>
    <link href="http://mmyblog.cn/2020/05/16/Transformer-XL/"/>
    <id>http://mmyblog.cn/2020/05/16/Transformer-XL/</id>
    <published>2020-05-16T00:34:49.000Z</published>
    <updated>2020-06-09T01:32:52.403Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context"><a href="#Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context" class="headerlink" title="Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"></a>Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</h2><p><a href="https://arxiv.org/pdf/1901.02860.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.02860.pdf</a></p><p>ç›¸è¾ƒäºä¼ ç»Ÿtransformer decoderï¼Œå¼•å…¥ä¸¤ä¸ªæ–°æ¨¡å—</p><ul><li>segment-level recurrence mechanism</li></ul><p><img src="https://uploader.shimo.im/f/DpNe30kuahkbOeW5.png!thumbnail" alt="img"></p><ul><li><p>a novel positional encoding scheme</p></li><li><p>è€ƒè™‘æˆ‘ä»¬åœ¨attentionæœºåˆ¶ä¸­å¦‚ä½•ä½¿ç”¨positional encoding</p></li></ul><p>(E_{x_i}^T+U_i^T)W_q^TW_kE_{x_j}U_j</p><p><img src="https://uploader.shimo.im/f/5zNU9yZQtQMClNiY.png!thumbnail" alt="img"></p><ul><li><p>Rä»–ä»¬é‡‡ç”¨çš„æ˜¯transformerå½“ä¸­çš„positional encoding</p></li><li><p>uå’Œvæ˜¯éœ€è¦è®­ç»ƒçš„æ¨¡å‹å‚æ•°</p></li></ul><p>æœ€ç»ˆTransformer XLæ¨¡å‹</p><p><img src="https://uploader.shimo.im/f/Nm1uk49MIjUys1aK.png!thumbnail" alt="img"></p><p>ä»£ç </p><p><a href="https://github.com/kimiyoung/transformer-xl" target="_blank" rel="noopener">https://github.com/kimiyoung/transformer-xl</a></p><h2 id="XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding"><a href="#XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding" class="headerlink" title="XLNet: Generalized Autoregressive Pretraining for Language Understanding"></a>XLNet: Generalized Autoregressive Pretraining for Language Understanding</h2><p><a href="https://arxiv.org/pdf/1906.08237.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1906.08237.pdf</a></p><p>èƒŒæ™¯çŸ¥è¯†</p><ul><li><p>è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼ˆAutoregressive Language Modelï¼‰ï¼šé‡‡ç”¨ä»å·¦å¾€å³æˆ–ä»å³å¾€å·¦çš„è¯­è¨€æ¨¡å‹ï¼Œæ ¹æ®ä¸Šæ–‡é¢„æµ‹ä¸‹æ–‡ã€‚</p></li><li><p>ç¼ºç‚¹ï¼šåªåˆ©ç”¨äº†é¢„æµ‹å•è¯å·¦è¾¹æˆ–å³è¾¹çš„ä¿¡æ¯ï¼Œæ— æ³•åŒæ—¶åˆ©ç”¨ä¸¤è¾¹çš„ä¿¡æ¯ã€‚ELMoåœ¨ä¸€å®šç¨‹åº¦ä¸Šè§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚</p></li><li><p><img src="https://uploader.shimo.im/f/cpfGbeRfzf8c1ga8.png!thumbnail" alt="img"></p></li><li><p>è‡ªç¼–ç æ¨¡å‹ï¼ˆDenoising Auto Encoder, DAEï¼‰ï¼šåœ¨è¾“å…¥ä¸­éšæœºmaskä¸€äº›å•è¯ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡æ¥é¢„æµ‹è¢«maskæ‰çš„å•è¯ã€‚BERTé‡‡ç”¨äº†è¿™ä¸€æ€è·¯ã€‚</p></li><li><p><img src="https://uploader.shimo.im/f/za1FnG3zHdsbm5gD.png!thumbnail" alt="img"></p></li></ul><p>ä¸¤ä¸ªæ¨¡å‹çš„é—®é¢˜</p><p><img src="https://uploader.shimo.im/f/A1rO6rAR1nAQqqvu.png!thumbnail" alt="img"></p><p>XLNetçš„ç›®æ ‡æ˜¯èåˆä»¥ä¸Šä¸¤ç§æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œè§£å†³å®ƒä»¬å„è‡ªå­˜åœ¨çš„é—®é¢˜ã€‚</p><p>XLNetæ¨¡å‹ï¼šPermutation Language Modeling</p><p><img src="https://uploader.shimo.im/f/LdaKeEgG8XwH3iNj.png!thumbnail" alt="img"></p><p>Two-Stream Self-Attention</p><p><img src="https://uploader.shimo.im/f/TdQVsxOeYMoakBW0.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/iLMqF1WinQI6wOsW.png!thumbnail" alt="img"></p><p>å‚è€ƒèµ„æ–™</p><p><a href="https://zhuanlan.zhihu.com/p/70257427" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/70257427</a></p><p>ä»£ç </p><p><a href="https://github.com/zihangdai/xlnet" target="_blank" rel="noopener">https://github.com/zihangdai/xlnet</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context&quot;&gt;&lt;a href=&quot;#Transformer-XL-Attentive-Language-Models-Beyond-a-
      
    
    </summary>
    
      <category term="NLP" scheme="http://mmyblog.cn/categories/NLP/"/>
    
    
      <category term="Transformer-XL" scheme="http://mmyblog.cn/tags/Transformer-XL/"/>
    
      <category term="XLNet" scheme="http://mmyblog.cn/tags/XLNet/"/>
    
  </entry>
  
  <entry>
    <title>è‹±æ–‡ä¹¦ç±wordçº§åˆ«çš„æ–‡æœ¬ç”Ÿæˆä»£ç æ³¨é‡Š</title>
    <link href="http://mmyblog.cn/2020/05/12/%E8%8B%B1%E6%96%87%E4%B9%A6%E7%B1%8Dword%E7%BA%A7%E5%88%AB%E7%9A%84%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E4%BB%A3%E7%A0%81%E6%B3%A8%E9%87%8A/"/>
    <id>http://mmyblog.cn/2020/05/12/è‹±æ–‡ä¹¦ç±wordçº§åˆ«çš„æ–‡æœ¬ç”Ÿæˆä»£ç æ³¨é‡Š/</id>
    <published>2020-05-12T10:52:06.000Z</published>
    <updated>2020-06-09T01:23:39.894Z</updated>
    
    <content type="html"><![CDATA[<p><strong>å…ˆçœ‹ä¸˜å‰å°”çš„äººç‰©ä¼ è®°charçº§åˆ«çš„æ–‡æœ¬ç”Ÿæˆ</strong></p><p>ä¸¾ä¸ªå°å°çš„ä¾‹å­ï¼Œæ¥çœ‹çœ‹LSTMæ˜¯æ€ä¹ˆç©çš„</p><p>æˆ‘ä»¬è¿™é‡Œä¸å†ç”¨charçº§åˆ«ï¼Œæˆ‘ä»¬ç”¨wordçº§åˆ«æ¥åšã€‚æˆ‘ä»¬è¿™é‡Œçš„æ–‡æœ¬é¢„æµ‹å°±æ˜¯ï¼Œç»™äº†å‰é¢çš„å•è¯ä»¥åï¼Œä¸‹ä¸€ä¸ªå•è¯æ˜¯è°ï¼Ÿ</p><p>æ¯”å¦‚ï¼Œhello from the other, ç»™å‡º side</p><p>ç¬¬ä¸€æ­¥ï¼Œä¸€æ ·ï¼Œå…ˆå¯¼å…¥å„ç§åº“</p><h3 id="å¯¼å…¥æ•°æ®å¹¶åˆ†è¯"><a href="#å¯¼å…¥æ•°æ®å¹¶åˆ†è¯" class="headerlink" title="å¯¼å…¥æ•°æ®å¹¶åˆ†è¯"></a>å¯¼å…¥æ•°æ®å¹¶åˆ†è¯</h3><p>In [1]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dropout</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> LSTM</span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ModelCheckpoint</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> gensim.models.word2vec <span class="keyword">import</span> Word2Vec</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Using TensorFlow backend.</span><br></pre></td></tr></table></figure><p>In [8]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># è¿è¡Œèµ„æºå……è¶³çš„å¯ä»¥è¯•è¯•ä¸‹é¢çš„ä»£ç </span></span><br><span class="line"><span class="comment"># raw_text = ''</span></span><br><span class="line"><span class="comment"># for file in os.listdir("./input/"):</span></span><br><span class="line"><span class="comment">#     # os.listdiråˆ—å‡ºè·¯å¾„ä¸‹çš„æ‰€æœ‰æ–‡ä»¶çš„åå­—</span></span><br><span class="line"><span class="comment">#     if file.endswith(".txt"): # å–å‡ºåç¼€.txtçš„æ–‡ä»¶</span></span><br><span class="line"><span class="comment">#         raw_text += open("./input/"+file, errors='ignore').read() + '\n\n'</span></span><br><span class="line">raw_text = open(<span class="string">'./input/Winston_Churchil.txt'</span>).read()</span><br><span class="line"><span class="comment"># æˆ‘ä»¬ä»ç”¨ä¸˜å‰å°”çš„è¯­æ–™ç”Ÿæˆæ–‡æœ¬</span></span><br><span class="line">raw_text = raw_text.lower()</span><br><span class="line">sentensor = nltk.data.load(<span class="string">'tokenizers/punkt/english.pickle'</span>)   </span><br><span class="line"><span class="comment"># åŠ è½½è‹±æ–‡çš„åˆ’åˆ†å¥å­çš„æ¨¡å‹</span></span><br><span class="line">sents = sentensor.tokenize(raw_text)</span><br><span class="line"><span class="comment"># .tokenizeå¯¹ä¸€æ®µæ–‡æœ¬è¿›è¡Œåˆ†å¥ï¼Œåˆ†æˆå„ä¸ªå¥å­ç»„æˆçš„åˆ—è¡¨ã€‚è¯¦è§£çœ‹ä¸‹è¿™ä¸ªåšå®¢ï¼Œè›®æœ‰æ„æ€çš„</span></span><br><span class="line"><span class="comment"># https://blog.csdn.net/ustbbsy/article/details/80053307</span></span><br><span class="line">print(sents[:<span class="number">2</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&apos;\ufeffproject gutenbergâ€™s real soldiers of fortune, by richard harding davis\n\nthis ebook is for the use of anyone anywhere at no cost and with\nalmost no restrictions whatsoever.&apos;, &apos;you may copy it, give it away or\nre-use it under the terms of the project gutenberg license included\nwith this ebook or online at www.gutenberg.org\n\n\ntitle: real soldiers of fortune\n\nauthor: richard harding davis\n\nposting date: february 22, 2009 [ebook #3029]\nlast updated: september 26, 2016\n\nlanguage: english\n\ncharacter set encoding: utf-8\n\n*** start of this project gutenberg ebook real soldiers of fortune ***\n\n\n\n\nproduced by david reed, and ronald j. wilson\n\n\n\n\n\nreal soldiers of fortune\n\n\nby richard harding davis\n\n\n\n\n\nmajor-general henry ronald douglas maciver\n\nany sunny afternoon, on fifth avenue, or at night in the _table dâ€™hote_\nrestaurants of university place, you may meet the soldier of fortune who\nof all his brothers in arms now living is the most remarkable.&apos;]</span><br></pre></td></tr></table></figure><p>In [9]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">corpus = []</span><br><span class="line"><span class="keyword">for</span> sen <span class="keyword">in</span> sents: <span class="comment"># é’ˆå¯¹æ¯ä¸ªå¥å­ï¼Œå†æ¬¡è¿›è¡Œåˆ†è¯ã€‚</span></span><br><span class="line">    corpus.append(nltk.word_tokenize(sen))</span><br><span class="line"></span><br><span class="line">print(len(corpus))</span><br><span class="line">print(corpus[:<span class="number">2</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1792</span><br><span class="line">[[&apos;\ufeffproject&apos;, &apos;gutenberg&apos;, &apos;â€™&apos;, &apos;s&apos;, &apos;real&apos;, &apos;soldiers&apos;, &apos;of&apos;, &apos;fortune&apos;, &apos;,&apos;, &apos;by&apos;, &apos;richard&apos;, &apos;harding&apos;, &apos;davis&apos;, &apos;this&apos;, &apos;ebook&apos;, &apos;is&apos;, &apos;for&apos;, &apos;the&apos;, &apos;use&apos;, &apos;of&apos;, &apos;anyone&apos;, &apos;anywhere&apos;, &apos;at&apos;, &apos;no&apos;, &apos;cost&apos;, &apos;and&apos;, &apos;with&apos;, &apos;almost&apos;, &apos;no&apos;, &apos;restrictions&apos;, &apos;whatsoever&apos;, &apos;.&apos;], [&apos;you&apos;, &apos;may&apos;, &apos;copy&apos;, &apos;it&apos;, &apos;,&apos;, &apos;give&apos;, &apos;it&apos;, &apos;away&apos;, &apos;or&apos;, &apos;re-use&apos;, &apos;it&apos;, &apos;under&apos;, &apos;the&apos;, &apos;terms&apos;, &apos;of&apos;, &apos;the&apos;, &apos;project&apos;, &apos;gutenberg&apos;, &apos;license&apos;, &apos;included&apos;, &apos;with&apos;, &apos;this&apos;, &apos;ebook&apos;, &apos;or&apos;, &apos;online&apos;, &apos;at&apos;, &apos;www.gutenberg.org&apos;, &apos;title&apos;, &apos;:&apos;, &apos;real&apos;, &apos;soldiers&apos;, &apos;of&apos;, &apos;fortune&apos;, &apos;author&apos;, &apos;:&apos;, &apos;richard&apos;, &apos;harding&apos;, &apos;davis&apos;, &apos;posting&apos;, &apos;date&apos;, &apos;:&apos;, &apos;february&apos;, &apos;22&apos;, &apos;,&apos;, &apos;2009&apos;, &apos;[&apos;, &apos;ebook&apos;, &apos;#&apos;, &apos;3029&apos;, &apos;]&apos;, &apos;last&apos;, &apos;updated&apos;, &apos;:&apos;, &apos;september&apos;, &apos;26&apos;, &apos;,&apos;, &apos;2016&apos;, &apos;language&apos;, &apos;:&apos;, &apos;english&apos;, &apos;character&apos;, &apos;set&apos;, &apos;encoding&apos;, &apos;:&apos;, &apos;utf-8&apos;, &apos;***&apos;, &apos;start&apos;, &apos;of&apos;, &apos;this&apos;, &apos;project&apos;, &apos;gutenberg&apos;, &apos;ebook&apos;, &apos;real&apos;, &apos;soldiers&apos;, &apos;of&apos;, &apos;fortune&apos;, &apos;***&apos;, &apos;produced&apos;, &apos;by&apos;, &apos;david&apos;, &apos;reed&apos;, &apos;,&apos;, &apos;and&apos;, &apos;ronald&apos;, &apos;j.&apos;, &apos;wilson&apos;, &apos;real&apos;, &apos;soldiers&apos;, &apos;of&apos;, &apos;fortune&apos;, &apos;by&apos;, &apos;richard&apos;, &apos;harding&apos;, &apos;davis&apos;, &apos;major-general&apos;, &apos;henry&apos;, &apos;ronald&apos;, &apos;douglas&apos;, &apos;maciver&apos;, &apos;any&apos;, &apos;sunny&apos;, &apos;afternoon&apos;, &apos;,&apos;, &apos;on&apos;, &apos;fifth&apos;, &apos;avenue&apos;, &apos;,&apos;, &apos;or&apos;, &apos;at&apos;, &apos;night&apos;, &apos;in&apos;, &apos;the&apos;, &apos;_table&apos;, &apos;d&apos;, &apos;â€™&apos;, &apos;hote_&apos;, &apos;restaurants&apos;, &apos;of&apos;, &apos;university&apos;, &apos;place&apos;, &apos;,&apos;, &apos;you&apos;, &apos;may&apos;, &apos;meet&apos;, &apos;the&apos;, &apos;soldier&apos;, &apos;of&apos;, &apos;fortune&apos;, &apos;who&apos;, &apos;of&apos;, &apos;all&apos;, &apos;his&apos;, &apos;brothers&apos;, &apos;in&apos;, &apos;arms&apos;, &apos;now&apos;, &apos;living&apos;, &apos;is&apos;, &apos;the&apos;, &apos;most&apos;, &apos;remarkable&apos;, &apos;.&apos;]]</span><br></pre></td></tr></table></figure><h1 id="word2vecç”Ÿæˆè¯å‘é‡"><a href="#word2vecç”Ÿæˆè¯å‘é‡" class="headerlink" title="word2vecç”Ÿæˆè¯å‘é‡"></a>word2vecç”Ÿæˆè¯å‘é‡</h1><p>In [45]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">w2v_model = Word2Vec(corpus, size=<span class="number">128</span>, window=<span class="number">5</span>, min_count=<span class="number">2</span>, workers=<span class="number">4</span>)</span><br><span class="line"><span class="comment"># Word2Vec()å‚æ•°çœ‹è¿™ä¸ªåšå®¢ï¼šhttps://www.cnblogs.com/pinard/p/7278324.html</span></span><br><span class="line"><span class="comment"># sizeï¼šè¯å‘é‡çš„ç»´åº¦</span></span><br><span class="line"><span class="comment"># windowï¼šå³è¯å‘é‡ä¸Šä¸‹æ–‡æœ€å¤§è·ç¦»ï¼Œwindowè¶Šå¤§ï¼Œåˆ™å’ŒæŸä¸€è¯è¾ƒè¿œçš„è¯ä¹Ÿä¼šäº§ç”Ÿä¸Šä¸‹æ–‡å…³ç³»ã€‚é»˜è®¤å€¼ä¸º5ã€‚</span></span><br><span class="line"><span class="comment"># min_countï¼šéœ€è¦è®¡ç®—è¯å‘é‡çš„æœ€å°è¯é¢‘ã€‚è¿™ä¸ªå€¼å¯ä»¥å»æ‰ä¸€äº›å¾ˆç”Ÿåƒ»çš„ä½é¢‘è¯ï¼Œé»˜è®¤æ˜¯5ã€‚å¦‚æœæ˜¯å°è¯­æ–™ï¼Œå¯ä»¥è°ƒä½è¿™ä¸ªå€¼ã€‚</span></span><br><span class="line"><span class="comment"># workersï¼šç”¨äºæ§åˆ¶è®­ç»ƒçš„å¹¶è¡Œæ•°ã€‚</span></span><br><span class="line"></span><br><span class="line">print(w2v_model[<span class="string">'office'</span>][:<span class="number">20</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[-0.03379476 -0.22743131 -0.17660786 -0.00957653 -0.10752155 -0.14298159</span><br><span class="line">  0.02914934 -0.08970737 -0.15872304 -0.05246524 -0.00084796 -0.05634443</span><br><span class="line"> -0.1461402   0.03880814 -0.12331649 -0.06511988 -0.08555544 -0.2300725</span><br><span class="line"> -0.0083805   0.02204316]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).</span><br></pre></td></tr></table></figure><h3 id="æ„é€ è®­ç»ƒé›†"><a href="#æ„é€ è®­ç»ƒé›†" class="headerlink" title="æ„é€ è®­ç»ƒé›†"></a>æ„é€ è®­ç»ƒé›†</h3><p>In [46]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">raw_input = [item <span class="keyword">for</span> sublist <span class="keyword">in</span> corpus <span class="keyword">for</span> item <span class="keyword">in</span> sublist]</span><br><span class="line">print(len(raw_input)) <span class="comment"># åŸå§‹è¯­æ–™åº“é‡Œçš„è¯è¯­æ€»æ•°</span></span><br><span class="line">text_stream = []</span><br><span class="line">vocab = w2v_model.wv.vocab <span class="comment"># æŸ¥çœ‹w2v_modelç”Ÿæˆçš„è¯å‘é‡</span></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> raw_input:</span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> vocab:</span><br><span class="line">        text_stream.append(word)</span><br><span class="line">print(len(text_stream))  </span><br><span class="line"><span class="comment"># æŸ¥çœ‹å»æ‰ä½é¢‘è¯åçš„æ€»çš„è¯æ•°ï¼Œå› ä¸ºmin_countæŠŠä½é¢‘è¯å»æ‰äº†</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">55562</span><br><span class="line">51876</span><br></pre></td></tr></table></figure><p>In [47]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># å¤„ç†æ–¹å¼åŒcharçº§åˆ«çš„æ–‡æœ¬ç”Ÿæˆ</span></span><br><span class="line">seq_length = <span class="number">10</span> </span><br><span class="line">x = []</span><br><span class="line">y = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(text_stream) - seq_length):</span><br><span class="line">    given = text_stream[i:i + seq_length]</span><br><span class="line">    predict = text_stream[i + seq_length]</span><br><span class="line">    x.append([w2v_model[word] <span class="keyword">for</span> word <span class="keyword">in</span> given])</span><br><span class="line">    y.append(w2v_model[predict])</span><br><span class="line"></span><br><span class="line">x = np.reshape(x, (<span class="number">-1</span>, seq_length, <span class="number">128</span>))</span><br><span class="line">y = np.reshape(y, (<span class="number">-1</span>,<span class="number">128</span>))</span><br><span class="line">print(x.shape)</span><br><span class="line">print(y.shape)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).</span><br><span class="line">  </span><br><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).</span><br><span class="line">  if __name__ == &apos;__main__&apos;:</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(51866, 10, 128)</span><br><span class="line">(51866, 128)</span><br></pre></td></tr></table></figure><h3 id="æ„å»ºå’Œè®­ç»ƒæ¨¡å‹"><a href="#æ„å»ºå’Œè®­ç»ƒæ¨¡å‹" class="headerlink" title="æ„å»ºå’Œè®­ç»ƒæ¨¡å‹"></a>æ„å»ºå’Œè®­ç»ƒæ¨¡å‹</h3><p>In [53]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(LSTM(<span class="number">256</span>, input_shape=(seq_length, <span class="number">128</span>),dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.2</span>))</span><br><span class="line"><span class="comment"># ç¬¬ä¸€ä¸ªdropoutæ˜¯xå’Œhiddenä¹‹é—´çš„dropout</span></span><br><span class="line"><span class="comment"># ç¬¬äºŒä¸ªrecurrent_dropoutï¼Œè¿™é‡Œæˆ‘ç†è§£ä¸ºæ˜¯æ¨ªå‘ä¸åŒæ—¶åˆ»éšè—å±‚ä¹‹é—´çš„dropout</span></span><br><span class="line">model.add(Dropout(<span class="number">0.2</span>)) <span class="comment"># ç¬¬ä¸‰ä¸ªï¼Œè¿™é‡Œæˆ‘ç†è§£ä¸ºçºµå‘å±‚ä¸å±‚ä¹‹é—´çš„dropout</span></span><br><span class="line">model.add(Dense(<span class="number">128</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.compile(loss=<span class="string">'mse'</span>, optimizer=<span class="string">'adam'</span>)</span><br><span class="line"><span class="comment"># æŸå¤±ç”¨çš„å‡æ–¹å·®æŸå¤±ï¼Œä¼˜åŒ–å™¨adam</span></span><br></pre></td></tr></table></figure><p>In [54]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(x, y, nb_epoch=<span class="number">10</span>, batch_size=<span class="number">4096</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.</span><br><span class="line">  &quot;&quot;&quot;Entry point for launching an IPython kernel.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/10</span><br><span class="line">51866/51866 [==============================] - 28s 539us/step - loss: 0.3177</span><br><span class="line">Epoch 2/10</span><br><span class="line">51866/51866 [==============================] - 28s 542us/step - loss: 0.1405</span><br><span class="line">Epoch 3/10</span><br><span class="line">51866/51866 [==============================] - 29s 560us/step - loss: 0.1329</span><br><span class="line">Epoch 4/10</span><br><span class="line">51866/51866 [==============================] - 30s 584us/step - loss: 0.1318</span><br><span class="line">Epoch 5/10</span><br><span class="line">51866/51866 [==============================] - 28s 548us/step - loss: 0.1313</span><br><span class="line">Epoch 6/10</span><br><span class="line">51866/51866 [==============================] - 30s 574us/step - loss: 0.1309</span><br><span class="line">Epoch 7/10</span><br><span class="line">51866/51866 [==============================] - 30s 570us/step - loss: 0.1306</span><br><span class="line">Epoch 8/10</span><br><span class="line">51866/51866 [==============================] - 29s 551us/step - loss: 0.1303</span><br><span class="line">Epoch 9/10</span><br><span class="line">51866/51866 [==============================] - 27s 524us/step - loss: 0.1299</span><br><span class="line">Epoch 10/10</span><br><span class="line">51866/51866 [==============================] - 27s 512us/step - loss: 0.1296</span><br></pre></td></tr></table></figure><p>Out[54]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;keras.callbacks.History at 0x1a32c9a2b0&gt;</span><br></pre></td></tr></table></figure><h3 id="é¢„æµ‹æ¨¡å‹"><a href="#é¢„æµ‹æ¨¡å‹" class="headerlink" title="é¢„æµ‹æ¨¡å‹"></a>é¢„æµ‹æ¨¡å‹</h3><p>In [55]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ä»£ç æ³¨é‡ŠåŒä¸˜å‰å°”çš„äººç‰©ä¼ è®°charçº§åˆ«çš„æ–‡æœ¬ç”Ÿæˆ</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_next</span><span class="params">(input_array)</span>:</span></span><br><span class="line">    x = np.reshape(input_array, (<span class="number">-1</span>,seq_length,<span class="number">128</span>))</span><br><span class="line">    y = model.predict(x)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">string_to_index</span><span class="params">(raw_input)</span>:</span></span><br><span class="line">    raw_input = raw_input.lower()</span><br><span class="line">    input_stream = nltk.word_tokenize(raw_input)</span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> input_stream[(len(input_stream)-seq_length):]:</span><br><span class="line">        res.append(w2v_model[word])</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">y_to_word</span><span class="params">(y)</span>:</span></span><br><span class="line">    word = w2v_model.most_similar(positive=y, topn=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> word</span><br></pre></td></tr></table></figure><p>In [56]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_article</span><span class="params">(init, rounds=<span class="number">30</span>)</span>:</span></span><br><span class="line">    in_string = init.lower()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(rounds):</span><br><span class="line">        n = y_to_word(predict_next(string_to_index(in_string)))</span><br><span class="line">        in_string += <span class="string">' '</span> + n[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> in_string</span><br></pre></td></tr></table></figure><p>In [58]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">init = <span class="string">'His object in coming to New York was to engage officers for that service. He came at an  moment'</span></span><br><span class="line">article = generate_article(init)</span><br><span class="line">print(article) <span class="comment"># è¯­æ–™åº“è¾ƒå°ï¼Œå¯ä»¥çœ‹åˆ°é‡å¤äº†</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).</span><br><span class="line">  if sys.path[0] == &apos;&apos;:</span><br><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).</span><br><span class="line">  app.launch_new_instance()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">his object in coming to new york was to engage officers for that service. he came at an  moment battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery battery</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;å…ˆçœ‹ä¸˜å‰å°”çš„äººç‰©ä¼ è®°charçº§åˆ«çš„æ–‡æœ¬ç”Ÿæˆ&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ä¸¾ä¸ªå°å°çš„ä¾‹å­ï¼Œæ¥çœ‹çœ‹LSTMæ˜¯æ€ä¹ˆç©çš„&lt;/p&gt;
&lt;p&gt;æˆ‘ä»¬è¿™é‡Œä¸å†ç”¨charçº§åˆ«ï¼Œæˆ‘ä»¬ç”¨wordçº§åˆ«æ¥åšã€‚æˆ‘ä»¬è¿™é‡Œçš„æ–‡æœ¬é¢„æµ‹å°±æ˜¯ï¼Œç»™äº†å‰é¢çš„å•è¯ä»¥åï¼Œä¸‹ä¸€ä¸ªå•è¯æ˜¯è°ï¼Ÿ&lt;/p&gt;
&lt;p
      
    
    </summary>
    
      <category term="NLP" scheme="http://mmyblog.cn/categories/NLP/"/>
    
    
      <category term="LSTM" scheme="http://mmyblog.cn/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>æ–‡æœ¬ç”Ÿæˆä»»åŠ¡</title>
    <link href="http://mmyblog.cn/2020/05/10/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1/"/>
    <id>http://mmyblog.cn/2020/05/10/æ–‡æœ¬ç”Ÿæˆä»»åŠ¡/</id>
    <published>2020-05-10T00:39:03.000Z</published>
    <updated>2020-06-09T01:22:20.753Z</updated>
    
    <content type="html"><![CDATA[<p>ä¸»è¦è®¨è®º</p><ul><li><p>æ–‡æœ¬ç”Ÿæˆçš„æ–¹æ³•ï¼š<strong>inference</strong></p></li><li><p>å¢åŠ æ–‡æœ¬ç”Ÿæˆçš„å¤šæ ·æ€§ï¼š<strong>variational auto encoder</strong></p></li><li><p>å¯ä»¥<strong>æ§åˆ¶çš„æ–‡æœ¬ç”Ÿæˆã€æ–‡æœ¬é£æ ¼è¿ç§»</strong></p></li><li><p>Generative Adversarial Networks</p></li><li><p>Data to text</p></li></ul><p>log loss:</p><ul><li><p>[s1, s2, â€¦, s_n] â€“&gt; softmax(s) = exp(s_i) / sum_i exp(s_i)  </p></li><li><p>p_i log q_i</p></li></ul><h1 id="å…³äºæ–‡æœ¬ç”Ÿæˆ"><a href="#å…³äºæ–‡æœ¬ç”Ÿæˆ" class="headerlink" title="å…³äºæ–‡æœ¬ç”Ÿæˆ"></a>å…³äºæ–‡æœ¬ç”Ÿæˆ</h1><p>ä¹‹å‰çš„è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦è®¨è®ºäº†Natural Language Understandingï¼Œä¹Ÿå°±æ˜¯ç»™ä½ ä¸€æ®µæ–‡å­—ï¼Œå¦‚ä½•ä»å„ä¸ªæ–¹é¢å»ç†è§£å®ƒã€‚å¸¸è§çš„NLUä»»åŠ¡æœ‰ï¼šæ–‡æœ¬åˆ†ç±»ï¼Œæƒ…æ„Ÿåˆ†ç±»ï¼Œ<strong>å‘½åå®ä½“è¯†åˆ«ï¼ˆNamed Entity Recognition, NERï¼‰ï¼ŒRelation Extraction</strong>ç­‰ç­‰ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä»æ–‡å­—ä¸­æå–å‡ºæˆ‘ä»¬æƒ³è¦äº†è§£çš„å…³é”®ä¿¡æ¯ã€‚</p><p>è¿™èŠ‚è¯¾æˆ‘ä»¬æ¥è®¨è®ºæ–‡æœ¬ç”Ÿæˆçš„ä¸€äº›æ–¹æ³•ã€‚</p><p>å¯¹äºæ–‡æœ¬ç”Ÿæˆï¼Œæˆ‘ä»¬å…³å¿ƒå“ªäº›é—®é¢˜ï¼Ÿ</p><ul><li><p>ä¸æ–‡æœ¬ç†è§£ç›¸åï¼Œæˆ‘ä»¬æœ‰ä¸€äº›æƒ³è¦è¡¨è¾¾çš„ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯å¯èƒ½æ¥è‡ªäºå¯¹è¯çš„å†å²ï¼Œå¯èƒ½æ¥è‡ªäºç»“æ„åŒ–çš„æ•°æ® (structured data, data-to-text generation)ã€‚ç°åœ¨æˆ‘ä»¬è¦è€ƒè™‘çš„æ˜¯å¦‚ä½•æŠŠè¿™äº›æˆ‘ä»¬æƒ³è¦è¡¨è¾¾çš„ä¿¡æ¯è½¬æ¢æˆè‡ªç„¶è¯­è¨€çš„æ–¹å¼ã€‚è¿™ä¸€ä»»åŠ¡åœ¨æ„å»ºèŠå¤©æœºå™¨äººä¸­æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚ç›®å‰çœ‹æ¥ï¼ŒåŸºäº<strong>æ¨¡æ¿ (template)</strong> çš„æ–¹æ³•ä»ç„¶æ˜¯æœ€ä¿é™©çš„ï¼Œä½†æ˜¯åœ¨ç ”ç©¶é¢†åŸŸä¸­ï¼Œäººä»¬è¶Šæ¥è¶Šå…³æ³¨<strong>åŸºäºç¥ç»ç½‘ç»œçš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•</strong>ã€‚</p></li><li><p>åŸºäºä¸Šæ–‡çš„æ–‡æœ¬è¡¥å…¨ä»»åŠ¡ï¼Œæ•…äº‹ç”Ÿæˆï¼Œç”Ÿæˆå¼èŠå¤©æœºå™¨äºº</p></li><li><p>äººä»¬ä¸€ç›´å¸Œæœ›è®¡ç®—æœºå¯ä»¥å®Œæˆä¸€äº›äººç±»æ‰å¯ä»¥å®Œæˆçš„åˆ›é€ æ€§ä»»åŠ¡ï¼Œä¾‹å¦‚ä½œç”»ã€‚AIä½œç”»å®é™…ä¸Šå·²ç»ä¸æ˜¯ä»€ä¹ˆæ–°é—»äº†ï¼ŒPortrait of Edmond de Belamyï¼Œä¸€å¹…AIåˆ›ä½œçš„ç”»åƒï¼Œæ‹å–å‡ºäº†43.2ä¸‡ç¾é‡‘çš„é«˜ä»·ã€‚</p></li><li><p>é‚£ä¹ˆAIèƒ½ä¸èƒ½å†™æ–‡ç« è®²æ•…äº‹å‘¢ï¼Ÿå…³äºæ–‡æœ¬ç”Ÿæˆçš„ç ”ç©¶ç›¸å¯¹æ¥è¯´æ²¡æœ‰ç‰¹åˆ«å®¢è§‚çš„è¯„ä»·æŒ‡æ ‡ï¼Œæ‰€ä»¥å¾ˆå¤šæ—¶å€™äººä»¬ä¼šæŒ‰ç…§è‡ªå·±çš„ä¸»è§‚è¯„ä»·æ¥åˆ¤æ–­æ¨¡å‹çš„å¥½åã€‚ä¾‹å¦‚ç»™å®šæ•…äº‹çš„ä¸Šæ–‡ï¼ŒAIç³»ç»Ÿèƒ½ä¸èƒ½å¾ˆå¥½åœ°è¡¥å…¨è¿™ä¸ªæ•…äº‹å‘¢ï¼Ÿ</p></li><li><p>æ–‡æœ¬è¡¥å…¨è¿™ä¸ªä»»åŠ¡æœ¬è´¨ä¸Šå°±æ˜¯è®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œå½“ç„¶ä¹Ÿæœ‰äººå°è¯•ä½¿ç”¨Seq2Seqçš„æ–¹æ³•åšæ–‡æœ¬ç”Ÿæˆã€‚ç›®å‰çœ‹æ¥æœ€å¼ºçš„æ¨¡å‹æ˜¯åŸºäºGPT-2é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ã€‚å¾ˆå¤šç ”ç©¶è€…ä½¿ç”¨GPT-2æ¥è¿›è¡Œæ–‡æœ¬ç”Ÿæˆç›¸å…³çš„å®éªŒã€‚ç”±äºè®­ç»ƒGPT-2è¿™æ ·è§„æ¨¡çš„è¯­è¨€æ¨¡å‹éœ€è¦å¤§é‡çš„ç®—åŠ›å’Œæ•°æ®èµ„æºï¼Œæ‰€ä»¥å¤§éƒ¨åˆ†çš„ç ”ç©¶éƒ½å…³æ³¨åœ¨å¦‚ä½•ä½¿ç”¨æ¨¡å‹ï¼Œä¹Ÿå°±æ˜¯inferenceçš„æ­¥éª¤ï¼Œè€Œä¸åœ¨äºæ¨¡å‹çš„è®­ç»ƒç¯èŠ‚ã€‚</p></li></ul><h2 id="Greedy-Decoding"><a href="#Greedy-Decoding" class="headerlink" title="Greedy Decoding"></a>Greedy Decoding</h2><p><strong>autoregressive</strong>: åŸºäºä¹‹å‰ç”Ÿæˆçš„æ–‡å­—æ¥ç”Ÿæˆåç»­çš„æ–‡å­—? </p><p>P(y_i | y_1, â€¦ y_{i-1})</p><p>parallel generation</p><p>å¤§éƒ¨åˆ†åŸºäºç¥ç»ç½‘ç»œçš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹é‡‡ç”¨çš„æ˜¯ä¸€ç§æ¡ä»¶è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬æœ‰ä¸€äº›å…ˆå†³æ¡ä»¶ï¼Œä¾‹å¦‚ auto encoder ä¸­çš„éšå‘é‡ï¼Œç„¶åæˆ‘ä»¬åŸºäºè¿™ä¸ªéšå‘é‡æ¥ç”Ÿæˆå¥å­ã€‚</p><p>å¤§éƒ¨åˆ†è¯­è¨€æ¨¡å‹çš„åŸºæœ¬å‡è®¾æ˜¯ä»å·¦å¾€å³çš„æ¡ä»¶æ¦‚ç‡æ¨¡å‹ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œç»™å®šäº†å•è¯1è‡³n-1ï¼Œæˆ‘ä»¬å¸Œæœ›ç”Ÿæˆç¬¬nä¸ªå•è¯ã€‚å‡è®¾æˆ‘ä»¬ç°åœ¨é‡‡ç”¨ä¸€ä¸ªåŸºäºLSTMçš„è¯­è¨€æ¨¡å‹ï¼Œåœ¨å½“å‰ç¬¬iä¸ªä½ç½®ä¸Šï¼Œæˆ‘ä»¬é¢„æµ‹ä¸‹ä¸€ä¸ªç”Ÿæˆå•è¯çš„æ¦‚ç‡åˆ†å¸ƒä¸º p = (p_1, <strong>p_2</strong>, â€¦ p_|V|)ï¼Œé‚£ä¹ˆåœ¨å½“å‰ä½ç½®ä¸Šæˆ‘ä»¬åº”è¯¥ç”Ÿæˆä»€ä¹ˆå•è¯å‘¢ï¼Ÿ</p><p>argmax_i p_i = 2</p><p>ä¸€ä¸ªæœ€ç®€å•çš„æ–¹æ³•æ˜¯ä½¿ç”¨Greedy Decodingï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬ç›´æ¥é‡‡ç”¨ argmax_i (p_i) å³å¯ã€‚å½“ç„¶ï¼ŒåŒå­¦ä»¬å¾ˆå®¹æ˜“è”æƒ³åˆ°ï¼Œè¿™ç§decodingçš„æ–¹æ³•æ˜¯æœ‰é—®é¢˜çš„ï¼Œå› ä¸ºæ¯æ¬¡éƒ½é€‰æ‹©æœ€å¤§æ¦‚ç‡çš„å•è¯å¹¶ä¸èƒ½ä¿è¯æˆ‘ä»¬ç”Ÿæˆå‡ºæ¥çš„å¥å­çš„æ€»ä½“æ¦‚ç‡åˆ†å¸ƒæ˜¯æœ€å¤§çš„ã€‚äº‹å®ä¸Šï¼Œå¤§éƒ¨åˆ†æ—¶å€™è¿™æ ·ç”Ÿæˆçš„å¥å­å…¶å®æ˜¯ä¸å¥½çš„ã€‚ç„¶è€Œæˆ‘ä»¬æ²¡æœ‰åŠæ³•éå†æ‰€æœ‰å¯èƒ½çš„å¥å­ï¼šé¦–å…ˆå¥å­çš„é•¿åº¦æ˜¯ä¸ç¡®å®šçš„ï¼›å³ä½¿æˆ‘ä»¬å‡å®šè‡ªå·±çŸ¥é“å¥å­çš„é•¿åº¦ lï¼Œå¦‚æœåœ¨æ¯ä¸ªä½ç½®ä¸Šè€ƒè™‘æ¯ä¸ªå¯èƒ½çš„å•è¯ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘ |V|^l ç§å¯èƒ½çš„æƒ…å†µï¼Œåœ¨è®¡ç®—èµ„æºä¸Šä¹Ÿæ˜¯ä¸ç°å®çš„ã€‚</p><p>ä¸€ç§å¦¥åçš„æ–¹æ³•æ˜¯é‡‡ç”¨ <strong>Beam Search</strong> ï¼ˆ<a href="https://shimo.im/docs/rHwdq8wd8txyXjP6ï¼‰ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨decodingçš„æ¯ä¸ªæ­¥éª¤ï¼Œæˆ‘ä»¬éƒ½ä¿ç•™ç€" target="_blank" rel="noopener">https://shimo.im/docs/rHwdq8wd8txyXjP6ï¼‰ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨decodingçš„æ¯ä¸ªæ­¥éª¤ï¼Œæˆ‘ä»¬éƒ½ä¿ç•™ç€</a> <strong>top K</strong> ä¸ªå¯èƒ½çš„å€™é€‰å•è¯ï¼Œç„¶ååˆ°äº†ä¸‹ä¸€ä¸ªæ­¥éª¤çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯¹è¿™ K ä¸ªå•è¯éƒ½åšä¸‹ä¸€æ­¥ decodingï¼Œåˆ†åˆ«é€‰å‡º top Kï¼Œç„¶åå¯¹è¿™ K^2 ä¸ªå€™é€‰å¥å­å†æŒ‘é€‰å‡º <strong>top K ä¸ªå¥å­</strong>ã€‚ä»¥æ­¤ç±»æ¨ä¸€ç›´åˆ° decoding ç»“æŸä¸ºæ­¢ã€‚å½“ç„¶ Beam Search æœ¬è´¨ä¸Šä¹Ÿæ˜¯ä¸€ä¸ª greedy decoding çš„æ–¹æ³•ï¼Œæ‰€ä»¥æˆ‘ä»¬æ— æ³•ä¿è¯è‡ªå·±ä¸€å®šå¯ä»¥å¾—åˆ°æœ€å¥½çš„ decoding ç»“æœã€‚</p><p>p(x_1, x_2, â€¦, x_n) = log (p(x_1) * p(x_2 | x_1) â€¦ p(x_n | x_1, â€¦, x_{n-1})) / n</p><p><strong>Greedy Decoding</strong>çš„é—®é¢˜</p><ul><li><p>å®¹æ˜“å‡ºç°å¾ˆæ— èŠçš„å›ç­”ï¼šI donâ€™t know. </p></li><li><p>å®¹æ˜“é‡å¤è‡ªå·±ï¼šI donâ€™t know. I donâ€™t know. I donâ€™t know. I donâ€™t know. I donâ€™t know. I donâ€™t know. </p></li><li><p>Beam search K = 200</p></li></ul><h2 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h2><p>argmax ä¸ä¸€å®šæ˜¯æœ€å¥½çš„</p><p>vocab(y_i) = [<strong>0.9</strong>, 0.05, 0.01, 0.01, 0.01, â€¦., 0.01]  softmax(logits/temperature)</p><p>sample(vocab(y_i))</p><p>sampleå¾ˆå¤šä¸ªå¥å­ï¼Œç„¶åç”¨å¦ä¸€ä¸ªæ¨¡å‹æ¥æ‰“åˆ†ï¼Œæ‰¾å‡ºæœ€ä½³generated text</p><p>sampling over the full vocabularyï¼šæˆ‘ä»¬å¯ä»¥åœ¨ç”Ÿæˆæ–‡æœ¬çš„æ—¶å€™å¼•å…¥ä¸€äº›éšæœºæ€§ã€‚ä¾‹å¦‚ç°åœ¨è¯­è¨€æ¨¡å‹å‘Šè¯‰æˆ‘ä»¬ä¸‹ä¸€ä¸ªå•è¯åœ¨æ•´ä¸ªå•è¯è¡¨ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒæ˜¯ p = (p_1, p_2, â€¦ p_|V|)ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥æŒ‰ç…§è¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒè¿›è¡Œéšæœºé‡‡æ ·ï¼Œç„¶åå†³å®šä¸‹ä¸€ä¸ªå•è¯ç”Ÿæˆä»€ä¹ˆã€‚é‡‡æ ·ç›¸å¯¹äºgreedyæ–¹æ³•çš„å¥½å¤„æ˜¯ï¼Œæˆ‘ä»¬ç”Ÿæˆçš„æ–‡å­—å¼€å§‹æœ‰äº†ä¸€äº›éšæœºæ€§ï¼Œä¸ä¼šæ€»æ˜¯ç”Ÿæˆå¾ˆæœºæ¢°çš„å›å¤äº†ã€‚</p><p>1 - 0.98^n</p><p>Samplingçš„é—®é¢˜</p><ul><li><p>ç”Ÿæˆçš„è¯å®¹æ˜“ä¸è¿è´¯ï¼Œä¸Šä¸‹æ–‡æ¯”è¾ƒçŸ›ç›¾ã€‚</p></li><li><p>å®¹æ˜“ç”Ÿæˆå¥‡æ€ªçš„è¯ï¼Œå‡ºç°<strong>ç½•è§è¯</strong>ã€‚</p></li></ul><p>top-k sampling å¯ä»¥ç¼“è§£ç”Ÿæˆç½•è§å•è¯çš„é—®é¢˜ã€‚æ¯”å¦‚è¯´ï¼Œæˆ‘ä»¬å¯ä»¥æ¯æ¬¡åªåœ¨æ¦‚ç‡æœ€é«˜çš„50ä¸ªå•è¯ä¸­æŒ‰ç…§æ¦‚ç‡åˆ†å¸ƒåšé‡‡æ ·ã€‚</p><p>æˆ‘åªä¿ç•™top-kä¸ªprobabilityçš„å•è¯ï¼Œç„¶ååœ¨è¿™äº›å•è¯ä¸­æ ¹æ®æ¦‚ç‡åšsampling</p><h2 id="Neucleus-Sampling"><a href="#Neucleus-Sampling" class="headerlink" title="Neucleus Sampling"></a>Neucleus Sampling</h2><h3 id="The-Curious-Case-of-Neural-Text-Degeneration"><a href="#The-Curious-Case-of-Neural-Text-Degeneration" class="headerlink" title="The Curious Case of Neural Text Degeneration"></a>The Curious Case of Neural Text Degeneration</h3><p><a href="https://arxiv.org/pdf/1904.09751.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.09751.pdf</a></p><p><img src="https://uploader.shimo.im/f/8IjYXmjBFmAwnJy3.png!thumbnail" alt="img"></p><p>è¿™ç¯‡æ–‡ç« åœ¨å‰äº›æ—¥å­å¼•èµ·äº†ä¸å°çš„å…³æ³¨ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åšsamplingçš„æ–¹æ³•ï¼Œå«åš Neucleus Samplingã€‚</p><p>Neucleus Samplingçš„åŸºæœ¬æ€æƒ³æ˜¯ï¼Œæˆ‘ä»¬ä¸åšbeam searchï¼Œè€Œæ˜¯åštop p samplingã€‚</p><p>è®¾ç½®ä¸€ä¸ªthresholdï¼Œp=0.95</p><p>top-k sampling å’Œ neucleus sampling çš„ä»£ç ï¼š<a href="https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317" target="_blank" rel="noopener">https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317</a></p><h1 id="Variational-Auto-Encoder-VAE"><a href="#Variational-Auto-Encoder-VAE" class="headerlink" title="Variational Auto Encoder (VAE)"></a>Variational Auto Encoder (VAE)</h1><h2 id="Auto-Encoder-è‡ªç¼–ç å™¨"><a href="#Auto-Encoder-è‡ªç¼–ç å™¨" class="headerlink" title="Auto Encoder è‡ªç¼–ç å™¨"></a>Auto Encoder è‡ªç¼–ç å™¨</h2><p>NLPä¸­çš„ä¸€ä¸ªé‡è¦é—®é¢˜æ˜¯è·å¾—ä¸€ç§è¯­è¨€çš„è¡¨ç¤ºï¼Œæ— è®ºæ˜¯å•è¯çš„è¡¨ç¤ºè¿˜æ˜¯å¥å­çš„è¡¨ç¤ºã€‚ä¸ºäº†è·å¾—å¥å­çš„è¡¨ç¤ºï¼Œä¸€ç§ç›´è§‚çš„æ€è·¯æ˜¯è®­ç»ƒä¸€ä¸ªauto encoderï¼Œä¹Ÿå°±æ˜¯è¯´ä¸€ä¸ªencoderç”¨æ¥ç¼–ç ä¸€ä¸ªå¥å­ï¼ŒæŠŠä¸€ä¸ªå¥å­è½¬æ¢æˆä¸€ä¸ªvectorï¼›å¦ä¸€ä¸ªdecoderç”¨æ¥è§£ç ä¸€ä¸ªå¥å­ï¼Œä¹Ÿå°±æ˜¯è¯´æŠŠä¸€ä¸ªvectorè§£ç æˆä¸€ä¸ªå¥å­ã€‚auto encoder äº‹å®ä¸Šæ˜¯ä¸€ç§æ•°æ®å‹ç¼©çš„æ–¹æ³•ã€‚</p><p>Encoder(text) â€“&gt; vector</p><p>Decoder(vector) â€“&gt; text</p><p>Encoderï¼šå¾—åˆ°å¾ˆå¥½çš„æ–‡æœ¬è¡¨ç¤ºï¼Œè¿™ä¸ªæ–‡æœ¬è¡¨ç¤ºä½ å¯ç”¨ç”¨äºä»»ä½•å…¶ä»–çš„ä»»åŠ¡ã€‚</p><p>Decoder: conditional language model</p><p>generalizeèƒ½åŠ›ä¸ä¸€å®šå¥½ã€‚è¿‡æ‹Ÿåˆã€‚</p><p>é¢„æœŸï¼šå¸Œæœ›ç±»ä¼¼çš„å¥å­ï¼Œèƒ½å¤Ÿå˜æˆæ¯”è¾ƒç›¸è¿‘çš„vectorã€‚ä¸ç±»ä¼¼çš„å¥å­ï¼Œèƒ½å¤Ÿè·ç¦»æ¯”è¾ƒè¿œã€‚</p><p>Decoder(0,200,-23, 122) â€“&gt; text?</p><p>æˆ‘çˆ±[MASK]ç„¶è¯­[MASK]å¤„ç† â€“&gt; vector â€“&gt; æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†</p><p>åœ¨ auto encoder çš„åŸºç¡€ä¸Šåˆè¡ç”Ÿå‡ºäº†å„ç§ç±»å‹çš„ auto encoderï¼Œä¾‹å¦‚ <strong>denoising</strong> auto encoder ï¼ˆ<a href="https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdfï¼‰ã€‚denoising" target="_blank" rel="noopener">https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdfï¼‰ã€‚denoising</a> auto encoder çš„åŸºæœ¬æ€æƒ³æ˜¯è¦åŠ å¼º auto encoder çš„ robustnessã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›æŠŠè¾“å…¥å¥å­çš„ä¸€éƒ¨åˆ†ç»™â€œæ±¡æŸ“â€ (corrupt) äº†ï¼Œä½†æ˜¯æˆ‘ä»¬å¸Œæœ›åœ¨ç»è¿‡ç¼–ç å’Œè§£ç çš„è¿‡ç¨‹ä¹‹åï¼Œæˆ‘ä»¬èƒ½å¤Ÿå¾—åˆ°åŸæ¥çš„æ­£ç¡®çš„å¥å­ã€‚äº‹å®ä¸Š BERT çš„ masking å°±æ˜¯ä¸€ç§â€œæ±¡æŸ“â€çš„æ‰‹æ®µã€‚</p><p>Encoder(corrupt(text)) â€“&gt; vector</p><p>Decoder(vector) â€“&gt; text</p><p>éšæœºäº§ç”Ÿä¸€ä¸ªvector â€“&gt; decoder â€“&gt; ç”Ÿæˆä¸€ä¸ªå¥å­</p><p>mapping </p><p>N(0, 1) â€“&gt; å„ç§å„æ ·çš„æ–‡å­—</p><p>ä»ä¸€ä¸ªåˆ†å¸ƒå»ç”Ÿæˆä¸€äº›ä¸œè¥¿</p><p>ä¸ºäº†è®­ç»ƒå‡ºå¯ä»¥ç”¨æ¥sampleæ–‡å­—çš„æ¨¡å‹ï¼Œäººä»¬å‘æ˜äº†variational auto encoder (VAE)ã€‚VAEä¸æ™®é€šauto encoderçš„ä¸åŒä¹‹å¤„åœ¨äºï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ªconstraintï¼Œå¸Œæœ›encoderç¼–ç çš„æ¯ä¸ªå¥å­éƒ½èƒ½å¤Ÿå±€é™åœ¨æŸäº›ç‰¹å®šçš„ä½ç½®ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è¦æ±‚æ¯ä¸ªå¥å­çš„encodingåœ¨ç©ºé—´ä¸Šæ»¡è¶³ä¸€ä¸ªå¤šç»´æ ‡å‡†é«˜æ–¯åˆ†å¸ƒã€‚</p><p><strong>vector ~ N(0, 1)</strong></p><h2 id="ä»€ä¹ˆæ˜¯VAEï¼Ÿ"><a href="#ä»€ä¹ˆæ˜¯VAEï¼Ÿ" class="headerlink" title="ä»€ä¹ˆæ˜¯VAEï¼Ÿ"></a>ä»€ä¹ˆæ˜¯VAEï¼Ÿ</h2><p>ç½‘ä¸Šæœ‰å¾ˆå¤šVAEçš„è®ºæ–‡ï¼Œåšå®¢ï¼Œå»ºè®®æ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥é€‰æ‹©æ€§é˜…è¯»ã€‚æˆ‘ä»¬è¿™èŠ‚è¯¾ä¸ä¼šè®¨è®ºå¤ªå¤šçš„æ•°å­¦å…¬å¼ï¼Œè€Œæ˜¯ä»æ¯”è¾ƒhigh levelçš„å±‚é¢ä»‹ç»ä¸€ä¸‹VAEæ¨¡å‹ä»¥åŠå®ƒæ‰€è§£å†³çš„ä¸€äº›é—®é¢˜ã€‚</p><p>ç®€å•æ¥è¯´ï¼ŒVAEæœ¬è´¨ä¸Šæ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿé€šè¿‡éšå‘é‡zç”Ÿæˆæ•°æ®æ ·æœ¬xã€‚åœ¨æ–‡æœ¬ç”Ÿæˆçš„é—®é¢˜ä¸­ï¼Œè¿™ä¸ªxå¾€å¾€è¡¨ç¤ºçš„æ˜¯ä¸€äº›æ–‡æœ¬/å¥å­ç­‰å†…å®¹ã€‚</p><p><img src="https://uploader.shimo.im/f/OoUmk9RItWAALQ69.png!thumbnail" alt="img"></p><p>ä¸‹é¢æ˜¯ Kingma åœ¨ <strong>VAE</strong> è®ºæ–‡ä¸­å®šä¹‰çš„ä¼˜åŒ–ç›®æ ‡ã€‚</p><p>æ–‡æœ¬â€“&gt; å‘é‡è¡¨ç¤º â€“&gt; æ–‡æœ¬</p><p>auto encoder: sentence â€“&gt; vector â€“&gt; sentence</p><p>Loss = -log P_{sentence}(dec(enc(sentence)))</p><p><img src="https://uploader.shimo.im/f/1HQEntdhGB8sSbbd.png!thumbnail" alt="img"></p><p>z -&gt; zâ€™ -&gt; decoder(z) â€“&gt; ä¸€ä¸ªå¥å­</p><p><strong>crossentropyloss(decoder(encoder(x)), x)</strong></p><p><strong>æˆ‘ä»¬å¯¹zæ²¡æœ‰ä»»ä½•çš„çº¦æŸæ¡ä»¶</strong></p><p>q: encoder</p><p>p: decoder</p><p>KL divergence: è®¡ç®—ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å·®å€¼</p><p>z: æŠŠå¥å­å˜æˆä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ</p><p>z: (\mu, \sigma) â€“&gt; æ­£æ€åˆ†å¸ƒçš„å‚æ•°</p><p>ç”¨zåšé‡‡æ ·</p><p>KL Divergenceçš„å®šä¹‰</p><p><img src="https://uploader.shimo.im/f/YQtU3e2EeBc0e4VH.png!thumbnail" alt="img"></p><p>sampling</p><p>N(0,1): sampling: 0.1, 0.05, 0.2, -0.1, -100</p><p>æˆ‘ä»¬å¯ä»¥å‘ç°ï¼ŒVAEæ¨¡å‹æœ¬è´¨ä¸Šå°±æ˜¯è¦æœ€å¤§åŒ–æ ·æœ¬çš„ç”Ÿæˆæ¦‚ç‡ï¼Œå¹¶ä¸”æœ€å°åŒ–æ ·æœ¬encodeä¹‹åçš„å‚æ•°è¡¨ç¤ºä¸æŸç§åˆ†å¸ƒ(æ­£æ€åˆ†å¸ƒ)çš„KLæ•£åº¦ã€‚ä¹‹æ‰€ä»¥æˆ‘ä»¬ä¼šé™åˆ¶æ•°æ®è¢«ç¼–ç åçš„å‘é‡æœä»æŸä¸ªå±€éƒ¨çš„æ­£æ€åˆ†å¸ƒï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬ä¸å¸Œæœ›è¿™äº›æ•°æ®è¢«ç¼–ç ä¹‹åæ‚ä¹±åœ°æ•£å¸ƒåœ¨ä¸€ä¸ªç©ºé—´ä¸Šï¼Œè€Œæ˜¯å¸Œæœ›ä¿¡æ¯èƒ½å¤Ÿå¾—åˆ°ä¸€å®šç¨‹åº¦ä¸Šçš„å‹ç¼©ã€‚ä¹‹æ‰€ä»¥è®©ä»–ä»¬æœä»ä¸€ä¸ªåˆ†å¸ƒè€Œä¸æ˜¯ä¸€äº›å›ºå®šçš„å€¼ï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬å¸Œæœ›æ¨¡å‹ä¸­èƒ½å¤Ÿæœ‰ä¸€äº›éšæœºæ€§ï¼Œå¥½è®©æ¨¡å‹çš„è§£ç å™¨èƒ½å¤Ÿç”Ÿæˆå„ç§å„æ ·çš„å¥å­ã€‚</p><p>æœ‰äº†è¿™ä¸ªVAEæ¨¡å‹çš„æ¶æ„ä¹‹åï¼Œäººä»¬å°±å¯ä»¥åœ¨å„ç§ä»»åŠ¡ä¸Šç©å‡ºå„ç§ä¸åŒçš„èŠ±æ ·äº†ã€‚</p><p>ä¾‹å¦‚å¯¹äºå›¾åƒæ¥è¯´ï¼Œè¿™é‡Œçš„<img src="https://uploader.shimo.im/f/FSNiIgsmVLc0HyUH.png!thumbnail" alt="img">å’Œ<img src="https://uploader.shimo.im/f/XpeVKp5c144d4RWl.png!thumbnail" alt="img">å¯èƒ½æ˜¯CNNæ¨¡å‹ï¼Œå¯¹äºè‡ªç„¶è¯­è¨€æ¥è¯´ï¼Œå®ƒä»¬å¯èƒ½æ˜¯ä¸€äº›RNN/LSTMä¹‹ç±»çš„æ¨¡å‹ã€‚</p><p>ä¸‹é¢æˆ‘ä»¬æ¥çœ‹ä¸€äº›VAEåœ¨NLPé¢†åŸŸçš„å…·ä½“æ¨¡å‹ã€‚</p><h3 id="Generating-Sentences-from-a-Continuous-Space"><a href="#Generating-Sentences-from-a-Continuous-Space" class="headerlink" title="Generating Sentences from a Continuous Space"></a>Generating Sentences from a Continuous Space</h3><p><a href="https://arxiv.org/pdf/1511.06349.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1511.06349.pdf</a></p><p><img src="https://uploader.shimo.im/f/LeCZ7dpW9JcvYC6T.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/qAE9oBPi2Lg7LJro.png!thumbnail" alt="img"></p><p>ä»ä¸Šå›¾å¯ä»¥çœ‹åˆ°ï¼Œè¿™ç¯‡è®ºæ–‡çš„æ€è·¯éå¸¸ç®€å•ï¼Œå°±æ˜¯æŠŠä¸€ä¸ªå¥å­ç”¨RNNç¼–ç èµ·æ¥ï¼Œç¼–ç ä¹‹åå¾—åˆ°çš„éšå‘é‡è¾“å‡ºä¸¤ä¸ªä¿¡æ¯\muå’Œ\simgaï¼Œåˆ†åˆ«è¡¨ç¤ºä¸€ä¸ªæ­£å¤ªåˆ†å¸ƒçš„å¹³å‡å€¼å’Œæ ‡å‡†å·®ã€‚ç„¶åè¿™ä¸ªåˆ†å¸ƒåº”è¯¥å°½å¯èƒ½åœ°æ¥è¿‘æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œåœ¨KLæ•£åº¦çš„è¡¨ç¤ºä¸‹ã€‚å¹¶ä¸”å¦‚æœæˆ‘ä»¬ç”¨è¿™ä¸ªåˆ†å¸ƒå»é‡‡æ ·å¾—åˆ°æ–°çš„å‘é‡è¡¨ç¤ºï¼Œé‚£ä¹ˆdecoderåº”è¯¥è¦å°½å¯èƒ½å¥½åœ°å¤åŸæˆ‘ä»¬åŸæ¥çš„è¿™ä¸ªå¥å­ã€‚</p><p>å…·ä½“çš„å®éªŒç»†èŠ‚æˆ‘ä»¬å°±ä¸å±•å¼€äº†ï¼Œä½†æ˜¯æˆ‘ä»¬çœ‹ä¸€äº›è®ºæ–‡ä¸­å±•ç¤ºçš„ç”Ÿæˆçš„å¥å­ã€‚</p><p><img src="https://uploader.shimo.im/f/WTJqo8usWYYxMR8k.png!thumbnail" alt="img"></p><p>ä¸‹é¢çœ‹çœ‹VAEå½“ä¸­ç¼–ç çš„ç©ºé—´æ˜¯å¦å…·æœ‰æŸç§è¿ç»­æ€§ã€‚</p><p><img src="https://uploader.shimo.im/f/uLmDTf81yPUZJbae.png!thumbnail" alt="img"></p><p>ä»£ç é˜…è¯»ï¼š</p><ul><li><p><a href="https://github.com/timbmg/Sentence-VAE/blob/master/model.py" target="_blank" rel="noopener">https://github.com/timbmg/Sentence-VAE/blob/master/model.py</a></p></li><li><p><strong>ç»ƒä¹ </strong>ï¼šè¿™ä»½ä»£ç å·²ç»ä¸€å¹´å¤šæ²¡æœ‰æ›´æ–°äº†ï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥æŠŠå®ƒæ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬çš„PyTorchä¸Šï¼Œä½œä¸ºå†™ä»£ç ç»ƒä¹ ï¼Œå¹¶ä¸”åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šåšä¸€äº›å®éªŒï¼Œçœ‹çœ‹èƒ½å¦å¾—åˆ°ä¸è®ºæ–‡ä¸­ç±»ä¼¼çš„æ•ˆæœï¼ˆsentence interpolationï¼‰ã€‚</p></li></ul><p>GAN: generative adversarial networks</p><ul><li><p>generator: G(z) â€“&gt; x ä¸€å¼ é€¼çœŸçš„æ±½è½¦ç…§ç‰‡</p></li><li><p>discriminator: D(x) â€“&gt; è¿™ä¸ªåˆ°åº•æ˜¯ä¸æ˜¯ä¸€å¼ æ±½è½¦çš„ç…§ç‰‡ äºŒåˆ†ç±»</p></li></ul><p>Discriminatorçš„ç›®æ ‡</p><p>D(G(z)) â€“&gt; False</p><p>D(true photo) â€“&gt; True</p><p>Generator çš„ç›®æ ‡ D(G(z)) â€“&gt; True</p><h2 id="å¯æ§åˆ¶çš„æ–‡æœ¬ç”Ÿæˆ"><a href="#å¯æ§åˆ¶çš„æ–‡æœ¬ç”Ÿæˆ" class="headerlink" title="å¯æ§åˆ¶çš„æ–‡æœ¬ç”Ÿæˆ"></a>å¯æ§åˆ¶çš„æ–‡æœ¬ç”Ÿæˆ</h2><h3 id="Toward-Controlled-Generation-of-Text"><a href="#Toward-Controlled-Generation-of-Text" class="headerlink" title="Toward Controlled Generation of Text"></a>Toward Controlled Generation of Text</h3><p><a href="https://arxiv.org/pdf/1703.00955.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1703.00955.pdf</a></p><ul><li><p>Controlled Text Generation: æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„ä¸€äº›ç‰¹å¾</p></li><li><p>Learning disentangled latent representations: å¯¹äºæ–‡æœ¬ä¸åŒçš„ç‰¹å¾æœ‰ä¸åŒçš„å‘é‡è¡¨ç¤º</p></li></ul><p>æ¨¡å‹</p><p><img src="https://uploader.shimo.im/f/NejrcnoWDRgz7k58.png!thumbnail" alt="img"></p><p>To model and control the attributes of interest in an interpretable way, we augment the unstructured variables z with a set of structured variables c each of which targets a salient and independent semantic feature of sentences.</p><p>è¿™ç¯‡æ–‡ç« è¯•å›¾è§£å†³è¿™æ ·ä¸€ä¸ªé—®é¢˜ï¼Œèƒ½ä¸èƒ½æŠŠä¸€å¥è¯ç¼–ç æˆå‡ ä¸ªå‘é‡(zå’Œc)ã€‚zå’Œcåˆ†åˆ«åŒ…å«äº†ä¸€äº›ä¸åŒçš„å…³äºå¥å­çš„ä¿¡æ¯ã€‚</p><p><img src="https://uploader.shimo.im/f/LWIaBAzxmwkLY0pj.png!thumbnail" alt="img"></p><p>æ¨¡å‹åŒ…å«å‡ ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ªgeneratorå¯ä»¥åŸºäºè‹¥å¹²ä¸ªå‘é‡(zå’Œc)ç”Ÿæˆå¥å­ï¼Œå‡ ä¸ªencoderå¯ä»¥ä»å¥å­ç”Ÿæˆzå’Œcçš„åˆ†å¸ƒï¼Œå‡ ä¸ªdiscriminatorç”¨æ¥åˆ¤æ–­æ¨¡å‹ç¼–ç å‡ºçš„å‘é‡(c)æ˜¯å¦ç¬¦åˆexampleçš„æ­£ç¡®åˆ†ç±»ã€‚è¿™ä¸ªæ¨¡å‹çš„å¥½å¤„æ˜¯ï¼Œæˆ‘ä»¬åœ¨æŸç§ç¨‹åº¦ä¸Šåˆ†ç¦»äº†å¥å­çš„ä¿¡æ¯ã€‚ä¾‹å¦‚å¦‚æœå‘é‡cç”¨æ¥è¡¨ç¤ºçš„æ˜¯å¥å­çš„æƒ…æ„Ÿæ­£è´Ÿï¼Œé‚£ä¹ˆæ¨¡å‹å°±å…·å¤‡äº†ç”Ÿæˆæ­£é¢æƒ…æ„Ÿçš„å¥å­å’Œè´Ÿé¢æƒ…æ„Ÿå¥å­çš„èƒ½åŠ›ã€‚</p><p><img src="https://uploader.shimo.im/f/IGw674vLSf4tiqHe.png!thumbnail" alt="img"></p><p>å‚è€ƒä»£ç </p><p><a href="https://github.com/wiseodd/controlled-text-generation" target="_blank" rel="noopener">https://github.com/wiseodd/controlled-text-generation</a></p><p>æ›´å¤šé˜…è¯»</p><p>VAEè®ºæ–‡ï¼šAuto-Encoding Variational Bayes <a href="https://arxiv.org/pdf/1312.6114.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1312.6114.pdf</a></p><p>An Introduction to Variational Autoencoders <a href="https://arxiv.org/pdf/1906.02691.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1906.02691.pdf</a></p><p>Stype Transfer</p><p>æ–‡æœ¬ â€“&gt; å†…å®¹zï¼Œé£æ ¼c</p><p>z, æ¢ä¸€ä¸ªé£æ ¼câ€™ â€“&gt; åŒæ ·å†…å®¹ï¼Œä¸åŒé£æ ¼çš„æ–‡æœ¬</p><h1 id="æ–‡æœ¬ç”Ÿæˆçš„åº”ç”¨ï¼šæ–‡æœ¬é£æ ¼è¿ç§»"><a href="#æ–‡æœ¬ç”Ÿæˆçš„åº”ç”¨ï¼šæ–‡æœ¬é£æ ¼è¿ç§»" class="headerlink" title="æ–‡æœ¬ç”Ÿæˆçš„åº”ç”¨ï¼šæ–‡æœ¬é£æ ¼è¿ç§»"></a>æ–‡æœ¬ç”Ÿæˆçš„åº”ç”¨ï¼šæ–‡æœ¬é£æ ¼è¿ç§»</h1><h3 id="Style-Transfer-from-Non-Parallel-Text-by-Cross-Alignment"><a href="#Style-Transfer-from-Non-Parallel-Text-by-Cross-Alignment" class="headerlink" title="Style Transfer from Non-Parallel Text by Cross-Alignment"></a>Style Transfer from Non-Parallel Text by Cross-Alignment</h3><p>è®ºæ–‡ï¼š<a href="https://papers.nips.cc/paper/7259-style-transfer-from-non-parallel-text-by-cross-alignment.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/7259-style-transfer-from-non-parallel-text-by-cross-alignment.pdf</a></p><p>ä»£ç ï¼š<a href="https://github.com/shentianxiao/language-style-transfer/blob/master/code/style_transfer.py" target="_blank" rel="noopener">https://github.com/shentianxiao/language-style-transfer/blob/master/code/style_transfer.py</a></p><p>style transfer å…¶å®ä¹Ÿæ˜¯controlled text generationçš„ä¸€ç§ï¼Œåªæ˜¯å®ƒcontrolçš„æ˜¯æ–‡æœ¬çš„é£æ ¼ã€‚æ–‡æœ¬é£æ ¼æœ‰å¾ˆå¤šç§ï¼Œä¾‹å¦‚æƒ…æ„Ÿçš„æ­£è´Ÿé¢ï¼Œæ–‡ç« æ˜¯éšæ„çš„è¿˜æ˜¯ä¸¥è‚ƒçš„ã€‚</p><p><img src="https://uploader.shimo.im/f/2BUGTCxcajoerOmj.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/fE6u9Ap33JIRO8So.png!thumbnail" alt="img"></p><p>ä¸€ä¸ªå¾ˆå¥½çš„repoï¼Œæ€»ç»“äº†æ–‡æœ¬é£æ ¼è¿ç§»é¢†åŸŸçš„paper</p><p><a href="https://github.com/fuzhenxin/Style-Transfer-in-Text" target="_blank" rel="noopener">https://github.com/fuzhenxin/Style-Transfer-in-Text</a></p><h1 id="Generative-Adversarial-Networks-GAN-åœ¨NLPä¸Šçš„åº”ç”¨"><a href="#Generative-Adversarial-Networks-GAN-åœ¨NLPä¸Šçš„åº”ç”¨" class="headerlink" title="Generative Adversarial Networks (GAN) åœ¨NLPä¸Šçš„åº”ç”¨"></a>Generative Adversarial Networks (GAN) åœ¨NLPä¸Šçš„åº”ç”¨</h1><p>æœ€æ—©Ian Goodfellowçš„å…³äºGANçš„æ–‡ç« ï¼Œå…¶åŸºæœ¬åšæ³•å°±æ˜¯ä¸€ä¸ª<strong>generator</strong>å’Œä¸€ä¸ª<strong>discriminator</strong>(è¾…åŠ©è§’è‰²)ï¼Œç„¶åè®©ä¸¤ä¸ªæ¨¡å‹äº’ç›¸ç«äº‰å¯¹æŠ—ï¼Œåœ¨å¯¹æŠ—çš„è¿‡ç¨‹ä¸­é€æ¸æå‡å„è‡ªçš„æ¨¡å‹èƒ½åŠ›ã€‚è€Œå…¶ä¸­çš„generatorå°±æ˜¯æˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿæœ€ç»ˆoptimizeå¹¶ä¸”è¢«æ‹¿æ¥ä½¿ç”¨çš„æ¨¡å‹ã€‚</p><p>æ—©æœŸGANä¸»è¦æˆåŠŸåº”ç”¨éƒ½åœ¨äºå›¾åƒé¢†åŸŸã€‚å…¶å…³é”®åŸå› åœ¨äºï¼Œå›¾åƒçš„æ¯ä¸ªåƒç´ éƒ½æ˜¯ä¸‰ä¸ªè¿ç»­çš„RGBæ•°å€¼ã€‚discriminatorå¦‚æœç»™å›¾åƒè®¡ç®—ä¸€ä¸ªæ¦‚ç‡åˆ†æ•°ï¼Œå½“æˆ‘ä»¬åœ¨ä¼˜åŒ–generatorå¸Œæœ›æé«˜è¿™ä¸ªåˆ†æ•°çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨Back Propagationç®—æ³•è®¡ç®—æ¢¯åº¦ï¼Œç„¶ååšæ¢¯åº¦ä¸Šå‡/ä¸‹é™æ¥å®Œæˆæˆ‘ä»¬æƒ³è¦ä¼˜åŒ–çš„ç›®æ ‡ã€‚</p><p>discriminator: äºŒåˆ†ç±»é—®é¢˜ å›¾ç‰‡â€“&gt;åˆ†ç±»</p><p>D(G(z)) â€“&gt; cross entropyloss â€“&gt; backprop åˆ°generator</p><p>æ–‡æœ¬â€“&gt; </p><p>LSTM â€“&gt; P_vocab() â€“&gt; <strong>argmax æ–‡å­—</strong> â€“&gt; discriminator</p><p>LSTM â€“&gt; P_vocab() â€“&gt; discriminator</p><p>è€Œæ–‡æœ¬ç”Ÿæˆæ˜¯ä¸€ä¸ªä¸åŒçš„é—®é¢˜ï¼Œå…¶ç‰¹æ®Šä¹‹å¤„åœ¨äºæˆ‘ä»¬åœ¨åšæ–‡æœ¬ç”Ÿæˆçš„æ—¶å€™æœ‰ä¸€æ­¥<strong>argmax</strong>çš„æ“ä½œï¼Œä¹Ÿå°±æ˜¯è¯´å½“æˆ‘ä»¬åšinferenceç”Ÿæˆæ–‡å­—çš„æ—¶å€™ï¼Œåœ¨è¾“å‡ºå±‚ä½¿ç”¨äº†argmaxæˆ–è€…samplingçš„æ“ä½œã€‚å½“æˆ‘ä»¬æŠŠargmaxæˆ–è€…samplingå¾—åˆ°çš„æ–‡å­—ä¼ ç»™discriminatoræ‰“åˆ†çš„æ—¶å€™ï¼Œæˆ‘ä»¬æ— æ³•ç”¨è¿™ä¸ªåˆ†æ•°åš<strong>back propagation</strong>å¯¹ç”Ÿæˆå™¨åšä¼˜åŒ–æ“ä½œã€‚</p><p>çœŸæ­£çš„sample â€“&gt; one hot vector ([1, 0, 0, 0, 0, 0])</p><p>é¢„æµ‹ä¸€ä¸ªè¾“å‡ºå•è¯çš„æ—¶å€™ï¼š([0.8, 0.1, 0, 0.05, 0, 0.05]) â€“&gt; gumbel_softmax â€“&gt; discriminatoråˆ¤æ–­ä¸€ä¸‹</p><p>ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œäººä»¬å¤§è‡´èµ°äº†ä¸¤æ¡è·¯çº¿ï¼Œä¸€æ¡æ˜¯å°†æ™®é€šçš„argmaxè½¬å˜æˆå¯å¯¼çš„Gumbel-softmaxï¼Œç„¶åæˆ‘ä»¬å°±å¯ä»¥åŒæ—¶ä¼˜åŒ–generatorå’Œdiscriminatoräº†ã€‚</p><p>é¢„æµ‹ä¸€ä¸ªè¾“å‡ºå•è¯çš„æ—¶å€™ï¼š([0.8, 0.1, 0, 0.05, 0, 0.05]) â€“&gt; gumbel_softmax â€“&gt; discriminatoråˆ¤æ–­ä¸€ä¸‹</p><p><a href="https://arxiv.org/pdf/1611.04051.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1611.04051.pdf</a></p><p><a href="https://www.zhihu.com/question/62631725" target="_blank" rel="noopener">https://www.zhihu.com/question/62631725</a></p><p>å¦å¤–ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨<strong>Reinforcement Learning</strong>ä¸­çš„<strong>Policy Gradient</strong>æ¥ä¼°ç®—æ¨¡å‹çš„gradientï¼Œå¹¶åšä¼˜åŒ–ã€‚</p><p>æ ¹æ®å½“å‰çš„policyæ¥<strong>sample</strong> stepsã€‚</p><p>NLPï¼š policyå°±æ˜¯æˆ‘ä»¬çš„<strong>è¯­è¨€æ¨¡å‹</strong>ï¼Œä¹Ÿå°±æ˜¯è¯´æ ¹æ®å½“å‰çš„hidden state, å†³å®šæˆ‘ä¸‹ä¸€æ­¥è¦ç”Ÿæˆä»€ä¹ˆå•è¯ã€‚</p><p>P_vocab â€“&gt; argmax</p><p>P_vocab â€“&gt; sampling</p><p>backpropagation â€“&gt; æ²¡æœ‰åŠæ³•æ›´æ–°æ¨¡å‹</p><p>æ–‡æœ¬ç¿»è¯‘ â€“ ä¼˜åŒ–<strong>BLEU?</strong></p><p>è®­ç»ƒï¼Ÿ cross entropy loss</p><p>policy gradientç›´æ¥ä¼˜åŒ–BLEU</p><p>å¯ä»¥ä¸å¯ä»¥æ‰¾ä¸ªæ–¹æ³•ä¼°ç®—gradientã€‚</p><p>Policy: å½“å‰æ‰§è¡Œçš„ç­–ç•¥,åœ¨æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œè¿™ä¸ªPolicyä¸€èˆ¬å°±æ˜¯æŒ‡æˆ‘ä»¬çš„decoder(LSTM)</p><p>Policy Gradient: æ ¹æ®å½“å‰çš„policyæ‰§è¡Œä»»åŠ¡ï¼Œç„¶åå¾—åˆ°rewardï¼Œå¹¶ä¼°ç®—æ¯ä¸ªå‚æ•°çš„gradient, SGD</p><p>è¿™é‡Œå°±æ¶‰åŠåˆ°ä¸€äº›Reinforcement Learningå½“ä¸­çš„åŸºæœ¬çŸ¥è¯†ã€‚æˆ‘ä»¬å¯ä»¥è®¤ä¸ºä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚LSTMï¼Œæ˜¯åœ¨åšä¸€è¿ä¸²è¿ç»­çš„å†³ç­–ã€‚æ¯ä¸€ä¸ªdecodingçš„æ­¥éª¤ï¼Œæ¯ä¸ªhidden stateå¯¹åº”ä¸€ä¸ª<strong>çŠ¶æ€state</strong>ï¼Œæ¯ä¸ªè¾“å‡ºå¯¹åº”ä¸€ä¸ª<strong>observation</strong>ã€‚å¦‚æœæˆ‘ä»¬æ¯æ¬¡è¾“å‡ºä¸€ä¸ªæ–‡å­—çš„æ—¶å€™ä½¿ç”¨samplingçš„æ–¹æ³•ï¼ŒReinforcement Learningæœ‰ä¸€å¥—æˆç†Ÿçš„ç®—æ³•å¯ä»¥å¸®åŠ©æˆ‘ä»¬ä¼°ç®—æ¨¡å‹çš„æ¢¯åº¦ï¼Œè¿™ç§ç®—æ³•å«åšpolicy gradientã€‚å¦‚æœé‡‡ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚</p><p><a href="https://arxiv.org/pdf/1609.05473.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1609.05473.pdf</a></p><p>è¿™ä¸€å¥—policy gradientçš„åšæ³•åœ¨å¾ˆå¤šæ–‡æœ¬ç”Ÿæˆï¼ˆä¾‹å¦‚ç¿»è¯‘ï¼Œimage captioningï¼‰çš„ä¼˜åŒ–é—®é¢˜ä¸Šä¹Ÿç»å¸¸è§åˆ°ã€‚</p><p>ç¿»è¯‘ï¼šä¼˜åŒ–BLEU</p><p>Improved Image Captioning via Policy Gradient optimization of SPIDEr</p><p><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Improved_Image_Captioning_ICCV_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Improved_Image_Captioning_ICCV_2017_paper.pdf</a></p><p>è¿˜æœ‰ä¸€äº›æ–¹æ³•æ˜¯ï¼Œæˆ‘ä»¬ä¸åšæœ€ç»ˆçš„æ–‡æœ¬é‡‡æ ·ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨æ¨¡å‹è¾“å‡ºçš„åœ¨å•è¯è¡¨ä¸Šçš„è¾“å‡ºåˆ†å¸ƒï¼Œæˆ–è€…æ˜¯ä½¿ç”¨LSTMä¸­çš„ä¸€äº›hidden vectoræ¥ä¼ ç»™discriminatorï¼Œå¹¶ç›´æ¥ä¼˜åŒ–è¯­è¨€æ¨¡å‹ã€‚</p><p>æˆ‘ä¸ªäººçš„çœ‹æ³•æ˜¯GANåœ¨æ–‡æœ¬ç”Ÿæˆä¸Šçš„ä½œç”¨å¤§å°è¿˜ä¸æ˜ç¡®ï¼Œä¸€éƒ¨åˆ†åŸå› åœ¨äºæˆ‘ä»¬æ²¡æœ‰ä¸€ç§å¾ˆå¥½çš„æœºåˆ¶å»è¯„ä¼°æ–‡æœ¬ç”Ÿæˆçš„å¥½åã€‚æˆ‘ä»¬çœ‹åˆ°å¾ˆå¤šè®ºæ–‡å…¶å®å¯¹æ¨¡å‹çš„å¥½åæ²¡æœ‰æ˜ç¡®çš„è¯„ä»·ï¼Œå¾ˆå¤šæ—¶å€™æ˜¯éšæœºäº§ç”Ÿå‡ ä¸ªå¥å­ï¼Œç„¶åç”±ä½œè€…æ¥è¯„ä»·ä¸€ä¸‹ç”Ÿæˆå¥å­çš„å¥½åã€‚</p><h1 id="Data-to-text"><a href="#Data-to-text" class="headerlink" title="Data-to-text"></a>Data-to-text</h1><p><img src="https://uploader.shimo.im/f/Fo4ylW4dnbgm68U9.png!thumbnail" alt="img"></p><ul><li><p>Content selection: é€‰æ‹©ä»€ä¹ˆæ•°æ®éœ€è¦è¿›å…¥åˆ°æˆ‘ä»¬çš„æ–‡æœ¬ä¹‹ä¸­</p></li><li><p>Sentence planning: å†³å®šå¥å­çš„ç»“æ„</p></li><li><p>Surface realization: æŠŠå¥å­ç»“æ„è½¬åŒ–æˆå…·ä½“çš„å­—ç¬¦ä¸²</p></li></ul><p><img src="https://uploader.shimo.im/f/9QQQLucVUMsIVS6P.png!thumbnail" alt="img"></p><p>é—®é¢˜å®šä¹‰</p><ul><li><p>è¾“å…¥: A table of recordsã€‚æ¯ä¸ªrecordåŒ…å«å››ä¸ªfeatures: type, entity, value, home or away</p></li><li><p>è¾“å‡º: ä¸€æ®µæ–‡å­—æè¿°</p></li></ul><h2 id="ç›¸å…³èµ„æ–™"><a href="#ç›¸å…³èµ„æ–™" class="headerlink" title="ç›¸å…³èµ„æ–™"></a>ç›¸å…³èµ„æ–™</h2><p><a href="https://github.com/Morde-kaiser/LearningNotes/blob/master/GAN-Overview-Chinese.pdf" target="_blank" rel="noopener">https://github.com/Morde-kaiser/LearningNotes/blob/master/GAN-Overview-Chinese.pdf</a></p><p>William Wangå…³äºGAN in NLPçš„slides: <a href="http://sameersingh.org/files/ppts/naacl19-advnlp-part1-william-slides.pdf" target="_blank" rel="noopener">http://sameersingh.org/files/ppts/naacl19-advnlp-part1-william-slides.pdf</a></p><p>è¿™ç¯‡åšæ–‡ä¹Ÿè®²çš„å¾ˆå¥½</p><p><a href="https://zhuanlan.zhihu.com/p/29168803" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29168803</a></p><p>å‚è€ƒè¯¥çŸ¥ä¹ä¸“æ æ–‡ç«  <a href="https://zhuanlan.zhihu.com/p/36880287" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/36880287</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ä¸»è¦è®¨è®º&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;æ–‡æœ¬ç”Ÿæˆçš„æ–¹æ³•ï¼š&lt;strong&gt;inference&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;å¢åŠ æ–‡æœ¬ç”Ÿæˆçš„å¤šæ ·æ€§ï¼š&lt;strong&gt;variational auto encoder&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;
      
    
    </summary>
    
      <category term="NLP" scheme="http://mmyblog.cn/categories/NLP/"/>
    
    
      <category term="inference" scheme="http://mmyblog.cn/tags/inference/"/>
    
  </entry>
  
  <entry>
    <title>BERT&amp;ELMo&amp;co</title>
    <link href="http://mmyblog.cn/2020/05/09/%E5%B8%B8%E8%A7%81%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    <id>http://mmyblog.cn/2020/05/09/å¸¸è§é¢„è®­ç»ƒæ¨¡å‹/</id>
    <published>2020-05-09T00:25:57.000Z</published>
    <updated>2020-06-09T00:56:26.079Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ã€è¯‘ã€‘The-Illustrated-BERT-ELMo-and-co"><a href="#ã€è¯‘ã€‘The-Illustrated-BERT-ELMo-and-co" class="headerlink" title="ã€è¯‘ã€‘The Illustrated BERT, ELMo, and co."></a>ã€è¯‘ã€‘The Illustrated BERT, ELMo, and co.</h2><p><a href="https://www.mihaileric.com/posts/deep-contextualized-word-representations-elmo/" target="_blank" rel="noopener">ELMo: Contextualized Word Vectors</a></p><p>æœ¬æ–‡ç”±Adam Liuæˆæƒè½¬è½½ï¼Œæºé“¾æ¥ <a href="https://blog.csdn.net/qq_41664845/article/details/84787969#comments" target="_blank" rel="noopener">https://blog.csdn.net/qq_41664845/article/details/84787969</a></p><p>åŸæ–‡é“¾æ¥ï¼šThe Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</p><p>ä½œè€…ï¼šJay Alammar</p><p>ä¿®æ”¹ï¼šè¤šåˆ™ä¼Ÿ <a href="mailto:zeweichu@gmail.com" target="_blank" rel="noopener">zeweichu@gmail.com</a></p><p>BERTè®ºæ–‡åœ°å€ï¼šBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding <a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">https://arxiv.org/abs/1810.04805</a></p><h1 id="å‰è¨€"><a href="#å‰è¨€" class="headerlink" title="å‰è¨€"></a>å‰è¨€</h1><p>2018å¹´å¯è°“æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„å…ƒå¹´ï¼Œåœ¨æˆ‘ä»¬å¦‚ä½•ä»¥æœ€èƒ½æ•æ‰æ½œåœ¨è¯­ä¹‰å…³ç³»çš„æ–¹å¼  æ¥è¾…åŠ©è®¡ç®—æœºå¯¹çš„å¥å­æ¦‚å¿µæ€§çš„ç†è§£ è¿™æ–¹é¢å–å¾—äº†æå¤§çš„å‘å±•è¿›æ­¥ã€‚æ­¤å¤–ï¼Œ NLPé¢†åŸŸçš„ä¸€äº›å¼€æºç¤¾åŒºå·²ç»å‘å¸ƒäº†å¾ˆå¤šå¼ºå¤§çš„ç»„ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è‡ªå·±çš„æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å…è´¹çš„ä¸‹è½½ä½¿ç”¨ã€‚ï¼ˆå¯ä»¥è¯´ä»Šå¹´æ˜¯NLPçš„ImageNetæ—¶åˆ»ï¼Œå› ä¸ºè¿™å’Œå‡ å¹´å‰è®¡ç®—æœºè§†è§‰çš„å‘å±•å¾ˆç›¸ä¼¼ï¼‰</p><p><img src="https://uploader.shimo.im/f/Z0tgsQt24GAoKjkj.png!thumbnail" alt="img"></p><p>ä¸Šå›¾ä¸­ï¼Œæœ€æ–°å‘å¸ƒçš„BERTæ˜¯ä¸€ä¸ªNLPä»»åŠ¡çš„é‡Œç¨‹ç¢‘å¼æ¨¡å‹ï¼Œå®ƒçš„å‘å¸ƒåŠ¿å¿…ä¼šå¸¦æ¥ä¸€ä¸ªNLPçš„æ–°æ—¶ä»£ã€‚BERTæ˜¯ä¸€ä¸ªç®—æ³•æ¨¡å‹ï¼Œå®ƒçš„å‡ºç°æ‰“ç ´äº†å¤§é‡çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„è®°å½•ã€‚åœ¨BERTçš„è®ºæ–‡å‘å¸ƒä¸ä¹…åï¼ŒGoogleçš„ç ”å‘å›¢é˜Ÿè¿˜å¼€æ”¾äº†è¯¥æ¨¡å‹çš„ä»£ç ï¼Œå¹¶æä¾›äº†ä¸€äº›åœ¨å¤§é‡æ•°æ®é›†ä¸Šé¢„è®­ç»ƒå¥½çš„ç®—æ³•æ¨¡å‹ä¸‹è½½æ–¹å¼ã€‚Gooleå¼€æºè¿™ä¸ªæ¨¡å‹ï¼Œå¹¶æä¾›é¢„è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¿™ä½¿å¾—æ‰€æœ‰äººéƒ½å¯ä»¥é€šè¿‡å®ƒæ¥æ„å»ºä¸€ä¸ªæ¶‰åŠNLPçš„ç®—æ³•æ¨¡å‹ï¼ŒèŠ‚çº¦äº†å¤§é‡è®­ç»ƒè¯­è¨€æ¨¡å‹æ‰€éœ€çš„æ—¶é—´ï¼Œç²¾åŠ›ï¼ŒçŸ¥è¯†å’Œèµ„æºã€‚</p><p><img src="https://uploader.shimo.im/f/6xxJC31NvvYDCGFQ.png!thumbnail" alt="img"></p><p>BERTé›†æˆäº†æœ€è¿‘ä¸€æ®µæ—¶é—´å†…NLPé¢†åŸŸä¸­çš„ä¸€äº›é¡¶å°–çš„æ€æƒ³ï¼ŒåŒ…æ‹¬ä½†ä¸é™äº Semi-supervised Sequence Learning (by Andrew Dai and Quoc Le), ELMo (by Matthew Peters and researchers from AI2 and UW CSE), ULMFiT (by fast.ai founder Jeremy Howard and Sebastian Ruder), and the OpenAI transformer (by OpenAI researchers Radford, Narasimhan, Salimans, and Sutskever), and the Transformer (Vaswani et al).ã€‚</p><p>ä½ éœ€è¦æ³¨æ„ä¸€äº›äº‹æƒ…æ‰èƒ½æ°å½“çš„ç†è§£BERTçš„å†…å®¹ï¼Œä¸è¿‡ï¼Œåœ¨ä»‹ç»æ¨¡å‹æ¶‰åŠçš„æ¦‚å¿µä¹‹å‰å¯ä»¥ä½¿ç”¨BERTçš„æ–¹æ³•ã€‚ </p><h2 id="ç¤ºä¾‹ï¼šå¥å­åˆ†ç±»"><a href="#ç¤ºä¾‹ï¼šå¥å­åˆ†ç±»" class="headerlink" title="ç¤ºä¾‹ï¼šå¥å­åˆ†ç±»"></a>ç¤ºä¾‹ï¼šå¥å­åˆ†ç±»</h2><p>ä½¿ç”¨BERTæœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯åšä¸€ä¸ªæ–‡æœ¬åˆ†ç±»æ¨¡å‹ï¼Œè¿™æ ·çš„æ¨¡å‹ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="https://uploader.shimo.im/f/8T7zkJ6MWgwE98oi.png!thumbnail" alt="img"></p><p>ä¸ºäº†è®­ç»ƒä¸€ä¸ªè¿™æ ·çš„æ¨¡å‹ï¼Œï¼ˆä¸»è¦æ˜¯è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼‰ï¼Œåœ¨è®­ç»ƒé˜¶æ®µBERTæ¨¡å‹å‘ç”Ÿçš„å˜åŒ–å¾ˆå°ã€‚è¯¥è®­ç»ƒè¿‡ç¨‹ç§°ä¸ºå¾®è°ƒï¼Œå¹¶ä¸”æºäº Semi-supervised Sequence Learning å’Œ ULMFiT.ã€‚</p><p>ä¸ºäº†æ›´æ–¹ä¾¿ç†è§£ï¼Œæˆ‘ä»¬ä¸‹é¢ä¸¾ä¸€ä¸ªåˆ†ç±»å™¨çš„ä¾‹å­ã€‚åˆ†ç±»å™¨æ˜¯å±äºç›‘ç£å­¦ä¹ é¢†åŸŸçš„ï¼Œè¿™æ„å‘³ç€ä½ éœ€è¦ä¸€äº›æ ‡è®°çš„æ•°æ®æ¥è®­ç»ƒè¿™äº›æ¨¡å‹ã€‚å¯¹äºåƒåœ¾é‚®ä»¶åˆ†ç±»å™¨çš„ç¤ºä¾‹ï¼Œæ ‡è®°çš„æ•°æ®é›†ç”±é‚®ä»¶çš„å†…å®¹å’Œé‚®ä»¶çš„ç±»åˆ«2éƒ¨åˆ†ç»„æˆï¼ˆç±»åˆ«åˆ†ä¸ºâ€œåƒåœ¾é‚®ä»¶â€æˆ–â€œéåƒåœ¾é‚®ä»¶â€ï¼‰ã€‚</p><h1 id="æ¨¡å‹æ¶æ„"><a href="#æ¨¡å‹æ¶æ„" class="headerlink" title="æ¨¡å‹æ¶æ„"></a>æ¨¡å‹æ¶æ„</h1><p>ç°åœ¨æ‚¨å·²ç»äº†è§£äº†å¦‚ä½•ä½¿ç”¨BERTçš„ç¤ºä¾‹ï¼Œè®©æˆ‘ä»¬ä»”ç»†äº†è§£ä¸€ä¸‹ä»–çš„å·¥ä½œåŸç†ã€‚</p><p><img src="https://uploader.shimo.im/f/1jNYmPwPIDEzhsLv.png!thumbnail" alt="img"></p><p>BERTçš„è®ºæ–‡ä¸­ä»‹ç»äº†2ç§ç‰ˆæœ¬ï¼š</p><ul><li><p>BERT BASE - ä¸OpenAI Transformerçš„å°ºå¯¸ç›¸å½“ï¼Œä»¥ä¾¿æ¯”è¾ƒæ€§èƒ½</p></li><li><p>BERT LARGE - ä¸€ä¸ªéå¸¸åºå¤§çš„æ¨¡å‹ï¼Œå®ƒå®Œæˆäº†æœ¬æ–‡ä»‹ç»çš„æœ€å…ˆè¿›çš„ç»“æœã€‚</p></li></ul><p>BERTçš„åŸºç¡€é›†æˆå•å…ƒæ˜¯Transformerçš„Encoderã€‚å…³äºTransformerçš„ä»‹ç»å¯ä»¥é˜…è¯»ä½œè€…ä¹‹å‰çš„æ–‡ç« ï¼šThe Illustrated Transformerï¼Œè¯¥æ–‡ç« è§£é‡Šäº†Transformeræ¨¡å‹ - BERTçš„åŸºæœ¬æ¦‚å¿µä»¥åŠæˆ‘ä»¬æ¥ä¸‹æ¥è¦è®¨è®ºçš„æ¦‚å¿µã€‚</p><p>2ä¸ªBERTçš„æ¨¡å‹éƒ½æœ‰ä¸€ä¸ªå¾ˆå¤§çš„ç¼–ç å™¨å±‚æ•°ï¼Œï¼ˆè®ºæ–‡é‡Œé¢å°†æ­¤ç§°ä¸ºTransformer Blocksï¼‰ - åŸºç¡€ç‰ˆæœ¬å°±æœ‰12å±‚ï¼Œè¿›é˜¶ç‰ˆæœ¬æœ‰24å±‚ã€‚åŒæ—¶å®ƒä¹Ÿæœ‰å¾ˆå¤§çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆ 768å’Œ1024ä¸ªéšè—å±‚ç¥ç»å…ƒï¼‰ï¼Œè¿˜æœ‰å¾ˆå¤šattention headsï¼ˆ12-16ä¸ªï¼‰ã€‚è¿™è¶…è¿‡äº†Transformerè®ºæ–‡ä¸­çš„å‚è€ƒé…ç½®å‚æ•°ï¼ˆ6ä¸ªç¼–ç å™¨å±‚ï¼Œ512ä¸ªéšè—å±‚å•å…ƒï¼Œå’Œ8ä¸ªæ³¨æ„å¤´ï¼‰</p><h2 id="æ¨¡å‹è¾“å…¥"><a href="#æ¨¡å‹è¾“å…¥" class="headerlink" title="æ¨¡å‹è¾“å…¥"></a>æ¨¡å‹è¾“å…¥</h2><p>è¾“å…¥çš„ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸º[CLS]ï¼Œåœ¨è¿™é‡Œå­—ç¬¦[CLS]è¡¨è¾¾çš„æ„æ€å¾ˆç®€å• - Classification ï¼ˆåˆ†ç±»ï¼‰ã€‚</p><p>BERTä¸Transformer çš„ç¼–ç æ–¹å¼ä¸€æ ·ã€‚å°†å›ºå®šé•¿åº¦çš„å­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥ï¼Œæ•°æ®ç”±ä¸‹è€Œä¸Šä¼ é€’è®¡ç®—ï¼Œæ¯ä¸€å±‚éƒ½ç”¨åˆ°äº†self attentionï¼Œå¹¶é€šè¿‡å‰é¦ˆç¥ç»ç½‘ç»œä¼ é€’å…¶ç»“æœï¼Œå°†å…¶äº¤ç»™ä¸‹ä¸€ä¸ªç¼–ç å™¨ã€‚</p><p><img src="https://uploader.shimo.im/f/4VwjFpmDltoJInZh.png!thumbnail" alt="img"></p><p>è¿™æ ·çš„æ¶æ„ï¼Œä¼¼ä¹æ˜¯æ²¿ç”¨äº†Transformer çš„æ¶æ„ï¼ˆé™¤äº†å±‚æ•°ï¼Œä¸è¿‡è¿™æ˜¯æˆ‘ä»¬å¯ä»¥è®¾ç½®çš„å‚æ•°ï¼‰ã€‚é‚£ä¹ˆBERTä¸Transformer ä¸åŒä¹‹å¤„åœ¨å“ªé‡Œå‘¢ï¼Ÿå¯èƒ½åœ¨æ¨¡å‹çš„è¾“å‡ºä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ä¸€äº›ç«¯å€ªã€‚</p><h2 id="æ¨¡å‹è¾“å‡º"><a href="#æ¨¡å‹è¾“å‡º" class="headerlink" title="æ¨¡å‹è¾“å‡º"></a>æ¨¡å‹è¾“å‡º</h2><p>æ¯ä¸ªä½ç½®è¿”å›çš„è¾“å‡ºéƒ½æ˜¯ä¸€ä¸ªéšè—å±‚å¤§å°çš„å‘é‡ï¼ˆåŸºæœ¬ç‰ˆæœ¬BERTä¸º768ï¼‰ã€‚ä»¥æ–‡æœ¬åˆ†ç±»ä¸ºä¾‹ï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨ç¬¬ä¸€ä¸ªä½ç½®ä¸Šçš„è¾“å‡ºï¼ˆç¬¬ä¸€ä¸ªä½ç½®æ˜¯åˆ†ç±»æ ‡è¯†[CLS]ï¼‰ ã€‚å¦‚ä¸‹å›¾</p><p>è¯¥å‘é‡ç°åœ¨å¯ä»¥ç”¨ä½œæˆ‘ä»¬é€‰æ‹©çš„åˆ†ç±»å™¨çš„è¾“å…¥ï¼Œåœ¨è®ºæ–‡ä¸­æŒ‡å‡ºä½¿ç”¨å•å±‚ç¥ç»ç½‘ç»œä½œä¸ºåˆ†ç±»å™¨å°±å¯ä»¥å–å¾—å¾ˆå¥½çš„æ•ˆæœã€‚åŸç†å¦‚ä¸‹ï¼š</p><p><img src="https://uploader.shimo.im/f/X6bsq7gTFDERfO9s.png!thumbnail" alt="img"></p><p>ä¾‹å­ä¸­åªæœ‰åƒåœ¾é‚®ä»¶å’Œéåƒåœ¾é‚®ä»¶ï¼Œå¦‚æœä½ æœ‰æ›´å¤šçš„labelï¼Œä½ åªéœ€è¦å¢åŠ è¾“å‡ºç¥ç»å…ƒçš„ä¸ªæ•°å³å¯ï¼Œå¦å¤–æŠŠæœ€åçš„æ¿€æ´»å‡½æ•°æ¢æˆsoftmaxå³å¯ã€‚</p><h2 id="Parallels-with-Convolutional-Netsï¼ˆBERT-VSå·ç§¯ç¥ç»ç½‘ç»œï¼‰"><a href="#Parallels-with-Convolutional-Netsï¼ˆBERT-VSå·ç§¯ç¥ç»ç½‘ç»œï¼‰" class="headerlink" title="Parallels with Convolutional Netsï¼ˆBERT VSå·ç§¯ç¥ç»ç½‘ç»œï¼‰"></a>Parallels with Convolutional Netsï¼ˆBERT VSå·ç§¯ç¥ç»ç½‘ç»œï¼‰</h2><p>å¯¹äºé‚£äº›å…·æœ‰è®¡ç®—æœºè§†è§‰èƒŒæ™¯çš„äººæ¥è¯´ï¼Œè¿™ä¸ªçŸ¢é‡åˆ‡æ¢åº”è¯¥è®©äººè”æƒ³åˆ°VGGNetç­‰ç½‘ç»œçš„å·ç§¯éƒ¨åˆ†ä¸ç½‘ç»œæœ«ç«¯çš„å®Œå…¨è¿æ¥çš„åˆ†ç±»éƒ¨åˆ†ä¹‹é—´å‘ç”Ÿçš„äº‹æƒ…ã€‚ä½ å¯ä»¥è¿™æ ·ç†è§£ï¼Œå®è´¨ä¸Šè¿™æ ·ç†è§£ä¹Ÿå¾ˆæ–¹ä¾¿ã€‚</p><p><img src="https://uploader.shimo.im/f/SIlXQTqB9vM4DEDi.png!thumbnail" alt="img"></p><h1 id="è¯åµŒå…¥çš„æ–°æ—¶ä»£ã€œ"><a href="#è¯åµŒå…¥çš„æ–°æ—¶ä»£ã€œ" class="headerlink" title="è¯åµŒå…¥çš„æ–°æ—¶ä»£ã€œ"></a>è¯åµŒå…¥çš„æ–°æ—¶ä»£ã€œ</h1><p>BERTçš„å¼€æºéšä¹‹è€Œæ¥çš„æ˜¯ä¸€ç§è¯åµŒå…¥çš„æ›´æ–°ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œè¯åµŒå…¥å·²ç»æˆä¸ºNLPæ¨¡å‹å¤„ç†è‡ªç„¶è¯­è¨€çš„ä¸»è¦ç»„æˆéƒ¨åˆ†ã€‚è¯¸å¦‚Word2vecå’ŒGlove ç­‰æ–¹æ³•å·²ç»å¹¿æ³›çš„ç”¨äºå¤„ç†è¿™äº›é—®é¢˜ï¼Œåœ¨æˆ‘ä»¬ä½¿ç”¨æ–°çš„è¯åµŒå…¥ä¹‹å‰ï¼Œæˆ‘ä»¬æœ‰å¿…è¦å›é¡¾ä¸€ä¸‹å…¶å‘å±•ã€‚</p><h2 id="Word-Embedding-Recap"><a href="#Word-Embedding-Recap" class="headerlink" title="Word Embedding Recap"></a>Word Embedding Recap</h2><p>ä¸ºäº†è®©æœºå™¨å¯ä»¥å­¦ä¹ åˆ°æ–‡æœ¬çš„ç‰¹å¾å±æ€§ï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›å°†æ–‡æœ¬æ•°å€¼åŒ–çš„è¡¨ç¤ºçš„æ–¹å¼ã€‚Word2vecç®—æ³•é€šè¿‡ä½¿ç”¨ä¸€ç»„å›ºå®šç»´åº¦çš„å‘é‡æ¥è¡¨ç¤ºå•è¯ï¼Œè®¡ç®—å…¶æ–¹å¼å¯ä»¥æ•è·åˆ°å•è¯çš„è¯­ä¹‰åŠå•è¯ä¸å•è¯ä¹‹é—´çš„å…³ç³»ã€‚ä½¿ç”¨Word2vecçš„å‘é‡åŒ–è¡¨ç¤ºæ–¹å¼å¯ä»¥ç”¨äºåˆ¤æ–­å•è¯æ˜¯å¦ç›¸ä¼¼ï¼Œå¯¹ç«‹ï¼Œæˆ–è€…è¯´åˆ¤æ–­â€œç”·äººâ€˜ä¸â€™å¥³äººâ€çš„å…³ç³»å°±å¦‚åŒâ€œå›½ç‹â€ä¸â€œç‹åâ€ã€‚ï¼ˆè¿™äº›è¯æ˜¯ä¸æ˜¯å¬è…»äº†ã€œ emmmæ°´æ–‡å¿…å¤‡ï¼‰ã€‚å¦å¤–è¿˜èƒ½æ•è·åˆ°ä¸€äº›è¯­æ³•çš„å…³ç³»ï¼Œè¿™ä¸ªåœ¨è‹±è¯­ä¸­å¾ˆå®ç”¨ã€‚ä¾‹å¦‚â€œhadâ€ä¸â€œhasâ€çš„å…³ç³»å¦‚åŒâ€œwasâ€ä¸â€œisâ€çš„å…³ç³»ã€‚</p><p>è¿™æ ·çš„åšæ³•ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¤§é‡çš„æ–‡æœ¬æ•°æ®æ¥é¢„è®­ç»ƒä¸€ä¸ªè¯åµŒå…¥æ¨¡å‹ï¼Œè€Œè¿™ä¸ªè¯åµŒå…¥æ¨¡å‹å¯ä»¥å¹¿æ³›ç”¨äºå…¶ä»–NLPçš„ä»»åŠ¡ï¼Œè¿™æ˜¯ä¸ªå¥½ä¸»æ„ï¼Œè¿™ä½¿å¾—ä¸€äº›åˆåˆ›å…¬å¸æˆ–è€…è®¡ç®—èµ„æºä¸è¶³çš„å…¬å¸ï¼Œä¹Ÿèƒ½é€šè¿‡ä¸‹è½½å·²ç»å¼€æºçš„è¯åµŒå…¥æ¨¡å‹æ¥å®ŒæˆNLPçš„ä»»åŠ¡ã€‚</p><h2 id="ELMoï¼šè¯­å¢ƒé—®é¢˜"><a href="#ELMoï¼šè¯­å¢ƒé—®é¢˜" class="headerlink" title="ELMoï¼šè¯­å¢ƒé—®é¢˜"></a>ELMoï¼šè¯­å¢ƒé—®é¢˜</h2><p>ä¸Šé¢ä»‹ç»çš„è¯åµŒå…¥æ–¹å¼æœ‰ä¸€ä¸ªå¾ˆæ˜æ˜¾çš„é—®é¢˜ï¼Œå› ä¸ºä½¿ç”¨é¢„è®­ç»ƒå¥½çš„è¯å‘é‡æ¨¡å‹ï¼Œé‚£ä¹ˆæ— è®ºä¸Šä¸‹æ–‡çš„è¯­å¢ƒå…³ç³»å¦‚ä½•ï¼Œæ¯ä¸ªå•è¯éƒ½åªæœ‰ä¸€ä¸ªå”¯ä¸€çš„ä¸”å·²ç»å›ºå®šä¿å­˜çš„å‘é‡åŒ–å½¢å¼â€œã€‚Wait a minute â€œ - å‡ºè‡ª(Peters et. al., 2017, McCann et. al., 2017, and yet again Peters et. al., 2018 in the ELMo paper )</p><blockquote><p>â€œ Wait a minute â€è¿™æ˜¯ä¸€ä¸ªæ¬§ç¾æ—¥å¸¸æ¢—ï¼Œç¤ºä¾‹ï¼š</p></blockquote><blockquote><p>â€‹                         æˆ‘ï¼šå…„å¼Ÿï¼Œä½ è®¤çœŸå­¦ä¹ æ·±åº¦ï¼Œæ²¡å‡†èƒ½æ‹¿80Wå¹´è–ªå•Šã€‚</p></blockquote><blockquote><p>â€‹                         ä½ ï¼šWait a minuteï¼Œè¿™ä¹ˆå¥½ï¼Œä½ ä¸ºå•¥ä¸åšã€‚ </p></blockquote><p>è¿™å’Œä¸­æ–‡çš„åŒéŸ³å­—å…¶å®ä¹Ÿç±»ä¼¼ï¼Œç”¨è¿™ä¸ªä¸¾ä¸€ä¸ªä¾‹å­å§ï¼Œ â€˜é•¿â€™ è¿™ä¸ªå­—ï¼Œåœ¨ â€˜é•¿åº¦â€™ è¿™ä¸ªè¯ä¸­è¡¨ç¤ºåº¦é‡ï¼Œåœ¨ â€˜é•¿é«˜â€™ è¿™ä¸ªè¯ä¸­è¡¨ç¤ºå¢åŠ ã€‚é‚£ä¹ˆä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸é€šè¿‡â€é•¿â€™å‘¨å›´æ˜¯åº¦æˆ–è€…æ˜¯é«˜æ¥åˆ¤æ–­å®ƒçš„è¯»éŸ³æˆ–è€…å®ƒçš„è¯­ä¹‰å‘¢ï¼Ÿå—–å˜ï¼Œè¿™ä¸ªé—®é¢˜å°±æ´¾ç”Ÿå‡ºè¯­å¢ƒåŒ–çš„è¯åµŒå…¥æ¨¡å‹ã€‚</p><p><img src="https://uploader.shimo.im/f/AahBpyq3tDodAsMn.png!thumbnail" alt="img"></p><p>EMLoæ”¹å˜Word2vecç±»çš„å°†å•è¯å›ºå®šä¸ºæŒ‡å®šé•¿åº¦çš„å‘é‡çš„å¤„ç†æ–¹å¼ï¼Œå®ƒæ˜¯åœ¨ä¸ºæ¯ä¸ªå•è¯åˆ†é…è¯å‘é‡ä¹‹å‰å…ˆæŸ¥çœ‹æ•´ä¸ªå¥å­ï¼Œç„¶åä½¿ç”¨bi-LSTMæ¥è®­ç»ƒå®ƒå¯¹åº”çš„è¯å‘é‡ã€‚</p><p><img src="https://uploader.shimo.im/f/IPV3LOYXmr8m8GN7.png!thumbnail" alt="img"></p><p>ELMoä¸ºè§£å†³NLPçš„è¯­å¢ƒé—®é¢˜ä½œå‡ºäº†é‡è¦çš„è´¡çŒ®ï¼Œå®ƒçš„LSTMå¯ä»¥ä½¿ç”¨ä¸æˆ‘ä»¬ä»»åŠ¡ç›¸å…³çš„å¤§é‡æ–‡æœ¬æ•°æ®æ¥è¿›è¡Œè®­ç»ƒï¼Œç„¶åå°†è®­ç»ƒå¥½çš„æ¨¡å‹ç”¨ä½œå…¶ä»–NLPä»»åŠ¡çš„è¯å‘é‡çš„åŸºå‡†ã€‚</p><p>ELMoçš„ç§˜å¯†æ˜¯ä»€ä¹ˆï¼Ÿ</p><p>ELMoä¼šè®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹æ¥å—ä¸€ä¸ªå¥å­æˆ–è€…å•è¯çš„è¾“å…¥,è¾“å‡ºæœ€æœ‰å¯èƒ½å‡ºç°åœ¨åé¢çš„ä¸€ä¸ªå•è¯ã€‚æƒ³æƒ³è¾“å…¥æ³•ï¼Œå¯¹å•¦ï¼Œå°±æ˜¯è¿™æ ·çš„é“ç†ã€‚è¿™ä¸ªåœ¨NLPä¸­æˆ‘ä»¬ä¹Ÿç§°ä½œLanguage Modelingã€‚è¿™æ ·çš„æ¨¡å‹å¾ˆå®¹æ˜“å®ç°ï¼Œå› ä¸ºæˆ‘ä»¬æ‹¥æœ‰å¤§é‡çš„æ–‡æœ¬æ•°æ®ä¸”æˆ‘ä»¬å¯ä»¥åœ¨ä¸éœ€è¦æ ‡ç­¾çš„æƒ…å†µä¸‹å»å­¦ä¹ ã€‚</p><p><img src="https://uploader.shimo.im/f/7z7sv9ALI24kQSst.png!thumbnail" alt="img"></p><p>ä¸Šå›¾ä»‹ç»äº†ELMoé¢„è®­ç»ƒçš„è¿‡ç¨‹çš„æ­¥éª¤çš„ä¸€éƒ¨åˆ†ï¼š</p><p>æˆ‘ä»¬éœ€è¦å®Œæˆä¸€ä¸ªè¿™æ ·çš„ä»»åŠ¡ï¼šè¾“å…¥â€œLets stick toâ€ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªæœ€å¯èƒ½å‡ºç°çš„å•è¯ï¼Œå¦‚æœåœ¨è®­ç»ƒé˜¶æ®µä½¿ç”¨å¤§é‡çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œé‚£ä¹ˆåœ¨é¢„æµ‹é˜¶æ®µæˆ‘ä»¬å¯èƒ½å‡†ç¡®çš„é¢„æµ‹å‡ºæˆ‘ä»¬æœŸå¾…çš„ä¸‹ä¸€ä¸ªå•è¯ã€‚æ¯”å¦‚è¾“å…¥â€œæœºå™¨â€ï¼Œåœ¨â€˜â€™å­¦ä¹ â€˜å’Œâ€˜ä¹°èœâ€™ä¸­å®ƒæœ€æœ‰å¯èƒ½çš„è¾“å‡ºä¼šæ˜¯â€˜å­¦ä¹ â€™è€Œä¸æ˜¯â€˜ä¹°èœâ€™ã€‚</p><p>ä»ä¸Šå›¾å¯ä»¥å‘ç°ï¼Œæ¯ä¸ªå±•å¼€çš„LSTMéƒ½åœ¨æœ€åä¸€æ­¥å®Œæˆé¢„æµ‹ã€‚</p><p>å¯¹äº†çœŸæ­£çš„ELMoä¼šæ›´è¿›ä¸€æ­¥ï¼Œå®ƒä¸ä»…èƒ½åˆ¤æ–­ä¸‹ä¸€ä¸ªè¯ï¼Œè¿˜èƒ½é¢„æµ‹å‰ä¸€ä¸ªè¯ã€‚ï¼ˆBi-Lstmï¼‰</p><p><img src="https://uploader.shimo.im/f/HWw1FQCwDbUJkIi5.png!thumbnail" alt="img"></p><p>ELMoé€šè¿‡ä¸‹å›¾çš„æ–¹å¼å°†hidden statesï¼ˆçš„åˆå§‹çš„åµŒå…¥ï¼‰ç»„åˆå’‹å­ä¸€èµ·æ¥æç‚¼å‡ºå…·æœ‰è¯­å¢ƒæ„ä¹‰çš„è¯åµŒå…¥æ–¹å¼ï¼ˆå…¨è¿æ¥ååŠ æƒæ±‚å’Œï¼‰</p><p><img src="https://uploader.shimo.im/f/ZldUQJvmyjsiR5fx.png!thumbnail" alt="img"></p><p>ELMo pretrained embeddingå¯ä»¥åœ¨AllenNLPçš„repoä¸‹æ‰¾åˆ°</p><p><a href="https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md" target="_blank" rel="noopener">https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md</a></p><p>é¡ºä¾¿è¯´ä¸€ä¸‹AllenNLPæœ‰ä¸ªéå¸¸ä¸é”™çš„å…³äºNLPçš„æ•™ç¨‹</p><p><a href="https://github.com/allenai/writing-code-for-nlp-research-emnlp2018" target="_blank" rel="noopener">https://github.com/allenai/writing-code-for-nlp-research-emnlp2018</a></p><p>ELMoçš„å‡ ä½ä½œè€…éƒ½æ˜¯NLPåœˆå†…çš„çŸ¥åäººå£«</p><ul><li><p><a href="https://people.cs.umass.edu/~miyyer/" target="_blank" rel="noopener">Mohit Iyyer: UMass</a></p></li><li><p><a href="https://www.cs.washington.edu/people/faculty/lsz/" target="_blank" rel="noopener">Luke Zettlemoyer: UWashington</a></p></li><li><p><a href="https://matt-gardner.github.io/" target="_blank" rel="noopener">Matt Gardner: Allan AI</a></p></li></ul><h3 id="æ›´å¤šELMoçš„æ¨¡å‹å›¾ç‰‡"><a href="#æ›´å¤šELMoçš„æ¨¡å‹å›¾ç‰‡" class="headerlink" title="æ›´å¤šELMoçš„æ¨¡å‹å›¾ç‰‡"></a>æ›´å¤šELMoçš„æ¨¡å‹å›¾ç‰‡</h3><p><img src="https://uploader.shimo.im/f/khgCdxx0pNIaAVe3.png!thumbnail" alt="img"></p><p>å›¾ç‰‡æ¥æºï¼ˆ<a href="https://tsenghungchen.github.io/posts/elmo/ï¼‰" target="_blank" rel="noopener">https://tsenghungchen.github.io/posts/elmo/ï¼‰</a></p><p><img src="https://uploader.shimo.im/f/TId9a8gwTE0DTjua.png!thumbnail" alt="img"></p><p>å›¾ç‰‡æ¥æºï¼ˆ<a href="https://www.mihaileric.com/posts/deep-contextualized-word-representations-elmo/ï¼‰" target="_blank" rel="noopener">https://www.mihaileric.com/posts/deep-contextualized-word-representations-elmo/ï¼‰</a></p><h2 id="ULM-FiTï¼šNLPé¢†åŸŸåº”ç”¨è¿ç§»å­¦ä¹ "><a href="#ULM-FiTï¼šNLPé¢†åŸŸåº”ç”¨è¿ç§»å­¦ä¹ " class="headerlink" title="ULM-FiTï¼šNLPé¢†åŸŸåº”ç”¨è¿ç§»å­¦ä¹ "></a>ULM-FiTï¼šNLPé¢†åŸŸåº”ç”¨è¿ç§»å­¦ä¹ </h2><p>ULM-FiTæœºåˆ¶è®©æ¨¡å‹çš„é¢„è®­ç»ƒå‚æ•°å¾—åˆ°æ›´å¥½çš„åˆ©ç”¨ã€‚æ‰€åˆ©ç”¨çš„å‚æ•°ä¸ä»…é™äºembeddingsï¼Œä¹Ÿä¸ä»…é™äºè¯­å¢ƒembeddingï¼ŒULM-FiTå¼•å…¥äº†Language Modelå’Œä¸€ä¸ªæœ‰æ•ˆå¾®è°ƒè¯¥Language Modelæ¥æ‰§è¡Œå„ç§NLPä»»åŠ¡çš„æµç¨‹ã€‚è¿™ä½¿å¾—NLPä»»åŠ¡ä¹Ÿèƒ½åƒè®¡ç®—æœºè§†è§‰ä¸€æ ·æ–¹ä¾¿çš„ä½¿ç”¨è¿ç§»å­¦ä¹ ã€‚</p><h2 id="The-Transformerï¼šè¶…è¶ŠLSTMçš„ç»“æ„"><a href="#The-Transformerï¼šè¶…è¶ŠLSTMçš„ç»“æ„" class="headerlink" title="The Transformerï¼šè¶…è¶ŠLSTMçš„ç»“æ„"></a>The Transformerï¼šè¶…è¶ŠLSTMçš„ç»“æ„</h2><p>Transformerè®ºæ–‡å’Œä»£ç çš„å‘å¸ƒï¼Œä»¥åŠå…¶åœ¨æœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ä¸Šå–å¾—çš„ä¼˜å¼‚æˆæœï¼Œè®©ä¸€äº›ç ”ç©¶äººå‘˜è®¤ä¸ºå®ƒæ˜¯LSTMçš„æ›¿ä»£å“ï¼Œäº‹å®ä¸Šå´æ˜¯Transformeræ¯”LSTMæ›´å¥½çš„å¤„ç†long-term dependanciesï¼ˆé•¿ç¨‹ä¾èµ–ï¼‰é—®é¢˜ã€‚Transformer Encodingå’ŒDecodingçš„ç»“æ„éå¸¸é€‚åˆæœºå™¨ç¿»è¯‘ï¼Œä½†æ˜¯æ€ä¹ˆåˆ©ç”¨ä»–æ¥åšæ–‡æœ¬åˆ†ç±»çš„ä»»åŠ¡å‘¢ï¼Ÿå®é™…ä¸Šä½ åªç”¨ä½¿ç”¨å®ƒæ¥é¢„è®­ç»ƒå¯ä»¥é’ˆå¯¹å…¶ä»–ä»»åŠ¡å¾®è°ƒçš„è¯­è¨€æ¨¡å‹å³å¯ã€‚</p><h2 id="OpenAI-Transformerï¼šç”¨äºè¯­è¨€æ¨¡å‹çš„Transformerè§£ç å™¨é¢„è®­ç»ƒ"><a href="#OpenAI-Transformerï¼šç”¨äºè¯­è¨€æ¨¡å‹çš„Transformerè§£ç å™¨é¢„è®­ç»ƒ" class="headerlink" title="OpenAI Transformerï¼šç”¨äºè¯­è¨€æ¨¡å‹çš„Transformerè§£ç å™¨é¢„è®­ç»ƒ"></a>OpenAI Transformerï¼šç”¨äºè¯­è¨€æ¨¡å‹çš„Transformerè§£ç å™¨é¢„è®­ç»ƒ</h2><p>äº‹å®è¯æ˜ï¼Œæˆ‘ä»¬å¹¶ä¸éœ€è¦ä¸€ä¸ªå®Œæ•´çš„transformerç»“æ„æ¥ä½¿ç”¨è¿ç§»å­¦ä¹ å’Œä¸€ä¸ªå¾ˆå¥½çš„è¯­è¨€æ¨¡å‹æ¥å¤„ç†NLPä»»åŠ¡ã€‚æˆ‘ä»¬åªéœ€è¦Transformerçš„è§£ç å™¨å°±è¡Œäº†ã€‚The decoder is a good choice because itâ€™s a natural choice for language modeling (predicting the next word) since itâ€™s built to mask future tokens â€“ a valuable feature when itâ€™s generating a translation word by word.</p><p><img src="https://uploader.shimo.im/f/cBZwrEweFIQLuP0O.png!thumbnail" alt="img"></p><p>è¯¥æ¨¡å‹å †å äº†åäºŒä¸ªDecoderå±‚ã€‚ ç”±äºåœ¨è¯¥è®¾ç½®ä¸­æ²¡æœ‰Encoderï¼Œå› æ­¤è¿™äº›Decoderå°†ä¸å…·æœ‰Transformer Decoderå±‚å…·æœ‰çš„Encoder - Decoder attentionå±‚ã€‚ ç„¶è€Œï¼Œå–è€Œä»£ä¹‹çš„æ˜¯ä¸€ä¸ªself attentionå±‚ï¼ˆmasked so it doesnâ€™t peak at future tokensï¼‰ã€‚</p><p>é€šè¿‡è¿™ç§ç»“æ„è°ƒæ•´ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­åœ¨ç›¸ä¼¼çš„è¯­è¨€æ¨¡å‹ä»»åŠ¡ä¸Šè®­ç»ƒæ¨¡å‹ï¼šä½¿ç”¨å¤§é‡çš„æœªæ ‡è®°æ•°æ®é›†è®­ç»ƒï¼Œæ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚ä¸¾ä¸ªåˆ—å­ï¼šä½ é‚£7000æœ¬ä¹¦å–‚ç»™ä½ çš„æ¨¡å‹ï¼Œï¼ˆä¹¦ç±æ˜¯æå¥½çš„è®­ç»ƒæ ·æœ¬~æ¯”åšå®¢å’Œæ¨æ–‡å¥½å¾ˆå¤šã€‚ï¼‰è®­ç»ƒæ¡†æ¶å¦‚ä¸‹ï¼š</p><p><img src="https://uploader.shimo.im/f/KdcfSdkeNBIb5iRT.png!thumbnail" alt="img"></p><h2 id="Transfer-Learning-to-Downstream-Tasks"><a href="#Transfer-Learning-to-Downstream-Tasks" class="headerlink" title="Transfer Learning to Downstream Tasks"></a>Transfer Learning to Downstream Tasks</h2><p>é€šè¿‡OpenAIçš„transformerçš„é¢„è®­ç»ƒå’Œä¸€äº›å¾®è°ƒåï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œç”¨äºå…¶ä»–ä¸‹æ¸¸NLPä»»åŠ¡å•¦ã€‚ï¼ˆæ¯”å¦‚è®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œç„¶åæ‹¿ä»–çš„hidden stateæ¥åšåˆ†ç±»ã€‚ï¼‰ï¼Œä¸‹é¢å°±ä»‹ç»ä¸€ä¸‹è¿™ä¸ªéªšæ“ä½œã€‚ï¼ˆè¿˜æ˜¯å¦‚ä¸Šé¢ä¾‹å­ï¼šåˆ†ä¸ºåƒåœ¾é‚®ä»¶å’Œéåƒåœ¾é‚®ä»¶ï¼‰</p><p><img src="https://uploader.shimo.im/f/7x6X4ngskaEUd6sY.png!thumbnail" alt="img"></p><p>OpenAIè®ºæ–‡æ¦‚è¿°äº†è®¸å¤šTransformerä½¿ç”¨è¿ç§»å­¦ä¹ æ¥å¤„ç†ä¸åŒç±»å‹NLPä»»åŠ¡çš„ä¾‹å­ã€‚å¦‚ä¸‹å›¾ä¾‹å­æ‰€ç¤ºï¼š</p><p><img src="https://uploader.shimo.im/f/P4V9NbGQz9Q2k213.png!thumbnail" alt="img"></p><h2 id="BERT-From-Decoders-to-Encoders"><a href="#BERT-From-Decoders-to-Encoders" class="headerlink" title="BERT: From Decoders to Encoders"></a>BERT: From Decoders to Encoders</h2><p>OpenAI transformerä¸ºæˆ‘ä»¬æä¾›äº†åŸºäºTransformerçš„ç²¾å¯†çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚ä½†æ˜¯ä»LSTMåˆ°Transformerçš„è¿‡æ¸¡ä¸­ï¼Œæˆ‘ä»¬å‘ç°å°‘äº†äº›ä¸œè¥¿ã€‚ELMoçš„è¯­è¨€æ¨¡å‹æ˜¯åŒå‘çš„ï¼Œä½†æ˜¯OpenAIçš„transformeræ˜¯å‰å‘è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬èƒ½å¦è®©æˆ‘ä»¬çš„Transformeræ¨¡å‹ä¹Ÿå…·æœ‰Bi-Lstmçš„ç‰¹æ€§å‘¢ï¼Ÿ</p><p>R-BERTï¼šâ€œHold my beerâ€</p><h2 id="Masked-Language-Model"><a href="#Masked-Language-Model" class="headerlink" title="Masked Language Model"></a>Masked Language Model</h2><p>BERTè¯´ï¼šâ€œæˆ‘è¦ç”¨ transformer çš„ encodersâ€</p><p>Ernieä¸å±‘é“ï¼šâ€œå‘µå‘µï¼Œä½ ä¸èƒ½åƒBi-Lstmä¸€æ ·è€ƒè™‘æ–‡ç« â€</p><p>BERTè‡ªä¿¡å›ç­”é“ï¼šâ€œæˆ‘ä»¬ä¼šç”¨masksâ€</p><blockquote><p>è§£é‡Šä¸€ä¸‹Maskï¼š</p></blockquote><blockquote></blockquote><blockquote><p>è¯­è¨€æ¨¡å‹ä¼šæ ¹æ®å‰é¢å•è¯æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œä½†æ˜¯self-attentionçš„æ³¨æ„åŠ›åªä¼šæ”¾åœ¨è‡ªå·±èº«ä¸Šï¼Œé‚£ä¹ˆè¿™æ ·100%é¢„æµ‹åˆ°è‡ªå·±ï¼Œæ¯«æ— æ„ä¹‰ï¼Œæ‰€ä»¥ç”¨Maskï¼ŒæŠŠéœ€è¦é¢„æµ‹çš„è¯ç»™æŒ¡ä½ã€‚</p></blockquote><p>å¦‚ä¸‹å›¾ï¼š</p><p><img src="https://uploader.shimo.im/f/jvcJ8SPeBEwszR8M.png!thumbnail" alt="img"></p><h2 id="Two-sentence-Tasks"><a href="#Two-sentence-Tasks" class="headerlink" title="Two-sentence Tasks"></a>Two-sentence Tasks</h2><p>æˆ‘ä»¬å›é¡¾ä¸€ä¸‹OpenAI transformerå¤„ç†ä¸åŒä»»åŠ¡çš„è¾“å…¥è½¬æ¢ï¼Œä½ ä¼šå‘ç°åœ¨æŸäº›ä»»åŠ¡ä¸Šæˆ‘ä»¬éœ€è¦2ä¸ªå¥å­ä½œä¸ºè¾“å…¥ï¼Œå¹¶åšä¸€äº›æ›´ä¸ºæ™ºèƒ½çš„åˆ¤æ–­ï¼Œæ¯”å¦‚æ˜¯å¦ç›¸ä¼¼ï¼Œæ¯”å¦‚ ç»™å‡ºä¸€ä¸ªç»´åŸºç™¾ç§‘çš„å†…å®¹ä½œä¸ºè¾“å…¥ï¼ŒåŒæ—¶åœ¨æ”¾å…¥ä¸€æ¡é’ˆå¯¹è¯¥æ¡ç›®çš„é—®é¢˜ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„ç®—æ³•æ¨¡å‹èƒ½å¤Ÿå¤„ç†è¿™ä¸ªé—®é¢˜å—ï¼Ÿ</p><p>ä¸ºäº†ä½¿BERTæ›´å¥½çš„å¤„ç†2ä¸ªå¥å­ä¹‹é—´çš„å…³ç³»ï¼Œé¢„è®­ç»ƒçš„è¿‡ç¨‹è¿˜æœ‰ä¸€ä¸ªé¢å¤–çš„ä»»åŠ¡ï¼šç»™å®š2ä¸ªå¥å­ï¼ˆAå’ŒBï¼‰,Aä¸Bæ˜¯å¦ç›¸ä¼¼ï¼Ÿï¼ˆ0æˆ–è€…1ï¼‰</p><h2 id="ç‰¹æ®ŠNLPä»»åŠ¡"><a href="#ç‰¹æ®ŠNLPä»»åŠ¡" class="headerlink" title="ç‰¹æ®ŠNLPä»»åŠ¡"></a>ç‰¹æ®ŠNLPä»»åŠ¡</h2><p>BERTçš„è®ºæ–‡ä¸ºæˆ‘ä»¬ä»‹ç»äº†å‡ ç§BERTå¯ä»¥å¤„ç†çš„NLPä»»åŠ¡ï¼š</p><ol><li><p>çŸ­æ–‡æœ¬ç›¸ä¼¼ </p></li><li><p>æ–‡æœ¬åˆ†ç±»</p></li><li><p>QAæœºå™¨äºº</p></li><li><p>è¯­ä¹‰æ ‡æ³¨</p></li></ol><p><img src="https://uploader.shimo.im/f/yKFxOevBvMQXvjnv.png!thumbnail" alt="img"></p><h2 id="BERTç”¨åšç‰¹å¾æå–"><a href="#BERTç”¨åšç‰¹å¾æå–" class="headerlink" title="BERTç”¨åšç‰¹å¾æå–"></a>BERTç”¨åšç‰¹å¾æå–</h2><p>å¾®è°ƒæ–¹æ³•å¹¶ä¸æ˜¯ä½¿ç”¨BERTçš„å”¯ä¸€æ–¹æ³•ï¼Œå°±åƒELMoä¸€æ ·ï¼Œä½ å¯ä»¥ä½¿ç”¨é¢„é€‰è®­ç»ƒå¥½çš„BERTæ¥åˆ›å»ºè¯­å¢ƒåŒ–è¯åµŒå…¥ã€‚ç„¶åä½ å¯ä»¥å°†è¿™äº›åµŒå…¥æä¾›ç»™ç°æœ‰çš„æ¨¡å‹ã€‚</p><p><img src="https://uploader.shimo.im/f/uKUkG73gELQGry4L.png!thumbnail" alt="img"></p><p>å“ªä¸ªå‘é‡æœ€é€‚åˆä½œä¸ºä¸Šä¸‹æ–‡åµŒå…¥ï¼Ÿ æˆ‘è®¤ä¸ºè¿™å–å†³äºä»»åŠ¡ã€‚ æœ¬æ–‡è€ƒå¯Ÿäº†å…­ç§é€‰æ‹©ï¼ˆä¸å¾®è°ƒæ¨¡å‹ç›¸æ¯”ï¼Œå¾—åˆ†ä¸º96.4ï¼‰ï¼š</p><p><img src="https://uploader.shimo.im/f/bfpUyWE9YCEP9IU2.png!thumbnail" alt="img"></p><h1 id="å¦‚ä½•ä½¿ç”¨BERT"><a href="#å¦‚ä½•ä½¿ç”¨BERT" class="headerlink" title="å¦‚ä½•ä½¿ç”¨BERT"></a>å¦‚ä½•ä½¿ç”¨BERT</h1><p>ä½¿ç”¨BERTçš„æœ€ä½³æ–¹å¼æ˜¯é€šè¿‡ BERT FineTuning with Cloud TPUs (<a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb" target="_blank" rel="noopener">https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb</a>) è°·æ­Œäº‘ä¸Šæ‰˜ç®¡çš„ç¬”è®°ã€‚å¦‚æœä½ æœªä½¿ç”¨è¿‡è°·æ­Œäº‘TPUå¯ä»¥è¯•è¯•çœ‹ï¼Œè¿™æ˜¯ä¸ªä¸é”™çš„å°è¯•ã€‚å¦å¤–BERTä¹Ÿé€‚ç”¨äºTPUï¼ŒCPUå’ŒGPU</p><p>ä¸‹ä¸€æ­¥æ˜¯æŸ¥çœ‹BERTä»“åº“ä¸­çš„ä»£ç ï¼š</p><ol><li><p>è¯¥æ¨¡å‹åœ¨modeling.pyï¼ˆBertModelç±»ï¼‰ä¸­æ„å»ºï¼Œä¸vanilla Transformerç¼–ç å™¨å®Œå…¨ç›¸åŒã€‚</p></li><li><p>run_classifier.pyæ˜¯å¾®è°ƒè¿‡ç¨‹çš„ä¸€ä¸ªç¤ºä¾‹ã€‚å®ƒè¿˜æ„å»ºäº†ç›‘ç£æ¨¡å‹çš„åˆ†ç±»å±‚ã€‚å¦‚æœè¦æ„å»ºè‡ªå·±çš„åˆ†ç±»å™¨ï¼Œè¯·æŸ¥çœ‹è¯¥æ–‡ä»¶ä¸­çš„create_model()æ–¹æ³•ã€‚</p></li><li><p>å¯ä»¥ä¸‹è½½å‡ ç§é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹ã€‚æ¶µç›–102ç§è¯­è¨€çš„å¤šè¯­è¨€æ¨¡å‹ï¼Œè¿™äº›è¯­è¨€éƒ½æ˜¯åœ¨ç»´åŸºç™¾ç§‘çš„æ•°æ®åŸºç¡€ä¸Šè®­ç»ƒè€Œæˆçš„ã€‚</p></li><li><p>BERTä¸ä¼šå°†å•è¯è§†ä¸ºtokensã€‚ç›¸åï¼Œå®ƒæ³¨é‡WordPiecesã€‚ tokenization.pyæ˜¯å°†ä½ çš„å•è¯è½¬æ¢ä¸ºé€‚åˆBERTçš„wordPiecesçš„tokensizerã€‚</p></li></ol><p>æˆ‘è‡ªå·±ç»™BERTçš„ä»£ç å¢åŠ äº†ä¸€äº›æ³¨è§£</p><p><a href="https://github.com/ZeweiChu/bert/blob/master/modeling.py" target="_blank" rel="noopener">https://github.com/ZeweiChu/bert/blob/master/modeling.py</a></p><p>é‡ç‚¹å…³æ³¨å…¶ä¸­çš„ï¼š</p><ul><li><p>attention_layer: <a href="https://github.com/ZeweiChu/bert/blob/master/modeling.py#L638" target="_blank" rel="noopener">https://github.com/ZeweiChu/bert/blob/master/modeling.py#L638</a></p></li><li><p>transformer_model: <a href="https://github.com/ZeweiChu/bert/blob/master/modeling.py#L868" target="_blank" rel="noopener">https://github.com/ZeweiChu/bert/blob/master/modeling.py#L868</a></p></li></ul><p>BERTçš„å¾ˆå¤šä»»åŠ¡åŸºäºGLUE benchmark</p><p><a href="https://gluebenchmark.com/tasks/" target="_blank" rel="noopener">https://gluebenchmark.com/tasks/</a></p><p><a href="https://openreview.net/pdf?id=rJ4km2R5t7" target="_blank" rel="noopener">https://openreview.net/pdf?id=rJ4km2R5t7</a></p><p>æœ€è¿‘è¿˜æœ‰ä¸€ä¸ªSuperGLUE</p><p><a href="https://w4ngatang.github.io/static/papers/superglue.pdf" target="_blank" rel="noopener">https://w4ngatang.github.io/static/papers/superglue.pdf</a></p><p>æ‚¨è¿˜å¯ä»¥æŸ¥çœ‹BERTçš„PyTorchå®ç° (<a href="https://github.com/huggingface/pytorch-transformers)ã€‚" target="_blank" rel="noopener">https://github.com/huggingface/pytorch-transformers)ã€‚</a> AllenNLPåº“ä½¿ç”¨æ­¤å®ç°å…è®¸å°†BERTåµŒå…¥ä¸ä»»ä½•æ¨¡å‹ä¸€èµ·ä½¿ç”¨ã€‚</p><p>æœ€è¿‘NVIDIAå¼€æºäº†ä»–ä»¬53åˆ†é’Ÿè®­ç»ƒBERTçš„ä»£ç </p><p><a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT" target="_blank" rel="noopener">https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT</a></p><hr><h1 id="å‚è€ƒæ–‡çŒ®"><a href="#å‚è€ƒæ–‡çŒ®" class="headerlink" title="å‚è€ƒæ–‡çŒ®"></a>å‚è€ƒæ–‡çŒ®</h1><p>BERTå…¨æ–‡ç¿»è¯‘æˆä¸­æ–‡</p><p><a href="https://zhuanlan.zhihu.com/p/59775981" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/59775981</a></p><p>å›¾è§£ BERT æ¨¡å‹ï¼šä»é›¶å¼€å§‹æ„å»º BERT</p><p><a href="https://flashgene.com/archives/20062.html" target="_blank" rel="noopener">https://flashgene.com/archives/20062.html</a></p><p>NLPå¿…è¯»ï¼šååˆ†é’Ÿè¯»æ‡‚è°·æ­ŒBERTæ¨¡å‹</p><p><a href="https://zhuanlan.zhihu.com/p/51413773" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/51413773</a></p><p>BERT Explained: State of the art language model for NLP</p><p><a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270" target="_blank" rel="noopener">https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;ã€è¯‘ã€‘The-Illustrated-BERT-ELMo-and-co&quot;&gt;&lt;a href=&quot;#ã€è¯‘ã€‘The-Illustrated-BERT-ELMo-and-co&quot; class=&quot;headerlink&quot; title=&quot;ã€è¯‘ã€‘The Illustrated BER
      
    
    </summary>
    
      <category term="NLP" scheme="http://mmyblog.cn/categories/NLP/"/>
    
    
      <category term="BERT" scheme="http://mmyblog.cn/tags/BERT/"/>
    
      <category term="ELMo" scheme="http://mmyblog.cn/tags/ELMo/"/>
    
  </entry>
  
  <entry>
    <title>å¤§è§„æ¨¡æ— ç›‘ç£é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸åº”ç”¨ä¸Š</title>
    <link href="http://mmyblog.cn/2020/05/01/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%97%A0%E7%9B%91%E7%9D%A3%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%BA%94%E7%94%A8%E4%B8%8A/"/>
    <id>http://mmyblog.cn/2020/05/01/å¤§è§„æ¨¡æ— ç›‘ç£é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸åº”ç”¨ä¸Š/</id>
    <published>2020-05-01T00:23:44.000Z</published>
    <updated>2020-06-09T00:57:54.232Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Subword-Modeling"><a href="#Subword-Modeling" class="headerlink" title="Subword Modeling"></a>Subword Modeling</h3><p>ä»¥å•è¯ä½œä¸ºæ¨¡å‹çš„åŸºæœ¬å•ä½æœ‰ä¸€äº›é—®é¢˜ï¼š</p><ul><li><p>å•è¯é‡æœ‰é™ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä¼šæŠŠå•è¯é‡å›ºå®šåœ¨50k-300kï¼Œç„¶åæ²¡æœ‰è§è¿‡çš„å•è¯åªèƒ½ç”¨<strong>UNK</strong>è¡¨ç¤º</p></li><li><p>zipf distribution: given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.: the rank-frequency distribution is an inverse relation. </p></li><li><p>æ¨¡å‹å‚æ•°é‡å¤ªå¤§ï¼Œ100K * 300 = 30Mä¸ªå‚æ•°ï¼Œä»…ä»…æ˜¯embeddingå±‚</p></li><li><p>å¯¹äºå¾ˆå¤šè¯­è¨€ï¼Œä¾‹å¦‚è‹±è¯­æ¥è¯´ï¼Œå¾ˆå¤šæ—¶å€™å•è¯æ˜¯ç”±å‡ ä¸ªsubwordæ‹¼æ¥è€Œæˆçš„</p></li><li><p>å¯¹äºä¸­æ–‡æ¥è¯´ï¼Œå¾ˆå¤šå¸¸ç”¨çš„æ¨¡å‹ä¼šé‡‡ç”¨åˆ†è¯åå¾—åˆ°çš„è¯è¯­ä½œä¸ºæ¨¡å‹çš„åŸºæœ¬å•å…ƒï¼ŒåŒæ ·å­˜åœ¨ä¸Šè¿°é—®é¢˜</p></li></ul><p>å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼š</p><ul><li><p>ä½¿ç”¨subword informationï¼Œä¾‹å¦‚å­—æ¯ä½œä¸ºè¯­è¨€çš„åŸºæœ¬å•å…ƒ Char-CNN</p></li><li><p>ç”¨wordpiece</p></li></ul><h2 id="è§£å†³æ–¹æ¡ˆï¼šcharacter-level-modeling"><a href="#è§£å†³æ–¹æ¡ˆï¼šcharacter-level-modeling" class="headerlink" title="è§£å†³æ–¹æ¡ˆï¼šcharacter level modeling"></a>è§£å†³æ–¹æ¡ˆï¼šcharacter level modeling</h2><ul><li>ä½¿ç”¨å­—æ¯ä½œä¸ºæ¨¡å‹çš„åŸºæœ¬è¾“å…¥å•å…ƒ</li></ul><h3 id="Ling-et-al-Finding-Function-in-Form-Compositional-Character-Models-for-Open-Vocabulary-Word-Representation"><a href="#Ling-et-al-Finding-Function-in-Form-Compositional-Character-Models-for-Open-Vocabulary-Word-Representation" class="headerlink" title="Ling et. al, Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation"></a>Ling et. al, <a href="https://aclweb.org/anthology/D15-1176" target="_blank" rel="noopener">Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation</a></h3><p>ç”¨BiLSTMæŠŠå•è¯ä¸­çš„æ¯ä¸ªå­—æ¯encodeåˆ°ä¸€èµ·</p><p><img src="https://uploader.shimo.im/f/V49ti0noVOsqeLRH.png!thumbnail" alt="img"></p><h3 id="Yoon-Kim-et-al-Character-Aware-Neural-Language-Models"><a href="#Yoon-Kim-et-al-Character-Aware-Neural-Language-Models" class="headerlink" title="Yoon Kim et. al, Character-Aware Neural Language Models"></a>Yoon Kim et. al, <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/viewFile/12489/12017" target="_blank" rel="noopener">Character-Aware Neural Language Models</a></h3><p><img src="https://uploader.shimo.im/f/bsR8NzGROvs0scpq.png!thumbnail" alt="img"></p><p>æ ¹æ®ä»¥ä¸Šæ¨¡å‹ç¤ºæ„å›¾æ€è€ƒä»¥ä¸‹é—®é¢˜ï¼š</p><ul><li><p>character emebddingçš„çš„ç»´åº¦æ˜¯å¤šå°‘ï¼Ÿ4</p></li><li><p>æœ‰å‡ ä¸ªcharacter 4-gramçš„filterï¼Ÿfilter-size=4? çº¢è‰²çš„ 5ä¸ªfilter</p></li><li><p>max-over-time pooling: 3-gram 4ç»´ï¼Œ 2-gram 3ç»´ 4-gram 55ç»´</p></li><li><p>ä¸ºä»€ä¹ˆä¸åŒçš„filter (kernel size)é•¿åº¦ä¼šå¯¼è‡´ä¸åŒé•¿åº¦çš„feature map?  seq_length - kernel_size + 1</p></li></ul><p>fastText</p><ul><li>ä¸word2vecç±»ä¼¼ï¼Œä½†æ˜¯æ¯ä¸ªå•è¯æ˜¯å®ƒçš„character n-gram embeddings + word emebdding</li></ul><h2 id="è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨subwordä½œä¸ºæ¨¡å‹çš„åŸºæœ¬å•å…ƒ"><a href="#è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨subwordä½œä¸ºæ¨¡å‹çš„åŸºæœ¬å•å…ƒ" class="headerlink" title="è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨subwordä½œä¸ºæ¨¡å‹çš„åŸºæœ¬å•å…ƒ"></a>è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨subwordä½œä¸ºæ¨¡å‹çš„åŸºæœ¬å•å…ƒ</h2><h3 id="Botha-amp-Blunsom-2014-Composional-Morphology-for-Word-Representations-and-Language-Modelling"><a href="#Botha-amp-Blunsom-2014-Composional-Morphology-for-Word-Representations-and-Language-Modelling" class="headerlink" title="Botha &amp; Blunsom (2014): Composional Morphology for Word Representations    and    Language Modelling"></a>Botha &amp; Blunsom (2014): <a href="http://proceedings.mlr.press/v32/botha14.pdf" target="_blank" rel="noopener">Composional Morphology for Word Representations    and    Language Modelling</a></h3><p><img src="https://uploader.shimo.im/f/FplKX422O5owOuVV.png!thumbnail" alt="img"></p><p>subword embedding</p><p><img src="https://uploader.shimo.im/f/3nQUw9cZGvwNSCyx.png!thumbnail" alt="img"></p><h3 id="Byte-Pair-Encoding-éœ€è¦çŸ¥é“ä»€ä¹ˆæ˜¯BPE"><a href="#Byte-Pair-Encoding-éœ€è¦çŸ¥é“ä»€ä¹ˆæ˜¯BPE" class="headerlink" title="Byte Pair Encoding (éœ€è¦çŸ¥é“ä»€ä¹ˆæ˜¯BPE)"></a>Byte Pair Encoding (éœ€è¦çŸ¥é“ä»€ä¹ˆæ˜¯BPE)</h3><p><a href="https://www.aclweb.org/anthology/P16-1162" target="_blank" rel="noopener">Neural Machine Translation of Rare Words with Subword Units</a></p><p>å…³äºä»€ä¹ˆæ˜¯BPEå¯ä»¥å‚è€ƒä¸‹é¢çš„æ–‡ç« </p><p><a href="https://www.cnblogs.com/huangyc/p/10223075.html" target="_blank" rel="noopener">https://www.cnblogs.com/huangyc/p/10223075.html</a></p><p><a href="https://leimao.github.io/blog/Byte-Pair-Encoding/" target="_blank" rel="noopener">https://leimao.github.io/blog/Byte-Pair-Encoding/</a></p><ul><li><p>é¦–å…ˆå®šä¹‰æ‰€æœ‰å¯èƒ½çš„åŸºæœ¬å­—ç¬¦ï¼ˆabcdeâ€¦ï¼‰</p></li><li><p>ç„¶åå¼€å§‹å¾ªç¯æ•°å‡ºæœ€ç»å¸¸å‡ºç°çš„pairsï¼ŒåŠ å…¥åˆ°æˆ‘ä»¬çš„å€™é€‰å­—ç¬¦ï¼ˆåŸºæœ¬ç»„æˆå•å…ƒï¼‰ä¸­å»</p></li></ul><p>a, b, c, d, â€¦, z, A, B, â€¦., Z.. !, @, ?, st, est, lo, low, </p><p>æ§åˆ¶å•è¯è¡¨çš„å¤§å°</p><ul><li>æˆ‘åªè¦ç¡®å®šiterationçš„æ¬¡æ•° 30000ä¸ªiteartionï¼Œ30000+åŸå§‹å­—æ¯è¡¨å½“ä¸­çš„å­—æ¯æ•° ä¸ªå•è¯</li></ul><p>happiest</p><p>h a p p i est</p><p>LSTM</p><p>emb(h), emb(a), emb(p), emb(p), emb(i), emb(est)</p><p>happ, iest</p><p>emb(happ), emb(iest)</p><p><img src="https://uploader.shimo.im/f/0zx2ooI2uzoWLfOg.png!thumbnail" alt="img"></p><p><a href="https://www.aclweb.org/anthology/P16-1162.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/P16-1162.pdf</a></p><h2 id="ä¸­æ–‡è¯å‘é‡"><a href="#ä¸­æ–‡è¯å‘é‡" class="headerlink" title="ä¸­æ–‡è¯å‘é‡"></a>ä¸­æ–‡è¯å‘é‡</h2><h3 id="Meng-et-al-Is-Word-Segmentation-Necessary-for-Deep-Learning-of-Chinese-Representations"><a href="#Meng-et-al-Is-Word-Segmentation-Necessary-for-Deep-Learning-of-Chinese-Representations" class="headerlink" title="Meng et. al, Is Word Segmentation Necessary for Deep Learning of Chinese Representations?"></a>Meng et. al, <a href="https://arxiv.org/pdf/1905.05526.pdf" target="_blank" rel="noopener">Is Word Segmentation Necessary for Deep Learning of Chinese Representations?</a></h3><p>ç®€å•æ¥è¯´ï¼Œè¿™ç¯‡æ–‡ç« çš„ä½œè€…ç”Ÿæˆé€šè¿‡ä»–ä»¬çš„å®éªŒå‘ç°Chinese Word Segmentationå¯¹äºè¯­è¨€æ¨¡å‹ã€æ–‡æœ¬åˆ†ç±»ï¼Œç¿»è¯‘å’Œæ–‡æœ¬å…³ç³»åˆ†ç±»å¹¶æ²¡æœ‰ä»€ä¹ˆå¸®åŠ©ï¼Œç›´æ¥ä½¿ç”¨å•ä¸ªå­—ä½œä¸ºæ¨¡å‹çš„è¾“å…¥å¯ä»¥è¾¾åˆ°æ›´å¥½çš„æ•ˆæœã€‚</p><blockquote><p>We benchmark neural word-based models which rely on word segmentation against neural char-based models which do not involve word segmentation in four end-to-end NLP benchmark tasks: language modeling, machine translation, sentence matching/paraphrase and text classification. Through direct comparisons between these two types of models, we find that charbased models consistently outperform wordbased models.</p></blockquote><blockquote></blockquote><blockquote><p>word-based models are more vulnerable to data sparsity and the presence of out-of-vocabulary (OOV) words, and thus more prone to overfitting</p></blockquote><p>Jiwei Li</p><p><a href="https://nlp.stanford.edu/~bdlijiwei/" target="_blank" rel="noopener">https://nlp.stanford.edu/~bdlijiwei/</a></p><h3 id="ä¸­æ–‡åˆ†è¯å·¥å…·"><a href="#ä¸­æ–‡åˆ†è¯å·¥å…·" class="headerlink" title="ä¸­æ–‡åˆ†è¯å·¥å…·"></a>ä¸­æ–‡åˆ†è¯å·¥å…·</h3><p>å»ºè®®åŒå­¦ä»¬å¯ä»¥åœ¨è‡ªå·±çš„é¡¹ç›®ä¸­å°è¯•ä»¥ä¸‹å·¥å…·</p><ul><li><p>åŒ—å¤§ä¸­æ–‡åˆ†è¯å·¥å…· </p></li><li><p><a href="https://github.com/lancopku/pkuseg-python" target="_blank" rel="noopener">https://github.com/lancopku/pkuseg-python</a> </p></li><li><p>æœºå™¨ä¹‹å¿ƒæŠ¥é“ <a href="https://www.jiqizhixin.com/articles/2019-01-09-12" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2019-01-09-12</a></p></li><li><p>æ¸…ååˆ†è¯å·¥å…· <a href="https://github.com/thunlp/THULAC-Python" target="_blank" rel="noopener">https://github.com/thunlp/THULAC-Python</a></p></li><li><p>ç»“å·´ <a href="https://github.com/fxsjy/jieba" target="_blank" rel="noopener">https://github.com/fxsjy/jieba</a></p></li></ul><h1 id="é¢„è®­ç»ƒå¥å­-æ–‡æ¡£å‘é‡"><a href="#é¢„è®­ç»ƒå¥å­-æ–‡æ¡£å‘é‡" class="headerlink" title="é¢„è®­ç»ƒå¥å­/æ–‡æ¡£å‘é‡"></a>é¢„è®­ç»ƒå¥å­/æ–‡æ¡£å‘é‡</h1><p>æ—¢ç„¶æœ‰è¯å‘é‡ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ˜¯å¦å¯ä»¥æ›´è¿›ä¸€æ­¥ï¼ŒæŠŠå¥å­ç”šè‡³ä¸€æ•´ä¸ªæ–‡æ¡£ä¹Ÿç¼–ç æˆä¸€ä¸ªå‘é‡å‘¢ï¼Ÿ</p><p>åœ¨ä¹‹å‰çš„è¯¾ç¨‹ä¸­æˆ‘ä»¬å·²ç»æ¶‰åŠåˆ°äº†ä¸€äº›å¥å­çº§åˆ«çš„ä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ†ç±»ï¼Œå¸¸å¸¸å°±æ˜¯æŠŠä¸€å¥æˆ–è€…è‹¥å¹²å¥æ–‡æœ¬åˆ†ç±»æˆä¸€å®šçš„ç±»åˆ«ã€‚æ­¤ç±»æ¨¡å‹çš„ä¸€èˆ¬å®ç°æ–¹å¼æ˜¯é¦–å…ˆæŠŠæ–‡æœ¬ç¼–ç æˆæŸç§æ–‡æœ¬è¡¨ç¤ºæ–¹å¼ï¼Œä¾‹å¦‚averaged word embeddingsï¼Œæˆ–è€…åŒå‘LSTMå¤´å°¾æ‹¼æ¥ï¼Œæˆ–è€…CNNæ¨¡å‹ç­‰ç­‰ã€‚</p><p>æ–‡æœ¬åˆ†ç±»</p><ul><li><p>æ–‡æœ¬é€šè¿‡æŸç§æ–¹å¼å˜æˆä¸€ä¸ªå‘é‡</p></li><li><p>WORDAVG</p></li><li><p>LSTM</p></li><li><p>CNN</p></li><li><p>æœ€åæ˜¯ä¸€ä¸ªlinear layer 300ç»´å¥å­å‘é‡ â€“ã€‹ 2 æƒ…æ„Ÿåˆ†ç±»</p></li></ul><p>çŒ«å›¾ç‰‡/ç‹—å›¾ç‰‡</p><p>å›¾ç‰‡ â€“&gt; <strong>ResNet</strong> â€“&gt; 2048ç»´å‘é‡ â€“&gt; (2, 2048) â€“&gt; 2ç»´å‘é‡ binary cross entropy loss</p><p><strong>ResNet</strong> é¢„è®­ç»ƒæ¨¡å‹</p><p>æ–‡æœ¬ â€“&gt; TextResNet â€“&gt; 2048ç»´å‘é‡</p><p>apply to any downstream tasks</p><p>TextResNetï¼šLSTMæ¨¡å‹</p><p>ä¸åŒçš„ä»»åŠ¡ï¼ˆä¾‹å¦‚ä¸åŒçš„æ–‡æœ¬åˆ†ç±»ï¼šæƒ…æ„Ÿåˆ†ç±»ï¼Œè¯é¢˜åˆ†ç±»ï¼‰è™½ç„¶æœ€ç»ˆçš„è¾“å‡ºä¸åŒï¼Œä½†æ˜¯å¾€å¾€æ‹¥æœ‰ç€ç›¸ä¼¼ç”šè‡³å®Œå…¨ä¸€æ ·çš„ç¼–ç å±‚ã€‚å¦‚æœæˆ‘ä»¬èƒ½å¤Ÿé¢„è®­ç»ƒä¸€ä¸ªéå¸¸å¥½çš„ç¼–ç å±‚ï¼Œé‚£ä¹ˆåç»­æ¨¡å‹çš„è´Ÿæ‹…å°±å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šå¾—åˆ°é™ä½ã€‚è¿™æ ·çš„æ€æƒ³å¾ˆå¤šæ˜¯æ¥è‡ªå›¾åƒå¤„ç†çš„ç›¸å…³å·¥ä½œã€‚ä¾‹å¦‚äººä»¬åœ¨å„ç±»å›¾åƒä»»åŠ¡ä¸­å‘ç°ï¼Œå¦‚æœä½¿ç”¨åœ¨ImageNetä¸Šé¢„è®­ç»ƒè¿‡çš„æ·±å±‚CNNç½‘ç»œï¼ˆä¾‹å¦‚ResNetï¼‰ï¼ŒåªæŠŠæœ€ç»ˆçš„è¾“å‡ºå±‚æ›¿æ¢æˆè‡ªå·±éœ€è¦çš„æ ·å­ï¼Œå¾€å¾€å¯ä»¥å–å¾—éå¸¸å¥½çš„æ•ˆæœï¼Œä¸”å¯ä»¥åœ¨å°‘é‡æ•°æ®çš„æƒ…å†µä¸‹è®­ç»ƒå‡ºä¼˜è´¨çš„æ¨¡å‹ã€‚</p><p>åœ¨å¥å­/æ–‡æœ¬å‘é‡é¢„è®­ç»ƒçš„é¢†åŸŸæ¶Œç°å‡ºäº†ä¸€ç³»åˆ—çš„å·¥ä½œï¼Œä¸‹é¢æˆ‘ä»¬é€‰å–ä¸€äº›æœ‰ä»£è¡¨æ€§çš„å·¥ä½œä¾›å¤§å®¶å­¦ä¹ å‚è€ƒã€‚</p><h2 id="Skip-Thought"><a href="#Skip-Thought" class="headerlink" title="Skip-Thought"></a>Skip-Thought</h2><p>Kiros et. al, <a href="https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf" target="_blank" rel="noopener">Skip-Thought Vectors</a></p><p>skip-gram: distributional semantics of words ç”¨ä¸­å¿ƒè¯â€“ã€‹å‘¨å›´è¯</p><p>skip-thought: distributional semantics of sentences ç”¨ä¸­å¿ƒå¥â€“ã€‹å‘¨å›´å¥</p><p>ä¸¤ä¸ªå¥å­å¦‚æœæ€»æ˜¯åœ¨åŒä¸€ä¸ªç¯å¢ƒä¸‹å‡ºç°ï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªå¥å­å¯èƒ½æœ‰æŸç§å«ä¹‰ä¸Šçš„è”ç³»</p><p>å¦‚ä½•æŠŠå¥å­mapæˆä¸€ä¸ªå‘é‡ï¼šcompositional modelï¼ŒRNN, LSTM, CNN, WordAvg, <strong>GRU</strong></p><p>Skip-thought æ¨¡å‹çš„æ€æƒ³éå¸¸ç®€å•ï¼Œæˆ‘ä»¬è®­ç»ƒä¸€ä¸ªåŸºäºGRUçš„æ¨¡å‹ä½œä¸ºå¥å­çš„ç¼–ç å™¨ã€‚äº‹å®ä¸ŠSkip-thoughtè¿™ä¸ªåå­—ä¸Skip-gramæœ‰ç€åƒä¸ä¸‡ç¼•çš„è”ç³»ï¼Œå®ƒä»¬åŸºäºä¸€ä¸ªå…±åŒçš„æ€æƒ³ï¼Œå°±æ˜¯ä¸€å¥è¯ï¼ˆä¸€ä¸ªå•è¯ï¼‰çš„å«ä¹‰ä¸å®ƒæ‰€å¤„çš„ç¯å¢ƒï¼ˆcontextï¼Œå‘¨å›´å¥å­/å•è¯ï¼‰é«˜åº¦ç›¸å…³ã€‚</p><p>å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒSkipthoughté‡‡ç”¨ä¸€ä¸ªGRU encoderï¼Œä½¿ç”¨ç¼–ç å™¨æœ€åä¸€ä¸ªhidden stateæ¥è¡¨ç¤ºæ•´ä¸ªå¥å­ã€‚ç„¶åä½¿ç”¨è¿™ä¸ªhidden stateä½œä¸ºåˆå§‹çŠ¶æ€æ¥è§£ç å®ƒä¹‹å‰å’Œä¹‹åçš„å¥å­ã€‚</p><p>decoder: ä¸¤ä¸ªconditionalè¯­è¨€æ¨¡å‹ã€‚</p><p>åŸºäºä¸­å¿ƒå¥çš„å¥å­å‘é‡ï¼Œä¼˜åŒ–conditional log likelihood</p><p><img src="https://uploader.shimo.im/f/keW15vUeJX4E7gam.png!thumbnail" alt="img"></p><p>ä¸€ä¸ªencoder GRU</p><p><img src="https://uploader.shimo.im/f/XbXbCNlxSpk5PLuh.png!thumbnail" alt="img"></p><p>ä¸¤ä¸ªdecoder GRU</p><p><img src="https://uploader.shimo.im/f/atuHcc6hYNIE2QOd.png!thumbnail" alt="img"></p><p>è®­ç»ƒç›®æ ‡</p><p><img src="https://uploader.shimo.im/f/XCVPs561UVADzFkO.png!thumbnail" alt="img"></p><p>ç„¶åæˆ‘ä»¬å°±å¯ä»¥æŠŠencoderå½“åšfeature extractoräº†ã€‚</p><p>ç±»ä¼¼çš„å·¥ä½œè¿˜æœ‰<a href="https://arxiv.org/pdf/1602.03483.pdf" target="_blank" rel="noopener">FastSent</a>ã€‚FastSentç›´æ¥ä½¿ç”¨è¯å‘é‡ä¹‹å’Œæ¥è¡¨ç¤ºæ•´ä¸ªå¥å­ï¼Œç„¶åç”¨è¯¥å¥å­å‘é‡æ¥è§£ç å‘¨å›´å¥å­ä¸­çš„å•ä¸ªå•è¯ä»¬ã€‚</p><h2 id="InferSent"><a href="#InferSent" class="headerlink" title="InferSent"></a>InferSent</h2><p><a href="https://www.aclweb.org/anthology/D17-1070" target="_blank" rel="noopener">Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</a></p><p>Natural Language Inference (NLI)</p><ul><li><p>ç»™å®šä¸¤ä¸ªå¥å­ï¼Œåˆ¤æ–­è¿™ä¸¤ä¸ªå¥å­ä¹‹é—´çš„å…³ç³»</p></li><li><p>entailment æ‰¿æ¥å…³ç³»</p></li><li><p>neutral æ²¡æœ‰å…³ç³»</p></li><li><p>contradiction çŸ›ç›¾</p></li><li><p>(non_entailment)</p></li></ul><h3 id="SNLIä»»åŠ¡"><a href="#SNLIä»»åŠ¡" class="headerlink" title="SNLIä»»åŠ¡"></a>SNLIä»»åŠ¡</h3><p>ç»™å®šä¸¤ä¸ªå¥å­ï¼Œé¢„æµ‹è¿™ä¸¤ä¸ªå¥å­çš„å…³ç³»æ˜¯entailment, contradictionï¼Œè¿˜æ˜¯neutral  </p><p>ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¨¡å‹</p><p><img src="https://uploader.shimo.im/f/bdClv9vmULUCXgcc.png!thumbnail" alt="img"></p><p>Encoderæ˜¯BiLSTM + max pooling</p><p><img src="https://uploader.shimo.im/f/Uiw1y8KX5pEw5Lri.png!thumbnail" alt="img"></p><p>æ¨¡å‹æ•ˆæœ</p><p><img src="https://uploader.shimo.im/f/kzvejsQ7DMI3369N.png!thumbnail" alt="img"></p><h2 id="SentEval"><a href="#SentEval" class="headerlink" title="SentEval"></a>SentEval</h2><p><a href="https://www.aclweb.org/anthology/L18-1269" target="_blank" rel="noopener">SentEval: An Evaluation Toolkit for Universal Sentence Representations</a></p><p>ä¸€ä¸ªéå¸¸é€šç”¨çš„benchmarkï¼Œç”¨æ¥è¯„ä¼°å¥å­embeddingæ˜¯å¦èƒ½å¤Ÿå¾ˆå¥½åœ°åº”ç”¨äºdownstream tasksã€‚</p><p>Github: <a href="https://github.com/facebookresearch/SentEval" target="_blank" rel="noopener">https://github.com/facebookresearch/SentEval</a></p><h2 id="Document-Vector"><a href="#Document-Vector" class="headerlink" title="Document Vector"></a>Document Vector</h2><p>äº‹å®ä¸Šç ”ç©¶è€…åœ¨å¥å­å‘é‡ä¸Šçš„å„ç§å°è¯•æ˜¯ä¸å¤ªæˆåŠŸçš„ã€‚ä¸»è¦ä½“ç°åœ¨è¿™äº›é¢„è®­ç»ƒå‘é‡å¹¶ä¸èƒ½éå¸¸å¥½åœ°æå‡æ¨¡å‹åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œäººä»¬å¤§å¤šæ•°æ—¶å€™è¿˜æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚</p><p>åœ¨document vectorä¸Šçš„å°è¯•å°±æ›´ä¸å°½å¦‚äººæ„äº†ï¼Œå› ä¸ºä¸€ä¸ªæ–‡æœ¬å¾€å¾€åŒ…å«éå¸¸ä¸°å¯Œçš„ä¿¡æ¯ï¼Œè€Œä¸€ä¸ªå‘é‡èƒ½å¤Ÿç¼–ç çš„ä¿¡æ¯é‡å®åœ¨å¤ªå°ã€‚</p><p>Learning Deep Structured Semantic Models for Web Search using Clickthrough Data</p><p><a href="https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/" target="_blank" rel="noopener">https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/</a></p><p>Hierarchical Attention Networks for Document Classification</p><p><a href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf" target="_blank" rel="noopener">https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf</a></p><h1 id="ELMo-BERT"><a href="#ELMo-BERT" class="headerlink" title="ELMo, BERT"></a><a href="https://shimo.im/docs/Y6q3gX8yGGjpWqXx" target="_blank" rel="noopener">ELMo, BERT</a></h1><p>ELMO paper: <a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1802.05365.pdf</a></p><h1 id="Transformerä¸­çš„Encoder"><a href="#Transformerä¸­çš„Encoder" class="headerlink" title="Transformerä¸­çš„Encoder"></a><a href="https://shimo.im/docs/gPwkqCXrkJyRW89V" target="_blank" rel="noopener">Transformerä¸­çš„Encoder</a></h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Subword-Modeling&quot;&gt;&lt;a href=&quot;#Subword-Modeling&quot; class=&quot;headerlink&quot; title=&quot;Subword Modeling&quot;&gt;&lt;/a&gt;Subword Modeling&lt;/h3&gt;&lt;p&gt;ä»¥å•è¯ä½œä¸ºæ¨¡å‹çš„åŸºæœ¬å•ä½æœ‰ä¸€
      
    
    </summary>
    
      <category term="NLP" scheme="http://mmyblog.cn/categories/NLP/"/>
    
    
      <category term="BERT" scheme="http://mmyblog.cn/tags/BERT/"/>
    
      <category term="Transformer" scheme="http://mmyblog.cn/tags/Transformer/"/>
    
      <category term="ELMo" scheme="http://mmyblog.cn/tags/ELMo/"/>
    
  </entry>
  
  <entry>
    <title>word2vec</title>
    <link href="http://mmyblog.cn/2020/04/24/word2vec/"/>
    <id>http://mmyblog.cn/2020/04/24/word2vec/</id>
    <published>2020-04-24T05:29:22.000Z</published>
    <updated>2020-05-08T07:27:37.853Z</updated>
    
    <content type="html"><![CDATA[<h4 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h4><ul><li>æœ‰ä¸€ä¸ªå¾ˆå¤§çš„è¯è¡¨åº“</li><li>åœ¨è¯è¡¨ä¸­çš„æ¯ä¸ªè¯éƒ½å¯ä»¥é€šè¿‡å‘é‡è¡¨å¾</li><li>æœ‰ä¸€ä¸ªä¸­å¿ƒè¯cï¼Œæœ‰ä¸€ä¸ªè¾“å‡ºè¯o</li><li>ç”¨è¯cå’Œoçš„ç›¸ä¼¼åº¦æ¥è®¡ç®—ä»–ä»¬ä¹‹é—´åŒæ—¶å‡ºç°çš„æ¦‚ç‡</li><li>è°ƒæ•´è¿™ä¸ªè¯å‘é‡æ¥è·å¾—æœ€å¤§è¾“å‡ºæ¦‚ç‡</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;word2vec&quot;&gt;&lt;a href=&quot;#word2vec&quot; class=&quot;headerlink&quot; title=&quot;word2vec&quot;&gt;&lt;/a&gt;word2vec&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;æœ‰ä¸€ä¸ªå¾ˆå¤§çš„è¯è¡¨åº“&lt;/li&gt;
&lt;li&gt;åœ¨è¯è¡¨ä¸­çš„æ¯ä¸ªè¯éƒ½å¯ä»¥é€šè¿‡å‘é‡è¡¨å¾&lt;/li
      
    
    </summary>
    
      <category term="word2vec" scheme="http://mmyblog.cn/categories/word2vec/"/>
    
    
      <category term="skip-gram" scheme="http://mmyblog.cn/tags/skip-gram/"/>
    
      <category term="cbow" scheme="http://mmyblog.cn/tags/cbow/"/>
    
      <category term="hierarchical softmax" scheme="http://mmyblog.cn/tags/hierarchical-softmax/"/>
    
      <category term="negative sampling" scheme="http://mmyblog.cn/tags/negative-sampling/"/>
    
  </entry>
  
  <entry>
    <title>ç‰¹å¾å·¥ç¨‹ä¸æ¨¡å‹è°ƒä¼˜</title>
    <link href="http://mmyblog.cn/2020/04/20/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B8%8E%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98/"/>
    <id>http://mmyblog.cn/2020/04/20/ç‰¹å¾å·¥ç¨‹ä¸æ¨¡å‹è°ƒä¼˜/</id>
    <published>2020-04-20T06:17:41.000Z</published>
    <updated>2020-04-24T05:05:56.398Z</updated>
    
    <content type="html"><![CDATA[<h2 id="æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹"><a href="#æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹" class="headerlink" title="æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹"></a>æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹</h2><h3 id="æœºå™¨å­¦ä¹ æµç¨‹ä¸æ¦‚å¿µ"><a href="#æœºå™¨å­¦ä¹ æµç¨‹ä¸æ¦‚å¿µ" class="headerlink" title="æœºå™¨å­¦ä¹ æµç¨‹ä¸æ¦‚å¿µ"></a>æœºå™¨å­¦ä¹ æµç¨‹ä¸æ¦‚å¿µ</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBncGV.jpg" alt></p><h3 id="æœºå™¨å­¦ä¹ å»ºæ¨¡æµç¨‹"><a href="#æœºå™¨å­¦ä¹ å»ºæ¨¡æµç¨‹" class="headerlink" title="æœºå™¨å­¦ä¹ å»ºæ¨¡æµç¨‹"></a>æœºå™¨å­¦ä¹ å»ºæ¨¡æµç¨‹</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBn2xU.png" alt></p><h3 id="æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹ä¸€è§ˆ"><a href="#æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹ä¸€è§ˆ" class="headerlink" title="æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹ä¸€è§ˆ"></a>æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹ä¸€è§ˆ</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBnOMD.jpg" alt></p><h3 id="æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹ä»‹ç»"><a href="#æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹ä»‹ç»" class="headerlink" title="æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹ä»‹ç»"></a>æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹ä»‹ç»</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBnjqH.jpg" alt></p><h3 id="ç‰¹å¾æ¸…æ´—"><a href="#ç‰¹å¾æ¸…æ´—" class="headerlink" title="ç‰¹å¾æ¸…æ´—"></a>ç‰¹å¾æ¸…æ´—</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBumon.jpg" alt><br><img src="https://s1.ax1x.com/2020/04/24/JBuKJ0.jpg" alt><br><img src="https://s1.ax1x.com/2020/04/24/JBu3yF.jpg" alt></p><h3 id="æ•°å€¼å‹æ•°æ®ä¸Šçš„ç‰¹å¾å·¥ç¨‹"><a href="#æ•°å€¼å‹æ•°æ®ä¸Šçš„ç‰¹å¾å·¥ç¨‹" class="headerlink" title="æ•°å€¼å‹æ•°æ®ä¸Šçš„ç‰¹å¾å·¥ç¨‹"></a>æ•°å€¼å‹æ•°æ®ä¸Šçš„ç‰¹å¾å·¥ç¨‹</h3><p>æ•°å€¼å‹æ•°æ®é€šå¸¸ä»¥æ ‡é‡çš„å½¢å¼è¡¨ç¤ºæ•°æ®ï¼Œæè¿°è§‚æµ‹å€¼ã€è®°å½•æˆ–è€…æµ‹é‡å€¼ã€‚æœ¬æ–‡çš„æ•°å€¼å‹æ•°æ®æ˜¯æŒ‡è¿ç»­å‹æ•°æ®è€Œä¸æ˜¯ç¦»æ•£å‹æ•°æ®ï¼Œè¡¨ç¤ºä¸åŒç±»ç›®çš„æ•°æ®å°±æ˜¯åè€…ã€‚æ•°å€¼å‹æ•°æ®ä¹Ÿå¯ä»¥ç”¨å‘é‡æ¥è¡¨ç¤ºï¼Œå‘é‡çš„æ¯ä¸ªå€¼æˆ–åˆ†é‡ä»£è¡¨ä¸€ä¸ªç‰¹å¾ã€‚æ•´æ•°å’Œæµ®ç‚¹æ•°æ˜¯è¿ç»­å‹æ•°å€¼æ•°æ®ä¸­æœ€å¸¸è§ä¹Ÿæ˜¯æœ€å¸¸ä½¿ç”¨çš„æ•°å€¼å‹æ•°æ®ç±»å‹ã€‚å³ä½¿æ•°å€¼å‹æ•°æ®å¯ä»¥ç›´æ¥è¾“å…¥åˆ°æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­ï¼Œä½ ä»éœ€è¦åœ¨å»ºæ¨¡å‰è®¾è®¡ä¸åœºæ™¯ã€é—®é¢˜å’Œé¢†åŸŸç›¸å…³çš„ç‰¹å¾ã€‚å› æ­¤ä»éœ€è¦ç‰¹å¾å·¥ç¨‹ã€‚è®©æˆ‘ä»¬åˆ©ç”¨ python æ¥çœ‹çœ‹åœ¨æ•°å€¼å‹æ•°æ®ä¸Šåšç‰¹å¾å·¥ç¨‹çš„ä¸€äº›ç­–ç•¥ã€‚æˆ‘ä»¬é¦–å…ˆåŠ è½½ä¸‹é¢ä¸€äº›å¿…è¦çš„ä¾èµ–ï¼ˆé€šå¸¸åœ¨ <a href="http://jupyter.org/" target="_blank" rel="noopener"><strong>Jupyter</strong> </a> botebook ä¸Šï¼‰ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">&gt;</span><br><span class="line">&gt; <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">&gt;</span><br><span class="line">&gt; <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">&gt;</span><br><span class="line">&gt; <span class="keyword">import</span> scipy.stats <span class="keyword">as</span> spstats</span><br><span class="line">&gt;</span><br><span class="line">&gt; %matplotlib inline</span><br></pre></td></tr></table></figure><p>åŸå§‹åº¦é‡</p><p>æ­£å¦‚æˆ‘ä»¬å…ˆå‰æåˆ°çš„ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡å’Œæ•°æ®çš„æ ¼å¼ï¼ŒåŸå§‹æ•°å€¼å‹æ•°æ®é€šå¸¸å¯ç›´æ¥è¾“å…¥åˆ°æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­ã€‚åŸå§‹çš„åº¦é‡æ–¹æ³•é€šå¸¸ç”¨æ•°å€¼å‹å˜é‡æ¥ç›´æ¥è¡¨ç¤ºä¸ºç‰¹å¾ï¼Œè€Œä¸éœ€è¦ä»»ä½•å½¢å¼çš„å˜æ¢æˆ–ç‰¹å¾å·¥ç¨‹ã€‚é€šå¸¸è¿™äº›ç‰¹å¾å¯ä»¥è¡¨ç¤ºä¸€äº›å€¼æˆ–æ€»æ•°ã€‚è®©æˆ‘ä»¬åŠ è½½å››ä¸ªæ•°æ®é›†ä¹‹ä¸€çš„ <a href="https://www.kaggle.com/abcsds/pokemon/data" target="_blank" rel="noopener">Pokemon </a>æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä¹Ÿåœ¨ <a href="https://www.kaggle.com/abcsds/pokemon/data" target="_blank" rel="noopener">Kaggle </a>ä¸Šå…¬å¸ƒäº†ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">poke_df = pd.read_csv(<span class="string">'datasets/Pokemon.csv'</span>, encoding=<span class="string">'utf-8'</span>) </span><br><span class="line"></span><br><span class="line">poke_df.head()</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f55514768e.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><h5 id="æˆ‘ä»¬çš„Pokemonæ•°æ®é›†æˆªå›¾"><a href="#æˆ‘ä»¬çš„Pokemonæ•°æ®é›†æˆªå›¾" class="headerlink" title="æˆ‘ä»¬çš„Pokemonæ•°æ®é›†æˆªå›¾"></a>æˆ‘ä»¬çš„Pokemonæ•°æ®é›†æˆªå›¾</h5><p>Pokemon æ˜¯ä¸€ä¸ªå¤§å‹å¤šåª’ä½“æ¸¸æˆï¼ŒåŒ…å«äº†å„ç§å£è¢‹å¦–æ€ªï¼ˆPokemonï¼‰è§’è‰²ã€‚ç®€è€Œè¨€ä¹‹ï¼Œä½ å¯ä»¥è®¤ä¸ºä»–ä»¬æ˜¯å¸¦æœ‰è¶…èƒ½åŠ›çš„åŠ¨ç‰©ï¼è¿™äº›æ•°æ®é›†ç”±è¿™äº›å£è¢‹å¦–æ€ªè§’è‰²æ„æˆï¼Œæ¯ä¸ªè§’è‰²å¸¦æœ‰å„ç§ç»Ÿè®¡ä¿¡æ¯ã€‚</p><h4 id="æ•°å€¼"><a href="#æ•°å€¼" class="headerlink" title="æ•°å€¼"></a>æ•°å€¼</h4><p>å¦‚æœä½ ä»”ç»†åœ°è§‚å¯Ÿä¸Šå›¾ä¸­è¿™äº›æ•°æ®ï¼Œä½ ä¼šçœ‹åˆ°å‡ ä¸ªä»£è¡¨æ•°å€¼å‹åŸå§‹å€¼çš„å±æ€§ï¼Œå®ƒå¯ä»¥è¢«ç›´æ¥ä½¿ç”¨ã€‚ä¸‹é¢çš„è¿™è¡Œä»£ç æŒ‘å‡ºäº†å…¶ä¸­ä¸€äº›é‡ç‚¹ç‰¹å¾ã€‚</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poke_df[[&apos;HP&apos;, &apos;Attack&apos;, &apos;Defense&apos;]].head()</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f557552811.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><h5 id="å¸¦ï¼ˆè¿ç»­å‹ï¼‰æ•°å€¼æ•°æ®çš„ç‰¹å¾"><a href="#å¸¦ï¼ˆè¿ç»­å‹ï¼‰æ•°å€¼æ•°æ®çš„ç‰¹å¾" class="headerlink" title="å¸¦ï¼ˆè¿ç»­å‹ï¼‰æ•°å€¼æ•°æ®çš„ç‰¹å¾"></a>å¸¦ï¼ˆè¿ç»­å‹ï¼‰æ•°å€¼æ•°æ®çš„ç‰¹å¾</h5><p>è¿™æ ·ï¼Œä½ å¯ä»¥ç›´æ¥å°†è¿™äº›å±æ€§ä½œä¸ºç‰¹å¾ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºã€‚è¿™äº›ç‰¹å¾åŒ…æ‹¬ Pokemon çš„ HPï¼ˆè¡€é‡ï¼‰ï¼ŒAttackï¼ˆæ”»å‡»ï¼‰å’Œ Defenseï¼ˆé˜²å¾¡ï¼‰çŠ¶æ€ã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥åŸºäºè¿™äº›å­—æ®µè®¡ç®—å‡ºä¸€äº›åŸºæœ¬çš„ç»Ÿè®¡é‡ã€‚</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poke_df[[&apos;HP&apos;, &apos;Attack&apos;, &apos;Defense&apos;]].describe()</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f559f61c14.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p><strong>æ•°å€¼ç‰¹å¾å½¢å¼çš„åŸºæœ¬æè¿°æ€§ç»Ÿè®¡é‡</strong></p><p>è¿™æ ·ä½ å°±å¯¹ç‰¹å¾ä¸­çš„ç»Ÿè®¡é‡å¦‚æ€»æ•°ã€å¹³å‡å€¼ã€æ ‡å‡†å·®å’Œå››åˆ†ä½æ•°æœ‰äº†ä¸€ä¸ªå¾ˆå¥½çš„å°è±¡ã€‚</p><h4 id="è®°æ•°"><a href="#è®°æ•°" class="headerlink" title="è®°æ•°"></a>è®°æ•°</h4><p>åŸå§‹åº¦é‡çš„å¦ä¸€ç§å½¢å¼åŒ…æ‹¬ä»£è¡¨é¢‘ç‡ã€æ€»æ•°æˆ–ç‰¹å¾å±æ€§å‘ç”Ÿæ¬¡æ•°çš„ç‰¹å¾ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ <a href="https://labrosa.ee.columbia.edu/millionsong/" target="_blank" rel="noopener"><strong>millionsong</strong></a> <strong>æ•°æ®é›†</strong>ä¸­çš„ä¸€ä¸ªä¾‹å­ï¼Œå…¶æè¿°äº†æŸä¸€æ­Œæ›²è¢«å„ç§ç”¨æˆ·æ”¶å¬çš„æ€»æ•°æˆ–é¢‘æ•°ã€‚</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">popsong_df = pd.read_csv(&apos;datasets/song_views.csv&apos;,encoding=&apos;utf-8&apos;)</span><br><span class="line"></span><br><span class="line">popsong_df.head(10)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f55bf6176f.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><h5 id="æ•°å€¼ç‰¹å¾å½¢å¼çš„æ­Œæ›²æ”¶å¬æ€»æ•°"><a href="#æ•°å€¼ç‰¹å¾å½¢å¼çš„æ­Œæ›²æ”¶å¬æ€»æ•°" class="headerlink" title="æ•°å€¼ç‰¹å¾å½¢å¼çš„æ­Œæ›²æ”¶å¬æ€»æ•°"></a>æ•°å€¼ç‰¹å¾å½¢å¼çš„æ­Œæ›²æ”¶å¬æ€»æ•°</h5><p>æ ¹æ®è¿™å¼ æˆªå›¾ï¼Œæ˜¾è€Œæ˜“è§ listen_count å­—æ®µå¯ä»¥ç›´æ¥ä½œä¸ºåŸºäºæ•°å€¼å‹ç‰¹å¾çš„é¢‘æ•°æˆ–æ€»æ•°ã€‚</p><h4 id="äºŒå€¼åŒ–"><a href="#äºŒå€¼åŒ–" class="headerlink" title="äºŒå€¼åŒ–"></a>äºŒå€¼åŒ–</h4><p>åŸºäºè¦è§£å†³çš„é—®é¢˜æ„å»ºæ¨¡å‹æ—¶ï¼Œé€šå¸¸åŸå§‹é¢‘æ•°æˆ–æ€»æ•°å¯èƒ½ä¸æ­¤ä¸ç›¸å…³ã€‚æ¯”å¦‚å¦‚æœæˆ‘è¦å»ºç«‹ä¸€ä¸ªæ¨èç³»ç»Ÿç”¨æ¥æ¨èæ­Œæ›²ï¼Œæˆ‘åªå¸Œæœ›çŸ¥é“ä¸€ä¸ªäººæ˜¯å¦æ„Ÿå…´è¶£æˆ–æ˜¯å¦å¬è¿‡æŸæ­Œæ›²ã€‚æˆ‘ä¸éœ€è¦çŸ¥é“ä¸€é¦–æ­Œè¢«å¬è¿‡çš„æ¬¡æ•°ï¼Œå› ä¸ºæˆ‘æ›´å…³å¿ƒçš„æ˜¯ä¸€ä¸ªäººæ‰€å¬è¿‡çš„å„ç§å„æ ·çš„æ­Œæ›²ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒäºŒå€¼åŒ–çš„ç‰¹å¾æ¯”åŸºäºè®¡æ•°çš„ç‰¹å¾æ›´åˆé€‚ã€‚æˆ‘ä»¬äºŒå€¼åŒ– listen_count å­—æ®µå¦‚ä¸‹ã€‚</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; watched = np.array(popsong_df[&apos;listen_count&apos;])</span><br><span class="line">&gt;</span><br><span class="line">&gt; watched[watched &gt;= 1] = 1</span><br><span class="line">&gt;</span><br><span class="line">&gt; popsong_df[&apos;watched&apos;] = watched</span><br></pre></td></tr></table></figure><p>ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ scikit-learn ä¸­ preprocessing æ¨¡å—çš„ Binarizer ç±»æ¥æ‰§è¡ŒåŒæ ·çš„ä»»åŠ¡ï¼Œè€Œä¸ä¸€å®šä½¿ç”¨ numpy æ•°ç»„ã€‚</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import Binarizer</span><br><span class="line"></span><br><span class="line">bn = Binarizer(threshold=0.9)</span><br><span class="line"></span><br><span class="line">pd_watched =bn.transform([popsong_df[&apos;listen_count&apos;]])[0]</span><br><span class="line"></span><br><span class="line">popsong_df[&apos;pd_watched&apos;] = pd_watched</span><br><span class="line"></span><br><span class="line">popsong_df.head(11)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56505e8ff.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><h5 id="æ­Œæ›²æ”¶å¬æ€»æ•°çš„äºŒå€¼åŒ–ç»“æ„"><a href="#æ­Œæ›²æ”¶å¬æ€»æ•°çš„äºŒå€¼åŒ–ç»“æ„" class="headerlink" title="æ­Œæ›²æ”¶å¬æ€»æ•°çš„äºŒå€¼åŒ–ç»“æ„"></a>æ­Œæ›²æ”¶å¬æ€»æ•°çš„äºŒå€¼åŒ–ç»“æ„</h5><p>ä½ å¯ä»¥ä»ä¸Šé¢çš„æˆªå›¾ä¸­æ¸…æ¥šåœ°çœ‹åˆ°ï¼Œä¸¤ä¸ªæ–¹æ³•å¾—åˆ°äº†ç›¸åŒçš„ç»“æœã€‚å› æ­¤æˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªäºŒå€¼åŒ–çš„ç‰¹å¾æ¥è¡¨ç¤ºä¸€é¦–æ­Œæ˜¯å¦è¢«æ¯ä¸ªç”¨æˆ·å¬è¿‡ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ç›¸å…³çš„æ¨¡å‹ä¸­ä½¿ç”¨å®ƒã€‚</p><h4 id="æ•°æ®èˆå…¥"><a href="#æ•°æ®èˆå…¥" class="headerlink" title="æ•°æ®èˆå…¥"></a>æ•°æ®èˆå…¥</h4><p>å¤„ç†è¿ç»­å‹æ•°å€¼å±æ€§å¦‚æ¯”ä¾‹æˆ–ç™¾åˆ†æ¯”æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸ä¸éœ€è¦é«˜ç²¾åº¦çš„åŸå§‹æ•°å€¼ã€‚å› æ­¤é€šå¸¸æœ‰å¿…è¦å°†è¿™äº›é«˜ç²¾åº¦çš„ç™¾åˆ†æ¯”èˆå…¥ä¸ºæ•´æ•°å‹æ•°å€¼ã€‚è¿™äº›æ•´æ•°å¯ä»¥ç›´æ¥ä½œä¸ºåŸå§‹æ•°å€¼ç”šè‡³åˆ†ç±»å‹ç‰¹å¾ï¼ˆåŸºäºç¦»æ•£ç±»çš„ï¼‰ä½¿ç”¨ã€‚è®©æˆ‘ä»¬è¯•ç€å°†è¿™ä¸ªè§‚å¿µåº”ç”¨åˆ°ä¸€ä¸ªè™šæ‹Ÿæ•°æ®é›†ä¸Šï¼Œè¯¥æ•°æ®é›†æè¿°äº†åº“å­˜é¡¹å’Œä»–ä»¬çš„æµè¡Œåº¦ç™¾åˆ†æ¯”ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">items_popularity =pd.read_csv(<span class="string">'datasets/item_popularity.csv'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">items_popularity[<span class="string">'popularity_scale_10'</span>] = np.array(np.round((items_popularity[<span class="string">'pop_percent'</span>] * <span class="number">10</span>)),dtype=<span class="string">'int'</span>)</span><br><span class="line"></span><br><span class="line">items_popularity[<span class="string">'popularity_scale_100'</span>] = np.array(np.round((items_popularity[<span class="string">'pop_percent'</span>] * <span class="number">100</span>)),dtype=<span class="string">'int'</span>)</span><br><span class="line"></span><br><span class="line">items_popularity</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f566e30ad2.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><h5 id="ä¸åŒå°ºåº¦ä¸‹æµè¡Œåº¦èˆå…¥ç»“æœ"><a href="#ä¸åŒå°ºåº¦ä¸‹æµè¡Œåº¦èˆå…¥ç»“æœ" class="headerlink" title="ä¸åŒå°ºåº¦ä¸‹æµè¡Œåº¦èˆå…¥ç»“æœ"></a>ä¸åŒå°ºåº¦ä¸‹æµè¡Œåº¦èˆå…¥ç»“æœ</h5><p>åŸºäºä¸Šé¢çš„è¾“å‡ºï¼Œä½ å¯èƒ½çŒœåˆ°æˆ‘ä»¬è¯•äº†ä¸¤ç§ä¸åŒçš„èˆå…¥æ–¹å¼ã€‚è¿™äº›ç‰¹å¾è¡¨æ˜é¡¹ç›®æµè¡Œåº¦çš„ç‰¹å¾ç°åœ¨æ—¢æœ‰ 1-10 çš„å°ºåº¦ä¹Ÿæœ‰ 1-100 çš„å°ºåº¦ã€‚åŸºäºè¿™ä¸ªåœºæ™¯æˆ–é—®é¢˜ä½ å¯ä»¥ä½¿ç”¨è¿™äº›å€¼åŒæ—¶ä½œä¸ºæ•°å€¼å‹æˆ–åˆ†ç±»å‹ç‰¹å¾ã€‚</p><h4 id="ç›¸å…³æ€§"><a href="#ç›¸å…³æ€§" class="headerlink" title="ç›¸å…³æ€§"></a>ç›¸å…³æ€§</h4><p>é«˜çº§æœºå™¨å­¦ä¹ æ¨¡å‹é€šå¸¸ä¼šå¯¹ä½œä¸ºè¾“å…¥ç‰¹å¾å˜é‡å‡½æ•°çš„è¾“å‡ºå“åº”å»ºæ¨¡ï¼ˆç¦»æ•£ç±»åˆ«æˆ–è¿ç»­æ•°å€¼ï¼‰ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’æ–¹ç¨‹å¯ä»¥è¡¨ç¤ºä¸º</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56ab22fb7.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>å…¶ä¸­è¾“å…¥ç‰¹å¾ç”¨å˜é‡è¡¨ç¤ºä¸º</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56c69ac66.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>æƒé‡æˆ–ç³»æ•°å¯ä»¥åˆ†åˆ«è¡¨ç¤ºä¸º</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56de74ee7.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>ç›®æ ‡æ˜¯é¢„æµ‹å“åº” <strong>*y*</strong>.</p><p>åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œä»…ä»…æ ¹æ®å•ä¸ªçš„ã€åˆ†ç¦»çš„è¾“å…¥ç‰¹å¾ï¼Œè¿™ä¸ªç®€å•çš„çº¿æ€§æ¨¡å‹æè¿°äº†è¾“å‡ºä¸è¾“å…¥ä¹‹é—´çš„å…³ç³»ã€‚</p><p>ç„¶è€Œï¼Œåœ¨ä¸€äº›çœŸå®åœºæ™¯ä¸­ï¼Œæœ‰å¿…è¦è¯•ç€æ•è·è¿™äº›è¾“å…¥ç‰¹å¾é›†ä¸€éƒ¨åˆ†çš„ç‰¹å¾å˜é‡ä¹‹é—´çš„ç›¸å…³æ€§ã€‚ä¸Šè¿°å¸¦æœ‰ç›¸å…³ç‰¹å¾çš„çº¿æ€§å›å½’æ–¹ç¨‹çš„å±•å¼€å¼å¯ä»¥ç®€å•è¡¨ç¤ºä¸º</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f5701419ee.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>æ­¤å¤„ç‰¹å¾å¯è¡¨ç¤ºä¸º</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f57162d4f7.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>è¡¨ç¤ºäº†ç›¸å…³ç‰¹å¾ã€‚ç°åœ¨è®©æˆ‘ä»¬è¯•ç€åœ¨ Pokemon æ•°æ®é›†ä¸Šè®¾è®¡ä¸€äº›ç›¸å…³ç‰¹å¾ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">atk_def = poke_df[[<span class="string">'Attack'</span>, <span class="string">'Defense'</span>]]</span><br><span class="line"></span><br><span class="line">atk_def.head()</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f572bad2cc.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>ä»è¾“å‡ºæ•°æ®æ¡†ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬æœ‰ä¸¤ä¸ªæ•°å€¼å‹ï¼ˆè¿ç»­çš„ï¼‰ç‰¹å¾ï¼ŒAttack å’Œ Defenceã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥åˆ©ç”¨ scikit-learn å»ºç«‹äºŒåº¦ç‰¹å¾ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">pf = PolynomialFeatures(degree=<span class="number">2</span>,</span><br><span class="line"></span><br><span class="line">interaction_only=<span class="literal">False</span>,include_bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">res = pf.fit_transform(atk_def)</span><br><span class="line"></span><br><span class="line">res</span><br><span class="line"></span><br><span class="line">**Output**</span><br><span class="line"></span><br><span class="line">**------**</span><br><span class="line"></span><br><span class="line">array([[ <span class="number">49.</span>, <span class="number">49.</span>, <span class="number">2401.</span>, <span class="number">2401.</span>, <span class="number">2401.</span>],</span><br><span class="line"></span><br><span class="line">  [ <span class="number">62.</span>, <span class="number">63.</span>, <span class="number">3844.</span>, <span class="number">3906.</span>, <span class="number">3969.</span>],</span><br><span class="line"></span><br><span class="line">  [ <span class="number">82.</span>, <span class="number">83.</span>, <span class="number">6724.</span>, <span class="number">6806.</span>, <span class="number">6889.</span>],</span><br><span class="line"></span><br><span class="line">  ...,</span><br><span class="line"></span><br><span class="line">  [ <span class="number">110.</span>, <span class="number">60.</span>, <span class="number">12100.</span>, <span class="number">6600.</span>, <span class="number">3600.</span>],</span><br><span class="line"></span><br><span class="line">  [ <span class="number">160.</span>, <span class="number">60.</span>, <span class="number">25600.</span>, <span class="number">9600.</span>, <span class="number">3600.</span>],</span><br><span class="line"></span><br><span class="line">[ <span class="number">110.</span>, <span class="number">120.</span>, <span class="number">12100.</span>, <span class="number">13200.</span>, <span class="number">14400.</span>]])</span><br></pre></td></tr></table></figure><p>ä¸Šé¢çš„ç‰¹å¾çŸ©é˜µä¸€å…±æè¿°äº† 5 ä¸ªç‰¹å¾ï¼Œå…¶ä¸­åŒ…æ‹¬æ–°çš„ç›¸å…³ç‰¹å¾ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸Šè¿°çŸ©é˜µä¸­æ¯ä¸ªç‰¹å¾çš„åº¦ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.DataFrame(pf.powers_, columns=[<span class="string">'Attack_degree'</span>,<span class="string">'Defense_degree'</span>])</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f575a65683.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>åŸºäºè¿™ä¸ªè¾“å‡ºï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥é€šè¿‡æ¯ä¸ªç‰¹å¾çš„åº¦çŸ¥é“å®ƒå®é™…ä¸Šä»£è¡¨ä»€ä¹ˆã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥å¯¹æ¯ä¸ªç‰¹å¾è¿›è¡Œå‘½åå¦‚ä¸‹ã€‚è¿™ä»…ä»…æ˜¯ä¸ºäº†ä¾¿äºç†è§£ï¼Œä½ å¯ä»¥ç»™è¿™äº›ç‰¹å¾å–æ›´å¥½çš„ã€å®¹æ˜“ä½¿ç”¨å’Œç®€å•çš„åå­—ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">intr_features = pd.DataFrame(res, columns=[<span class="string">'Attack'</span>,<span class="string">'Defense'</span>,<span class="string">'Attack^2'</span>,<span class="string">'Attack x Defense'</span>,<span class="string">'Defense^2'</span>])</span><br><span class="line"></span><br><span class="line">intr_features.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f576e91376.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><h5 id="æ•°å€¼å‹ç‰¹å¾åŠå…¶ç›¸å…³ç‰¹å¾"><a href="#æ•°å€¼å‹ç‰¹å¾åŠå…¶ç›¸å…³ç‰¹å¾" class="headerlink" title="æ•°å€¼å‹ç‰¹å¾åŠå…¶ç›¸å…³ç‰¹å¾"></a>æ•°å€¼å‹ç‰¹å¾åŠå…¶ç›¸å…³ç‰¹å¾</h5><p>å› æ­¤ä¸Šè¿°æ•°æ®ä»£è¡¨äº†æˆ‘ä»¬åŸå§‹çš„ç‰¹å¾ä»¥åŠå®ƒä»¬çš„ç›¸å…³ç‰¹å¾ã€‚</p><h4 id="åˆ†åŒºé—´å¤„ç†æ•°æ®"><a href="#åˆ†åŒºé—´å¤„ç†æ•°æ®" class="headerlink" title="åˆ†åŒºé—´å¤„ç†æ•°æ®"></a>åˆ†åŒºé—´å¤„ç†æ•°æ®</h4><p>å¤„ç†åŸå§‹ã€è¿ç»­çš„æ•°å€¼å‹ç‰¹å¾é—®é¢˜é€šå¸¸ä¼šå¯¼è‡´è¿™äº›ç‰¹å¾å€¼çš„åˆ†å¸ƒè¢«ç ´åã€‚è¿™è¡¨æ˜æœ‰äº›å€¼ç»å¸¸å‡ºç°è€Œå¦ä¸€äº›å€¼å‡ºç°éå¸¸å°‘ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå¦ä¸€ä¸ªé—®é¢˜æ˜¯è¿™äº›ç‰¹å¾çš„å€¼çš„å˜åŒ–èŒƒå›´ã€‚æ¯”å¦‚æŸä¸ªéŸ³ä¹è§†é¢‘çš„è§‚çœ‹æ€»æ•°ä¼šéå¸¸å¤§ï¼ˆ<a href="https://www.youtube.com/watch?v=kJQP7kiw5Fk" target="_blank" rel="noopener">Despacito</a>ï¼Œè¯´ä½ å‘¢ï¼‰è€Œä¸€äº›å€¼ä¼šéå¸¸å°ã€‚ç›´æ¥ä½¿ç”¨è¿™äº›ç‰¹å¾ä¼šäº§ç”Ÿå¾ˆå¤šé—®é¢˜ï¼Œåè€Œä¼šå½±å“æ¨¡å‹è¡¨ç°ã€‚å› æ­¤å‡ºç°äº†å¤„ç†è¿™äº›é—®é¢˜çš„æŠ€å·§ï¼ŒåŒ…æ‹¬åˆ†åŒºé—´æ³•å’Œå˜æ¢ã€‚</p><p>åˆ†åŒºé—´ï¼ˆBiningï¼‰ï¼Œä¹Ÿå«åšé‡åŒ–ï¼Œç”¨äºå°†è¿ç»­å‹æ•°å€¼ç‰¹å¾è½¬æ¢ä¸ºç¦»æ•£å‹ç‰¹å¾ï¼ˆç±»åˆ«ï¼‰ã€‚å¯ä»¥è®¤ä¸ºè¿™äº›ç¦»æ•£å€¼æˆ–æ•°å­—æ˜¯ç±»åˆ«æˆ–åŸå§‹çš„è¿ç»­å‹æ•°å€¼è¢«åˆ†åŒºé—´æˆ–åˆ†ç»„ä¹‹åçš„æ•°ç›®ã€‚æ¯ä¸ªä¸åŒçš„åŒºé—´å¤§å°ä»£è¡¨æŸç§å¯†åº¦ï¼Œå› æ­¤ä¸€ä¸ªç‰¹å®šèŒƒå›´çš„è¿ç»­å‹æ•°å€¼ä¼šè½åœ¨é‡Œé¢ã€‚å¯¹æ•°æ®åšåˆ†åŒºé—´çš„å…·ä½“æŠ€å·§åŒ…æ‹¬ç­‰å®½åˆ†åŒºé—´ä»¥åŠè‡ªé€‚åº”åˆ†åŒºé—´ã€‚æˆ‘ä»¬ä½¿ç”¨ä» <a href="https://github.com/freeCodeCamp/2016-new-coder-survey" target="_blank" rel="noopener">2016 å¹´ FreeCodeCamp å¼€å‘è€…å’Œç¼–ç å‘˜è°ƒæŸ¥æŠ¥å‘Š</a>ä¸­æŠ½å–å‡ºæ¥çš„ä¸€ä¸ªå­é›†ä¸­çš„æ•°æ®ï¼Œæ¥è®¨è®ºå„ç§é’ˆå¯¹ç¼–ç å‘˜å’Œè½¯ä»¶å¼€å‘è€…çš„å±æ€§ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df =pd.read_csv(<span class="string">'datasets/fcc_2016_coder_survey_subset.csv'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'EmploymentField'</span>, <span class="string">'Age'</span>,<span class="string">'Income'</span>]].head()</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f578e01139.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><h5 id="æ¥è‡ªFCCç¼–ç å‘˜è°ƒæŸ¥æ•°æ®é›†çš„æ ·æœ¬å±æ€§"><a href="#æ¥è‡ªFCCç¼–ç å‘˜è°ƒæŸ¥æ•°æ®é›†çš„æ ·æœ¬å±æ€§" class="headerlink" title="æ¥è‡ªFCCç¼–ç å‘˜è°ƒæŸ¥æ•°æ®é›†çš„æ ·æœ¬å±æ€§"></a>æ¥è‡ªFCCç¼–ç å‘˜è°ƒæŸ¥æ•°æ®é›†çš„æ ·æœ¬å±æ€§</h5><p>å¯¹äºæ¯ä¸ªå‚åŠ è°ƒæŸ¥çš„ç¼–ç å‘˜æˆ–å¼€å‘è€…ï¼ŒID.x å˜é‡åŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªå”¯ä¸€çš„æ ‡è¯†ç¬¦è€Œå…¶ä»–å­—æ®µæ˜¯å¯è‡ªæˆ‘è§£é‡Šçš„ã€‚</p><h4 id="ç­‰å®½åˆ†åŒºé—´"><a href="#ç­‰å®½åˆ†åŒºé—´" class="headerlink" title="ç­‰å®½åˆ†åŒºé—´"></a>ç­‰å®½åˆ†åŒºé—´</h4><p>å°±åƒåå­—è¡¨æ˜çš„é‚£æ ·ï¼Œåœ¨ç­‰å®½åˆ†åŒºé—´æ–¹æ³•ä¸­ï¼Œæ¯ä¸ªåŒºé—´éƒ½æ˜¯å›ºå®šå®½åº¦çš„ï¼Œé€šå¸¸å¯ä»¥é¢„å…ˆåˆ†ææ•°æ®è¿›è¡Œå®šä¹‰ã€‚åŸºäºä¸€äº›é¢†åŸŸçŸ¥è¯†ã€è§„åˆ™æˆ–çº¦æŸï¼Œæ¯ä¸ªåŒºé—´æœ‰ä¸ªé¢„å…ˆå›ºå®šçš„å€¼çš„èŒƒå›´ï¼Œåªæœ‰å¤„äºèŒƒå›´å†…çš„æ•°å€¼æ‰è¢«åˆ†é…åˆ°è¯¥åŒºé—´ã€‚åŸºäºæ•°æ®èˆå…¥æ“ä½œçš„åˆ†åŒºé—´æ˜¯ä¸€ç§æ–¹å¼ï¼Œä½ å¯ä»¥ä½¿ç”¨æ•°æ®èˆå…¥æ“ä½œæ¥å¯¹åŸå§‹å€¼è¿›è¡Œåˆ†åŒºé—´ï¼Œæˆ‘ä»¬å‰é¢å·²ç»è®²è¿‡ã€‚</p><p>ç°åœ¨æˆ‘ä»¬åˆ†æç¼–ç å‘˜è°ƒæŸ¥æŠ¥å‘Šæ•°æ®é›†çš„ Age ç‰¹å¾å¹¶çœ‹çœ‹å®ƒçš„åˆ†å¸ƒã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Age'</span>].hist(color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Age Histogram'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Age'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f57b05846b.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><h5 id="æè¿°å¼€å‘è€…å¹´é¾„åˆ†å¸ƒçš„ç›´æ–¹å›¾"><a href="#æè¿°å¼€å‘è€…å¹´é¾„åˆ†å¸ƒçš„ç›´æ–¹å›¾" class="headerlink" title="æè¿°å¼€å‘è€…å¹´é¾„åˆ†å¸ƒçš„ç›´æ–¹å›¾"></a>æè¿°å¼€å‘è€…å¹´é¾„åˆ†å¸ƒçš„ç›´æ–¹å›¾</h5><p>ä¸Šé¢çš„ç›´æ–¹å›¾è¡¨æ˜ï¼Œå¦‚é¢„æœŸé‚£æ ·ï¼Œå¼€å‘è€…å¹´é¾„åˆ†å¸ƒä»¿ä½›å¾€å·¦ä¾§å€¾æ–œï¼ˆä¸Šå¹´çºªçš„å¼€å‘è€…åå°‘ï¼‰ã€‚ç°åœ¨æˆ‘ä»¬æ ¹æ®ä¸‹é¢çš„æ¨¡å¼ï¼Œå°†è¿™äº›åŸå§‹å¹´é¾„å€¼åˆ†é…åˆ°ç‰¹å®šçš„åŒºé—´ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Age Range: Bin</span><br><span class="line"></span><br><span class="line">\---------------</span><br><span class="line"></span><br><span class="line"><span class="number">0</span> - <span class="number">9</span> : <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="number">10</span> - <span class="number">19</span> : <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="number">20</span> - <span class="number">29</span> : <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="number">30</span> - <span class="number">39</span> : <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="number">40</span> - <span class="number">49</span> : <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="number">50</span> - <span class="number">59</span> : <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="number">60</span> - <span class="number">69</span> : <span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="meta">... </span><span class="keyword">and</span> so on</span><br></pre></td></tr></table></figure><p>æˆ‘ä»¬å¯ä»¥ç®€å•åœ°ä½¿ç”¨æˆ‘ä»¬å…ˆå‰å­¦ä¹ åˆ°çš„æ•°æ®èˆå…¥éƒ¨åˆ†çŸ¥è¯†ï¼Œå…ˆå°†è¿™äº›åŸå§‹å¹´é¾„å€¼é™¤ä»¥ 10ï¼Œç„¶åé€šè¿‡ floor å‡½æ•°å¯¹åŸå§‹å¹´é¾„æ•°å€¼è¿›è¡Œæˆªæ–­ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df[<span class="string">'Age_bin_round'</span>] = np.array(np.floor(np.array(fcc_survey_df[<span class="string">'Age'</span>]) / <span class="number">10.</span>))</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>,<span class="string">'Age_bin_round'</span>]].iloc[<span class="number">1071</span>:<span class="number">1076</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f57d916a6f.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><h5 id="é€šè¿‡èˆå…¥æ³•åˆ†åŒºé—´"><a href="#é€šè¿‡èˆå…¥æ³•åˆ†åŒºé—´" class="headerlink" title="é€šè¿‡èˆå…¥æ³•åˆ†åŒºé—´"></a>é€šè¿‡èˆå…¥æ³•åˆ†åŒºé—´</h5><p>ä½ å¯ä»¥çœ‹åˆ°åŸºäºæ•°æ®èˆå…¥æ“ä½œçš„æ¯ä¸ªå¹´é¾„å¯¹åº”çš„åŒºé—´ã€‚ä½†æ˜¯å¦‚æœæˆ‘ä»¬éœ€è¦æ›´çµæ´»çš„æ“ä½œæ€ä¹ˆåŠï¼Ÿå¦‚æœæˆ‘ä»¬æƒ³åŸºäºæˆ‘ä»¬çš„è§„åˆ™æˆ–é€»è¾‘ï¼Œç¡®å®šæˆ–ä¿®æ”¹åŒºé—´çš„å®½åº¦æ€ä¹ˆåŠï¼ŸåŸºäºå¸¸ç”¨èŒƒå›´çš„åˆ†åŒºé—´æ–¹æ³•å°†å¸®åŠ©æˆ‘ä»¬å®Œæˆè¿™ä¸ªã€‚è®©æˆ‘ä»¬æ¥å®šä¹‰ä¸€äº›é€šç”¨å¹´é¾„æ®µä½ï¼Œä½¿ç”¨ä¸‹é¢çš„æ–¹å¼æ¥å¯¹å¼€å‘è€…å¹´é¾„åˆ†åŒºé—´ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Age Range : Bin</span><br><span class="line"></span><br><span class="line">\---------------</span><br><span class="line"></span><br><span class="line"><span class="number">0</span> - <span class="number">15</span> : <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="number">16</span> - <span class="number">30</span> : <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="number">31</span> - <span class="number">45</span> : <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="number">46</span> - <span class="number">60</span> : <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="number">61</span> - <span class="number">75</span> : <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="number">75</span> - <span class="number">100</span> : <span class="number">6</span></span><br></pre></td></tr></table></figure><p>åŸºäºè¿™äº›å¸¸ç”¨çš„åˆ†åŒºé—´æ–¹å¼ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥å¯¹æ¯ä¸ªå¼€å‘è€…å¹´é¾„å€¼çš„åŒºé—´æ‰“æ ‡ç­¾ï¼Œæˆ‘ä»¬å°†å­˜å‚¨åŒºé—´çš„èŒƒå›´å’Œç›¸åº”çš„æ ‡ç­¾ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">bin_ranges = [<span class="number">0</span>, <span class="number">15</span>, <span class="number">30</span>, <span class="number">45</span>, <span class="number">60</span>, <span class="number">75</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">bin_names = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Age_bin_custom_range'</span>] = pd.cut(np.array(fcc_survey_df[<span class="string">'Age'</span>]),bins=bin_ranges)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Age_bin_custom_label'</span>] = pd.cut(np.array(fcc_survey_df[<span class="string">'Age'</span>]),bins=bin_ranges, labels=bin_names)</span><br><span class="line"></span><br><span class="line">\<span class="comment"># view the binned features</span></span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Age_bin_round'</span>,<span class="string">'Age_bin_custom_range'</span>,<span class="string">'Age_bin_custom_label'</span>]].iloc[<span class="number">10</span>a71:<span class="number">1076</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58143c35f.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><h5 id="å¼€å‘è€…å¹´é¾„çš„å¸¸ç”¨åˆ†åŒºé—´æ–¹å¼"><a href="#å¼€å‘è€…å¹´é¾„çš„å¸¸ç”¨åˆ†åŒºé—´æ–¹å¼" class="headerlink" title="å¼€å‘è€…å¹´é¾„çš„å¸¸ç”¨åˆ†åŒºé—´æ–¹å¼"></a>å¼€å‘è€…å¹´é¾„çš„å¸¸ç”¨åˆ†åŒºé—´æ–¹å¼</h5><h4 id="è‡ªé€‚åº”åˆ†åŒºé—´"><a href="#è‡ªé€‚åº”åˆ†åŒºé—´" class="headerlink" title="è‡ªé€‚åº”åˆ†åŒºé—´"></a>è‡ªé€‚åº”åˆ†åŒºé—´</h4><p>ä½¿ç”¨ç­‰å®½åˆ†åŒºé—´çš„ä¸è¶³ä¹‹å¤„åœ¨äºï¼Œæˆ‘ä»¬æ‰‹åŠ¨å†³å®šäº†åŒºé—´çš„å€¼èŒƒå›´ï¼Œè€Œç”±äºè½åœ¨æŸä¸ªåŒºé—´ä¸­çš„æ•°æ®ç‚¹æˆ–å€¼çš„æ•°ç›®æ˜¯ä¸å‡åŒ€çš„ï¼Œå› æ­¤å¯èƒ½ä¼šå¾—åˆ°ä¸è§„åˆ™çš„åŒºé—´ã€‚ä¸€äº›åŒºé—´ä¸­çš„æ•°æ®å¯èƒ½ä¼šéå¸¸çš„å¯†é›†ï¼Œä¸€äº›åŒºé—´ä¼šéå¸¸ç¨€ç–ç”šè‡³æ˜¯ç©ºçš„ï¼è‡ªé€‚åº”åˆ†åŒºé—´æ–¹æ³•æ˜¯ä¸€ä¸ªæ›´å®‰å…¨çš„ç­–ç•¥ï¼Œåœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬è®©æ•°æ®è‡ªå·±è¯´è¯ï¼è¿™æ ·ï¼Œæˆ‘ä»¬ä½¿ç”¨æ•°æ®åˆ†å¸ƒæ¥å†³å®šåŒºé—´çš„èŒƒå›´ã€‚</p><p>åŸºäºåˆ†ä½æ•°çš„åˆ†åŒºé—´æ–¹æ³•æ˜¯è‡ªé€‚åº”åˆ†ç®±æ–¹æ³•ä¸­ä¸€ä¸ªå¾ˆå¥½çš„æŠ€å·§ã€‚é‡åŒ–å¯¹äºç‰¹å®šå€¼æˆ–åˆ‡ç‚¹æœ‰åŠ©äºå°†ç‰¹å®šæ•°å€¼åŸŸçš„è¿ç»­å€¼åˆ†å¸ƒåˆ’åˆ†ä¸ºç¦»æ•£çš„äº’ç›¸æŒ¨ç€çš„åŒºé—´ã€‚å› æ­¤ q åˆ†ä½æ•°æœ‰åŠ©äºå°†æ•°å€¼å±æ€§åˆ’åˆ†ä¸º q ä¸ªç›¸ç­‰çš„éƒ¨åˆ†ã€‚å…³äºé‡åŒ–æ¯”è¾ƒæµè¡Œçš„ä¾‹å­åŒ…æ‹¬ 2 åˆ†ä½æ•°ï¼Œä¹Ÿå«ä¸­å€¼ï¼Œå°†æ•°æ®åˆ†å¸ƒåˆ’åˆ†ä¸º2ä¸ªç›¸ç­‰çš„åŒºé—´ï¼›4 åˆ†ä½æ•°ï¼Œä¹Ÿç®€ç§°åˆ†ä½æ•°ï¼Œå®ƒå°†æ•°æ®åˆ’åˆ†ä¸º 4 ä¸ªç›¸ç­‰çš„åŒºé—´ï¼›ä»¥åŠ 10 åˆ†ä½æ•°ï¼Œä¹Ÿå«ååˆ†ä½æ•°ï¼Œåˆ›å»º 10 ä¸ªç›¸ç­‰å®½åº¦çš„åŒºé—´ï¼Œç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¼€å‘è€…æ•°æ®é›†çš„ Income å­—æ®µçš„æ•°æ®åˆ†å¸ƒã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>].hist(bins=<span class="number">30</span>, color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Income Histogram'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Developer Income'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f583631eff.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p><strong>æè¿°å¼€å‘è€…æ”¶å…¥åˆ†å¸ƒçš„ç›´æ–¹å›¾</strong></p><p>ä¸Šè¿°çš„åˆ†å¸ƒæè¿°äº†ä¸€ä¸ªåœ¨æ”¶å…¥ä¸Šå³æ­ªæ–œçš„åˆ†å¸ƒï¼Œå°‘æ•°äººèµšæ›´å¤šçš„é’±ï¼Œå¤šæ•°äººèµšæ›´å°‘çš„é’±ã€‚è®©æˆ‘ä»¬åŸºäºè‡ªé€‚åº”åˆ†ç®±æ–¹å¼åšä¸€ä¸ª 4-åˆ†ä½æ•°æˆ–åˆ†ä½æ•°ã€‚æˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°å¾—åˆ°å¦‚ä¸‹çš„åˆ†ä½æ•°ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">quantile_list = [<span class="number">0</span>, <span class="number">.25</span>, <span class="number">.5</span>, <span class="number">.75</span>, <span class="number">1.</span>]</span><br><span class="line"></span><br><span class="line">quantiles =</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>].quantile(quantile_list)</span><br><span class="line"></span><br><span class="line">quantiles</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**Output**</span><br><span class="line"></span><br><span class="line">**------**</span><br><span class="line"></span><br><span class="line"><span class="number">0.00</span> <span class="number">6000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">0.25</span> <span class="number">20000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">0.50</span> <span class="number">37000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">0.75</span> <span class="number">60000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">1.00</span> <span class="number">200000.0</span></span><br><span class="line"></span><br><span class="line">Name: Income, dtype: float64</span><br></pre></td></tr></table></figure><p>ç°åœ¨è®©æˆ‘ä»¬åœ¨åŸå§‹çš„åˆ†å¸ƒç›´æ–¹å›¾ä¸­å¯è§†åŒ–ä¸‹è¿™äº›åˆ†ä½æ•°ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>].hist(bins=<span class="number">30</span>, color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> quantile <span class="keyword">in</span> quantiles:</span><br><span class="line"></span><br><span class="line">qvl = plt.axvline(quantile, color=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">ax.legend([qvl], [<span class="string">'Quantiles'</span>], fontsize=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Income Histogram with Quantiles'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Developer Income'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f5853f1a2c.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><h5 id="å¸¦åˆ†ä½æ•°å½¢å¼æè¿°å¼€å‘è€…æ”¶å…¥åˆ†å¸ƒçš„ç›´æ–¹å›¾"><a href="#å¸¦åˆ†ä½æ•°å½¢å¼æè¿°å¼€å‘è€…æ”¶å…¥åˆ†å¸ƒçš„ç›´æ–¹å›¾" class="headerlink" title="å¸¦åˆ†ä½æ•°å½¢å¼æè¿°å¼€å‘è€…æ”¶å…¥åˆ†å¸ƒçš„ç›´æ–¹å›¾"></a>å¸¦åˆ†ä½æ•°å½¢å¼æè¿°å¼€å‘è€…æ”¶å…¥åˆ†å¸ƒçš„ç›´æ–¹å›¾</h5><p>ä¸Šé¢æè¿°çš„åˆ†å¸ƒä¸­çº¢è‰²çº¿ä»£è¡¨äº†åˆ†ä½æ•°å€¼å’Œæˆ‘ä»¬æ½œåœ¨çš„åŒºé—´ã€‚è®©æˆ‘ä»¬åˆ©ç”¨è¿™äº›çŸ¥è¯†æ¥æ„å»ºæˆ‘ä»¬åŸºäºåˆ†åŒºé—´ç­–ç•¥çš„åˆ†ä½æ•°ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">quantile_labels = [<span class="string">'0-25Q'</span>, <span class="string">'25-50Q'</span>, <span class="string">'50-75Q'</span>, <span class="string">'75-100Q'</span>]</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_quantile_range'</span>] = pd.qcut(</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>],q=quantile_list)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_quantile_label'</span>] = pd.qcut(</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>],q=quantile_list,labels=quantile_labels)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Income'</span>,<span class="string">'Income_quantile_range'</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">'Income_quantile_label'</span>]].iloc[<span class="number">4</span>:<span class="number">9</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f586dbd8f4.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><h5 id="åŸºäºåˆ†ä½æ•°çš„å¼€å‘è€…æ”¶å…¥çš„åŒºé—´èŒƒå›´å’Œæ ‡ç­¾"><a href="#åŸºäºåˆ†ä½æ•°çš„å¼€å‘è€…æ”¶å…¥çš„åŒºé—´èŒƒå›´å’Œæ ‡ç­¾" class="headerlink" title="åŸºäºåˆ†ä½æ•°çš„å¼€å‘è€…æ”¶å…¥çš„åŒºé—´èŒƒå›´å’Œæ ‡ç­¾"></a>åŸºäºåˆ†ä½æ•°çš„å¼€å‘è€…æ”¶å…¥çš„åŒºé—´èŒƒå›´å’Œæ ‡ç­¾</h5><p>é€šè¿‡è¿™ä¸ªä¾‹å­ï¼Œä½ åº”è¯¥å¯¹å¦‚ä½•åšåŸºäºåˆ†ä½æ•°çš„è‡ªé€‚åº”åˆ†åŒºé—´æ³•æœ‰äº†ä¸€ä¸ªå¾ˆå¥½çš„è®¤è¯†ã€‚ä¸€ä¸ªéœ€è¦é‡ç‚¹è®°ä½çš„æ˜¯ï¼Œåˆ†åŒºé—´çš„ç»“æœæ˜¯ç¦»æ•£å€¼ç±»å‹çš„åˆ†ç±»ç‰¹å¾ï¼Œå½“ä½ åœ¨æ¨¡å‹ä¸­ä½¿ç”¨åˆ†ç±»æ•°æ®ä¹‹å‰ï¼Œå¯èƒ½éœ€è¦é¢å¤–çš„ç‰¹å¾å·¥ç¨‹ç›¸å…³æ­¥éª¤ã€‚æˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ç®€è¦åœ°è®²è¿°åˆ†ç±»æ•°æ®çš„ç‰¹å¾å·¥ç¨‹æŠ€å·§ã€‚</p><h4 id="ç»Ÿè®¡å˜æ¢"><a href="#ç»Ÿè®¡å˜æ¢" class="headerlink" title="ç»Ÿè®¡å˜æ¢"></a>ç»Ÿè®¡å˜æ¢</h4><p>æˆ‘ä»¬è®¨è®ºä¸‹å…ˆå‰ç®€å•æåˆ°è¿‡çš„æ•°æ®åˆ†å¸ƒå€¾æ–œçš„è´Ÿé¢å½±å“ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥è€ƒè™‘å¦ä¸€ä¸ªç‰¹å¾å·¥ç¨‹æŠ€å·§ï¼Œå³åˆ©ç”¨ç»Ÿè®¡æˆ–æ•°å­¦å˜æ¢ã€‚æˆ‘ä»¬è¯•è¯•çœ‹ Log å˜æ¢å’Œ Box-Cox å˜æ¢ã€‚è¿™ä¸¤ç§å˜æ¢å‡½æ•°éƒ½å±äºå¹‚å˜æ¢å‡½æ•°ç°‡ï¼Œé€šå¸¸ç”¨æ¥åˆ›å»ºå•è°ƒçš„æ•°æ®å˜æ¢ã€‚å®ƒä»¬çš„ä¸»è¦ä½œç”¨åœ¨äºå®ƒèƒ½å¸®åŠ©ç¨³å®šæ–¹å·®ï¼Œå§‹ç»ˆä¿æŒåˆ†å¸ƒæ¥è¿‘äºæ­£æ€åˆ†å¸ƒå¹¶ä½¿å¾—æ•°æ®ä¸åˆ†å¸ƒçš„å¹³å‡å€¼æ— å…³ã€‚</p><h4 id="Logå˜æ¢"><a href="#Logå˜æ¢" class="headerlink" title="Logå˜æ¢"></a>Logå˜æ¢</h4><p>log å˜æ¢å±äºå¹‚å˜æ¢å‡½æ•°ç°‡ã€‚è¯¥å‡½æ•°ç”¨æ•°å­¦è¡¨è¾¾å¼è¡¨ç¤ºä¸º</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f588a0f6a5.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>è¯»ä¸ºä»¥ b ä¸ºåº• x çš„å¯¹æ•°ç­‰äº yã€‚è¿™å¯ä»¥å˜æ¢ä¸º</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f589e77242.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>è¡¨ç¤ºä»¥bä¸ºåº•æŒ‡æ•°å¿…é¡»è¾¾åˆ°å¤šå°‘æ‰ç­‰äºxã€‚è‡ªç„¶å¯¹æ•°ä½¿ç”¨ b=eï¼Œe=2.71828ï¼Œé€šå¸¸å«ä½œæ¬§æ‹‰å¸¸æ•°ã€‚ä½ å¯ä»¥ä½¿ç”¨é€šå¸¸åœ¨åè¿›åˆ¶ç³»ç»Ÿä¸­ä½¿ç”¨çš„ b=10 ä½œä¸ºåº•æ•°ã€‚</p><p><strong>å½“åº”ç”¨äºå€¾æ–œåˆ†å¸ƒæ—¶ Log å˜æ¢æ˜¯å¾ˆæœ‰ç”¨çš„ï¼Œå› ä¸ºä»–ä»¬å€¾å‘äºæ‹‰ä¼¸é‚£äº›è½åœ¨è¾ƒä½çš„å¹…åº¦èŒƒå›´å†…è‡ªå˜é‡å€¼çš„èŒƒå›´ï¼Œå€¾å‘äºå‹ç¼©æˆ–å‡å°‘æ›´é«˜å¹…åº¦èŒƒå›´å†…çš„è‡ªå˜é‡å€¼çš„èŒƒå›´</strong>ã€‚ä»è€Œä½¿å¾—å€¾æ–œåˆ†å¸ƒå°½å¯èƒ½çš„æ¥è¿‘æ­£æ€åˆ†å¸ƒã€‚è®©æˆ‘ä»¬å¯¹å…ˆå‰ä½¿ç”¨çš„å¼€å‘è€…æ•°æ®é›†çš„ Income ç‰¹å¾ä¸Šä½¿ç”¨logå˜æ¢ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df[<span class="string">'Income_log'</span>] = np.log((<span class="number">1</span>+fcc_survey_df[<span class="string">'Income'</span>]))</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Income'</span>,<span class="string">'Income_log'</span>]].iloc[<span class="number">4</span>:<span class="number">9</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58b3ed249.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><h5 id="å¼€å‘è€…æ”¶å…¥logå˜æ¢åç»“æ„"><a href="#å¼€å‘è€…æ”¶å…¥logå˜æ¢åç»“æ„" class="headerlink" title="å¼€å‘è€…æ”¶å…¥logå˜æ¢åç»“æ„"></a>å¼€å‘è€…æ”¶å…¥logå˜æ¢åç»“æ„</h5><p>Income_log å­—æ®µæè¿°äº†ç»è¿‡ log å˜æ¢åçš„ç‰¹å¾ã€‚ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹å­—æ®µå˜æ¢åæ•°æ®çš„åˆ†å¸ƒã€‚</p><p>åŸºäºä¸Šé¢çš„å›¾ï¼Œæˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°ä¸å…ˆå‰å€¾æ–œåˆ†å¸ƒç›¸æ¯”ï¼Œè¯¥åˆ†å¸ƒæ›´åŠ åƒæ­£æ€åˆ†å¸ƒæˆ–é«˜æ–¯åˆ†å¸ƒã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">income_log_mean =np.round(np.mean(fcc_survey_df[<span class="string">'Income_log'</span>]), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_log'</span>].hist(bins=<span class="number">30</span>,color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">plt.axvline(income_log_mean, color=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Income Histogram after Log Transform'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Developer Income (log scale)'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.text(<span class="number">11.5</span>, <span class="number">450</span>, <span class="string">r'$\mu$='</span>+str(income_log_mean),fontsize=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58cdaf02a.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p><strong>ç»è¿‡logå˜æ¢åæè¿°å¼€å‘è€…æ”¶å…¥åˆ†å¸ƒçš„ç›´æ–¹å›¾</strong></p><h4 id="Box-Coxå˜æ¢"><a href="#Box-Coxå˜æ¢" class="headerlink" title="Box-Coxå˜æ¢"></a>Box-Coxå˜æ¢</h4><p>Box-Cox å˜æ¢æ˜¯å¦ä¸€ä¸ªæµè¡Œçš„å¹‚å˜æ¢å‡½æ•°ç°‡ä¸­çš„ä¸€ä¸ªå‡½æ•°ã€‚è¯¥å‡½æ•°æœ‰ä¸€ä¸ªå‰ææ¡ä»¶ï¼Œå³æ•°å€¼å‹å€¼å¿…é¡»å…ˆå˜æ¢ä¸ºæ­£æ•°ï¼ˆä¸ log å˜æ¢æ‰€è¦æ±‚çš„ä¸€æ ·ï¼‰ã€‚ä¸‡ä¸€å‡ºç°æ•°å€¼æ˜¯è´Ÿçš„ï¼Œä½¿ç”¨ä¸€ä¸ªå¸¸æ•°å¯¹æ•°å€¼è¿›è¡Œåç§»æ˜¯æœ‰å¸®åŠ©çš„ã€‚æ•°å­¦ä¸Šï¼ŒBox-Cox å˜æ¢å‡½æ•°å¯ä»¥è¡¨ç¤ºå¦‚ä¸‹ã€‚</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58e556c08.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>ç”Ÿæˆçš„å˜æ¢åçš„è¾“å‡ºyæ˜¯è¾“å…¥ x å’Œå˜æ¢å‚æ•°çš„å‡½æ•°ï¼›å½“ Î»=0 æ—¶ï¼Œè¯¥å˜æ¢å°±æ˜¯è‡ªç„¶å¯¹æ•° log å˜æ¢ï¼Œå‰é¢æˆ‘ä»¬å·²ç»æåˆ°è¿‡äº†ã€‚Î» çš„æœ€ä½³å–å€¼é€šå¸¸ç”±æœ€å¤§ä¼¼ç„¶æˆ–æœ€å¤§å¯¹æ•°ä¼¼ç„¶ç¡®å®šã€‚ç°åœ¨è®©æˆ‘ä»¬åœ¨å¼€å‘è€…æ•°æ®é›†çš„æ”¶å…¥ç‰¹å¾ä¸Šåº”ç”¨ Box-Cox å˜æ¢ã€‚é¦–å…ˆæˆ‘ä»¬ä»æ•°æ®åˆ†å¸ƒä¸­ç§»é™¤éé›¶å€¼å¾—åˆ°æœ€ä½³çš„å€¼ï¼Œç»“æœå¦‚ä¸‹ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">income = np.array(fcc_survey_df[<span class="string">'Income'</span>])</span><br><span class="line"></span><br><span class="line">income_clean = income[~np.isnan(income)]</span><br><span class="line"></span><br><span class="line">l, opt_lambda = spstats.boxcox(income_clean)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Optimal lambda value:'</span>, opt_lambda)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**Output**</span><br><span class="line"></span><br><span class="line">**------**</span><br><span class="line"></span><br><span class="line">Optimal <span class="keyword">lambda</span> value: <span class="number">0.117991239456</span></span><br></pre></td></tr></table></figure><p>ç°åœ¨æˆ‘ä»¬å¾—åˆ°äº†æœ€ä½³çš„å€¼ï¼Œè®©æˆ‘ä»¬åœ¨å–å€¼ä¸º 0 å’Œ Î»ï¼ˆæœ€ä½³å–å€¼ Î» ï¼‰æ—¶ä½¿ç”¨ Box-Cox å˜æ¢å¯¹å¼€å‘è€…æ”¶å…¥ç‰¹å¾è¿›è¡Œå˜æ¢ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df[<span class="string">'Income_boxcox_lambda_0'</span>] = spstats.boxcox((<span class="number">1</span>+fcc_survey_df[<span class="string">'Income'</span>]),lmbda=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_boxcox_lambda_opt'</span>] = spstats.boxcox(fcc_survey_df[<span class="string">'Income'</span>],lmbda=opt_lambda)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Income'</span>, <span class="string">'Income_log'</span>,<span class="string">'Income_boxcox_lambda_0'</span>,<span class="string">'Income_boxcox_lambda_opt'</span>]].iloc[<span class="number">4</span>:<span class="number">9</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58fd7fd5e.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><h5 id="ç»è¿‡-Box-Cox-å˜æ¢åå¼€å‘è€…çš„æ”¶å…¥åˆ†å¸ƒ"><a href="#ç»è¿‡-Box-Cox-å˜æ¢åå¼€å‘è€…çš„æ”¶å…¥åˆ†å¸ƒ" class="headerlink" title="ç»è¿‡ Box-Cox å˜æ¢åå¼€å‘è€…çš„æ”¶å…¥åˆ†å¸ƒ"></a>ç»è¿‡ Box-Cox å˜æ¢åå¼€å‘è€…çš„æ”¶å…¥åˆ†å¸ƒ</h5><p>å˜æ¢åçš„ç‰¹å¾åœ¨ä¸Šè¿°æ•°æ®æ¡†ä¸­æè¿°äº†ã€‚å°±åƒæˆ‘ä»¬æœŸæœ›çš„é‚£æ ·ï¼ŒIncome_log å’Œ Income_boxcox_lamba_0å…·æœ‰ç›¸åŒçš„å–å€¼ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ç»è¿‡æœ€ä½³Î»å˜æ¢å Income ç‰¹å¾çš„åˆ†å¸ƒã€‚</p><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;income_boxcox_mean = np.round(np.mean(fcc_survey_df[<span class="string">'Income_boxcox_lambda_opt'</span>]),<span class="number">2</span>)</span><br><span class="line">&gt; </span><br><span class="line">&gt;fig, ax = plt.subplots()</span><br><span class="line">&gt; </span><br><span class="line">&gt;fcc_survey_df[<span class="string">'Income_boxcox_lambda_opt'</span>].hist(bins=<span class="number">30</span>,  color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>, grid=<span class="literal">False</span>)</span><br><span class="line">&gt;    plt.axvline(income_boxcox_mean, color=<span class="string">'r'</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.set_title(<span class="string">'Developer Income Histogram after Boxâ€“Cox Transform'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.set_xlabel(<span class="string">'Developer Income (Boxâ€“Cox transform)'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.text(<span class="number">24</span>, <span class="number">450</span>, <span class="string">r'$\mu$='</span>+str(income_boxcox_mean),fontsize=<span class="number">10</span>)       </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f591679bfb.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸Šç¯‡ - è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p><strong>ç»è¿‡Box-Coxå˜æ¢åæè¿°å¼€å‘è€…æ”¶å…¥åˆ†å¸ƒçš„ç›´æ–¹å›¾</strong></p><p> åˆ†å¸ƒçœ‹èµ·æ¥æ›´åƒæ˜¯æ­£æ€åˆ†å¸ƒï¼Œä¸æˆ‘ä»¬ç»è¿‡ log å˜æ¢åçš„åˆ†å¸ƒç›¸ä¼¼ã€‚</p><h3 id="ç±»åˆ«å‹æ•°æ®ä¸Šçš„ç‰¹å¾å·¥ç¨‹"><a href="#ç±»åˆ«å‹æ•°æ®ä¸Šçš„ç‰¹å¾å·¥ç¨‹" class="headerlink" title="ç±»åˆ«å‹æ•°æ®ä¸Šçš„ç‰¹å¾å·¥ç¨‹"></a>ç±»åˆ«å‹æ•°æ®ä¸Šçš„ç‰¹å¾å·¥ç¨‹</h3><p>åœ¨æ·±å…¥ç ”ç©¶ç‰¹å¾å·¥ç¨‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆäº†è§£ä¸€ä¸‹åˆ†ç±»æ•°æ®ã€‚é€šå¸¸ï¼Œåœ¨<strong>è‡ªç„¶ç•Œä¸­å¯åˆ†ç±»çš„ä»»æ„æ•°æ®å±æ€§éƒ½æ˜¯ç¦»æ•£å€¼ï¼Œè¿™æ„å‘³ç€å®ƒä»¬å±äºæŸä¸€ç‰¹å®šçš„æœ‰é™ç±»åˆ«</strong>ã€‚åœ¨æ¨¡å‹é¢„æµ‹çš„å±æ€§æˆ–è€…å˜é‡ï¼ˆé€šå¸¸è¢«ç§°ä¸º<strong>å“åº”å˜é‡ response variables</strong>ï¼‰ä¸­ï¼Œè¿™äº›ä¹Ÿç»å¸¸è¢«ç§°ä¸ºç±»åˆ«æˆ–è€…æ ‡ç­¾ã€‚è¿™äº›ç¦»æ•£å€¼åœ¨è‡ªç„¶ç•Œä¸­å¯ä»¥æ˜¯æ–‡æœ¬æˆ–è€…æ•°å­—ï¼ˆç”šè‡³æ˜¯è¯¸å¦‚å›¾åƒè¿™æ ·çš„éç»“æ„åŒ–æ•°æ®ï¼‰ã€‚åˆ†ç±»æ•°æ®æœ‰ä¸¤å¤§ç±»â€”â€”<strong>å®šç±»ï¼ˆNominalï¼‰å’Œå®šåºï¼ˆOrdinalï¼‰</strong>ã€‚</p><p>åœ¨ä»»æ„å®šç±»åˆ†ç±»æ•°æ®å±æ€§ä¸­ï¼Œè¿™äº›å±æ€§å€¼ä¹‹é—´<strong>æ²¡æœ‰é¡ºåºçš„æ¦‚å¿µ</strong>ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä¸¾ä¸ªç®€å•çš„ä¾‹å­ï¼Œå¤©æ°”åˆ†ç±»ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨è¿™ä¸ªç‰¹å®šçš„åœºæ™¯ä¸­ï¼Œä¸»è¦æœ‰å…­ä¸ªå¤§ç±»ï¼Œè€Œè¿™äº›ç±»ä¹‹é—´æ²¡æœ‰ä»»ä½•é¡ºåºä¸Šçš„å…³ç³»ï¼ˆåˆ®é£å¤©å¹¶ä¸æ€»æ˜¯å‘ç”Ÿåœ¨æ™´å¤©ä¹‹å‰ï¼Œå¹¶ä¸”ä¹Ÿä¸èƒ½è¯´æ¯”æ™´å¤©æ¥çš„æ›´å°æˆ–è€…æ›´å¤§ï¼‰</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af6bc87b4e.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸‹ç¯‡ - ç¦»æ•£æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>å°†å¤©æ°”ä½œä¸ºåˆ†ç±»å±æ€§</p><p>ä¸å¤©æ°”ç›¸ç±»ä¼¼çš„å±æ€§è¿˜æœ‰å¾ˆå¤šï¼Œæ¯”å¦‚ç”µå½±ã€éŸ³ä¹ã€ç”µå­æ¸¸æˆã€å›½å®¶ã€é£Ÿç‰©å’Œç¾é£Ÿç±»å‹ç­‰ç­‰ï¼Œè¿™äº›éƒ½å±äºå®šç±»åˆ†ç±»å±æ€§ã€‚</p><p>å®šåºåˆ†ç±»çš„å±æ€§å€¼åˆ™å­˜åœ¨ç€ä¸€å®šçš„é¡ºåºæ„ä¹‰æˆ–æ¦‚å¿µã€‚ä¾‹å¦‚ï¼Œä¸‹å›¾ä¸­çš„å­—æ¯æ ‡è¯†äº†è¡¬è¡«çš„å¤§å°ã€‚æ˜¾è€Œæ˜“è§çš„æ˜¯ï¼Œå½“æˆ‘ä»¬è€ƒè™‘è¡¬è¡«çš„æ—¶å€™ï¼Œå®ƒçš„â€œå¤§å°â€å±æ€§æ˜¯å¾ˆé‡è¦çš„ï¼ˆS ç æ¯” M ç æ¥çš„å°ï¼Œè€Œ M ç åˆå°äº L ç ç­‰ç­‰ï¼‰ã€‚</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af6d3b83ac.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸‹ç¯‡ - ç¦»æ•£æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>è¡¬è¡«å¤§å°ä½œä¸ºå®šåºåˆ†ç±»å±æ€§</p><p>é‹å·ã€å—æ•™è‚²æ°´å¹³å’Œå…¬å¸èŒä½åˆ™æ˜¯å®šåºåˆ†ç±»å±æ€§çš„ä¸€äº›å…¶å®ƒä¾‹å­ã€‚æ—¢ç„¶å·²ç»å¯¹åˆ†ç±»æ•°æ®æœ‰äº†ä¸€ä¸ªå¤§è‡´çš„ç†è§£ä¹‹åï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬æ¥çœ‹çœ‹ä¸€äº›ç‰¹å¾å·¥ç¨‹çš„ç­–ç•¥ã€‚</p><p>åœ¨æ¥å—åƒæ–‡æœ¬æ ‡ç­¾è¿™æ ·å¤æ‚çš„åˆ†ç±»æ•°æ®ç±»å‹é—®é¢˜ä¸Šï¼Œå„ç§æœºå™¨å­¦ä¹ æ¡†æ¶å‡å·²å–å¾—äº†è®¸å¤šçš„è¿›æ­¥ã€‚é€šå¸¸ï¼Œç‰¹å¾å·¥ç¨‹ä¸­çš„ä»»æ„æ ‡å‡†å·¥ä½œæµéƒ½æ¶‰åŠå°†è¿™äº›åˆ†ç±»å€¼è½¬æ¢ä¸ºæ•°å€¼æ ‡ç­¾çš„æŸç§å½¢å¼ï¼Œç„¶åå¯¹è¿™äº›å€¼åº”ç”¨ä¸€äº›<strong>ç¼–ç æ–¹æ¡ˆ</strong>ã€‚æˆ‘ä»¬å°†åœ¨å¼€å§‹ä¹‹å‰å¯¼å…¥å¿…è¦çš„å·¥å…·åŒ…ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h4 id="å®šç±»å±æ€§è½¬æ¢-LabelEncoding"><a href="#å®šç±»å±æ€§è½¬æ¢-LabelEncoding" class="headerlink" title="å®šç±»å±æ€§è½¬æ¢(LabelEncoding)"></a>å®šç±»å±æ€§è½¬æ¢(LabelEncoding)</h4><p><strong>å®šç±»å±æ€§ç”±ç¦»æ•£çš„åˆ†ç±»å€¼ç»„æˆï¼Œå®ƒä»¬æ²¡æœ‰å…ˆåé¡ºåºæ¦‚å¿µ</strong>ã€‚è¿™é‡Œçš„æ€æƒ³æ˜¯å°†è¿™äº›å±æ€§è½¬æ¢æˆæ›´å…·ä»£è¡¨æ€§çš„æ•°å€¼æ ¼å¼ï¼Œè¿™æ ·å¯ä»¥å¾ˆå®¹æ˜“è¢«ä¸‹æ¸¸çš„ä»£ç å’Œæµæ°´çº¿æ‰€ç†è§£ã€‚æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªå…³äºè§†é¢‘æ¸¸æˆé”€å”®çš„æ–°æ•°æ®é›†ã€‚è¿™ä¸ªæ•°æ®é›†ä¹Ÿå¯ä»¥åœ¨ <a href="https://www.kaggle.com/gregorut/videogamesales" target="_blank" rel="noopener">Kaggle</a> å’Œæˆ‘çš„ <a href="https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/notebooks/Ch04_Feature_Engineering_and_Selection" target="_blank" rel="noopener">GitHub</a> ä»“åº“ä¸­æ‰¾åˆ°ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vg_df = pd.read_csv(<span class="string">'datasets/vgsales.csv'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">vg_df[[<span class="string">'Name'</span>, <span class="string">'Platform'</span>, <span class="string">'Year'</span>, <span class="string">'Genre'</span>, <span class="string">'Publisher'</span>]].iloc[<span class="number">1</span>:<span class="number">7</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af756b687d.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸‹ç¯‡ - ç¦»æ•£æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>æ¸¸æˆé”€å”®æ•°æ®</p><p>è®©æˆ‘ä»¬é¦–å…ˆä¸“æ³¨äºä¸Šé¢æ•°æ®æ¡†ä¸­â€œè§†é¢‘æ¸¸æˆé£æ ¼ï¼ˆGenreï¼‰â€å±æ€§ã€‚æ˜¾è€Œæ˜“è§çš„æ˜¯ï¼Œè¿™æ˜¯ä¸€ä¸ªç±»ä¼¼äºâ€œå‘è¡Œå•†ï¼ˆPublisherï¼‰â€å’Œâ€œå¹³å°ï¼ˆPlatformï¼‰â€å±æ€§ä¸€æ ·çš„å®šç±»åˆ†ç±»å±æ€§ã€‚æˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“å¾—åˆ°ä¸€ä¸ªç‹¬ç‰¹çš„è§†é¢‘æ¸¸æˆé£æ ¼åˆ—è¡¨ï¼Œå¦‚ä¸‹ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">genres = np.unique(vg_df[<span class="string">'Genre'</span>])</span><br><span class="line"></span><br><span class="line">genres</span><br><span class="line"></span><br><span class="line">Output</span><br><span class="line"></span><br><span class="line">\------</span><br><span class="line"></span><br><span class="line">array([<span class="string">'Action'</span>, <span class="string">'Adventure'</span>, <span class="string">'Fighting'</span>, <span class="string">'Misc'</span>, <span class="string">'Platform'</span>, <span class="string">'Puzzle'</span>, <span class="string">'Racing'</span>, <span class="string">'Role-Playing'</span>, <span class="string">'Shooter'</span>, <span class="string">'Simulation'</span>, <span class="string">'Sports'</span>, <span class="string">'Strategy'</span>], dtype=object)</span><br></pre></td></tr></table></figure><p>è¾“å‡ºç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æœ‰ 12 ç§ä¸åŒçš„è§†é¢‘æ¸¸æˆé£æ ¼ã€‚æˆ‘ä»¬ç°åœ¨å¯ä»¥ç”Ÿæˆä¸€ä¸ªæ ‡ç­¾ç¼–ç æ–¹æ³•ï¼Œå³åˆ©ç”¨ scikit-learn å°†æ¯ä¸ªç±»åˆ«æ˜ å°„åˆ°ä¸€ä¸ªæ•°å€¼ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line">gle = LabelEncoder()</span><br><span class="line"></span><br><span class="line">genre_labels = gle.fit_transform(vg_df[<span class="string">'Genre'</span>])</span><br><span class="line"></span><br><span class="line">genre_mappings = &#123;index: label <span class="keyword">for</span> index, label <span class="keyword">in</span> enumerate(gle.classes_)&#125;</span><br><span class="line"></span><br><span class="line">genre_mappings</span><br><span class="line"></span><br><span class="line">Output</span><br><span class="line"></span><br><span class="line">\------</span><br><span class="line"></span><br><span class="line">&#123;<span class="number">0</span>: <span class="string">'Action'</span>, <span class="number">1</span>: <span class="string">'Adventure'</span>, <span class="number">2</span>: <span class="string">'Fighting'</span>, <span class="number">3</span>: <span class="string">'Misc'</span>, <span class="number">4</span>: <span class="string">'Platform'</span>, <span class="number">5</span>: <span class="string">'Puzzle'</span>, <span class="number">6</span>: <span class="string">'Racing'</span>, <span class="number">7</span>: <span class="string">'Role-Playing'</span>, <span class="number">8</span>: <span class="string">'Shooter'</span>, <span class="number">9</span>: <span class="string">'Simulation'</span>, <span class="number">10</span>: <span class="string">'Sports'</span>, <span class="number">11</span>: <span class="string">'Strategy'</span>&#125;</span><br></pre></td></tr></table></figure><p>å› æ­¤ï¼Œåœ¨ <em>LabelEncoder</em> ç±»çš„å®ä¾‹å¯¹è±¡ <em>gle</em> çš„å¸®åŠ©ä¸‹ç”Ÿæˆäº†ä¸€ä¸ªæ˜ å°„æ–¹æ¡ˆï¼ŒæˆåŠŸåœ°å°†æ¯ä¸ªé£æ ¼å±æ€§æ˜ å°„åˆ°ä¸€ä¸ªæ•°å€¼ã€‚è½¬æ¢åçš„æ ‡ç­¾å­˜å‚¨åœ¨ <em>genre_labels</em> ä¸­ï¼Œè¯¥å˜é‡å…è®¸æˆ‘ä»¬å°†å…¶å†™å›æ•°æ®è¡¨ä¸­ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vg_df[<span class="string">'GenreLabel'</span>] = genre_labels</span><br><span class="line"></span><br><span class="line">vg_df[[<span class="string">'Name'</span>, <span class="string">'Platform'</span>, <span class="string">'Year'</span>, <span class="string">'Genre'</span>, <span class="string">'GenreLabel'</span>]].iloc[<span class="number">1</span>:<span class="number">7</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af8164e6db.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸‹ç¯‡ - ç¦»æ•£æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>è§†é¢‘æ¸¸æˆé£æ ¼åŠå…¶ç¼–ç æ ‡ç­¾</p><p>å¦‚æœä½ æ‰“ç®—å°†å®ƒä»¬ç”¨ä½œé¢„æµ‹çš„å“åº”å˜é‡ï¼Œé‚£ä¹ˆè¿™äº›æ ‡ç­¾é€šå¸¸å¯ä»¥ç›´æ¥ç”¨äºè¯¸å¦‚ sikit-learn è¿™æ ·çš„æ¡†æ¶ã€‚ä½†æ˜¯å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬è¿˜éœ€è¦é¢å¤–çš„ç¼–ç æ­¥éª¤æ‰èƒ½å°†å®ƒä»¬ç”¨ä½œç‰¹å¾ã€‚</p><h4 id="å®šåºå±æ€§ç¼–ç "><a href="#å®šåºå±æ€§ç¼–ç " class="headerlink" title="å®šåºå±æ€§ç¼–ç "></a>å®šåºå±æ€§ç¼–ç </h4><p><strong>å®šåºå±æ€§æ˜¯ä¸€ç§å¸¦æœ‰å…ˆåé¡ºåºæ¦‚å¿µçš„åˆ†ç±»å±æ€§</strong>ã€‚è¿™é‡Œæˆ‘å°†ä»¥æœ¬ç³»åˆ—æ–‡ç« ç¬¬ä¸€éƒ¨åˆ†æ‰€ä½¿ç”¨çš„<a href="https://www.kaggle.com/abcsds/pokemon/data" target="_blank" rel="noopener">ç¥å¥‡å®è´æ•°æ®é›†</a>è¿›è¡Œè¯´æ˜ã€‚è®©æˆ‘ä»¬å…ˆä¸“æ³¨äº ã€Œä¸–ä»£ï¼ˆGenerationï¼‰ã€ å±æ€§ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; poke_df = pd.read_csv(<span class="string">'datasets/Pokemon.csv'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; poke_df = poke_df.sample(random_state=<span class="number">1</span>, frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; np.unique(poke_df[<span class="string">'Generation'</span>])</span><br><span class="line">&gt;</span><br><span class="line">&gt; Output</span><br><span class="line">&gt;</span><br><span class="line">&gt; \------</span><br><span class="line">&gt;</span><br><span class="line">&gt; array([<span class="string">'Gen 1'</span>, <span class="string">'Gen 2'</span>, <span class="string">'Gen 3'</span>, <span class="string">'Gen 4'</span>, <span class="string">'Gen 5'</span>, <span class="string">'Gen 6'</span>], dtype=object)</span><br></pre></td></tr></table></figure><p>æ ¹æ®ä¸Šé¢çš„è¾“å‡ºï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€å…±æœ‰ 6 ä»£ï¼Œå¹¶ä¸”æ¯ä¸ªç¥å¥‡å®è´é€šå¸¸å±äºè§†é¢‘æ¸¸æˆçš„ç‰¹å®šä¸–ä»£ï¼ˆä¾æ®å‘å¸ƒé¡ºåºï¼‰ï¼Œè€Œä¸”ç”µè§†ç³»åˆ—ä¹Ÿéµå¾ªäº†ç›¸ä¼¼çš„æ—¶é—´çº¿ã€‚è¿™ä¸ªå±æ€§é€šå¸¸æ˜¯å®šåºçš„ï¼ˆéœ€è¦ç›¸å…³çš„é¢†åŸŸçŸ¥è¯†æ‰èƒ½ç†è§£ï¼‰ï¼Œå› ä¸ºå±äºç¬¬ä¸€ä»£çš„å¤§å¤šæ•°ç¥å¥‡å®è´åœ¨ç¬¬äºŒä»£çš„è§†é¢‘æ¸¸æˆæˆ–è€…ç”µè§†èŠ‚ç›®ä¸­ä¹Ÿä¼šè¢«æ›´æ—©åœ°å¼•å…¥ã€‚ç¥å¥‡å®è´çš„ç²‰ä¸ä»¬å¯ä»¥çœ‹ä¸‹ä¸‹å›¾ï¼Œç„¶åè®°ä½æ¯ä¸€ä»£ä¸­ä¸€äº›æ¯”è¾ƒå—æ¬¢è¿çš„ç¥å¥‡å®è´ï¼ˆä¸åŒçš„ç²‰ä¸å¯èƒ½æœ‰ä¸åŒçš„çœ‹æ³•ï¼‰ã€‚</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af8f58f535.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸‹ç¯‡ - ç¦»æ•£æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>åŸºäºä¸åŒç±»å‹å’Œä¸–ä»£é€‰å‡ºçš„ä¸€äº›å—æ¬¢è¿çš„ç¥å¥‡å®è´</p><p>å› æ­¤ï¼Œå®ƒä»¬ä¹‹é—´å­˜åœ¨ç€å…ˆåé¡ºåºã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ²¡æœ‰é€šç”¨çš„æ¨¡å—æˆ–è€…å‡½æ•°å¯ä»¥æ ¹æ®è¿™äº›é¡ºåºè‡ªåŠ¨å°†è¿™äº›ç‰¹å¾è½¬æ¢å’Œæ˜ å°„åˆ°æ•°å€¼è¡¨ç¤ºã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è‡ªå®šä¹‰çš„ç¼–ç \æ˜ å°„æ–¹æ¡ˆã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">gen_ord_map = &#123;<span class="string">'Gen 1'</span>: <span class="number">1</span>, <span class="string">'Gen 2'</span>: <span class="number">2</span>, <span class="string">'Gen 3'</span>: <span class="number">3</span>, <span class="string">'Gen 4'</span>: <span class="number">4</span>, <span class="string">'Gen 5'</span>: <span class="number">5</span>, <span class="string">'Gen 6'</span>: <span class="number">6</span>&#125; </span><br><span class="line"></span><br><span class="line">poke_df[<span class="string">'GenerationLabel'</span>] = poke_df[<span class="string">'Generation'</span>].map(gen_ord_map)</span><br><span class="line"></span><br><span class="line">poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'GenerationLabel'</span>]].iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af95f94dc8.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸‹ç¯‡ - ç¦»æ•£æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>ç¥å¥‡å®è´ä¸–ä»£ç¼–ç </p><p>ä»ä¸Šé¢çš„ä»£ç ä¸­å¯ä»¥çœ‹å‡ºï¼Œæ¥è‡ª <em>pandas</em> åº“çš„ <em>map(â€¦)</em> å‡½æ•°åœ¨è½¬æ¢è¿™ç§å®šåºç‰¹å¾çš„æ—¶å€™éå¸¸æœ‰ç”¨ã€‚</p><h4 id="ç¼–ç åˆ†ç±»å±æ€§â€“ç‹¬çƒ­ç¼–ç æ–¹æ¡ˆï¼ˆOne-hot-Encoding-Schemeï¼‰"><a href="#ç¼–ç åˆ†ç±»å±æ€§â€“ç‹¬çƒ­ç¼–ç æ–¹æ¡ˆï¼ˆOne-hot-Encoding-Schemeï¼‰" class="headerlink" title="ç¼–ç åˆ†ç±»å±æ€§â€“ç‹¬çƒ­ç¼–ç æ–¹æ¡ˆï¼ˆOne-hot Encoding Schemeï¼‰"></a>ç¼–ç åˆ†ç±»å±æ€§â€“ç‹¬çƒ­ç¼–ç æ–¹æ¡ˆï¼ˆOne-hot Encoding Schemeï¼‰</h4><p>å¦‚æœä½ è¿˜è®°å¾—æˆ‘ä»¬ä¹‹å‰æåˆ°è¿‡çš„å†…å®¹ï¼Œé€šå¸¸å¯¹åˆ†ç±»æ•°æ®è¿›è¡Œç‰¹å¾å·¥ç¨‹å°±æ¶‰åŠåˆ°ä¸€ä¸ªè½¬æ¢è¿‡ç¨‹ï¼Œæˆ‘ä»¬åœ¨å‰ä¸€éƒ¨åˆ†æè¿°äº†ä¸€ä¸ªè½¬æ¢è¿‡ç¨‹ï¼Œè¿˜æœ‰ä¸€ä¸ªå¼ºåˆ¶ç¼–ç è¿‡ç¨‹ï¼Œæˆ‘ä»¬åº”ç”¨ç‰¹å®šçš„ç¼–ç æ–¹æ¡ˆä¸ºç‰¹å®šçš„æ¯ä¸ªç±»åˆ«åˆ›å»ºè™šæ‹Ÿå˜é‡æˆ–ç‰¹å¾åˆ†ç±»å±æ€§ã€‚</p><p>ä½ å¯èƒ½æƒ³çŸ¥é“ï¼Œæˆ‘ä»¬åˆšåˆšåœ¨ä¸Šä¸€èŠ‚è¯´åˆ°å°†ç±»åˆ«è½¬æ¢ä¸ºæ•°å­—æ ‡ç­¾ï¼Œä¸ºä»€ä¹ˆç°åœ¨æˆ‘ä»¬åˆéœ€è¦è¿™ä¸ªï¼ŸåŸå› å¾ˆç®€å•ã€‚è€ƒè™‘åˆ°è§†é¢‘æ¸¸æˆé£æ ¼ï¼Œå¦‚æœæˆ‘ä»¬ç›´æ¥å°† <em>GenereLabel</em> ä½œä¸ºå±æ€§ç‰¹å¾æä¾›ç»™æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œåˆ™æ¨¡å‹ä¼šè®¤ä¸ºå®ƒæ˜¯ä¸€ä¸ªè¿ç»­çš„æ•°å€¼ç‰¹å¾ï¼Œä»è€Œè®¤ä¸ºå€¼ 10 ï¼ˆä½“è‚²ï¼‰è¦å¤§äºå€¼ 6 ï¼ˆèµ›è½¦ï¼‰ï¼Œç„¶è€Œäº‹å®ä¸Šè¿™ç§ä¿¡æ¯æ˜¯æ¯«æ— æ„ä¹‰çš„ï¼Œå› ä¸º<em>ä½“è‚²ç±»å‹</em>æ˜¾ç„¶å¹¶ä¸å¤§äºæˆ–è€…å°äº<em>èµ›è½¦ç±»å‹</em>ï¼Œè¿™äº›ä¸åŒå€¼æˆ–è€…ç±»åˆ«æ— æ³•ç›´æ¥è¿›è¡Œæ¯”è¾ƒã€‚å› æ­¤æˆ‘ä»¬éœ€è¦å¦ä¸€å¥—ç¼–ç æ–¹æ¡ˆå±‚ï¼Œå®ƒè¦èƒ½ä¸ºæ¯ä¸ªå±æ€§çš„æ‰€æœ‰ä¸åŒç±»åˆ«ä¸­çš„æ¯ä¸ªå”¯ä¸€å€¼æˆ–ç±»åˆ«åˆ›å»ºè™šæ‹Ÿç‰¹å¾ã€‚</p><p>è€ƒè™‘åˆ°ä»»æ„å…·æœ‰ m ä¸ªæ ‡ç­¾çš„åˆ†ç±»å±æ€§ï¼ˆå˜æ¢ä¹‹åï¼‰çš„æ•°å­—è¡¨ç¤ºï¼Œç‹¬çƒ­ç¼–ç æ–¹æ¡ˆå°†è¯¥å±æ€§ç¼–ç æˆ–å˜æ¢æˆ m ä¸ªäºŒè¿›åˆ¶ç‰¹å¾å‘é‡ï¼ˆå‘é‡ä¸­çš„æ¯ä¸€ç»´çš„å€¼åªèƒ½ä¸º 0 æˆ– 1ï¼‰ã€‚é‚£ä¹ˆåœ¨è¿™ä¸ªåˆ†ç±»ç‰¹å¾ä¸­æ¯ä¸ªå±æ€§å€¼éƒ½è¢«è½¬æ¢æˆä¸€ä¸ª m ç»´çš„å‘é‡ï¼Œå…¶ä¸­åªæœ‰æŸä¸€ç»´çš„å€¼ä¸º 1ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹ç¥å¥‡å®è´æ•°æ®é›†çš„ä¸€ä¸ªå­é›†ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Legendary'</span>]].iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af9b37bf97.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸‹ç¯‡ - ç¦»æ•£æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>ç¥å¥‡å®è´æ•°æ®é›†å­é›†</p><p>è¿™é‡Œå…³æ³¨çš„å±æ€§æ˜¯ç¥å¥‡å®è´çš„ã€Œä¸–ä»£ï¼ˆGenerationï¼‰ã€å’Œã€Œä¼ å¥‡ï¼ˆLegendaryï¼‰ã€çŠ¶æ€ã€‚ç¬¬ä¸€æ­¥æ˜¯æ ¹æ®ä¹‹å‰å­¦åˆ°çš„å°†è¿™äº›å±æ€§è½¬æ¢ä¸ºæ•°å€¼è¡¨ç¤ºã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder, LabelEncoder</span><br><span class="line"></span><br><span class="line">\<span class="comment"># transform and map pokemon generations</span></span><br><span class="line"></span><br><span class="line">gen_le = LabelEncoder()</span><br><span class="line"></span><br><span class="line">gen_labels = gen_le.fit_transform(poke_df[<span class="string">'Generation'</span>])</span><br><span class="line"></span><br><span class="line">poke_df[<span class="string">'Gen_Label'</span>] = gen_labels</span><br><span class="line"></span><br><span class="line">\<span class="comment"># transform and map pokemon legendary status</span></span><br><span class="line"></span><br><span class="line">leg_le = LabelEncoder()</span><br><span class="line"></span><br><span class="line">leg_labels = leg_le.fit_transform(poke_df[<span class="string">'Legendary'</span>])</span><br><span class="line"></span><br><span class="line">poke_df[<span class="string">'Lgnd_Label'</span>] = leg_labels</span><br><span class="line"></span><br><span class="line">poke_df_sub = poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>, <span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>]]</span><br><span class="line"></span><br><span class="line">poke_df_sub.iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afa18d27fc.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸‹ç¯‡ - ç¦»æ•£æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>è½¬æ¢åçš„æ ‡ç­¾å±æ€§</p><p><em>Gen_Label</em> å’Œ <em>Lgnd_Label</em> ç‰¹å¾æè¿°äº†æˆ‘ä»¬åˆ†ç±»ç‰¹å¾çš„æ•°å€¼è¡¨ç¤ºã€‚ç°åœ¨è®©æˆ‘ä»¬åœ¨è¿™äº›ç‰¹å¾ä¸Šåº”ç”¨ç‹¬çƒ­ç¼–ç æ–¹æ¡ˆã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encode generation labels using one-hot encoding scheme</span></span><br><span class="line"></span><br><span class="line">gen_ohe = OneHotEncoder()</span><br><span class="line"></span><br><span class="line">gen_feature_arr = gen_ohe.fit_transform(poke_df[[<span class="string">'Gen_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">gen_feature_labels = list(gen_le.classes_)</span><br><span class="line"></span><br><span class="line">gen_features = pd.DataFrame(gen_feature_arr, columns=gen_feature_labels)</span><br><span class="line"></span><br><span class="line">\<span class="comment"># encode legendary status labels using one-hot encoding scheme</span></span><br><span class="line"></span><br><span class="line">leg_ohe = OneHotEncoder()</span><br><span class="line"></span><br><span class="line">leg_feature_arr = leg_ohe.fit_transform(poke_df[[<span class="string">'Lgnd_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">leg_feature_labels = [<span class="string">'Legendary_'</span>+str(cls_label) <span class="keyword">for</span> cls_label <span class="keyword">in</span> leg_le.classes_]</span><br><span class="line"></span><br><span class="line">leg_features = pd.DataFrame(leg_feature_arr, columns=leg_feature_labels)</span><br></pre></td></tr></table></figure><p>é€šå¸¸æ¥è¯´ï¼Œä½ å¯ä»¥ä½¿ç”¨ <em>fit_transform</em> å‡½æ•°å°†ä¸¤ä¸ªç‰¹å¾ä¸€èµ·ç¼–ç ï¼ˆé€šè¿‡å°†ä¸¤ä¸ªç‰¹å¾çš„äºŒç»´æ•°ç»„ä¸€èµ·ä¼ é€’ç»™å‡½æ•°ï¼Œè¯¦æƒ…<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" target="_blank" rel="noopener">æŸ¥çœ‹æ–‡æ¡£</a>ï¼‰ã€‚ä½†æ˜¯æˆ‘ä»¬åˆ†å¼€ç¼–ç æ¯ä¸ªç‰¹å¾ï¼Œè¿™æ ·å¯ä»¥æ›´æ˜“äºç†è§£ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥åˆ›å»ºå•ç‹¬çš„æ•°æ®è¡¨å¹¶ç›¸åº”åœ°æ ‡è®°å®ƒä»¬ã€‚ç°åœ¨è®©æˆ‘ä»¬é“¾æ¥è¿™äº›ç‰¹å¾è¡¨ï¼ˆFeature framesï¼‰ç„¶åçœ‹çœ‹æœ€ç»ˆçš„ç»“æœã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">poke_df_ohe = pd.concat([poke_df_sub, gen_features, leg_features], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">columns = sum([[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>], gen_feature_labels, [<span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>], leg_feature_labels], [])</span><br><span class="line"></span><br><span class="line">poke_df_ohe[columns].iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afab9940ae.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸‹ç¯‡ - ç¦»æ•£æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>ç¥å¥‡å®è´ä¸–ä»£å’Œä¼ å¥‡çŠ¶æ€çš„ç‹¬çƒ­ç¼–ç ç‰¹å¾</p><p>æ­¤æ—¶å¯ä»¥çœ‹åˆ°å·²ç»ä¸ºã€Œä¸–ä»£ï¼ˆGenerationï¼‰ã€ç”Ÿæˆ 6 ä¸ªè™šæ‹Ÿå˜é‡æˆ–è€…äºŒè¿›åˆ¶ç‰¹å¾ï¼Œå¹¶ä¸ºã€Œä¼ å¥‡ï¼ˆLegendaryï¼‰ã€ç”Ÿæˆäº† 2 ä¸ªç‰¹å¾ã€‚è¿™äº›ç‰¹å¾æ•°é‡æ˜¯è¿™äº›å±æ€§ä¸­ä¸åŒç±»åˆ«çš„æ€»æ•°ã€‚<strong>æŸä¸€ç±»åˆ«çš„æ¿€æ´»çŠ¶æ€é€šè¿‡å°†å¯¹åº”çš„è™šæ‹Ÿå˜é‡ç½® 1 æ¥è¡¨ç¤º</strong>ï¼Œè¿™ä»ä¸Šé¢çš„æ•°æ®è¡¨ä¸­å¯ä»¥éå¸¸æ˜æ˜¾åœ°ä½“ç°å‡ºæ¥ã€‚</p><p>è€ƒè™‘ä½ åœ¨è®­ç»ƒæ•°æ®ä¸Šå»ºç«‹äº†è¿™ä¸ªç¼–ç æ–¹æ¡ˆï¼Œå¹¶å»ºç«‹äº†ä¸€äº›æ¨¡å‹ï¼Œç°åœ¨ä½ æœ‰äº†ä¸€äº›æ–°çš„æ•°æ®ï¼Œè¿™äº›æ•°æ®å¿…é¡»åœ¨é¢„æµ‹ä¹‹å‰è¿›è¡Œå¦‚ä¸‹è®¾è®¡ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_poke_df = pd.DataFrame([[<span class="string">'PikaZoom'</span>, <span class="string">'Gen 3'</span>, <span class="literal">True</span>], [<span class="string">'CharMyToast'</span>, <span class="string">'Gen 4'</span>, <span class="literal">False</span>]], columns=[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Legendary'</span>])</span><br><span class="line"></span><br><span class="line">new_poke_df</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afaf0cb42e.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸‹ç¯‡ - ç¦»æ•£æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>æ–°æ•°æ®</p><p>ä½ å¯ä»¥é€šè¿‡è°ƒç”¨ä¹‹å‰æ„å»ºçš„ <em>LabelEncoder</em> å’Œ <em>OneHotEncoder</em> å¯¹è±¡çš„ <em>transform()</em> æ–¹æ³•æ¥å¤„ç†æ–°æ•°æ®ã€‚è¯·è®°å¾—æˆ‘ä»¬çš„å·¥ä½œæµç¨‹ï¼Œé¦–å…ˆæˆ‘ä»¬è¦åšè½¬æ¢ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">new_gen_labels = gen_le.transform(new_poke_df[<span class="string">'Generation'</span>])</span><br><span class="line"></span><br><span class="line">new_poke_df[<span class="string">'Gen_Label'</span>] = new_gen_labels</span><br><span class="line"></span><br><span class="line">new_leg_labels = leg_le.transform(new_poke_df[<span class="string">'Legendary'</span>])</span><br><span class="line"></span><br><span class="line">new_poke_df[<span class="string">'Lgnd_Label'</span>] = new_leg_labels</span><br><span class="line"></span><br><span class="line">new_poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>, <span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>]]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afb428f0dc.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸‹ç¯‡ - ç¦»æ•£æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>è½¬æ¢ä¹‹åçš„åˆ†ç±»å±æ€§</p><p>åœ¨å¾—åˆ°äº†æ•°å€¼æ ‡ç­¾ä¹‹åï¼Œæ¥ä¸‹æ¥è®©æˆ‘ä»¬åº”ç”¨ç¼–ç æ–¹æ¡ˆå§ï¼</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">new_gen_feature_arr = gen_ohe.transform(new_poke_df[[<span class="string">'Gen_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">new_gen_features = pd.DataFrame(new_gen_feature_arr, columns=gen_feature_labels)</span><br><span class="line"></span><br><span class="line">new_leg_feature_arr = leg_ohe.transform(new_poke_df[[<span class="string">'Lgnd_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">new_leg_features = pd.DataFrame(new_leg_feature_arr, columns=leg_feature_labels)</span><br><span class="line"></span><br><span class="line">new_poke_ohe = pd.concat([new_poke_df, new_gen_features, new_leg_features], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">columns = sum([[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>], gen_feature_labels, [<span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>], leg_feature_labels], [])</span><br><span class="line"></span><br><span class="line">new_poke_ohe[columns]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afb91bb3be.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸‹ç¯‡ - ç¦»æ•£æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>ç‹¬çƒ­ç¼–ç ä¹‹åçš„åˆ†ç±»å±æ€§</p><p>å› æ­¤ï¼Œé€šè¿‡åˆ©ç”¨ scikit-learn å¼ºå¤§çš„ APIï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“å°†ç¼–ç æ–¹æ¡ˆåº”ç”¨äºæ–°æ•°æ®ã€‚</p><p>ä½ ä¹Ÿå¯ä»¥é€šè¿‡åˆ©ç”¨æ¥è‡ª pandas çš„ <em>to_dummies()</em> å‡½æ•°è½»æ¾åº”ç”¨ç‹¬çƒ­ç¼–ç æ–¹æ¡ˆã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gen_onehot_features = pd.get_dummies(poke_df[<span class="string">'Generation'</span>])</span><br><span class="line"></span><br><span class="line">pd.concat([poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>]], gen_onehot_features], axis=<span class="number">1</span>).iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afbcc0ff2b.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸‹ç¯‡ - ç¦»æ•£æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>ä½¿ç”¨ pandas å®ç°çš„ç‹¬çƒ­ç¼–ç ç‰¹å¾</p><p>ä¸Šé¢çš„æ•°æ®è¡¨æè¿°äº†åº”ç”¨åœ¨ã€Œä¸–ä»£ï¼ˆGenerationï¼‰ã€å±æ€§ä¸Šçš„ç‹¬çƒ­ç¼–ç æ–¹æ¡ˆï¼Œç»“æœä¸ä¹‹å‰çš„ä¸€è‡´ã€‚</p><h4 id="åŒºé—´è®¡æ•°æ–¹æ¡ˆï¼ˆBin-counting-Schemeï¼‰"><a href="#åŒºé—´è®¡æ•°æ–¹æ¡ˆï¼ˆBin-counting-Schemeï¼‰" class="headerlink" title="åŒºé—´è®¡æ•°æ–¹æ¡ˆï¼ˆBin-counting Schemeï¼‰"></a>åŒºé—´è®¡æ•°æ–¹æ¡ˆï¼ˆBin-counting Schemeï¼‰</h4><p>åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æ‰€è®¨è®ºçš„ç¼–ç æ–¹æ¡ˆåœ¨åˆ†ç±»æ•°æ®æ–¹é¢æ•ˆæœè¿˜ä¸é”™ï¼Œä½†æ˜¯å½“ä»»æ„ç‰¹å¾çš„ä¸åŒç±»åˆ«æ•°é‡å˜å¾—å¾ˆå¤§çš„æ—¶å€™ï¼Œé—®é¢˜å¼€å§‹å‡ºç°ã€‚å¯¹äºå…·æœ‰ m ä¸ªä¸åŒæ ‡ç­¾çš„ä»»æ„åˆ†ç±»ç‰¹å¾è¿™ç‚¹éå¸¸é‡è¦ï¼Œä½ å°†å¾—åˆ° m ä¸ªç‹¬ç«‹çš„ç‰¹å¾ã€‚è¿™ä¼šå¾ˆå®¹æ˜“åœ°å¢åŠ ç‰¹å¾é›†çš„å¤§å°ï¼Œä»è€Œå¯¼è‡´åœ¨æ—¶é—´ã€ç©ºé—´å’Œå†…å­˜æ–¹é¢å‡ºç°å­˜å‚¨é—®é¢˜æˆ–è€…æ¨¡å‹è®­ç»ƒé—®é¢˜ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¿…é¡»å¤„ç†â€œ<a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" target="_blank" rel="noopener">ç»´åº¦è¯…å’’</a>â€é—®é¢˜ï¼Œé€šå¸¸æŒ‡çš„æ˜¯æ‹¥æœ‰å¤§é‡çš„ç‰¹å¾ï¼Œå´ç¼ºä¹è¶³å¤Ÿçš„ä»£è¡¨æ€§æ ·æœ¬ï¼Œç„¶åæ¨¡å‹çš„æ€§èƒ½å¼€å§‹å—åˆ°å½±å“å¹¶å¯¼è‡´è¿‡æ‹Ÿåˆã€‚</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afd0459749.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸‹ç¯‡ - ç¦»æ•£æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦é’ˆå¯¹é‚£äº›å¯èƒ½å…·æœ‰éå¸¸å¤šç§ç±»åˆ«çš„ç‰¹å¾ï¼ˆå¦‚ IP åœ°å€ï¼‰ï¼Œç ”ç©¶å…¶å®ƒåˆ†ç±»æ•°æ®ç‰¹å¾å·¥ç¨‹æ–¹æ¡ˆã€‚åŒºé—´è®¡æ•°æ–¹æ¡ˆæ˜¯å¤„ç†å…·æœ‰å¤šä¸ªç±»åˆ«çš„åˆ†ç±»å˜é‡çš„æœ‰æ•ˆæ–¹æ¡ˆã€‚åœ¨è¿™ä¸ªæ–¹æ¡ˆä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨<strong>åŸºäºæ¦‚ç‡çš„ç»Ÿè®¡ä¿¡æ¯å’Œåœ¨å»ºæ¨¡è¿‡ç¨‹ä¸­æ‰€è¦é¢„æµ‹çš„å®é™…ç›®æ ‡æˆ–è€…å“åº”å€¼</strong>ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å®é™…çš„æ ‡ç­¾å€¼è¿›è¡Œç¼–ç ã€‚ä¸€ä¸ªç®€å•çš„ä¾‹å­æ˜¯ï¼ŒåŸºäºè¿‡å»çš„ IP åœ°å€å†å²æ•°æ®å’Œ DDOS æ”»å‡»ä¸­æ‰€ä½¿ç”¨çš„å†å²æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºä»»ä¸€ IP åœ°å€ä¼šè¢« DDOS æ”»å‡»çš„å¯èƒ½æ€§å»ºç«‹æ¦‚ç‡æ¨¡å‹ã€‚ä½¿ç”¨è¿™äº›ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œç¼–ç ï¼Œè¯¥è¾“å…¥ç‰¹å¾æè¿°äº†å¦‚æœå°†æ¥å‡ºç°ç›¸åŒçš„ IP åœ°å€ï¼Œåˆ™å¼•èµ· DDOS æ”»å‡»çš„æ¦‚ç‡å€¼æ˜¯å¤šå°‘ã€‚<strong>è¿™ä¸ªæ–¹æ¡ˆéœ€è¦å†å²æ•°æ®ä½œä¸ºå…ˆå†³æ¡ä»¶ï¼Œå¹¶ä¸”è¦æ±‚æ•°æ®éå¸¸è¯¦å°½ã€‚</strong></p><h4 id="ç‰¹å¾å“ˆå¸Œæ–¹æ¡ˆ"><a href="#ç‰¹å¾å“ˆå¸Œæ–¹æ¡ˆ" class="headerlink" title="ç‰¹å¾å“ˆå¸Œæ–¹æ¡ˆ"></a>ç‰¹å¾å“ˆå¸Œæ–¹æ¡ˆ</h4><p>ç‰¹å¾å“ˆå¸Œæ–¹æ¡ˆï¼ˆFeature Hashing Schemeï¼‰æ˜¯å¤„ç†å¤§è§„æ¨¡åˆ†ç±»ç‰¹å¾çš„å¦ä¸€ä¸ªæœ‰ç”¨çš„ç‰¹å¾å·¥ç¨‹æ–¹æ¡ˆã€‚åœ¨è¯¥æ–¹æ¡ˆä¸­ï¼Œå“ˆå¸Œå‡½æ•°é€šå¸¸ä¸é¢„è®¾çš„ç¼–ç ç‰¹å¾çš„æ•°é‡ï¼ˆä½œä¸ºé¢„å®šä¹‰é•¿åº¦å‘é‡ï¼‰ä¸€èµ·ä½¿ç”¨ï¼Œä½¿å¾—ç‰¹å¾çš„å“ˆå¸Œå€¼è¢«ç”¨ä½œè¿™ä¸ªé¢„å®šä¹‰å‘é‡ä¸­çš„ç´¢å¼•ï¼Œå¹¶ä¸”å€¼ä¹Ÿè¦åšç›¸åº”çš„æ›´æ–°ã€‚ç”±äºå“ˆå¸Œå‡½æ•°å°†å¤§é‡çš„å€¼æ˜ å°„åˆ°ä¸€ä¸ªå°çš„æœ‰é™é›†åˆä¸­ï¼Œå› æ­¤<strong>å¤šä¸ªä¸åŒå€¼å¯èƒ½ä¼šåˆ›å»ºç›¸åŒçš„å“ˆå¸Œ</strong>ï¼Œè¿™ä¸€ç°è±¡ç§°ä¸º<strong>å†²çª</strong>ã€‚å…¸å‹åœ°ï¼Œä½¿ç”¨å¸¦ç¬¦å·çš„å“ˆå¸Œå‡½æ•°ï¼Œä½¿å¾—ä»å“ˆå¸Œè·å¾—çš„å€¼çš„ç¬¦å·è¢«ç”¨ä½œé‚£äº›åœ¨é€‚å½“çš„ç´¢å¼•å¤„å­˜å‚¨åœ¨æœ€ç»ˆç‰¹å¾å‘é‡ä¸­çš„å€¼çš„ç¬¦å·ã€‚è¿™æ ·èƒ½å¤Ÿç¡®ä¿å®ç°è¾ƒå°‘çš„å†²çªå’Œç”±äºå†²çªå¯¼è‡´çš„è¯¯å·®ç´¯ç§¯ã€‚</p><p>å“ˆå¸Œæ–¹æ¡ˆé€‚ç”¨äºå­—ç¬¦ä¸²ã€æ•°å­—å’Œå…¶å®ƒç»“æ„ï¼ˆå¦‚å‘é‡ï¼‰ã€‚ä½ å¯ä»¥å°†å“ˆå¸Œè¾“å‡ºçœ‹ä½œä¸€ä¸ªæœ‰é™çš„ <em>b bins</em> é›†åˆï¼Œä»¥ä¾¿äºå½“å°†å“ˆå¸Œå‡½æ•°åº”ç”¨äºç›¸åŒçš„å€¼\ç±»åˆ«æ—¶ï¼Œå“ˆå¸Œå‡½æ•°èƒ½æ ¹æ®å“ˆå¸Œå€¼å°†å…¶åˆ†é…åˆ° <em>b bins</em> ä¸­çš„åŒä¸€ä¸ª binï¼ˆæˆ–è€… bins çš„å­é›†ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥é¢„å…ˆå®šä¹‰ <em>b</em> çš„å€¼ï¼Œå®ƒæˆä¸ºæˆ‘ä»¬ä½¿ç”¨ç‰¹å¾å“ˆå¸Œæ–¹æ¡ˆç¼–ç çš„æ¯ä¸ªåˆ†ç±»å±æ€§çš„ç¼–ç ç‰¹å¾å‘é‡çš„æœ€ç»ˆå°ºå¯¸ã€‚</p><p>å› æ­¤ï¼Œå³ä½¿æˆ‘ä»¬æœ‰ä¸€ä¸ªç‰¹å¾æ‹¥æœ‰è¶…è¿‡ <strong>1000</strong> ä¸ªä¸åŒçš„ç±»åˆ«ï¼Œæˆ‘ä»¬è®¾ç½® <strong>b = 10</strong> ä½œä¸ºæœ€ç»ˆçš„ç‰¹å¾å‘é‡é•¿åº¦ï¼Œé‚£ä¹ˆæœ€ç»ˆè¾“å‡ºçš„ç‰¹å¾å°†åªæœ‰ 10 ä¸ªç‰¹å¾ã€‚è€Œé‡‡ç”¨ç‹¬çƒ­ç¼–ç æ–¹æ¡ˆåˆ™æœ‰ 1000 ä¸ªäºŒè¿›åˆ¶ç‰¹å¾ã€‚æˆ‘ä»¬æ¥è€ƒè™‘ä¸‹è§†é¢‘æ¸¸æˆæ•°æ®é›†ä¸­çš„ã€Œé£æ ¼ï¼ˆGenreï¼‰ã€å±æ€§ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">unique_genres = np.unique(vg_df[[<span class="string">'Genre'</span>]])</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Total game genres:"</span>, len(unique_genres))</span><br><span class="line"></span><br><span class="line">print(unique_genres)</span><br><span class="line"></span><br><span class="line">Output</span><br><span class="line"></span><br><span class="line">\------</span><br><span class="line"></span><br><span class="line">Total game genres: <span class="number">12</span></span><br><span class="line"></span><br><span class="line">[<span class="string">'Action'</span> <span class="string">'Adventure'</span> <span class="string">'Fighting'</span> <span class="string">'Misc'</span> <span class="string">'Platform'</span> <span class="string">'Puzzle'</span> <span class="string">'Racing'</span> <span class="string">'Role-Playing'</span> <span class="string">'Shooter'</span> <span class="string">'Simulation'</span> <span class="string">'Sports'</span> <span class="string">'Strategy'</span>]</span><br></pre></td></tr></table></figure><p>æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ€»å…±æœ‰ 12 ä¸­é£æ ¼çš„æ¸¸æˆã€‚å¦‚æœæˆ‘ä»¬åœ¨â€œé£æ ¼â€ç‰¹å¾ä¸­é‡‡ç”¨ç‹¬çƒ­ç¼–ç æ–¹æ¡ˆï¼Œåˆ™å°†å¾—åˆ° 12 ä¸ªäºŒè¿›åˆ¶ç‰¹å¾ã€‚è€Œè¿™æ¬¡ï¼Œæˆ‘ä»¬å°†é€šè¿‡ scikit-learn çš„ <em>FeatureHasher</em> ç±»æ¥ä½¿ç”¨ç‰¹å¾å“ˆå¸Œæ–¹æ¡ˆï¼Œè¯¥ç±»ä½¿ç”¨äº†ä¸€ä¸ªæœ‰ç¬¦å·çš„ 32 ä½ç‰ˆæœ¬çš„ <em>Murmurhash3</em> å“ˆå¸Œå‡½æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†é¢„å…ˆå®šä¹‰æœ€ç»ˆçš„ç‰¹å¾å‘é‡å¤§å°ä¸º 6ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> FeatureHasher</span><br><span class="line"></span><br><span class="line">fh = FeatureHasher(n_features=<span class="number">6</span>, input_type=<span class="string">'string'</span>)</span><br><span class="line"></span><br><span class="line">hashed_features = fh.fit_transform(vg_df[<span class="string">'Genre'</span>])</span><br><span class="line"></span><br><span class="line">hashed_features = hashed_features.toarray()pd.concat([vg_df[[<span class="string">'Name'</span>, <span class="string">'Genre'</span>]], pd.DataFrame(hashed_features)], axis=<span class="number">1</span>).iloc[<span class="number">1</span>:<span class="number">7</span>]</span><br></pre></td></tr></table></figure><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afd62f2a51.png?imageMogr2/format/jpg/quality/90" alt="ä¸ä¼šåšç‰¹å¾å·¥ç¨‹çš„ AI ç ”ç©¶å‘˜ä¸æ˜¯å¥½æ•°æ®ç§‘å­¦å®¶ï¼ä¸‹ç¯‡ - ç¦»æ•£æ•°æ®çš„å¤„ç†æ–¹æ³•"></p><p>é£æ ¼å±æ€§çš„ç‰¹å¾å“ˆå¸Œ</p><p>åŸºäºä¸Šè¿°è¾“å‡ºï¼Œã€Œé£æ ¼ï¼ˆGenreï¼‰ã€å±æ€§å·²ç»ä½¿ç”¨å“ˆå¸Œæ–¹æ¡ˆç¼–ç æˆ 6 ä¸ªç‰¹å¾è€Œä¸æ˜¯ 12 ä¸ªã€‚æˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°ï¼Œç¬¬ 1 è¡Œå’Œç¬¬ 6 è¡Œè¡¨ç¤ºç›¸åŒé£æ ¼çš„æ¸¸æˆã€Œå¹³å°ï¼ˆPlatformï¼‰ã€ï¼Œè€Œå®ƒä»¬ä¹Ÿè¢«æ­£ç¡®ç¼–ç æˆäº†ç›¸åŒçš„ç‰¹å¾å‘é‡ã€‚</p><h3 id="æ—¶é—´å‹"><a href="#æ—¶é—´å‹" class="headerlink" title="æ—¶é—´å‹"></a>æ—¶é—´å‹</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBQ8OS.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQrOU.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQcTJ.jpg" alt="avatar"></p><h3 id="æ–‡æœ¬å‹"><a href="#æ–‡æœ¬å‹" class="headerlink" title="æ–‡æœ¬å‹"></a>æ–‡æœ¬å‹</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBQhSx.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQIOO.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQzX8.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBlC7Q.jpg" alt="avatar"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹&quot;&gt;&lt;a href=&quot;#æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹&quot; class=&quot;headerlink&quot; title=&quot;æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹&quot;&gt;&lt;/a&gt;æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹&lt;/h2&gt;&lt;h3 id=&quot;æœºå™¨å­¦ä¹ æµç¨‹ä¸æ¦‚å¿µ&quot;&gt;&lt;a href=&quot;#æœºå™¨å­¦ä¹ æµç¨‹ä¸æ¦‚å¿µ&quot; class=&quot;he
      
    
    </summary>
    
      <category term="ç‰¹å¾å·¥ç¨‹" scheme="http://mmyblog.cn/categories/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="æ¨¡å‹è°ƒä¼˜" scheme="http://mmyblog.cn/categories/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98/"/>
    
    
      <category term="æ¨¡å‹è°ƒä¼˜" scheme="http://mmyblog.cn/tags/%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98/"/>
    
      <category term="python" scheme="http://mmyblog.cn/tags/python/"/>
    
      <category term="ç‰¹å¾å·¥ç¨‹" scheme="http://mmyblog.cn/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>è¯­è¨€æ¨¡å‹</title>
    <link href="http://mmyblog.cn/2020/04/18/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    <id>http://mmyblog.cn/2020/04/18/è¯­è¨€æ¨¡å‹/</id>
    <published>2020-04-18T11:00:36.000Z</published>
    <updated>2020-06-09T01:24:52.278Z</updated>
    
    <content type="html"><![CDATA[<h1 id="è¯­è¨€æ¨¡å‹"><a href="#è¯­è¨€æ¨¡å‹" class="headerlink" title="è¯­è¨€æ¨¡å‹"></a>è¯­è¨€æ¨¡å‹</h1><p>å­¦ä¹ ç›®æ ‡</p><ul><li>å­¦ä¹ è¯­è¨€æ¨¡å‹ï¼Œä»¥åŠå¦‚ä½•è®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡å‹</li><li>å­¦ä¹ torchtextçš„åŸºæœ¬ä½¿ç”¨æ–¹æ³•<ul><li>æ„å»º vocabulary</li><li>word to inde å’Œ index to word</li></ul></li><li>å­¦ä¹ torch.nnçš„ä¸€äº›åŸºæœ¬æ¨¡å‹<ul><li>Linear</li><li>RNN</li><li>LSTM</li><li>GRU</li></ul></li><li>RNNçš„è®­ç»ƒæŠ€å·§<ul><li>Gradient Clipping</li></ul></li><li>å¦‚ä½•ä¿å­˜å’Œè¯»å–æ¨¡å‹</li></ul><p>æˆ‘ä»¬ä¼šä½¿ç”¨ <a href="https://github.com/pytorch/text" target="_blank" rel="noopener">torchtext</a> æ¥åˆ›å»ºvocabulary, ç„¶åæŠŠæ•°æ®è¯»æˆbatchçš„æ ¼å¼ã€‚è¯·å¤§å®¶è‡ªè¡Œé˜…è¯»READMEæ¥å­¦ä¹ torchtextã€‚</p><p><strong>å…ˆäº†è§£ä¸‹torchtextåº“ï¼š<a href="https://blog.csdn.net/u012436149/article/details/79310176" target="_blank" rel="noopener">torchtextä»‹ç»å’Œä½¿ç”¨æ•™ç¨‹</a></strong></p><p>In [1]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">USE_CUDA = torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line"><span class="comment"># ä¸ºäº†ä¿è¯å®éªŒç»“æœå¯ä»¥å¤ç°ï¼Œæˆ‘ä»¬ç»å¸¸ä¼šæŠŠå„ç§random seedå›ºå®šåœ¨æŸä¸€ä¸ªå€¼</span></span><br><span class="line">random.seed(<span class="number">53113</span>)</span><br><span class="line">np.random.seed(<span class="number">53113</span>)</span><br><span class="line">torch.manual_seed(<span class="number">53113</span>)</span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    torch.cuda.manual_seed(<span class="number">53113</span>)</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">32</span> <span class="comment">#ä¸€ä¸ªbatchå¤šå°‘ä¸ªå¥å­</span></span><br><span class="line">EMBEDDING_SIZE = <span class="number">650</span>  <span class="comment">#æ¯ä¸ªå•è¯å¤šå°‘ç»´</span></span><br><span class="line">MAX_VOCAB_SIZE = <span class="number">50000</span>  <span class="comment">#å•è¯æ€»æ•°</span></span><br></pre></td></tr></table></figure><ul><li>æˆ‘ä»¬ä¼šç»§ç»­ä½¿ç”¨ä¸Šæ¬¡çš„text8ä½œä¸ºæˆ‘ä»¬çš„è®­ç»ƒï¼ŒéªŒè¯å’Œæµ‹è¯•æ•°æ®</li><li>torchtextæä¾›äº†LanguageModelingDatasetè¿™ä¸ªclassæ¥å¸®åŠ©æˆ‘ä»¬å¤„ç†è¯­è¨€æ¨¡å‹æ•°æ®é›†</li><li>BPTTIteratorå¯ä»¥è¿ç»­åœ°å¾—åˆ°è¿è´¯çš„å¥å­</li></ul><p>In [2]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">TEXT = torchtext.data.Field(lower=<span class="literal">True</span>) </span><br><span class="line"><span class="comment"># .Fieldè¿™ä¸ªå¯¹è±¡åŒ…å«äº†æˆ‘ä»¬æ‰“ç®—å¦‚ä½•é¢„å¤„ç†æ–‡æœ¬æ•°æ®çš„ä¿¡æ¯ï¼Œè¿™é‡Œå®šä¹‰å•è¯å…¨éƒ¨å°å†™</span></span><br><span class="line"></span><br><span class="line">train, val, test = \</span><br><span class="line">torchtext.datasets.LanguageModelingDataset.splits(</span><br><span class="line">    path=<span class="string">"."</span>, </span><br><span class="line">    train=<span class="string">"text8.train.txt"</span>, </span><br><span class="line">    validation=<span class="string">"text8.dev.txt"</span>, </span><br><span class="line">    test=<span class="string">"text8.test.txt"</span>, </span><br><span class="line">    text_field=TEXT)</span><br><span class="line"><span class="comment"># torchtextæä¾›äº†LanguageModelingDatasetè¿™ä¸ªclassæ¥å¸®åŠ©æˆ‘ä»¬å¤„ç†è¯­è¨€æ¨¡å‹æ•°æ®é›†</span></span><br><span class="line"></span><br><span class="line">TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)</span><br><span class="line"><span class="comment"># build_vocabå¯ä»¥æ ¹æ®æˆ‘ä»¬æä¾›çš„è®­ç»ƒæ•°æ®é›†æ¥åˆ›å»ºæœ€é«˜é¢‘å•è¯çš„å•è¯è¡¨ï¼Œmax_sizeå¸®åŠ©æˆ‘ä»¬é™å®šå•è¯æ€»é‡ã€‚</span></span><br><span class="line">print(<span class="string">"vocabulary size: &#123;&#125;"</span>.format(len(TEXT.vocab)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vocabulary size: 50002</span><br></pre></td></tr></table></figure><p>In [4]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test</span><br></pre></td></tr></table></figure><p>Out[4]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;torchtext.data.example.Example at 0x121738b00&gt;</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In [9]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(TEXT.vocab.itos[<span class="number">0</span>:<span class="number">50</span>]) </span><br><span class="line"><span class="comment"># è¿™é‡Œè¶Šé å‰è¶Šå¸¸è§ï¼Œå¢åŠ äº†ä¸¤ä¸ªç‰¹æ®Šçš„tokenï¼Œ&lt;unk&gt;è¡¨ç¤ºæœªçŸ¥çš„å•è¯ï¼Œ&lt;pad&gt;è¡¨ç¤ºpaddingã€‚</span></span><br><span class="line">print(<span class="string">"------"</span>*<span class="number">10</span>)</span><br><span class="line">print(list(TEXT.vocab.stoi.items())[<span class="number">0</span>:<span class="number">50</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&apos;&lt;unk&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;the&apos;, &apos;of&apos;, &apos;and&apos;, &apos;one&apos;, &apos;in&apos;, &apos;a&apos;, &apos;to&apos;, &apos;zero&apos;, &apos;nine&apos;, &apos;two&apos;, &apos;is&apos;, &apos;as&apos;, &apos;eight&apos;, &apos;for&apos;, &apos;s&apos;, &apos;five&apos;, &apos;three&apos;, &apos;was&apos;, &apos;by&apos;, &apos;that&apos;, &apos;four&apos;, &apos;six&apos;, &apos;seven&apos;, &apos;with&apos;, &apos;on&apos;, &apos;are&apos;, &apos;it&apos;, &apos;from&apos;, &apos;or&apos;, &apos;his&apos;, &apos;an&apos;, &apos;be&apos;, &apos;this&apos;, &apos;he&apos;, &apos;at&apos;, &apos;which&apos;, &apos;not&apos;, &apos;also&apos;, &apos;have&apos;, &apos;were&apos;, &apos;has&apos;, &apos;but&apos;, &apos;other&apos;, &apos;their&apos;, &apos;its&apos;, &apos;first&apos;, &apos;they&apos;, &apos;had&apos;]</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">[(&apos;&lt;unk&gt;&apos;, 0), (&apos;&lt;pad&gt;&apos;, 1), (&apos;the&apos;, 2), (&apos;of&apos;, 3), (&apos;and&apos;, 4), (&apos;one&apos;, 5), (&apos;in&apos;, 6), (&apos;a&apos;, 7), (&apos;to&apos;, 8), (&apos;zero&apos;, 9), (&apos;nine&apos;, 10), (&apos;two&apos;, 11), (&apos;is&apos;, 12), (&apos;as&apos;, 13), (&apos;eight&apos;, 14), (&apos;for&apos;, 15), (&apos;s&apos;, 16), (&apos;five&apos;, 17), (&apos;three&apos;, 18), (&apos;was&apos;, 19), (&apos;by&apos;, 20), (&apos;that&apos;, 21), (&apos;four&apos;, 22), (&apos;six&apos;, 23), (&apos;seven&apos;, 24), (&apos;with&apos;, 25), (&apos;on&apos;, 26), (&apos;are&apos;, 27), (&apos;it&apos;, 28), (&apos;from&apos;, 29), (&apos;or&apos;, 30), (&apos;his&apos;, 31), (&apos;an&apos;, 32), (&apos;be&apos;, 33), (&apos;this&apos;, 34), (&apos;he&apos;, 35), (&apos;at&apos;, 36), (&apos;which&apos;, 37), (&apos;not&apos;, 38), (&apos;also&apos;, 39), (&apos;have&apos;, 40), (&apos;were&apos;, 41), (&apos;has&apos;, 42), (&apos;but&apos;, 43), (&apos;other&apos;, 44), (&apos;their&apos;, 45), (&apos;its&apos;, 46), (&apos;first&apos;, 47), (&apos;they&apos;, 48), (&apos;had&apos;, 49)]</span><br></pre></td></tr></table></figure><p>In [10]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">VOCAB_SIZE = len(TEXT.vocab) <span class="comment"># 50002</span></span><br><span class="line">train_iter, val_iter, test_iter = \</span><br><span class="line">torchtext.data.BPTTIterator.splits(</span><br><span class="line">    (train, val, test), </span><br><span class="line">    batch_size=BATCH_SIZE, </span><br><span class="line">    device=<span class="number">-1</span>, </span><br><span class="line">    bptt_len=<span class="number">50</span>, <span class="comment"># åå‘ä¼ æ’­å¾€å›ä¼ çš„é•¿åº¦ï¼Œè¿™é‡Œæˆ‘æš‚æ—¶ç†è§£ä¸ºä¸€ä¸ªæ ·æœ¬æœ‰å¤šå°‘ä¸ªå•è¯ä¼ å…¥æ¨¡å‹</span></span><br><span class="line">    repeat=<span class="literal">False</span>, </span><br><span class="line">    shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># BPTTIteratorå¯ä»¥è¿ç»­åœ°å¾—åˆ°è¿è´¯çš„å¥å­ï¼ŒBPTTçš„å…¨ç§°æ˜¯back propagation through timeã€‚</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Iteratorï¼šæ ‡å‡†è¿­ä»£å™¨</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">BucketIeratorï¼šç›¸æ¯”äºæ ‡å‡†è¿­ä»£å™¨ï¼Œä¼šå°†ç±»ä¼¼é•¿åº¦çš„æ ·æœ¬å½“åšä¸€æ‰¹æ¥å¤„ç†ï¼Œ</span></span><br><span class="line"><span class="string">å› ä¸ºåœ¨æ–‡æœ¬å¤„ç†ä¸­ç»å¸¸ä¼šéœ€è¦å°†æ¯ä¸€æ‰¹æ ·æœ¬é•¿åº¦è¡¥é½ä¸ºå½“å‰æ‰¹ä¸­æœ€é•¿åºåˆ—çš„é•¿åº¦ï¼Œ</span></span><br><span class="line"><span class="string">å› æ­¤å½“æ ·æœ¬é•¿åº¦å·®åˆ«è¾ƒå¤§æ—¶ï¼Œä½¿ç”¨BucketIeratorå¯ä»¥å¸¦æ¥å¡«å……æ•ˆç‡çš„æé«˜ã€‚</span></span><br><span class="line"><span class="string">é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥åœ¨Fieldä¸­é€šè¿‡fix_lengthå‚æ•°æ¥å¯¹æ ·æœ¬è¿›è¡Œæˆªæ–­è¡¥é½æ“ä½œã€‚</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">BPTTIterator: åŸºäºBPTT(åŸºäºæ—¶é—´çš„åå‘ä¼ æ’­ç®—æ³•)çš„è¿­ä»£å™¨ï¼Œä¸€èˆ¬ç”¨äºè¯­è¨€æ¨¡å‹ä¸­ã€‚</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.</span><br><span class="line">The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.</span><br><span class="line">The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.</span><br></pre></td></tr></table></figure><p>Out[10]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;\nIteratorï¼šæ ‡å‡†è¿­ä»£å™¨\n\nBucketIeratorï¼šç›¸æ¯”äºæ ‡å‡†è¿­ä»£å™¨ï¼Œä¼šå°†ç±»ä¼¼é•¿åº¦çš„æ ·æœ¬å½“åšä¸€æ‰¹æ¥å¤„ç†ï¼Œ\nå› ä¸ºåœ¨æ–‡æœ¬å¤„ç†ä¸­ç»å¸¸ä¼šéœ€è¦å°†æ¯ä¸€æ‰¹æ ·æœ¬é•¿åº¦è¡¥é½ä¸ºå½“å‰æ‰¹ä¸­æœ€é•¿åºåˆ—çš„é•¿åº¦ï¼Œ\nå› æ­¤å½“æ ·æœ¬é•¿åº¦å·®åˆ«è¾ƒå¤§æ—¶ï¼Œä½¿ç”¨BucketIeratorå¯ä»¥å¸¦æ¥å¡«å……æ•ˆç‡çš„æé«˜ã€‚\né™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥åœ¨Fieldä¸­é€šè¿‡fix_lengthå‚æ•°æ¥å¯¹æ ·æœ¬è¿›è¡Œæˆªæ–­è¡¥é½æ“ä½œã€‚\n\nBPTTIterator: åŸºäºBPTT(åŸºäºæ—¶é—´çš„åå‘ä¼ æ’­ç®—æ³•)çš„è¿­ä»£å™¨ï¼Œä¸€èˆ¬ç”¨äºè¯­è¨€æ¨¡å‹ä¸­ã€‚\n&apos;</span><br></pre></td></tr></table></figure><p>In [11]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(next(iter(train_iter))) <span class="comment"># ä¸€ä¸ªbatchè®­ç»ƒé›†ç»´åº¦</span></span><br><span class="line">print(next(iter(val_iter))) <span class="comment"># ä¸€ä¸ªbatchéªŒè¯é›†ç»´åº¦</span></span><br><span class="line">print(next(iter(test_iter))) <span class="comment"># ä¸€ä¸ªbatchæµ‹è¯•é›†ç»´åº¦</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[torchtext.data.batch.Batch of size <span class="number">32</span>]</span><br><span class="line">[.text]:[torch.LongTensor of size <span class="number">50</span>x32]</span><br><span class="line">[.target]:[torch.LongTensor of size <span class="number">50</span>x32]</span><br><span class="line"></span><br><span class="line">[torchtext.data.batch.Batch of size <span class="number">32</span>]</span><br><span class="line">[.text]:[torch.LongTensor of size <span class="number">50</span>x32]</span><br><span class="line">[.target]:[torch.LongTensor of size <span class="number">50</span>x32]</span><br><span class="line"></span><br><span class="line">[torchtext.data.batch.Batch of size <span class="number">32</span>]</span><br><span class="line">[.text]:[torch.LongTensor of size <span class="number">50</span>x32]</span><br><span class="line">[.target]:[torch.LongTensor of size <span class="number">50</span>x32]</span><br></pre></td></tr></table></figure><p>æ¨¡å‹çš„è¾“å…¥æ˜¯ä¸€ä¸²æ–‡å­—ï¼Œæ¨¡å‹çš„è¾“å‡ºä¹Ÿæ˜¯ä¸€ä¸²æ–‡å­—ï¼Œä»–ä»¬ä¹‹é—´ç›¸å·®ä¸€ä¸ªä½ç½®ï¼Œå› ä¸ºè¯­è¨€æ¨¡å‹çš„ç›®æ ‡æ˜¯æ ¹æ®ä¹‹å‰çš„å•è¯é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚</p><p>In [12]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">it = iter(train_iter)</span><br><span class="line">batch = next(it)</span><br><span class="line">print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.text[:,<span class="number">1</span>].data])) <span class="comment"># æ‰“å°ä¸€ä¸ªè¾“å…¥çš„å¥å­</span></span><br><span class="line">print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.target[:,<span class="number">1</span>].data])) <span class="comment"># æ‰“å°ä¸€ä¸ªè¾“å‡ºçš„å¥å­</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">combine in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most typical forms of matter they had also suggested the possibility</span><br><span class="line">in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most typical forms of matter they had also suggested the possibility of</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In [13]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">5</span>): <span class="comment"># è¿™ç§å–æ³•æ˜¯åœ¨ä¸€ä¸ªå›ºå®šçš„batché‡Œå–æ•°æ®ï¼Œå‘ç°ä¸€ä¸ªbatché‡Œçš„æ•°æ®æ˜¯è¿ä¸èµ·æ¥çš„ã€‚</span></span><br><span class="line">    print(j)</span><br><span class="line">    print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.text[:,j].data]))</span><br><span class="line">    print(j)</span><br><span class="line">    print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.target[:,j].data]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">0</span><br><span class="line">anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans &lt;unk&gt; of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the</span><br><span class="line">0</span><br><span class="line">originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans &lt;unk&gt; of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization</span><br><span class="line">1</span><br><span class="line">combine in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most typical forms of matter they had also suggested the possibility</span><br><span class="line">1</span><br><span class="line">in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most typical forms of matter they had also suggested the possibility of</span><br><span class="line">2</span><br><span class="line">culture few living ainu settlements exist many authentic ainu villages advertised in hokkaido are simply tourist attractions language the ainu language is significantly different from japanese in its syntax phonology morphology and vocabulary although there have been attempts to show that they are related the vast majority of modern scholars</span><br><span class="line">2</span><br><span class="line">few living ainu settlements exist many authentic ainu villages advertised in hokkaido are simply tourist attractions language the ainu language is significantly different from japanese in its syntax phonology morphology and vocabulary although there have been attempts to show that they are related the vast majority of modern scholars reject</span><br><span class="line">3</span><br><span class="line">zero the apple iie card an expansion card for the lc line of macintosh computers was released essentially a miniaturized apple iie computer on a card utilizing the mega ii chip from the apple iigs it allowed the macintosh to run eight bit apple iie software through hardware emulation although</span><br><span class="line">3</span><br><span class="line">the apple iie card an expansion card for the lc line of macintosh computers was released essentially a miniaturized apple iie computer on a card utilizing the mega ii chip from the apple iigs it allowed the macintosh to run eight bit apple iie software through hardware emulation although video</span><br><span class="line">4</span><br><span class="line">in papers have been written arguing that the anthropic principle would explain the physical constants such as the fine structure constant the number of dimensions in the universe and the cosmological constant the three primary versions of the principle as stated by john d barrow and frank j &lt;unk&gt; one</span><br><span class="line">4</span><br><span class="line">papers have been written arguing that the anthropic principle would explain the physical constants such as the fine structure constant the number of dimensions in the universe and the cosmological constant the three primary versions of the principle as stated by john d barrow and frank j &lt;unk&gt; one nine</span><br></pre></td></tr></table></figure><p>In [14]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>): <span class="comment"># è¿™ç§å–æ³•æ˜¯åœ¨æ¯ä¸ªbatché‡Œå–æŸä¸€ä¸ªç›¸åŒä½ç½®æ•°æ®ï¼Œå‘ç°ä¸åŒbatché—´ç›¸åŒä½ç½®çš„æ•°æ®æ˜¯å¯ä»¥è¿èµ·æ¥çš„ã€‚è¿™é‡Œæœ‰ç‚¹å°ç–‘é—®ã€‚</span></span><br><span class="line">    batch = next(it)</span><br><span class="line">    print(i)</span><br><span class="line">    print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.text[:,<span class="number">2</span>].data]))</span><br><span class="line">    print(i)</span><br><span class="line">    print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.target[:,<span class="number">2</span>].data]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">0</span><br><span class="line">reject that the relationship goes beyond contact i e mutual borrowing of words between japanese and ainu in fact no attempt to show a relationship with ainu to any other language has gained wide acceptance and ainu is currently considered to be a language isolate culture traditional ainu culture is</span><br><span class="line">0</span><br><span class="line">that the relationship goes beyond contact i e mutual borrowing of words between japanese and ainu in fact no attempt to show a relationship with ainu to any other language has gained wide acceptance and ainu is currently considered to be a language isolate culture traditional ainu culture is quite</span><br><span class="line">1</span><br><span class="line">quite different from japanese culture never shaving after a certain age the men had full beards and &lt;unk&gt; men and women alike cut their hair level with the shoulders at the sides of the head but trimmed it &lt;unk&gt; behind the women tattooed their mouths arms &lt;unk&gt; and sometimes their</span><br><span class="line">1</span><br><span class="line">different from japanese culture never shaving after a certain age the men had full beards and &lt;unk&gt; men and women alike cut their hair level with the shoulders at the sides of the head but trimmed it &lt;unk&gt; behind the women tattooed their mouths arms &lt;unk&gt; and sometimes their &lt;unk&gt;</span><br><span class="line">2</span><br><span class="line">&lt;unk&gt; starting at the onset of puberty the soot deposited on a pot hung over a fire of birch bark was used for colour their traditional dress is a robe spun from the bark of the elm tree it has long sleeves reaches nearly to the feet is folded round</span><br><span class="line">2</span><br><span class="line">starting at the onset of puberty the soot deposited on a pot hung over a fire of birch bark was used for colour their traditional dress is a robe spun from the bark of the elm tree it has long sleeves reaches nearly to the feet is folded round the</span><br><span class="line">3</span><br><span class="line">the body and is tied with a girdle of the same material women also wear an &lt;unk&gt; of japanese cloth in winter the skins of animals were worn with &lt;unk&gt; of &lt;unk&gt; and boots made from the skin of dogs or salmon both sexes are fond of earrings which are</span><br><span class="line">3</span><br><span class="line">body and is tied with a girdle of the same material women also wear an &lt;unk&gt; of japanese cloth in winter the skins of animals were worn with &lt;unk&gt; of &lt;unk&gt; and boots made from the skin of dogs or salmon both sexes are fond of earrings which are said</span><br><span class="line">4</span><br><span class="line">said to have been made of grapevine in former times as also are bead necklaces called &lt;unk&gt; which the women prized highly their traditional cuisine consists of the flesh of bear fox wolf badger ox or horse as well as fish fowl millet vegetables herbs and roots they never ate</span><br><span class="line">4</span><br><span class="line">to have been made of grapevine in former times as also are bead necklaces called &lt;unk&gt; which the women prized highly their traditional cuisine consists of the flesh of bear fox wolf badger ox or horse as well as fish fowl millet vegetables herbs and roots they never ate raw</span><br></pre></td></tr></table></figure><h3 id="å®šä¹‰æ¨¡å‹"><a href="#å®šä¹‰æ¨¡å‹" class="headerlink" title="å®šä¹‰æ¨¡å‹"></a>å®šä¹‰æ¨¡å‹</h3><ul><li>ç»§æ‰¿nn.Module</li><li>åˆå§‹åŒ–å‡½æ•°</li><li>forwardå‡½æ•°</li><li>å…¶ä½™å¯ä»¥æ ¹æ®æ¨¡å‹éœ€è¦å®šä¹‰ç›¸å…³çš„å‡½æ•°</li></ul><p>In [15]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" ä¸€ä¸ªç®€å•çš„å¾ªç¯ç¥ç»ç½‘ç»œ"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        <span class="comment"># rnn_typeï¼›æœ‰ä¸¤ä¸ªå±‚ä¾›é€‰æ‹©'LSTM', 'GRU'</span></span><br><span class="line">        <span class="comment"># ntokenï¼šVOCAB_SIZE=50002</span></span><br><span class="line">        <span class="comment"># ninpï¼šEMBEDDING_SIZE = 650ï¼Œè¾“å…¥å±‚ç»´åº¦</span></span><br><span class="line">        <span class="comment"># nhidï¼šEMBEDDING_SIZE = 1000ï¼Œéšè—å±‚ç»´åº¦ï¼Œè¿™é‡Œæ˜¯æˆ‘è‡ªå·±è®¾ç½®çš„ï¼Œç”¨äºåŒºåˆ†ninpå±‚ã€‚</span></span><br><span class="line">        <span class="comment"># nlayersï¼šçºµå‘æœ‰å¤šå°‘å±‚ç¥ç»ç½‘ç»œ</span></span><br><span class="line"></span><br><span class="line">        <span class="string">''' è¯¥æ¨¡å‹åŒ…å«ä»¥ä¸‹å‡ å±‚:</span></span><br><span class="line"><span class="string">            - è¯åµŒå…¥å±‚</span></span><br><span class="line"><span class="string">            - ä¸€ä¸ªå¾ªç¯ç¥ç»ç½‘ç»œå±‚(RNN, LSTM, GRU)</span></span><br><span class="line"><span class="string">            - ä¸€ä¸ªçº¿æ€§å±‚ï¼Œä»hidden stateåˆ°è¾“å‡ºå•è¯è¡¨</span></span><br><span class="line"><span class="string">            - ä¸€ä¸ªdropoutå±‚ï¼Œç”¨æ¥åšregularization</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super(RNNModel, self).__init__()</span><br><span class="line">        self.drop = nn.Dropout(dropout)</span><br><span class="line">        self.encoder = nn.Embedding(ntoken, ninp)</span><br><span class="line">        <span class="comment"># å®šä¹‰è¾“å…¥çš„Embeddingå±‚ï¼Œç”¨æ¥æŠŠæ¯ä¸ªå•è¯è½¬åŒ–ä¸ºè¯å‘é‡</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> rnn_type <span class="keyword">in</span> [<span class="string">'LSTM'</span>, <span class="string">'GRU'</span>]: <span class="comment"># ä¸‹é¢ä»£ç ä»¥LSTMä¸¾ä¾‹</span></span><br><span class="line">            </span><br><span class="line">            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)</span><br><span class="line">            <span class="comment"># getattr(nn, rnn_type) ç›¸å½“äº nn.rnn_type</span></span><br><span class="line">            <span class="comment"># nlayersä»£è¡¨çºµå‘æœ‰å¤šå°‘å±‚ã€‚è¿˜æœ‰ä¸ªå‚æ•°æ˜¯bidirectional: æ˜¯å¦æ˜¯åŒå‘LSTMï¼Œé»˜è®¤false</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                nonlinearity = &#123;<span class="string">'RNN_TANH'</span>: <span class="string">'tanh'</span>, <span class="string">'RNN_RELU'</span>: <span class="string">'relu'</span>&#125;[rnn_type]</span><br><span class="line">            <span class="keyword">except</span> KeyError:</span><br><span class="line">                <span class="keyword">raise</span> ValueError( <span class="string">"""An invalid option for `--model` was supplied,</span></span><br><span class="line"><span class="string">                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']"""</span>)</span><br><span class="line">            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)</span><br><span class="line">        self.decoder = nn.Linear(nhid, ntoken)</span><br><span class="line">        <span class="comment"># æœ€åçº¿æ€§å…¨è¿æ¥éšè—å±‚çš„ç»´åº¦(1000,50002)</span></span><br><span class="line">      </span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">        self.rnn_type = rnn_type</span><br><span class="line">        self.nhid = nhid</span><br><span class="line">        self.nlayers = nlayers</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        initrange = <span class="number">0.1</span></span><br><span class="line">        self.encoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        self.decoder.bias.data.zero_()</span><br><span class="line">        self.decoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span> </span><br><span class="line">        </span><br><span class="line">        <span class="string">''' Forward pass:</span></span><br><span class="line"><span class="string">            - word embedding</span></span><br><span class="line"><span class="string">            - è¾“å…¥å¾ªç¯ç¥ç»ç½‘ç»œ</span></span><br><span class="line"><span class="string">            - ä¸€ä¸ªçº¿æ€§å±‚ä»hidden stateè½¬åŒ–ä¸ºè¾“å‡ºå•è¯è¡¨</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># input.shape = seg_length * batch = torch.Size([50, 32])</span></span><br><span class="line">        <span class="comment"># å¦‚æœè§‰å¾—æƒ³å˜æˆ32*50æ ¼å¼ï¼Œå¯ä»¥åœ¨LSTMé‡Œå®šä¹‰batch_first = True</span></span><br><span class="line">        <span class="comment"># hidden = (nlayers * 32 * hidden_size, nlayers * 32 * hidden_size)</span></span><br><span class="line">        <span class="comment"># hiddenæ˜¯ä¸ªå…ƒç»„ï¼Œè¾“å…¥æœ‰ä¸¤ä¸ªå‚æ•°ï¼Œä¸€ä¸ªæ˜¯åˆšå¼€å§‹çš„éšè—å±‚hçš„ç»´åº¦ï¼Œä¸€ä¸ªæ˜¯åˆšå¼€å§‹çš„ç”¨äºè®°å¿†çš„cçš„ç»´åº¦ï¼Œ</span></span><br><span class="line">        <span class="comment"># è¿™ä¸¤ä¸ªå±‚çš„ç»´åº¦ä¸€æ ·ï¼Œå¹¶ä¸”éœ€è¦å…ˆåˆå§‹åŒ–ï¼Œhidden_sizeçš„ç»´åº¦å’Œä¸Šé¢nhidçš„ç»´åº¦ä¸€æ · =1000ï¼Œæˆ‘ç†è§£è¿™ä¸¤ä¸ªæ˜¯åŒä¸€ä¸ªä¸œè¥¿ã€‚</span></span><br><span class="line">        emb = self.drop(self.encoder(input)) <span class="comment"># </span></span><br><span class="line">        <span class="comment"># emb.shape=torch.Size([50, 32, 650]) # è¾“å…¥æ•°æ®çš„ç»´åº¦</span></span><br><span class="line">        <span class="comment"># è¿™é‡Œè¿›è¡Œäº†è¿ç®—ï¼ˆ50ï¼Œ50002ï¼Œ650ï¼‰*(50, 32ï¼Œ50002)</span></span><br><span class="line">        output, hidden = self.rnn(emb, hidden)</span><br><span class="line">        <span class="comment"># output.shape = 50 * 32 * hidden_size # æœ€ç»ˆè¾“å‡ºæ•°æ®çš„ç»´åº¦ï¼Œ</span></span><br><span class="line">        <span class="comment"># hiddenæ˜¯ä¸ªå…ƒç»„ï¼Œè¾“å‡ºæœ‰ä¸¤ä¸ªå‚æ•°ï¼Œä¸€ä¸ªæ˜¯æœ€åçš„éšè—å±‚hçš„ç»´åº¦ï¼Œä¸€ä¸ªæ˜¯æœ€åçš„ç”¨äºè®°å¿†çš„cçš„ç»´åº¦ï¼Œè¿™ä¸¤ä¸ªå±‚ç»´åº¦ç›¸åŒ </span></span><br><span class="line">        <span class="comment"># hidden = (hå±‚ç»´åº¦ï¼šnlayers * 32 * hidden_size, cå±‚ç»´åº¦ï¼šnlayers * 32 * hidden_size)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        output = self.drop(output)</span><br><span class="line">        decoded = self.decoder(output.view(output.size(<span class="number">0</span>)*output.size(<span class="number">1</span>), output.size(<span class="number">2</span>)))</span><br><span class="line">        <span class="comment"># outputæœ€åçš„è¾“å‡ºå±‚ä¸€å®šè¦æ˜¯äºŒç»´çš„ï¼Œåªæ˜¯ä¸ºäº†èƒ½è¿›è¡Œå…¨è¿æ¥å±‚çš„è¿ç®—ï¼Œæ‰€ä»¥æŠŠå‰ä¸¤ä¸ªç»´åº¦æ‹¼åˆ°ä¸€èµ·ï¼Œï¼ˆ50*32,hidden_size)</span></span><br><span class="line">        <span class="comment"># decoded.shape=ï¼ˆ50*32,hidden_size)*(hidden_size,50002)=torch.Size([1600, 50002])</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> decoded.view(output.size(<span class="number">0</span>), output.size(<span class="number">1</span>), decoded.size(<span class="number">1</span>)), hidden</span><br><span class="line">               <span class="comment"># æˆ‘ä»¬è¦çŸ¥é“æ¯ä¸€ä¸ªä½ç½®é¢„æµ‹çš„æ˜¯å“ªä¸ªå•è¯ï¼Œæ‰€ä»¥æœ€ç»ˆè¾“å‡ºè¦æ¢å¤ç»´åº¦ = (50,32,50002)</span></span><br><span class="line">               <span class="comment"># hidden = (hå±‚ç»´åº¦ï¼š2 * 32 * 1000, cå±‚ç»´åº¦ï¼š2 * 32 * 1000)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, bsz, requires_grad=True)</span>:</span></span><br><span class="line">        <span class="comment"># è¿™æ­¥æˆ‘ä»¬åˆå§‹åŒ–ä¸‹éšè—å±‚å‚æ•°</span></span><br><span class="line">        weight = next(self.parameters())</span><br><span class="line">        <span class="comment"># weight = torch.Size([50002, 650])æ˜¯æ‰€æœ‰å‚æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°</span></span><br><span class="line">        <span class="comment"># æ‰€æœ‰å‚æ•°self.parameters()ï¼Œæ˜¯ä¸ªç”Ÿæˆå™¨ï¼ŒLSTMæ‰€æœ‰å‚æ•°ç»´åº¦ç§ç±»å¦‚ä¸‹ï¼š</span></span><br><span class="line">        <span class="comment"># print(list(iter(self.parameters())))</span></span><br><span class="line">        <span class="comment"># torch.Size([50002, 650])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000, 650])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000, 1000])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000]) # åç½®é¡¹</span></span><br><span class="line">        <span class="comment"># torch.Size([4000])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000, 1000])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000, 1000])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000])</span></span><br><span class="line">        <span class="comment"># torch.Size([50002, 1000])</span></span><br><span class="line">        <span class="comment"># torch.Size([50002])</span></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type == <span class="string">'LSTM'</span>:</span><br><span class="line">            <span class="keyword">return</span> (weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad),</span><br><span class="line">                    weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad))</span><br><span class="line">                   <span class="comment"># return = (2 * 32 * 1000, 2 * 32 * 1000)</span></span><br><span class="line">                   <span class="comment"># è¿™é‡Œä¸æ˜ç™½ä¸ºä»€ä¹ˆéœ€è¦weight.new_zerosï¼Œæˆ‘ä¼°è®¡æ˜¯æƒ³æ•´ä¸ªè®¡ç®—å›¾èƒ½é“¾æ¥èµ·æ¥</span></span><br><span class="line">                   <span class="comment"># è¿™é‡Œç‰¹åˆ«æ³¨æ„hiddençš„è¾“å…¥ä¸æ˜¯modelçš„å‚æ•°ï¼Œä¸å‚ä¸æ›´æ–°ï¼Œå°±è·Ÿè¾“å…¥æ•°æ®xä¸€æ ·</span></span><br><span class="line">                   </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad)</span><br><span class="line">            <span class="comment"># GRUç¥ç»ç½‘ç»œæŠŠhå±‚å’Œcå±‚åˆå¹¶äº†ï¼Œæ‰€ä»¥è¿™é‡Œåªæœ‰ä¸€å±‚ã€‚</span></span><br></pre></td></tr></table></figure><p>åˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹</p><p>In [16]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nhid = <span class="number">1000</span> <span class="comment"># æˆ‘è‡ªå·±è®¾ç½®çš„ç»´åº¦ï¼Œç”¨äºåŒºåˆ†embeding_size=650</span></span><br><span class="line">model = RNNModel(<span class="string">"LSTM"</span>, VOCAB_SIZE, EMBEDDING_SIZE, nhid, <span class="number">2</span>, dropout=<span class="number">0.5</span>)</span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    model = model.cuda()</span><br></pre></td></tr></table></figure><p>In [17]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model</span><br></pre></td></tr></table></figure><p>Out[17]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">RNNModel(</span><br><span class="line">  (drop): Dropout(p=<span class="number">0.5</span>)</span><br><span class="line">  (encoder): Embedding(<span class="number">50002</span>, <span class="number">650</span>)</span><br><span class="line">  (rnn): LSTM(<span class="number">650</span>, <span class="number">1000</span>, num_layers=<span class="number">2</span>, dropout=<span class="number">0.5</span>)</span><br><span class="line">  (decoder): Linear(in_features=<span class="number">1000</span>, out_features=<span class="number">50002</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>In [23]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list(model.parameters())[0].shape</span><br></pre></td></tr></table></figure><p>Out[23]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([50002, 650])</span><br></pre></td></tr></table></figure><ul><li>æˆ‘ä»¬é¦–å…ˆå®šä¹‰è¯„ä¼°æ¨¡å‹çš„ä»£ç ã€‚</li><li>æ¨¡å‹çš„è¯„ä¼°å’Œæ¨¡å‹çš„è®­ç»ƒé€»è¾‘åŸºæœ¬ç›¸åŒï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯æˆ‘ä»¬åªéœ€è¦forward passï¼Œä¸éœ€è¦backward pass</li></ul><p>In [68]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># å…ˆä»ä¸‹é¢è®­ç»ƒæ¨¡å¼çœ‹èµ·ï¼Œåœ¨çœ‹evaluate</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, data)</span>:</span></span><br><span class="line">    model.eval() <span class="comment"># é¢„æµ‹æ¨¡å¼</span></span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    it = iter(data)</span><br><span class="line">    total_count = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        hidden = model.init_hidden(BATCH_SIZE, requires_grad=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># è¿™é‡Œä¸ç®¡æ˜¯è®­ç»ƒæ¨¡å¼è¿˜æ˜¯é¢„æµ‹æ¨¡å¼ï¼Œhå±‚çš„è¾“å…¥éƒ½æ˜¯åˆå§‹åŒ–ä¸º0ï¼Œhiddençš„è¾“å…¥ä¸æ˜¯modelçš„å‚æ•°</span></span><br><span class="line"><span class="comment"># è¿™é‡Œmodelé‡Œçš„model.parameters()å·²ç»æ˜¯è®­ç»ƒè¿‡çš„å‚æ•°ã€‚</span></span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(it):</span><br><span class="line">            data, target = batch.text, batch.target</span><br><span class="line">            <span class="comment"># # å–å‡ºéªŒè¯é›†çš„è¾“å…¥çš„æ•°æ®å’Œè¾“å‡ºçš„æ•°æ®ï¼Œç›¸å½“äºç‰¹å¾å’Œæ ‡ç­¾</span></span><br><span class="line">            <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">                data, target = data.cuda(), target.cuda()</span><br><span class="line">            hidden = repackage_hidden(hidden) <span class="comment"># æˆªæ–­è®¡ç®—å›¾</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad(): <span class="comment"># éªŒè¯é˜¶æ®µä¸éœ€è¦æ›´æ–°æ¢¯åº¦</span></span><br><span class="line">                output, hidden = model(data, hidden)</span><br><span class="line">                <span class="comment">#è°ƒç”¨modelçš„forwardæ–¹æ³•è¿›è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œå¾—åˆ°returnè¾“å‡ºå€¼</span></span><br><span class="line">            loss = loss_fn(output.view(<span class="number">-1</span>, VOCAB_SIZE), target.view(<span class="number">-1</span>))</span><br><span class="line">            <span class="comment"># è®¡ç®—äº¤å‰ç†µæŸå¤±</span></span><br><span class="line">            </span><br><span class="line">            total_count += np.multiply(*data.size()) </span><br><span class="line"><span class="comment"># ä¸Šé¢è®¡ç®—äº¤å‰ç†µçš„æŸå¤±æ˜¯å¹³å‡è¿‡çš„ï¼Œè¿™é‡Œéœ€è¦è®¡ç®—ä¸‹æ€»çš„æŸå¤±</span></span><br><span class="line"><span class="comment"># total_countå…ˆè®¡ç®—éªŒè¯é›†æ ·æœ¬çš„å•è¯æ€»æ•°ï¼Œä¸€ä¸ªæ ·æœ¬æœ‰50ä¸ªå•è¯ï¼Œä¸€ä¸ªbatch32ä¸ªæ ·æœ¬</span></span><br><span class="line"><span class="comment"># np.multiply(*data.size()) =50*32=1600</span></span><br><span class="line">            total_loss += loss.item()*np.multiply(*data.size())</span><br><span class="line"><span class="comment"># æ¯æ¬¡batchå¹³å‡åçš„æŸå¤±ä¹˜ä»¥æ¯æ¬¡batchçš„æ ·æœ¬çš„æ€»çš„å•è¯æ•° = ä¸€æ¬¡batchæ€»çš„æŸå¤±</span></span><br><span class="line">            </span><br><span class="line">    loss = total_loss / total_count <span class="comment"># æ•´ä¸ªéªŒè¯é›†æ€»çš„æŸå¤±é™¤ä»¥æ€»çš„å•è¯æ•°</span></span><br><span class="line">    model.train() <span class="comment"># è®­ç»ƒæ¨¡å¼</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>In [9]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = torch.ones((<span class="number">5</span>,<span class="number">3</span>))</span><br><span class="line">print(a.size())</span><br><span class="line">np.multiply(*a.size())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([5, 3])</span><br></pre></td></tr></table></figure><p>Out[9]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">15</span><br></pre></td></tr></table></figure><p>æˆ‘ä»¬éœ€è¦å®šä¹‰ä¸‹é¢çš„ä¸€ä¸ªfunctionï¼Œå¸®åŠ©æˆ‘ä»¬æŠŠä¸€ä¸ªhidden stateå’Œè®¡ç®—å›¾ä¹‹å‰çš„å†å²åˆ†ç¦»ã€‚</p><p>In [69]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Remove this part</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">repackage_hidden</span><span class="params">(h)</span>:</span></span><br><span class="line">    <span class="string">"""Wraps hidden states in new Tensors, to detach them from their history."""</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(h, torch.Tensor): </span><br><span class="line">        <span class="comment"># è¿™ä¸ªæ˜¯GRUçš„æˆªæ–­ï¼Œå› ä¸ºåªæœ‰ä¸€ä¸ªéšè—å±‚</span></span><br><span class="line">        <span class="comment"># åˆ¤æ–­hæ˜¯ä¸æ˜¯torch.Tensor</span></span><br><span class="line">        <span class="keyword">return</span> h.detach() <span class="comment"># æˆªæ–­è®¡ç®—å›¾ï¼Œhæ˜¯å…¨çš„è®¡ç®—å›¾çš„å¼€å§‹ï¼Œåªæ˜¯ä¿ç•™äº†hçš„å€¼</span></span><br><span class="line">    <span class="keyword">else</span>: <span class="comment"># è¿™ä¸ªæ˜¯LSTMçš„æˆªæ–­ï¼Œæœ‰ä¸¤ä¸ªéšè—å±‚ï¼Œæ ¼å¼æ˜¯å…ƒç»„</span></span><br><span class="line">        <span class="keyword">return</span> tuple(repackage_hidden(v) <span class="keyword">for</span> v <span class="keyword">in</span> h)</span><br></pre></td></tr></table></figure><p>å®šä¹‰loss functionå’Œoptimizer</p><p>In [70]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss() <span class="comment"># äº¤å‰ç†µæŸå¤±</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, <span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># æ¯è°ƒç”¨ä¸€æ¬¡è¿™ä¸ªå‡½æ•°ï¼Œlenrning_rateå°±é™ä¸€åŠï¼Œ0.5å°±æ˜¯ä¸€åŠçš„æ„æ€</span></span><br></pre></td></tr></table></figure><p>è®­ç»ƒæ¨¡å‹ï¼š</p><ul><li>æ¨¡å‹ä¸€èˆ¬éœ€è¦è®­ç»ƒè‹¥å¹²ä¸ªepoch</li><li>æ¯ä¸ªepochæˆ‘ä»¬éƒ½æŠŠæ‰€æœ‰çš„æ•°æ®åˆ†æˆè‹¥å¹²ä¸ªbatch</li><li>æŠŠæ¯ä¸ªbatchçš„è¾“å…¥å’Œè¾“å‡ºéƒ½åŒ…è£…æˆcuda tensor</li><li>forward passï¼Œé€šè¿‡è¾“å…¥çš„å¥å­é¢„æµ‹æ¯ä¸ªå•è¯çš„ä¸‹ä¸€ä¸ªå•è¯</li><li>ç”¨æ¨¡å‹çš„é¢„æµ‹å’Œæ­£ç¡®çš„ä¸‹ä¸€ä¸ªå•è¯è®¡ç®—cross entropy loss</li><li>æ¸…ç©ºæ¨¡å‹å½“å‰gradient</li><li>backward pass</li><li>gradient clippingï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸</li><li>æ›´æ–°æ¨¡å‹å‚æ•°</li><li>æ¯éš”ä¸€å®šçš„iterationè¾“å‡ºæ¨¡å‹åœ¨å½“å‰iterationçš„lossï¼Œä»¥åŠåœ¨éªŒè¯é›†ä¸Šåšæ¨¡å‹çš„è¯„ä¼°</li></ul><p>In [13]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">GRAD_CLIP = <span class="number">1.</span></span><br><span class="line">NUM_EPOCHS = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">val_losses = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    model.train() <span class="comment"># è®­ç»ƒæ¨¡å¼</span></span><br><span class="line">    it = iter(train_iter) </span><br><span class="line">    <span class="comment"># iter,ç”Ÿæˆè¿­ä»£å™¨,è¿™é‡Œtrain_iterä¹Ÿæ˜¯è¿­ä»£å™¨ï¼Œä¸ç”¨iterä¹Ÿå¯ä»¥</span></span><br><span class="line">    hidden = model.init_hidden(BATCH_SIZE) </span><br><span class="line">    <span class="comment"># å¾—åˆ°hiddenåˆå§‹åŒ–åçš„ç»´åº¦</span></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(it):</span><br><span class="line">        data, target = batch.text, batch.target</span><br><span class="line">        <span class="comment"># å–å‡ºè®­ç»ƒé›†çš„è¾“å…¥çš„æ•°æ®å’Œè¾“å‡ºçš„æ•°æ®ï¼Œç›¸å½“äºç‰¹å¾å’Œæ ‡ç­¾</span></span><br><span class="line">        <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">            data, target = data.cuda(), target.cuda()</span><br><span class="line">        hidden = repackage_hidden(hidden)</span><br><span class="line"><span class="comment"># è¯­è¨€æ¨¡å‹æ¯ä¸ªbatchçš„éšè—å±‚çš„è¾“å‡ºå€¼æ˜¯è¦ç»§ç»­ä½œä¸ºä¸‹ä¸€ä¸ªbatchçš„éšè—å±‚çš„è¾“å…¥çš„</span></span><br><span class="line"><span class="comment"># å› ä¸ºbatchæ•°é‡å¾ˆå¤šï¼Œå¦‚æœä¸€ç›´å¾€åä¼ ï¼Œä¼šé€ æˆæ•´ä¸ªè®¡ç®—å›¾å¾ˆåºå¤§ï¼Œåå‘ä¼ æ’­ä¼šå†…å­˜å´©æºƒã€‚</span></span><br><span class="line"><span class="comment"># æ‰€æœ‰æ¯æ¬¡ä¸€ä¸ªbatchçš„è®¡ç®—å›¾è¿­ä»£å®Œæˆåï¼Œéœ€è¦æŠŠè®¡ç®—å›¾æˆªæ–­ï¼Œåªä¿ç•™éšè—å±‚çš„è¾“å‡ºå€¼ã€‚</span></span><br><span class="line"><span class="comment"># ä¸è¿‡åªæœ‰è¯­è¨€æ¨¡å‹æ‰è¿™ä¹ˆå¹²ï¼Œå…¶ä»–æ¯”å¦‚ç¿»è¯‘æ¨¡å‹ä¸éœ€è¦è¿™ä¹ˆåšã€‚</span></span><br><span class="line"><span class="comment"># repackage_hiddenè‡ªå®šä¹‰å‡½æ•°ç”¨æ¥æˆªæ–­è®¡ç®—å›¾çš„ã€‚</span></span><br><span class="line">        model.zero_grad() <span class="comment"># æ¢¯åº¦å½’é›¶ï¼Œä¸ç„¶æ¯æ¬¡è¿­ä»£æ¢¯åº¦ä¼šç´¯åŠ </span></span><br><span class="line">        output, hidden = model(data, hidden)</span><br><span class="line">        <span class="comment"># output = (50,32,50002)</span></span><br><span class="line">        loss = loss_fn(output.view(<span class="number">-1</span>, VOCAB_SIZE), target.view(<span class="number">-1</span>))</span><br><span class="line"><span class="comment"># output.view(-1, VOCAB_SIZE) = (1600,50002)</span></span><br><span class="line"><span class="comment"># target.view(-1) =(1600),å…³äºpytorchä¸­äº¤å‰ç†µçš„è®¡ç®—å…¬å¼è¯·çœ‹ä¸‹é¢é“¾æ¥ã€‚</span></span><br><span class="line"><span class="comment"># https://blog.csdn.net/geter_CS/article/details/84857220</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)</span><br><span class="line">        <span class="comment"># é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œè®¾å®šé˜ˆå€¼ï¼Œå½“æ¢¯åº¦å¤§äºé˜ˆå€¼æ—¶ï¼Œæ›´æ–°çš„æ¢¯åº¦ä¸ºé˜ˆå€¼</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"epoch"</span>, epoch, <span class="string">"iter"</span>, i, <span class="string">"loss"</span>, loss.item())</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            val_loss = evaluate(model, val_iter)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> len(val_losses) == <span class="number">0</span> <span class="keyword">or</span> val_loss &lt; min(val_losses):</span><br><span class="line">                <span class="comment"># å¦‚æœæ¯”ä¹‹å‰çš„lossè¦å°ï¼Œå°±ä¿å­˜æ¨¡å‹</span></span><br><span class="line">                print(<span class="string">"best model, val loss: "</span>, val_loss)</span><br><span class="line">                torch.save(model.state_dict(), <span class="string">"lm-best.th"</span>)</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># å¦åˆ™lossæ²¡æœ‰é™ä¸‹æ¥ï¼Œéœ€è¦ä¼˜åŒ–</span></span><br><span class="line">                scheduler.step() <span class="comment"># è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡</span></span><br><span class="line">                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line">                <span class="comment"># å­¦ä¹ ç‡è°ƒæ•´åéœ€è¦æ›´æ–°optimizerï¼Œä¸‹æ¬¡è®­ç»ƒå°±ç”¨æ›´æ–°åçš„</span></span><br><span class="line">            val_losses.append(val_loss) <span class="comment"># ä¿å­˜æ¯10000æ¬¡è¿­ä»£åçš„éªŒè¯é›†æŸå¤±æŸå¤±</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">epoch 0 iter 0 loss 10.821578979492188</span><br><span class="line">best model, val loss:  10.782116411285918</span><br><span class="line">epoch 0 iter 1000 loss 6.5122528076171875</span><br><span class="line">epoch 0 iter 2000 loss 6.3599748611450195</span><br><span class="line">epoch 0 iter 3000 loss 6.13856315612793</span><br><span class="line">epoch 0 iter 4000 loss 5.473214626312256</span><br><span class="line">epoch 0 iter 5000 loss 5.901871204376221</span><br><span class="line">epoch 0 iter 6000 loss 5.85321569442749</span><br><span class="line">epoch 0 iter 7000 loss 5.636535167694092</span><br><span class="line">epoch 0 iter 8000 loss 5.7489800453186035</span><br><span class="line">epoch 0 iter 9000 loss 5.464158058166504</span><br><span class="line">epoch 0 iter 10000 loss 5.554863452911377</span><br><span class="line">best model, val loss:  5.264891533569864</span><br><span class="line">epoch 0 iter 11000 loss 5.703625202178955</span><br><span class="line">epoch 0 iter 12000 loss 5.6448974609375</span><br><span class="line">epoch 0 iter 13000 loss 5.372857570648193</span><br><span class="line">epoch 0 iter 14000 loss 5.2639479637146</span><br><span class="line">epoch 1 iter 0 loss 5.696778297424316</span><br><span class="line">best model, val loss:  5.124550380139679</span><br><span class="line">epoch 1 iter 1000 loss 5.534722805023193</span><br><span class="line">epoch 1 iter 2000 loss 5.599489212036133</span><br><span class="line">epoch 1 iter 3000 loss 5.459986686706543</span><br><span class="line">epoch 1 iter 4000 loss 4.927192211151123</span><br><span class="line">epoch 1 iter 5000 loss 5.435710906982422</span><br><span class="line">epoch 1 iter 6000 loss 5.4059576988220215</span><br><span class="line">epoch 1 iter 7000 loss 5.308575630187988</span><br><span class="line">epoch 1 iter 8000 loss 5.405811786651611</span><br><span class="line">epoch 1 iter 9000 loss 5.1389055252075195</span><br><span class="line">epoch 1 iter 10000 loss 5.226413726806641</span><br><span class="line">best model, val loss:  4.946829228873176</span><br><span class="line">epoch 1 iter 11000 loss 5.379891395568848</span><br><span class="line">epoch 1 iter 12000 loss 5.360724925994873</span><br><span class="line">epoch 1 iter 13000 loss 5.176026344299316</span><br><span class="line">epoch 1 iter 14000 loss 5.110936641693115</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># åŠ è½½ä¿å­˜å¥½çš„æ¨¡å‹å‚æ•°</span></span><br><span class="line">best_model = RNNModel(<span class="string">"LSTM"</span>, VOCAB_SIZE, EMBEDDING_SIZE, nhid, <span class="number">2</span>, dropout=<span class="number">0.5</span>)</span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    best_model = best_model.cuda()</span><br><span class="line">best_model.load_state_dict(torch.load(<span class="string">"lm-best.th"</span>))</span><br><span class="line"><span class="comment"># æŠŠæ¨¡å‹å‚æ•°loadåˆ°best_modelé‡Œ</span></span><br></pre></td></tr></table></figure><h3 id="ä½¿ç”¨æœ€å¥½çš„æ¨¡å‹åœ¨validæ•°æ®ä¸Šè®¡ç®—perplexity"><a href="#ä½¿ç”¨æœ€å¥½çš„æ¨¡å‹åœ¨validæ•°æ®ä¸Šè®¡ç®—perplexity" class="headerlink" title="ä½¿ç”¨æœ€å¥½çš„æ¨¡å‹åœ¨validæ•°æ®ä¸Šè®¡ç®—perplexity"></a>ä½¿ç”¨æœ€å¥½çš„æ¨¡å‹åœ¨validæ•°æ®ä¸Šè®¡ç®—perplexity</h3><p>In [15]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val_loss = evaluate(best_model, val_iter)</span><br><span class="line">print(<span class="string">"perplexity: "</span>, np.exp(val_loss))</span><br><span class="line"><span class="comment"># è¿™é‡Œä¸æ¸…æ¥šè¯­è¨€æ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡perplexity = np.exp(val_loss)</span></span><br><span class="line"><span class="comment"># æ¸…æ¥šçš„æœ‹å‹æ¬¢è¿äº¤æµä¸‹</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">perplexity:  140.72803934425724</span><br></pre></td></tr></table></figure><h3 id="ä½¿ç”¨æœ€å¥½çš„æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®ä¸Šè®¡ç®—perplexity"><a href="#ä½¿ç”¨æœ€å¥½çš„æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®ä¸Šè®¡ç®—perplexity" class="headerlink" title="ä½¿ç”¨æœ€å¥½çš„æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®ä¸Šè®¡ç®—perplexity"></a>ä½¿ç”¨æœ€å¥½çš„æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®ä¸Šè®¡ç®—perplexity</h3><p>In [16]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_loss = evaluate(best_model, test_iter)</span><br><span class="line">print(<span class="string">"perplexity: "</span>, np.exp(test_loss))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">perplexity:  178.54742013696125</span><br></pre></td></tr></table></figure><p>ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆä¸€äº›å¥å­ã€‚</p><p>In [18]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">hidden = best_model.init_hidden(<span class="number">1</span>) <span class="comment"># batch_size = 1</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">input = torch.randint(VOCAB_SIZE, (<span class="number">1</span>, <span class="number">1</span>), dtype=torch.long).to(device)</span><br><span class="line"><span class="comment"># (1,1)è¡¨ç¤ºè¾“å‡ºæ ¼å¼æ˜¯1è¡Œ1åˆ—çš„2ç»´tensorï¼ŒVOCAB_SIZEè¡¨ç¤ºéšæœºå–çš„å€¼å°äºVOCAB_SIZE=50002</span></span><br><span class="line"><span class="comment"># æˆ‘ä»¬inputç›¸å½“äºå–çš„æ˜¯ä¸€ä¸ªå•è¯</span></span><br><span class="line">words = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    output, hidden = best_model(input, hidden)</span><br><span class="line">    <span class="comment"># output.shape = 1 * 1 * 50002</span></span><br><span class="line">    <span class="comment"># hidden = (2 * 1 * 1000, 2 * 1 * 1000)</span></span><br><span class="line">    word_weights = output.squeeze().exp().cpu()</span><br><span class="line">    <span class="comment"># .exp()çš„ä¸¤ä¸ªä½œç”¨ï¼šä¸€æ˜¯æŠŠæ¦‚ç‡æ›´å¤§çš„å˜å¾—æ›´å¤§ï¼ŒäºŒæ˜¯æŠŠè´Ÿæ•°ç»è¿‡eåå˜æˆæ­£æ•°ï¼Œä¸‹é¢.multinomialå‚æ•°éœ€è¦æ­£æ•°</span></span><br><span class="line">    word_idx = torch.multinomial(word_weights, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># æŒ‰ç…§word_weightsé‡Œé¢çš„æ¦‚ç‡éšæœºçš„å–å€¼ï¼Œæ¦‚ç‡å¤§çš„å–åˆ°çš„æœºä¼šå¤§ã€‚</span></span><br><span class="line">    <span class="comment"># torch.multinomialçœ‹è¿™ä¸ªåšå®¢ç†è§£ï¼šhttps://blog.csdn.net/monchin/article/details/79787621</span></span><br><span class="line">    <span class="comment"># è¿™é‡Œå¦‚æœé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„ï¼Œä¼šæ¯æ¬¡ç”Ÿæˆé‡å¤çš„å¥å­ã€‚</span></span><br><span class="line">    input.fill_(word_idx) <span class="comment"># é¢„æµ‹çš„å•è¯indexæ˜¯word_idxï¼Œç„¶åæŠŠword_idxä½œä¸ºä¸‹ä¸€ä¸ªå¾ªç¯é¢„æµ‹çš„inputè¾“å…¥</span></span><br><span class="line">    word = TEXT.vocab.itos[word_idx] <span class="comment"># æ ¹æ®word_idxå–å‡ºå¯¹åº”çš„å•è¯</span></span><br><span class="line">    words.append(word) </span><br><span class="line">print(<span class="string">" "</span>.join(words))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s influence clinton decision de gaulle is himself sappho s iv one family banquet was made published by paul &lt;unk&gt; and by a persuaded to prevent arcane of animate poverty based at copernicus bachelor in search services and in a cruise corps references eds the robin series july four one nine zero eight summer gutenberg one nine six four births one nine two eight deaths timeline of this method by the fourth amendment the german ioc known for his &lt;unk&gt; from &lt;unk&gt; one eight nine eight one seven eight nine management was established in one nine seven zero they had</span><br></pre></td></tr></table></figure><p>In [42]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.randint(50002, (1, 1))</span><br></pre></td></tr></table></figure><p>Out[42]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[11293]])</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;è¯­è¨€æ¨¡å‹&quot;&gt;&lt;a href=&quot;#è¯­è¨€æ¨¡å‹&quot; class=&quot;headerlink&quot; title=&quot;è¯­è¨€æ¨¡å‹&quot;&gt;&lt;/a&gt;è¯­è¨€æ¨¡å‹&lt;/h1&gt;&lt;p&gt;å­¦ä¹ ç›®æ ‡&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å­¦ä¹ è¯­è¨€æ¨¡å‹ï¼Œä»¥åŠå¦‚ä½•è®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡å‹&lt;/li&gt;
&lt;li&gt;å­¦ä¹ torchtextçš„åŸºæœ¬ä½¿
      
    
    </summary>
    
      <category term="NLP" scheme="http://mmyblog.cn/categories/NLP/"/>
    
    
      <category term="RNN" scheme="http://mmyblog.cn/tags/RNN/"/>
    
      <category term="Linear" scheme="http://mmyblog.cn/tags/Linear/"/>
    
      <category term="Gradient Clipping" scheme="http://mmyblog.cn/tags/Gradient-Clipping/"/>
    
  </entry>
  
  <entry>
    <title>SQuAD-BiDAF</title>
    <link href="http://mmyblog.cn/2020/04/17/SQuAD-BiDAF/"/>
    <id>http://mmyblog.cn/2020/04/17/SQuAD-BiDAF/</id>
    <published>2020-04-16T23:32:02.000Z</published>
    <updated>2020-06-09T01:31:28.801Z</updated>
    
    <content type="html"><![CDATA[<p>ä»£ç æ˜¯åœ¨github<a href="https://github.com/galsang/BiDAF-pytorch" target="_blank" rel="noopener">BiDAF-pytorch</a>ä¸Šä¸‹è½½çš„ï¼Œæˆ‘æŠŠä»£ç å¼„æˆäº†ä¸‹é¢jupyter notebookæ ¼å¼ï¼Œä»£ç æ˜¯åœ¨kaggle GPUè·‘çš„ï¼Œ</p><p>æ•°æ®é›†å¦‚æœä¸èƒ½ä¸‹è½½çš„å¯ä»¥åˆ°æˆ‘çš„ç½‘ç›˜ä¸‹è½½ï¼ŒåŒ…æ‹¬æ•°æ®é›†å’Œè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæ¯”è¾ƒå¤§ï¼š<a href="https://pan.baidu.com/s/1XCEUG6E2biCdqFyaxIGtBw" target="_blank" rel="noopener">ç™¾åº¦ç½‘ç›˜ä¸‹è½½åœ°å€</a></p><p>æ•´ä¸ªä»£ç è·‘ä¸‹æ¥ï¼Œè®­ç»ƒé›†å¯ä»¥è·‘é€šï¼Œæµ‹è¯•é›†å½“æ—¶è·‘çš„æ—¶å€™kaggleå†…å­˜ä¸å¤Ÿäº†ï¼ŒæŠ¥é”™äº†ï¼Œæœ‰å…´è¶£å¯ä»¥è¯•ä¸‹æœ€ç»ˆçš„æ•ˆæœã€‚</p><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This Python 3 environment comes with many helpful analytics libraries installed</span></span><br><span class="line"><span class="comment"># It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python</span></span><br><span class="line"><span class="comment"># For example, here's several helpful packages to load in </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># linear algebra</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># data processing, CSV file I/O (e.g. pd.read_csv)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Input data files are available in the "../input/" directory.</span></span><br><span class="line"><span class="comment"># For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">print(os.listdir(<span class="string">"../input"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Any results you write to the current directory are saved as output.</span></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!cp -r /kaggle/input/bidaf-pytorch-master/BiDAF-pytorch-master /kaggle/working</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.chdir(&quot;BiDAF-pytorch-master&quot;)</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.chdir(&quot;..&quot;)</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!ls</span><br><span class="line">!pwd</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> copy, json, os</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> gmtime, strftime</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> GloVe</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br></pre></td></tr></table></figure><h1 id="ä¸€ã€å®šä¹‰åˆå§‹å˜é‡å‚æ•°"><a href="#ä¸€ã€å®šä¹‰åˆå§‹å˜é‡å‚æ•°" class="headerlink" title="ä¸€ã€å®šä¹‰åˆå§‹å˜é‡å‚æ•°"></a>ä¸€ã€å®šä¹‰åˆå§‹å˜é‡å‚æ•°</h1><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># æœ‰å…³argparseçš„å®˜æ–¹æ–‡æ¡£æ“ä½œè¯·æŸ¥çœ‹ï¼šhttps://docs.python.org/3/library/argparse.html#module-argparseï¼Œ</span></span><br><span class="line"><span class="comment"># ä¸‹é¢çš„å‚æ•°ä»£ç æ³¨é‡Šï¼Œæˆ‘ä¹Ÿä¸æ˜¯ç‰¹åˆ«æ‡‚ï¼Œä»…ä¾›å‚è€ƒ</span></span><br><span class="line"><span class="comment"># å…³äºparser.add_argument(ï¼‰çš„è¯¦è§£è¯·æŸ¥çœ‹ï¼šhttps://blog.csdn.net/u013177568/article/details/62432761/</span></span><br><span class="line"><span class="comment"># å¯¹äºä¸‹é¢å‡½æ•°add_argument()ç¬¬ä¸€ä¸ªæ˜¯é€‰é¡¹æ˜¯å¿…é¡»å†™çš„å‚æ•°ï¼Œè¯¥å‚æ•°æ¥å—é€‰é¡¹å‚æ•°æˆ–è€…æ˜¯ä½ç½®å‚æ•°ï¼ˆä¸€ä¸²æ–‡ä»¶åï¼‰</span></span><br><span class="line"><span class="comment"># ç¬¬äºŒä¸ªæ˜¯defaulté»˜è®¤å€¼ï¼Œå¦‚æœç¬¬ä¸€ä¸ªé€‰é¡¹å‚æ•°æ²¡æœ‰å•ç‹¬æŒ‡å®šï¼Œé‚£é€‰é¡¹å‚æ•°çš„å€¼å°±æ˜¯é»˜è®¤å€¼</span></span><br><span class="line"><span class="comment"># ç¬¬ä¸‰ä¸ªæ˜¯å‚æ•°æ•°æ®ç±»å‹ï¼Œä»£è¡¨ä½ çš„é€‰é¡¹å‚æ•°å¿…é¡»æ˜¯æ˜¯intè¿˜æ˜¯floatå­—ç¬¦å‹æ•°æ®ã€‚</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_args</span><span class="params">()</span>:</span></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">'--char-dim'</span>, default=<span class="number">8</span>, type=int)</span><br><span class="line">    <span class="comment"># char-dim é»˜è®¤å€¼æ˜¯8</span></span><br><span class="line">    parser.add_argument(<span class="string">'--char-channel-width'</span>, default=<span class="number">5</span>, type=int)</span><br><span class="line">    <span class="comment"># char-channel-width é»˜è®¤å€¼æ˜¯5 ä»¥ä¸‹ç±»ä¼¼</span></span><br><span class="line">    parser.add_argument(<span class="string">'--char-channel-size'</span>, default=<span class="number">100</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--context-threshold'</span>, default=<span class="number">400</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--dev-batch-size'</span>, default=<span class="number">100</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--dev-file'</span>, default=<span class="string">'dev-v1.1.json'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--dropout'</span>, default=<span class="number">0.2</span>, type=float)</span><br><span class="line">    parser.add_argument(<span class="string">'--epoch'</span>, default=<span class="number">12</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--exp-decay-rate'</span>, default=<span class="number">0.999</span>, type=float)</span><br><span class="line">    parser.add_argument(<span class="string">'--gpu'</span>, default=<span class="number">0</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--hidden-size'</span>, default=<span class="number">100</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--learning-rate'</span>, default=<span class="number">0.5</span>, type=float)</span><br><span class="line">    parser.add_argument(<span class="string">'--print-freq'</span>, default=<span class="number">250</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--train-batch-size'</span>, default=<span class="number">60</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--train-file'</span>, default=<span class="string">'train-v1.1.json'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--word-dim'</span>, default=<span class="number">100</span>, type=int)</span><br><span class="line">    args = parser.parse_args(args=[]) </span><br><span class="line"><span class="comment"># .parse_args()æ˜¯å°†ä¹‹å‰æ‰€æœ‰add_argumentå®šä¹‰çš„å‚æ•°åœ¨æ‹¬å·é‡Œè¿›è¡Œèµ‹å€¼ï¼Œæ²¡æœ‰èµ‹å€¼(args=[])ï¼Œå°±è¿”å›å‚æ•°å„è‡ªdefaultçš„é»˜è®¤å€¼ã€‚</span></span><br><span class="line"><span class="comment"># è¿”å›å€¼argsç›¸å½“äºæ˜¯ä¸ªå‚æ•°å‘½åç©ºé—´çš„é›†åˆï¼Œå¯ä»¥è°ƒç”¨ä¸Šé¢ç¬¬ä¸€é¡¹é€‰é¡¹å‚æ•°çš„åå­—ï¼Œå°±å¯ä»¥å¾—åˆ°defaultå€¼äº†ã€‚</span></span><br><span class="line"><span class="comment"># æ¯”å¦‚è°ƒç”¨ä¸Šé¢å‚æ•°æ–¹å¼ï¼šargs.char_dim,args.char_channel_width....é»˜è®¤æƒ…å†µä¸‹ï¼Œä¸­åˆ’çº¿ä¼šè½¬æ¢ä¸ºä¸‹åˆ’çº¿.</span></span><br><span class="line">    <span class="keyword">return</span> args</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args = parse_args()</span><br></pre></td></tr></table></figure><h1 id="äºŒã€SQuADé—®ç­”æ•°æ®é¢„å¤„ç†"><a href="#äºŒã€SQuADé—®ç­”æ•°æ®é¢„å¤„ç†" class="headerlink" title="äºŒã€SQuADé—®ç­”æ•°æ®é¢„å¤„ç†"></a>äºŒã€SQuADé—®ç­”æ•°æ®é¢„å¤„ç†</h1><h2 id="1ã€æŸ¥çœ‹æ•°æ®é›†ç»“æ„"><a href="#1ã€æŸ¥çœ‹æ•°æ®é›†ç»“æ„" class="headerlink" title="1ã€æŸ¥çœ‹æ•°æ®é›†ç»“æ„"></a>1ã€æŸ¥çœ‹æ•°æ®é›†ç»“æ„</h2><p>SQuADé—®ç­”æ•°æ®ä»‹ç»ï¼š<a href="https://rajpurkar.github.io/SQuAD-explorer/" target="_blank" rel="noopener">https://rajpurkar.github.io/SQuAD-explorer/</a> è¿™ä¸ªæ•°æ®é›†æœ‰ä¸¤ä¸ªæ–‡ä»¶ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†ï¼štrain-v1.1.jsonï¼Œdev-v1.1.json</p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'data/squad/dev-v1.1.json'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                line = f.readline() <span class="comment"># è¯¥æ–¹æ³•æ¯æ¬¡è¯»å‡ºä¸€è¡Œå†…å®¹</span></span><br><span class="line">                <span class="keyword">if</span> line:</span><br><span class="line">                    print(<span class="string">"type(line)"</span>,type(line)) <span class="comment"># ç›´æ¥æ‰“å°å°±æ˜¯å­—ç¬¦ä¸²æ ¼å¼</span></span><br><span class="line">                    r = json.loads(line)</span><br><span class="line">                    print(<span class="string">"type(r)"</span>,type(r)) <span class="comment"># ä½¿ç”¨json.loadså°†å­—ç¬¦ä¸²è½¬åŒ–ä¸ºå­—å…¸</span></span><br><span class="line">                    print(r)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            f.close()</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># æ•°æ®æ¶æ„å¦‚ä¸‹</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"data"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"title"</span>: <span class="string">"Super_Bowl_50"</span>, <span class="comment"># ç¬¬ä¸€ä¸ªä¸»é¢˜</span></span><br><span class="line">            <span class="string">"paragraphs"</span>: [</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">"context"</span>: <span class="string">" numerals 50......."</span>, <span class="comment"># æ¯ä¸ªä¸»é¢˜ä¼šæœ‰å¾ˆå¤šcontextçŸ­æ–‡,è¿™é‡Œåªåˆ—å‡ºä¸€ä¸ª</span></span><br><span class="line">                    <span class="string">"qas"</span>: [  <span class="comment"># è¿™ä¸ªåˆ—è¡¨é‡Œæ”¾é—®é¢˜å’Œç­”æ¡ˆçš„ä½ç½®ï¼Œæ¯ç¯‡contextä¼šæœ‰å¾ˆæœ‰å¾ˆå¤šanswerå’Œquestionï¼Œè¿™é‡Œåªåˆ—å‡ºä¸€ä¸ª</span></span><br><span class="line">                        &#123;</span><br><span class="line">                            <span class="string">"answers"</span>: [  <span class="comment"># ä¸€ä¸ªé—®é¢˜ä¼šæœ‰ä¸‰ä¸ªç­”æ¡ˆï¼Œä¸‰ä¸ªç­”æ¡ˆéƒ½æ˜¯å¯¹çš„ï¼Œåªæ˜¯åœ¨contextä¸åŒæˆ–ç›¸åŒä½ç½®</span></span><br><span class="line">                                &#123;         <span class="comment"># ä¸‹é¢ä¸‰ä¸ªç­”æ¡ˆéƒ½åœ¨ç›¸åŒçš„ä½ç½®</span></span><br><span class="line">                                    <span class="string">"answer_start"</span>: <span class="number">177</span>,  <span class="comment"># ç­”æ¡ˆåœ¨æ–‡ä¸­çš„èµ·å§‹ä½ç½®æ˜¯ç¬¬177çš„å­—ç¬¦ã€‚</span></span><br><span class="line">                                    <span class="string">"text"</span>: <span class="string">"Denver Broncos"</span></span><br><span class="line">                                &#125;,</span><br><span class="line">                                &#123;</span><br><span class="line">                                    <span class="string">"answer_start"</span>: <span class="number">177</span>,</span><br><span class="line">                                    <span class="string">"text"</span>: <span class="string">"Denver Broncos"</span></span><br><span class="line">                                &#125;,</span><br><span class="line">                                &#123;</span><br><span class="line">                                    <span class="string">"answer_start"</span>: <span class="number">177</span>,</span><br><span class="line">                                    <span class="string">"text"</span>: <span class="string">"Denver Broncos"</span></span><br><span class="line">                                &#125;</span><br><span class="line">                            ],</span><br><span class="line">                            <span class="string">"question"</span>: <span class="string">"Which NFL team represented the AFC at Super Bowl 50?"</span>,</span><br><span class="line">                            <span class="string">"id"</span>: <span class="string">"56be4db0acb8001400a502ec"</span></span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                    ]</span><br><span class="line">                &#125;</span><br><span class="line">                </span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"title"</span>: <span class="string">"Warsaw"</span>, <span class="comment"># ç¬¬äºŒä¸ªä¸»é¢˜</span></span><br><span class="line">            <span class="string">"paragraphs"</span>:   </span><br><span class="line">        &#125;,</span><br><span class="line">        </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"title"</span>: <span class="string">"Normans"</span>, <span class="comment"># ç¬¬ä¸‰ä¸ªä¸»é¢˜</span></span><br><span class="line">            <span class="string">"paragraphs"</span>: </span><br><span class="line">        &#125;,</span><br><span class="line">        </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"title"</span>: <span class="string">"Nikola_Tesla"</span>, <span class="comment"># ç¬¬å››ä¸ªä¸»é¢˜</span></span><br><span class="line">            <span class="string">"paragraphs"</span>: </span><br><span class="line">        &#125;,</span><br><span class="line">        ........... <span class="comment"># è¿˜æœ‰å¾ˆå¤š</span></span><br><span class="line">        </span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"version"</span>: <span class="string">"1.1"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2ã€å®šä¹‰åˆ†è¯æ–¹æ³•"><a href="#2ã€å®šä¹‰åˆ†è¯æ–¹æ³•" class="headerlink" title="2ã€å®šä¹‰åˆ†è¯æ–¹æ³•"></a>2ã€å®šä¹‰åˆ†è¯æ–¹æ³•</h2><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_tokenize</span><span class="params">(tokens)</span>:</span></span><br><span class="line">    tokens = [token.replace(<span class="string">"''"</span>, <span class="string">'"'</span>).replace(<span class="string">"``"</span>, <span class="string">'"'</span>) <span class="keyword">for</span> token <span class="keyword">in</span> nltk.word_tokenize(tokens)]</span><br><span class="line">    <span class="comment"># nltk.word_tokenize(tokens)åˆ†è¯ï¼Œreplaceè§„èŒƒåŒ–å¼•å·ï¼Œæ–¹ä¾¿åé¢å¤„ç†</span></span><br><span class="line">    <span class="keyword">return</span> tokens</span><br></pre></td></tr></table></figure><h2 id="3ã€æ¸…æ´—æ•°æ®ï¼Œå¹¶ç”Ÿæˆæ•°æ®è¿­ä»£å™¨"><a href="#3ã€æ¸…æ´—æ•°æ®ï¼Œå¹¶ç”Ÿæˆæ•°æ®è¿­ä»£å™¨" class="headerlink" title="3ã€æ¸…æ´—æ•°æ®ï¼Œå¹¶ç”Ÿæˆæ•°æ®è¿­ä»£å™¨"></a>3ã€æ¸…æ´—æ•°æ®ï¼Œå¹¶ç”Ÿæˆæ•°æ®è¿­ä»£å™¨</h2><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SQuAD</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ä»¥ä¸‹å®šå¥½ä¸­é—´è¾“å‡ºç¼“å­˜æ–‡ä»¶çš„è·¯å¾„</span></span><br><span class="line">        path = <span class="string">'data/squad'</span> </span><br><span class="line">        dataset_path = path + <span class="string">'/torch_text/'</span> </span><br><span class="line">        train_examples_path = dataset_path + <span class="string">'train_examples.pt'</span></span><br><span class="line">        dev_examples_path = dataset_path + <span class="string">'dev_examples.pt'</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">"preprocessing data files..."</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.train_file&#125;</span>l'</span>):</span><br><span class="line">            <span class="comment"># å­—ç¬¦ä¸²å‰ä»¥få¼€å¤´è¡¨ç¤ºåœ¨å­—ç¬¦ä¸²å†…æ”¯æŒå¤§æ‹¬å·å†…çš„ python è¡¨è¾¾å¼</span></span><br><span class="line">            <span class="comment"># args.train_file = 'train-v1.1.json'</span></span><br><span class="line">            print(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.train_file&#125;</span>'</span>)</span><br><span class="line">            self.preprocess_file(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.train_file&#125;</span>'</span>)  </span><br><span class="line">            <span class="comment"># preprocess_fileä¸‹é¢å‡½æ•°æœ‰å®šä¹‰ï¼Œå®Œæˆæ–‡ä»¶çš„é¢„å¤„ç†</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.dev_file&#125;</span>l'</span>):</span><br><span class="line">            <span class="comment"># args.dev_file = 'dev-v1.1.json'</span></span><br><span class="line">            self.preprocess_file(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.dev_file&#125;</span>'</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># ä¸‹é¢æ˜¯ç”¨torchtextå¤„ç†æ•°æ®çš„æ­¥éª¤çœ‹ä¸æ‡‚äº†ï¼Œæœ‰çŸ¥é“çš„å¯ä»¥äº¤æµä¸‹   </span></span><br><span class="line">        self.RAW = data.RawField()<span class="comment"># è¿™ä¸ªæ˜¯å®Œå…¨ç©ºç™½çš„fieldï¼Œæ„å‘³ç€ä¸ç»è¿‡ä»»ä½•å¤„ç†</span></span><br><span class="line">        <span class="comment"># explicit declaration for torchtext compatibility</span></span><br><span class="line">        self.RAW.is_target = <span class="literal">False</span></span><br><span class="line">        self.CHAR_NESTING = data.Field(batch_first=<span class="literal">True</span>, tokenize=list, lower=<span class="literal">True</span>)</span><br><span class="line">        self.CHAR = data.NestedField(self.CHAR_NESTING, tokenize=word_tokenize)</span><br><span class="line">        self.WORD = data.Field(batch_first=<span class="literal">True</span>, tokenize=word_tokenize, lower=<span class="literal">True</span>, include_lengths=<span class="literal">True</span>)</span><br><span class="line">        self.LABEL = data.Field(sequential=<span class="literal">False</span>, unk_token=<span class="literal">None</span>, use_vocab=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        dict_fields = &#123;<span class="string">'id'</span>: (<span class="string">'id'</span>, self.RAW),</span><br><span class="line">                       <span class="string">'s_idx'</span>: (<span class="string">'s_idx'</span>, self.LABEL),</span><br><span class="line">                       <span class="string">'e_idx'</span>: (<span class="string">'e_idx'</span>, self.LABEL),</span><br><span class="line">                       <span class="string">'context'</span>: [(<span class="string">'c_word'</span>, self.WORD), (<span class="string">'c_char'</span>, self.CHAR)],</span><br><span class="line">                       <span class="string">'question'</span>: [(<span class="string">'q_word'</span>, self.WORD), (<span class="string">'q_char'</span>, self.CHAR)]&#125;</span><br><span class="line"></span><br><span class="line">        list_fields = [(<span class="string">'id'</span>, self.RAW), (<span class="string">'s_idx'</span>, self.LABEL), (<span class="string">'e_idx'</span>, self.LABEL),</span><br><span class="line">                       (<span class="string">'c_word'</span>, self.WORD), (<span class="string">'c_char'</span>, self.CHAR),</span><br><span class="line">                       (<span class="string">'q_word'</span>, self.WORD), (<span class="string">'q_char'</span>, self.CHAR)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(dataset_path):</span><br><span class="line">            print(<span class="string">"loading splits..."</span>)</span><br><span class="line">            train_examples = torch.load(train_examples_path)</span><br><span class="line">            dev_examples = torch.load(dev_examples_path)</span><br><span class="line"></span><br><span class="line">            self.train = data.Dataset(examples=train_examples, fields=list_fields)</span><br><span class="line">            self.dev = data.Dataset(examples=dev_examples, fields=list_fields)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"building splits..."</span>)</span><br><span class="line">             <span class="comment"># åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†</span></span><br><span class="line">            self.train, self.dev = data.TabularDataset.splits(</span><br><span class="line">                path=path,</span><br><span class="line">                train=<span class="string">f'<span class="subst">&#123;args.train_file&#125;</span>l'</span>,</span><br><span class="line">                validation=<span class="string">f'<span class="subst">&#123;args.dev_file&#125;</span>l'</span>,</span><br><span class="line">                format=<span class="string">'json'</span>,</span><br><span class="line">                fields=dict_fields)</span><br><span class="line"></span><br><span class="line">            os.makedirs(dataset_path)</span><br><span class="line">            torch.save(self.train.examples, train_examples_path)</span><br><span class="line">            torch.save(self.dev.examples, dev_examples_path)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#cut too long context in the training set for efficiency.</span></span><br><span class="line">        <span class="keyword">if</span> args.context_threshold &gt; <span class="number">0</span>:</span><br><span class="line">            self.train.examples = [e <span class="keyword">for</span> e <span class="keyword">in</span> self.train.examples <span class="keyword">if</span> len(e.c_word) &lt;= args.context_threshold]</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"building vocab..."</span>)</span><br><span class="line">        self.CHAR.build_vocab(self.train, self.dev) <span class="comment"># å­—ç¬¦å‘é‡æ²¡æœ‰è®¾ç½®vector</span></span><br><span class="line">        self.WORD.build_vocab(self.train, self.dev, vectors=GloVe(name=<span class="string">'6B'</span>, dim=args.word_dim))</span><br><span class="line">        <span class="comment"># åŠ è½½Gloveå‘é‡ï¼Œargs.word_dim = 100</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">"building iterators..."</span>)</span><br><span class="line">        device = torch.device(<span class="string">f"cuda:<span class="subst">&#123;args.gpu&#125;</span>"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">        <span class="comment"># ç”Ÿæˆè¿­ä»£å™¨</span></span><br><span class="line">        self.train_iter, self.dev_iter = \</span><br><span class="line">            data.BucketIterator.splits((self.train, self.dev),</span><br><span class="line">                                       batch_sizes=[args.train_batch_size, args.dev_batch_size],</span><br><span class="line">                                       device=device,</span><br><span class="line">                                       sort_key=<span class="keyword">lambda</span> x: len(x.c_word))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preprocess_file</span><span class="params">(self,path)</span>:</span></span><br><span class="line">        dump = []</span><br><span class="line">        abnormals = [<span class="string">' '</span>, <span class="string">'\n'</span>, <span class="string">'\u3000'</span>, <span class="string">'\u202f'</span>, <span class="string">'\u2009'</span>]</span><br><span class="line">        <span class="comment"># ç©ºç™½æ— æ•ˆå­—ç¬¦åˆ—è¡¨</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> open(path, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            data = json.load(f) <span class="comment"># ç›´æ¥æ–‡ä»¶å¥æŸ„è½¬åŒ–ä¸ºå­—å…¸</span></span><br><span class="line">            data = data[<span class="string">'data'</span>] <span class="comment"># è¿”å›å€¼dataæ˜¯ä¸ªåˆ—è¡¨ï¼Œå­—å…¸æ˜¯åˆ—è¡¨çš„å…ƒç´ </span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> article <span class="keyword">in</span> data:</span><br><span class="line">                <span class="comment"># æ¯ä¸ªarticleæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œä¸€ä¸ªå­—å…¸åŒ…å«ä¸€ä¸ªtitleçš„ä¿¡æ¯</span></span><br><span class="line">                <span class="keyword">for</span> paragraph <span class="keyword">in</span> article[<span class="string">'paragraphs'</span>]:</span><br><span class="line">                    <span class="comment"># æ¯ä¸ªparagraphæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œä¸€ä¸ªå­—å…¸é‡Œæœ‰ä¸€ä¸ªcontextå’Œqasçš„ä¿¡æ¯ï¼Œqasæ˜¯é—®é¢˜å’Œç­”æ¡ˆã€‚</span></span><br><span class="line">                    context = paragraph[<span class="string">'context'</span>]</span><br><span class="line">                    <span class="comment"># contextçš„å†…å®¹ï¼Œæ˜¯å­—ç¬¦ä¸²ï¼Œå¦‚ï¼š" numerals 50............."</span></span><br><span class="line">                    tokens = word_tokenize(context) <span class="comment"># å¯¹contextè¿›è¡Œåˆ†è¯</span></span><br><span class="line">                    <span class="keyword">for</span> qa <span class="keyword">in</span> paragraph[<span class="string">'qas'</span>]:</span><br><span class="line">                        <span class="comment"># æ¯ä¸ªqaæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œä¸€ä¸ªå­—å…¸åŒ…å«ä¸€å¯¹answerså’Œquestionçš„ä¿¡æ¯</span></span><br><span class="line">                        id = qa[<span class="string">'id'</span>]</span><br><span class="line">                        <span class="comment"># å–å‡ºè¿™å¯¹answerså’Œquestionçš„idä¿¡æ¯ï¼Œå¦‚ï¼š"56be4db0acb8001400a502ec"</span></span><br><span class="line">                        question = qa[<span class="string">'question'</span>]</span><br><span class="line">                        <span class="comment"># å–å‡ºquestionï¼Œå¦‚ï¼š"Which NFL team represented the AFC at Super Bowl 50?"</span></span><br><span class="line">                        <span class="keyword">for</span> ans <span class="keyword">in</span> qa[<span class="string">'answers'</span>]:</span><br><span class="line">                            <span class="comment"># ansä¸ºæ¯ä¸ªç­”æ¡ˆï¼Œå…±æœ‰ä¸‰ä¸ªæ ‡å‡†ç­”æ¡ˆï¼Œå¯ä»¥ç›¸åŒï¼Œå¯ä»¥ä¸åŒï¼Œç»Ÿä¸€ä¸º3ä¸ªã€‚</span></span><br><span class="line">                            answer = ans[<span class="string">'text'</span>]</span><br><span class="line">                            <span class="comment"># é—®é¢˜çš„æ¯ä¸ªå›ç­”ï¼Œå¦‚ï¼š"Denver Broncos"</span></span><br><span class="line">                            s_idx = ans[<span class="string">'answer_start'</span>]</span><br><span class="line">                            <span class="comment"># æ¯ä¸ªå›ç­”çš„startä½ç½®ï¼Œæ•°å€¼ä»£è¡¨contextä¸­ç¬¬å‡ ä¸ªå­—ç¬¦ï¼Œå¦‚ï¼š177</span></span><br><span class="line">                            e_idx = s_idx + len(answer)</span><br><span class="line">                            <span class="comment"># æ¯ä¸ªå›ç­”çš„endä½ç½®</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                            <span class="comment"># ä¸‹é¢é‡æ–°æ›´æ–°å­—ç¬¦çš„èµ·å§‹ä½ç½®ï¼Œä½¿ç”¨å­—ç¬¦è®¡ç®—ä½ç½®æ”¹ä¸ºä½¿ç”¨å•è¯è®¡ç®—ä½ç½®</span></span><br><span class="line">                            <span class="comment"># è¯·çœ‹ä¸‹é¢å•å…ƒæ ¼çš„ç¤ºä¾‹è¾“å‡ºæœ‰åŠ©ç†è§£ã€‚</span></span><br><span class="line">                            l = <span class="number">0</span></span><br><span class="line">                            s_found = <span class="literal">False</span></span><br><span class="line">                            <span class="keyword">for</span> i, t <span class="keyword">in</span> enumerate(tokens):</span><br><span class="line">                                <span class="comment"># å¾ªç¯tæ¬¡ï¼Œtä¸ºåˆ†è¯åçš„å•è¯æ•°é‡</span></span><br><span class="line">                                <span class="keyword">while</span> l &lt; len(context):</span><br><span class="line">                                    <span class="keyword">if</span> context[l] <span class="keyword">in</span> abnormals:</span><br><span class="line">                                        <span class="comment"># contextä¸­æœ‰ç©ºç™½æ— æ•ˆå­—ç¬¦ï¼Œå°±è®¡æ•°</span></span><br><span class="line">                                        l += <span class="number">1</span></span><br><span class="line">                                    <span class="keyword">else</span>:    <span class="comment"># ä¸€ç¢°åˆ°ä¸æ˜¯ç©ºç™½å­—ç¬¦çš„å°±break</span></span><br><span class="line">                                        <span class="keyword">break</span></span><br><span class="line">                                <span class="comment"># exceptional cases</span></span><br><span class="line">                                <span class="keyword">if</span> t[<span class="number">0</span>] == <span class="string">'"'</span> <span class="keyword">and</span> context[l:l + <span class="number">2</span>] == <span class="string">'\'\''</span>:</span><br><span class="line">                                    <span class="comment"># ä¸“é—¨è®¡ç®—context=''an è¿™ç§é•¿åº¦ï¼Œè¿™ä¸ªé•¿åº¦ä¸º4</span></span><br><span class="line">                                    t = <span class="string">'\'\''</span> + t[<span class="number">1</span>:] </span><br><span class="line">                                <span class="keyword">elif</span> t == <span class="string">'"'</span> <span class="keyword">and</span> context[l:l + <span class="number">2</span>] == <span class="string">'\'\''</span>:</span><br><span class="line">                                    <span class="comment"># ä¸“é—¨è®¡ç®—context='' è¿™ç§é•¿åº¦</span></span><br><span class="line">                                    <span class="comment"># ä¸Šé¢t[0] == '"'è¡¨è¾¾å¼åŒ…å«äº†è¿™ç§ï¼Œæ‰€ä»¥æˆ‘è®¤ä¸ºè¿™ä¸ªè¡¨è¾¾å¼æ²¡ç”¨ä¸Š</span></span><br><span class="line">                                    t = <span class="string">'\'\''</span></span><br><span class="line"></span><br><span class="line">                                l += len(t)</span><br><span class="line">                                <span class="keyword">if</span> l &gt; s_idx <span class="keyword">and</span> s_found == <span class="literal">False</span>:</span><br><span class="line">                                    <span class="comment"># åªè¦è®¡æ•°è¶…è¿‡èµ·å§‹ä½ç½®å€¼ï¼Œè¿™ä¸ªå•è¯å°±æ˜¯startçš„å•è¯</span></span><br><span class="line">                                    s_idx = i</span><br><span class="line">                                    s_found = <span class="literal">True</span></span><br><span class="line">                                <span class="keyword">if</span> l &gt;= e_idx:</span><br><span class="line">                                    <span class="comment"># è¿™é‡Œä¸å‡ºé”™çš„è¯ï¼Œç­‰äºe_idxå°±æ˜¯endçš„å•è¯</span></span><br><span class="line">                                    e_idx = i</span><br><span class="line">                                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">                            <span class="comment"># è¿™é‡ŒæŠŠä¸‰ä¸ªansweråˆ†å¼€ï¼Œæ¯ä¸ªansweréƒ½æ”¾è¿›å­—å…¸ä¸­,å¹¶ä½œä¸ºä¸€ä¸ªæ ·æœ¬</span></span><br><span class="line">                            dump.append(dict([(<span class="string">'id'</span>, id),</span><br><span class="line">                                              (<span class="string">'context'</span>, context),</span><br><span class="line">                                              (<span class="string">'question'</span>, question),</span><br><span class="line">                                              (<span class="string">'answer'</span>, answer),</span><br><span class="line">                                              (<span class="string">'s_idx'</span>, s_idx),</span><br><span class="line">                                              (<span class="string">'e_idx'</span>, e_idx)]))</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">f'<span class="subst">&#123;path&#125;</span>l'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> dump:</span><br><span class="line">                <span class="comment"># lineä¸ºå­—å…¸ï¼Œä¸€ä¸ªæ ·æœ¬å­˜å‚¨</span></span><br><span class="line">                json.dump(line, f)</span><br><span class="line">                <span class="comment">#dumpï¼šå°†dictç±»å‹è½¬æ¢ä¸ºjsonå­—ç¬¦ä¸²æ ¼å¼ï¼Œå†™å…¥åˆ°æ–‡ä»¶</span></span><br><span class="line">                print(<span class="string">''</span>, file=f) <span class="comment"># è¿™é‡Œprintçš„ä½œç”¨å°±æ˜¯æ¢è¡Œç”¨çš„ã€‚</span></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = SQuAD(args)</span><br></pre></td></tr></table></figure><h3 id="ä¸Šé¢ä¸å¤ªæ˜ç™½çš„ä¸¾ä¾‹å­"><a href="#ä¸Šé¢ä¸å¤ªæ˜ç™½çš„ä¸¾ä¾‹å­" class="headerlink" title="ä¸Šé¢ä¸å¤ªæ˜ç™½çš„ä¸¾ä¾‹å­"></a>ä¸Šé¢ä¸å¤ªæ˜ç™½çš„ä¸¾ä¾‹å­</h3><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ä¸¾ä¾‹å­</span></span><br><span class="line">a = <span class="string">" \u2009\n\u3000Super Bowl 50 was ''an'' American football     \u3000game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50."</span></span><br><span class="line">tokens = word_tokenize(a)</span><br><span class="line">print(nltk.word_tokenize(a)) <span class="comment"># æ‰€æœ‰çš„â€œ\u2009â€ï¼Œâ€œ\nâ€ï¼Œâ€œ\u3000â€ç­‰ç©ºç™½å­—ç¬¦éƒ½å»æ‰äº†</span></span><br><span class="line">print(<span class="string">"----"</span>*<span class="number">20</span>)</span><br><span class="line">print(tokens)</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># </span></span><br><span class="line">print(a[<span class="number">0</span>]) <span class="comment"># ç©ºç™½å­—ç¬¦æ‰“å°ä¸å‡ºæ¥</span></span><br><span class="line">print(a[<span class="number">1</span>]) <span class="comment"># ç©ºç™½å­—ç¬¦æ‰“å°ä¸å‡ºæ¥</span></span><br><span class="line">print(a[<span class="number">2</span>]) <span class="comment"># ç©ºç™½å­—ç¬¦æ‰“å°ä¸å‡ºæ¥</span></span><br><span class="line">print(a[<span class="number">3</span>]) <span class="comment"># ç©ºç™½å­—ç¬¦æ‰“å°ä¸å‡ºæ¥</span></span><br><span class="line">print(a[<span class="number">4</span>]) </span><br><span class="line">print(a[<span class="number">5</span>])</span><br><span class="line">a[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ä¸‹é¢ç‰¹åˆ«æ³¨æ„</span></span><br><span class="line">print(tokens[<span class="number">4</span>][<span class="number">0</span>]== <span class="string">'"'</span>) <span class="comment"># è™½ç„¶åˆ‡åˆ†åçœ‹èµ·æ¥æ˜¯"''"ï¼Œä½†å®é™…ä¸Šæ˜¯'"'</span></span><br><span class="line">print(tokens[<span class="number">4</span>][<span class="number">0</span>]== <span class="string">"''"</span>) </span><br><span class="line">print(tokens[<span class="number">5</span>]== <span class="string">'"'</span>)</span><br><span class="line">print(len(<span class="string">'"'</span>)) <span class="comment"># è¿™ç§é•¿åº¦ä¸º1</span></span><br><span class="line">print(len(<span class="string">"''"</span>)) <span class="comment"># è¿™ç§é•¿åº¦ä¸º2</span></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># æŸ¥çœ‹è¾“å‡ºç†è§£</span></span><br><span class="line">s_idx = <span class="number">177</span></span><br><span class="line">e_idx = s_idx + len(<span class="string">"Denver Broncos"</span>)</span><br><span class="line">l=<span class="number">0</span></span><br><span class="line">context = <span class="string">" \u2009\n\u3000Super Bowl 50 was ''an'' American football     \u3000game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50."</span></span><br><span class="line">tokens = word_tokenize(context)</span><br><span class="line">abnormals = [<span class="string">' '</span>, <span class="string">'\n'</span>, <span class="string">'\u3000'</span>, <span class="string">'\u202f'</span>, <span class="string">'\u2009'</span>]</span><br><span class="line">s_found = <span class="literal">False</span></span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> enumerate(tokens):</span><br><span class="line">    print(<span class="string">"t="</span>,t)</span><br><span class="line">    <span class="keyword">while</span> l &lt; len(context):</span><br><span class="line">        <span class="keyword">if</span> context[l] <span class="keyword">in</span> abnormals:</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    print(<span class="string">"l"</span>,l)</span><br><span class="line">    <span class="comment"># exceptional cases</span></span><br><span class="line">    <span class="keyword">if</span> t[<span class="number">0</span>] == <span class="string">'"'</span> <span class="keyword">and</span> context[l:l + <span class="number">2</span>] == <span class="string">'\'\''</span>:</span><br><span class="line">        print(<span class="string">"1111111111111111111"</span>)</span><br><span class="line">        print(t)</span><br><span class="line">        print(t[<span class="number">1</span>:])</span><br><span class="line">        t = <span class="string">'\'\''</span> + t[<span class="number">1</span>:]</span><br><span class="line">        print(t)</span><br><span class="line">    <span class="keyword">elif</span> t == <span class="string">'"'</span> <span class="keyword">and</span> context[l:l + <span class="number">2</span>] == <span class="string">'\'\''</span>:</span><br><span class="line">        <span class="comment"># çœ‹è¾“å‡ºç»“æœï¼Œè¿™ä¸ªè¡¨è¾¾å¼æ²¡æœ‰ç”¨åˆ°</span></span><br><span class="line">        print(<span class="string">"22222222222222222222"</span>)</span><br><span class="line">        print(t)</span><br><span class="line">        t = <span class="string">'\'\''</span></span><br><span class="line">    print(<span class="string">"len(t)"</span>,len(t))</span><br><span class="line">    l += len(t)</span><br><span class="line">    print(<span class="string">"l"</span>,l)</span><br><span class="line">    <span class="keyword">if</span> l &gt; s_idx <span class="keyword">and</span> s_found == <span class="literal">False</span>:</span><br><span class="line">        s_idx = i</span><br><span class="line">        print(<span class="string">"s_idx"</span>,s_idx)</span><br><span class="line">        s_found = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> l &gt;= e_idx:</span><br><span class="line">        e_idx = i</span><br><span class="line">        print(<span class="string">"e_idx"</span>,e_idx)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch = next(iter(data.train_iter)) <span class="comment">#ä¸€ä¸ªbatchçš„ä¿¡æ¯</span></span><br><span class="line">print(batch)</span><br><span class="line"><span class="comment"># è®­ç»ƒé›†çš„batch_sizes=60</span></span><br><span class="line"><span class="comment"># batch.c_word = 60x293ï¼Œ293æ˜¯60ä¸ªæ ·æœ¬ä¸­æœ€é•¿æ ·æœ¬tokençš„å•è¯æ•°</span></span><br><span class="line"><span class="comment"># batch.c_char = 60x293x25ï¼Œ25æ˜¯æŸä¸ªå•è¯å­—ç¬¦çš„æœ€å¤§çš„æ•°é‡</span></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(batch.q_word)</span><br><span class="line">print(batch.q_char[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ä¸‹é¢ä¸ºargsæ–°å¢å‚æ•°ï¼Œå¹¶èµ‹å€¼</span></span><br><span class="line"><span class="comment"># hasattr() getattr() setattr() å‡½æ•°ä½¿ç”¨æ–¹æ³•è¯¦è§£https://www.cnblogs.com/cenyu/p/5713686.html</span></span><br><span class="line">setattr(args, <span class="string">'char_vocab_size'</span>, len(data.CHAR.vocab)) <span class="comment"># è®¾ç½®å±æ€§args.char_vocab_sizeçš„å€¼ = len(data.CHAR.vocab)</span></span><br><span class="line">setattr(args, <span class="string">'word_vocab_size'</span>, len(data.WORD.vocab))</span><br><span class="line">setattr(args, <span class="string">'dataset_file'</span>, <span class="string">f'data/squad/<span class="subst">&#123;args.dev_file&#125;</span>'</span>)</span><br><span class="line">setattr(args, <span class="string">'prediction_file'</span>, <span class="string">f'prediction<span class="subst">&#123;args.gpu&#125;</span>.out'</span>)</span><br><span class="line">setattr(args, <span class="string">'model_time'</span>, strftime(<span class="string">'%H:%M:%S'</span>, gmtime())) <span class="comment"># æ—¶é—´</span></span><br><span class="line">print(<span class="string">'data loading complete!'</span>)</span><br></pre></td></tr></table></figure><h2 id="BIDAF"><a href="#BIDAF" class="headerlink" title="BIDAF"></a>BIDAF</h2><p><img src="https://s1.ax1x.com/2020/06/09/t4C2yd.jpg" alt="avatar"></p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTM</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, batch_first=False, num_layers=<span class="number">1</span>, bidirectional=False, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        <span class="comment"># input_size=args.hidden_size * 2 = 200,</span></span><br><span class="line">        <span class="comment"># hidden_size=args.hidden_size = 100,</span></span><br><span class="line">        <span class="comment"># bidirectional=True,</span></span><br><span class="line">        <span class="comment"># batch_first=True, </span></span><br><span class="line">        <span class="comment"># dropout=args.dropout = 0.2</span></span><br><span class="line">        super(LSTM, self).__init__()</span><br><span class="line">        self.rnn = nn.LSTM(input_size=input_size,</span><br><span class="line">                           hidden_size=hidden_size,</span><br><span class="line">                           num_layers=num_layers,</span><br><span class="line">                           bidirectional=bidirectional,</span><br><span class="line">                           batch_first=batch_first)</span><br><span class="line">        self.reset_params() <span class="comment"># é‡ç½®å‚æ•°</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_params</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.rnn.num_layers):</span><br><span class="line">            nn.init.orthogonal_(getattr(self.rnn, <span class="string">f'weight_hh_l<span class="subst">&#123;i&#125;</span>'</span>)) <span class="comment"># hidden-hidden weights</span></span><br><span class="line">            <span class="comment"># weight_hh_l&#123;i&#125;ã€weight_ih_l&#123;i&#125;ã€bias_hh_l&#123;i&#125;ã€bias_ih_l&#123;i&#125; éƒ½æ˜¯nn.LSTMæºç é‡Œçš„å‚æ•°</span></span><br><span class="line">            <span class="comment"># getattrå–å‡ºæºç é‡Œå‚æ•°çš„å€¼ï¼Œç”¨nn.init.orthogonal_æ­£äº¤è¿›è¡Œé‡æ–°åˆå§‹åŒ–</span></span><br><span class="line">            <span class="comment"># nn.initåˆå§‹åŒ–æ–¹æ³•çœ‹è¿™ä¸ªé“¾æ¥ï¼šhttps://www.aiuai.cn/aifarm613.html</span></span><br><span class="line">            nn.init.kaiming_normal_(getattr(self.rnn, <span class="string">f'weight_ih_l<span class="subst">&#123;i&#125;</span>'</span>)) <span class="comment"># input-hidden weights</span></span><br><span class="line">            nn.init.constant_(getattr(self.rnn, <span class="string">f'bias_hh_l<span class="subst">&#123;i&#125;</span>'</span>), val=<span class="number">0</span>) <span class="comment"># hidden-hidden bias</span></span><br><span class="line">            nn.init.constant_(getattr(self.rnn, <span class="string">f'bias_ih_l<span class="subst">&#123;i&#125;</span>'</span>), val=<span class="number">0</span>) <span class="comment"># input-hidden bias</span></span><br><span class="line">            getattr(self.rnn, <span class="string">f'bias_hh_l<span class="subst">&#123;i&#125;</span>'</span>).chunk(<span class="number">4</span>)[<span class="number">1</span>].fill_(<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># .chunkçœ‹ä¸‹è¿™ä¸ªé“¾æ¥ï¼šhttps://blog.csdn.net/XuM222222/article/details/92380538</span></span><br><span class="line">            <span class="comment"># .fill_(1),ä¸‹åˆ’çº¿ä»£è¡¨ç›´æ¥æ›¿æ¢ï¼Œçœ‹é“¾æ¥ï¼šhttps://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.fill.html</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.rnn.bidirectional: <span class="comment"># åŒå‘ï¼Œéœ€è¦åˆå§‹åŒ–åå‘çš„å‚æ•°</span></span><br><span class="line">                nn.init.orthogonal_(getattr(self.rnn, <span class="string">f'weight_hh_l<span class="subst">&#123;i&#125;</span>_reverse'</span>))</span><br><span class="line">                nn.init.kaiming_normal_(getattr(self.rnn, <span class="string">f'weight_ih_l<span class="subst">&#123;i&#125;</span>_reverse'</span>))</span><br><span class="line">                nn.init.constant_(getattr(self.rnn, <span class="string">f'bias_hh_l<span class="subst">&#123;i&#125;</span>_reverse'</span>), val=<span class="number">0</span>)</span><br><span class="line">                nn.init.constant_(getattr(self.rnn, <span class="string">f'bias_ih_l<span class="subst">&#123;i&#125;</span>_reverse'</span>), val=<span class="number">0</span>)</span><br><span class="line">                getattr(self.rnn, <span class="string">f'bias_hh_l<span class="subst">&#123;i&#125;</span>_reverse'</span>).chunk(<span class="number">4</span>)[<span class="number">1</span>].fill_(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># xæ˜¯ä¸€ä¸ªå…ƒç»„(c, c_lens)</span></span><br><span class="line">        x, x_len = x</span><br><span class="line">        <span class="comment"># x = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        <span class="comment"># x_len = (batch) ä¸€ä¸ªbatchä¸­æ‰€æœ‰contextæˆ–questionçš„æ ·æœ¬é•¿åº¦</span></span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ä¸‹é¢ä¸€é¡¿æ“ä½œå’Œç¬¬ä¸ƒè¯¾æœºå™¨ç¿»è¯‘çš„ä¸€æ ·ï¼Œ</span></span><br><span class="line">        <span class="comment"># çœ‹ä¸‹è¿™ç¯‡åšå®¢ç†è§£ï¼šhttps://www.cnblogs.com/sbj123456789/p/9834018.html</span></span><br><span class="line">        x_len_sorted, x_idx = torch.sort(x_len, descending=<span class="literal">True</span>)</span><br><span class="line">        x_sorted = x.index_select(dim=<span class="number">0</span>, index=x_idx)</span><br><span class="line">        _, x_ori_idx = torch.sort(x_idx)</span><br><span class="line"></span><br><span class="line">        x_packed = nn.utils.rnn.pack_padded_sequence(x_sorted, x_len_sorted, batch_first=<span class="literal">True</span>)</span><br><span class="line">        x_packed, (h, c) = self.rnn(x_packed)</span><br><span class="line"></span><br><span class="line">        x = nn.utils.rnn.pad_packed_sequence(x_packed, batch_first=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">        x = x.index_select(dim=<span class="number">0</span>, index=x_ori_idx)</span><br><span class="line">        h = h.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>).contiguous().view(<span class="number">-1</span>, h.size(<span class="number">0</span>) * h.size(<span class="number">2</span>)).squeeze()</span><br><span class="line">        h = h.index_select(dim=<span class="number">0</span>, index=x_ori_idx)</span><br><span class="line">        <span class="comment"># x = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        <span class="comment"># h = (1, batch, hidden_size * 2) è¿™ä¸ªç»´åº¦ä¸ç”¨ç®¡</span></span><br><span class="line">        <span class="keyword">return</span> x, h</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features, dropout=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(Linear, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.linear = nn.Linear(in_features=in_features, out_features=out_features)</span><br><span class="line">        <span class="comment"># in_features = hidden_size * 2</span></span><br><span class="line">        <span class="comment"># out_features = hidden_size * 2</span></span><br><span class="line">        <span class="keyword">if</span> dropout &gt; <span class="number">0</span>:</span><br><span class="line">            self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        self.reset_params()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_params</span><span class="params">(self)</span>:</span></span><br><span class="line">        nn.init.kaiming_normal_(self.linear.weight)</span><br><span class="line">        nn.init.constant_(self.linear.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> hasattr(self, <span class="string">'dropout'</span>): <span class="comment"># åˆ¤æ–­selfæœ‰æ²¡æœ‰'dropout'è¿™ä¸ªå‚æ•°ï¼Œè¿”å›boolå€¼</span></span><br><span class="line">            x = self.dropout(x)</span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args.char_dim</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># çœ‹è‹±æ–‡è®ºæ–‡æˆ–è¿™ç¯‡åšå®¢ç†è§£æ¨¡å‹ï¼šhttps://blog.csdn.net/u014665013/article/details/79793395</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiDAF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args, pretrained)</span>:</span></span><br><span class="line">        <span class="comment"># pretrained = data.WORD.vocab.vectors = (108777, 100)</span></span><br><span class="line">        super(BiDAF, self).__init__()</span><br><span class="line">        self.args = args</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. Character Embedding Layer æ˜¯æ¨¡å‹ç¤ºæ„å›¾å·¦è¾¹çš„å±‚çš„åå­—ï¼Œä»ä¸‹å¾€ä¸Š</span></span><br><span class="line">        <span class="comment"># å­—ç¬¦ç¼–ç å±‚</span></span><br><span class="line">        self.char_emb = nn.Embedding(args.char_vocab_size, args.char_dim, padding_idx=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># args.char_vocab_size = 1307ï¼Œargs.char_dim = 8</span></span><br><span class="line">        nn.init.uniform_(self.char_emb.weight, <span class="number">-0.001</span>, <span class="number">0.001</span>)</span><br><span class="line">        <span class="comment"># åˆå§‹åŒ–æƒé‡</span></span><br><span class="line"></span><br><span class="line">        self.char_conv = nn.Conv2d(<span class="number">1</span>, args.char_channel_size, (args.char_dim, args.char_channel_width))</span><br><span class="line">        <span class="comment"># args.char_channel_size = 100 å·ç§¯æ ¸æ•°é‡ </span></span><br><span class="line">        <span class="comment"># (args.char_dim, args.char_channel_width) = (8,5) è¿‡æ»¤å™¨å¤§å°</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. Word Embedding Layer</span></span><br><span class="line">        <span class="comment"># å•è¯ç¼–ç å±‚</span></span><br><span class="line">        <span class="comment"># initialize word embedding with GloVe</span></span><br><span class="line">        self.word_emb = nn.Embedding.from_pretrained(pretrained, freeze=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># åˆå§‹åŒ–è¯å‘é‡æƒé‡ï¼Œç”¨çš„Gloveå‘é‡</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># highway network</span></span><br><span class="line">        <span class="keyword">assert</span> self.args.hidden_size * <span class="number">2</span> == (self.args.char_channel_size + self.args.word_dim)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            setattr(self, <span class="string">f'highway_linear<span class="subst">&#123;i&#125;</span>'</span>,</span><br><span class="line">                    nn.Sequential(Linear(args.hidden_size * <span class="number">2</span>, args.hidden_size * <span class="number">2</span>),</span><br><span class="line">                                  nn.ReLU()))</span><br><span class="line">            <span class="comment"># è®¾ç½®highway_linear0 = nn.Sequential(Linear(args.hidden_size * 2, args.hidden_size * 2)</span></span><br><span class="line">            <span class="comment"># è®¾ç½®highway_linear1 = nn.Sequential(Linear(args.hidden_size * 2, args.hidden_size * 2)</span></span><br><span class="line">            <span class="comment"># args.hidden_size = 100</span></span><br><span class="line">                                </span><br><span class="line">            setattr(self, <span class="string">f'highway_gate<span class="subst">&#123;i&#125;</span>'</span>,</span><br><span class="line">                    nn.Sequential(Linear(args.hidden_size * <span class="number">2</span>, args.hidden_size * <span class="number">2</span>),</span><br><span class="line">                                  nn.Sigmoid()))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. Contextual Embedding Layer</span></span><br><span class="line">        <span class="comment"># ä¸Šä¸‹æ–‡ï¼Œå’Œç­”æ¡ˆåµŒå…¥å±‚ï¼Œç”¨çš„LSTM</span></span><br><span class="line">        <span class="comment"># ä¸‹é¢LSTMå®šä½åˆ°äº†è‡ªå®šä¹‰çš„class LSTM(nn.Module)ã€‚</span></span><br><span class="line">        self.context_LSTM = LSTM(input_size=args.hidden_size * <span class="number">2</span>,</span><br><span class="line">                                 hidden_size=args.hidden_size,</span><br><span class="line">                                 bidirectional=<span class="literal">True</span>,</span><br><span class="line">                                 batch_first=<span class="literal">True</span>,</span><br><span class="line">                                 dropout=args.dropout) </span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. Attention Flow Layer</span></span><br><span class="line">        <span class="comment"># æ³¨æ„åŠ›å±‚</span></span><br><span class="line">        self.att_weight_c = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.att_weight_q = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.att_weight_cq = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5. Modeling Layer</span></span><br><span class="line">        self.modeling_LSTM1 = LSTM(input_size=args.hidden_size * <span class="number">8</span>,</span><br><span class="line">                                   hidden_size=args.hidden_size,</span><br><span class="line">                                   bidirectional=<span class="literal">True</span>,</span><br><span class="line">                                   batch_first=<span class="literal">True</span>,</span><br><span class="line">                                   dropout=args.dropout)</span><br><span class="line"></span><br><span class="line">        self.modeling_LSTM2 = LSTM(input_size=args.hidden_size * <span class="number">2</span>,</span><br><span class="line">                                   hidden_size=args.hidden_size,</span><br><span class="line">                                   bidirectional=<span class="literal">True</span>,</span><br><span class="line">                                   batch_first=<span class="literal">True</span>,</span><br><span class="line">                                   dropout=args.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 6. Output Layer</span></span><br><span class="line">        self.p1_weight_g = Linear(args.hidden_size * <span class="number">8</span>, <span class="number">1</span>, dropout=args.dropout)</span><br><span class="line">        self.p1_weight_m = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>, dropout=args.dropout)</span><br><span class="line">        self.p2_weight_g = Linear(args.hidden_size * <span class="number">8</span>, <span class="number">1</span>, dropout=args.dropout)</span><br><span class="line">        self.p2_weight_m = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>, dropout=args.dropout)</span><br><span class="line"></span><br><span class="line">        self.output_LSTM = LSTM(input_size=args.hidden_size * <span class="number">2</span>,</span><br><span class="line">                                hidden_size=args.hidden_size,</span><br><span class="line">                                bidirectional=<span class="literal">True</span>,</span><br><span class="line">                                batch_first=<span class="literal">True</span>,</span><br><span class="line">                                dropout=args.dropout)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(p=args.dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, batch)</span>:</span></span><br><span class="line">        <span class="comment"># batché‡Œé¢æœ‰'id','s_idx','e_idx', 'c_word','c_char','q_word', 'q_char'æ•°æ®</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> More memory-efficient architecture</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">char_emb_layer</span><span class="params">(x)</span>:</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            :param x: (batch, seq_len, word_len)</span></span><br><span class="line"><span class="string">            :return: (batch, seq_len, char_channel_size)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            <span class="comment"># x = (batch_sizes,seq_len,word_len)</span></span><br><span class="line">            batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">            x = self.dropout(self.char_emb(x))</span><br><span class="line">            <span class="comment"># (batch, seq_len, word_len, char_dim)</span></span><br><span class="line">            x = x.view(<span class="number">-1</span>, self.args.char_dim, x.size(<span class="number">2</span>)).unsqueeze(<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># (batch * seq_len, 1, char_dim, word_len) 1æ˜¯è¾“å…¥çš„channelçš„ç»´åº¦</span></span><br><span class="line">            x = self.char_conv(x).squeeze()</span><br><span class="line">            <span class="comment"># (batch * seq_len, char_channel_size, 1, conv_len) -&gt; </span></span><br><span class="line">            <span class="comment"># (batch * seq_len, char_channel_size, conv_len) conv_lenä¸ç”¨ç®¡ï¼Œä¸‹ä¸€æ­¥éƒ½ä¼špoolæ‰</span></span><br><span class="line">            x = F.max_pool1d(x, x.size(<span class="number">2</span>)).squeeze()</span><br><span class="line">            <span class="comment"># (batch * seq_len, char_channel_size, 1) -&gt; (batch * seq_len, char_channel_size)</span></span><br><span class="line">            x = x.view(batch_size, <span class="number">-1</span>, self.args.char_channel_size)</span><br><span class="line">            <span class="comment"># (batch, seq_len, char_channel_size)</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">highway_network</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            :param x1: (batch, seq_len, char_channel_size)</span></span><br><span class="line"><span class="string">            :param x2: (batch, seq_len, word_dim)</span></span><br><span class="line"><span class="string">            :return: (batch, seq_len, hidden_size * 2)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            </span><br><span class="line">            x = torch.cat([x1, x2], dim=<span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># x = (batch, seq_len, char_channel_size + word_dim)</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">                h = getattr(self, <span class="string">f'highway_linear<span class="subst">&#123;i&#125;</span>'</span>)(x) <span class="comment"># è°ƒç”¨Linearçš„forwardæ–¹æ³•</span></span><br><span class="line">                <span class="comment"># h = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">                g = getattr(self, <span class="string">f'highway_gate<span class="subst">&#123;i&#125;</span>'</span>)(x)</span><br><span class="line">                <span class="comment"># g = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">                x = g * h + (<span class="number">1</span> - g) * x</span><br><span class="line">            <span class="comment"># (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">att_flow_layer</span><span class="params">(c, q)</span>:</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            :param c: (batch, c_len, hidden_size * 2)</span></span><br><span class="line"><span class="string">            :param q: (batch, q_len, hidden_size * 2)</span></span><br><span class="line"><span class="string">            :return: (batch, c_len, q_len)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            c_len = c.size(<span class="number">1</span>)</span><br><span class="line">            q_len = q.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (batch, c_len, q_len, hidden_size * 2)</span></span><br><span class="line">            <span class="comment">#c_tiled = c.unsqueeze(2).expand(-1, -1, q_len, -1)</span></span><br><span class="line">            <span class="comment"># (batch, c_len, q_len, hidden_size * 2)</span></span><br><span class="line">            <span class="comment">#q_tiled = q.unsqueeze(1).expand(-1, c_len, -1, -1)</span></span><br><span class="line">            <span class="comment"># (batch, c_len, q_len, hidden_size * 2)</span></span><br><span class="line">            <span class="comment">#cq_tiled = c_tiled * q_tiled</span></span><br><span class="line">            <span class="comment">#cq_tiled = c.unsqueeze(2).expand(-1, -1, q_len, -1) * q.unsqueeze(1).expand(-1, c_len, -1, -1)</span></span><br><span class="line"><span class="comment">#        # 4. Attention Flow Layer</span></span><br><span class="line"><span class="comment">#         # æ³¨æ„åŠ›å±‚</span></span><br><span class="line"><span class="comment">#         self.att_weight_c = Linear(args.hidden_size * 2, 1)</span></span><br><span class="line"><span class="comment">#         self.att_weight_q = Linear(args.hidden_size * 2, 1)</span></span><br><span class="line"><span class="comment">#         self.att_weight_cq = Linear(args.hidden_size * 2, 1)</span></span><br><span class="line">            cq = []</span><br><span class="line">            <span class="comment"># 1ã€ç›¸ä¼¼åº¦è®¡ç®—æ–¹å¼ï¼Œçœ‹ä¸‹è¿™ç¯‡åšå®¢ç†è§£ï¼šhttps://blog.csdn.net/u014665013/article/details/79793395</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(q_len):</span><br><span class="line">                qi = q.select(<span class="number">1</span>, i).unsqueeze(<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># (batch, 1, hidden_size * 2)</span></span><br><span class="line">                <span class="comment"># .selectçœ‹è¿™ä¸ªï¼šhttps://blog.csdn.net/hungryof/article/details/51802829</span></span><br><span class="line">                ci = self.att_weight_cq(c * qi).squeeze()</span><br><span class="line">                <span class="comment"># (batch, c_len, 1)</span></span><br><span class="line">                cq.append(ci)</span><br><span class="line">            cq = torch.stack(cq, dim=<span class="number">-1</span>) </span><br><span class="line">            <span class="comment"># (batch, c_len, q_len) cpæ˜¯å…±äº«ç›¸ä¼¼åº¦çŸ©é˜µ</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 2ã€è®¡ç®—å¯¹æ¯ä¸€ä¸ª context word è€Œè¨€å“ªäº› query words å’Œå®ƒæœ€ç›¸å…³ã€‚</span></span><br><span class="line">            <span class="comment"># context-to-query attention(C2Q):</span></span><br><span class="line">            s = self.att_weight_c(c).expand(<span class="number">-1</span>, <span class="number">-1</span>, q_len) + \</span><br><span class="line">                self.att_weight_q(q).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).expand(<span class="number">-1</span>, c_len, <span class="number">-1</span>) + cq</span><br><span class="line">            <span class="comment"># (batch, c_len, q_len) </span></span><br><span class="line">            a = F.softmax(s, dim=<span class="number">2</span>) </span><br><span class="line">            <span class="comment"># (batch, c_len, q_len)</span></span><br><span class="line">            c2q_att = torch.bmm(a, q) </span><br><span class="line">            <span class="comment"># (batch, c_len, q_len) * (batch, q_len, hidden_size * 2) -&gt; (batch, c_len, hidden_size * 2)</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 3ã€è®¡ç®—å¯¹æ¯ä¸€ä¸ª query word è€Œè¨€å“ªäº› context words å’Œå®ƒæœ€ç›¸å…³</span></span><br><span class="line">            <span class="comment"># query-to-context attention(Q2C):</span></span><br><span class="line">            b = F.softmax(torch.max(s, dim=<span class="number">2</span>)[<span class="number">0</span>], dim=<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># (batch, 1, c_len)</span></span><br><span class="line">            q2c_att = torch.bmm(b, c).squeeze()</span><br><span class="line">            <span class="comment"># (batch, 1, c_len) * (batch, c_len, hidden_size * 2) -&gt; (batch, hidden_size * 2)</span></span><br><span class="line">            q2c_att = q2c_att.unsqueeze(<span class="number">1</span>).expand(<span class="number">-1</span>, c_len, <span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># (batch, c_len, hidden_size * 2) (tiled)</span></span><br><span class="line">            <span class="comment"># q2c_att = torch.stack([q2c_att] * c_len, dim=1)</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 4ã€æœ€åå°†context embeddingå’ŒC2Qã€Q2Cçš„ç»“æœï¼ˆä¸‰ä¸ªçŸ©é˜µï¼‰æ‹¼æ¥èµ·æ¥</span></span><br><span class="line">            x = torch.cat([c, c2q_att, c * c2q_att, c * q2c_att], dim=<span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># (batch, c_len, hidden_size * 8)</span></span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">output_layer</span><span class="params">(g, m, l)</span>:</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            :param g: (batch, c_len, hidden_size * 8)</span></span><br><span class="line"><span class="string">            :param m: (batch, c_len ,hidden_size * 2)</span></span><br><span class="line"><span class="string">             #  l = c_lens</span></span><br><span class="line"><span class="string">            :return: p1: (batch, c_len), p2: (batch, c_len)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            p1 = (self.p1_weight_g(g) + self.p1_weight_m(m)).squeeze()</span><br><span class="line">            <span class="comment"># (batch, c_len)</span></span><br><span class="line">            m2 = self.output_LSTM((m, l))[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># (batch, c_len, hidden_size * 2)</span></span><br><span class="line">            p2 = (self.p2_weight_g(g) + self.p2_weight_m(m2)).squeeze()</span><br><span class="line">            <span class="comment"># (batch, c_len)</span></span><br><span class="line">            <span class="keyword">return</span> p1, p2</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. Character Embedding Layer</span></span><br><span class="line">        <span class="comment"># ä»¤:ä¸€ä¸ªbatchä¸­å•è¯æ•°é‡æœ€å¤šçš„æ ·æœ¬é•¿åº¦ä¸ºseq_len</span></span><br><span class="line">        <span class="comment"># ä»¤:ä¸€ä¸ªbatchä¸­æŸä¸ªå•è¯é•¿åº¦æœ€é•¿çš„å•è¯é•¿åº¦ä¸ºword_len</span></span><br><span class="line">        </span><br><span class="line">        c_char = char_emb_layer(batch.c_char) </span><br><span class="line">        <span class="comment"># batch.c_char = (batch,seq_len,word_len) åä¸¤ä¸ªç»´åº¦å¯¹åº”context</span></span><br><span class="line">        <span class="comment"># c_char = (batch, seq_len, char_channel_size)</span></span><br><span class="line"></span><br><span class="line">        q_char = char_emb_layer(batch.q_char)</span><br><span class="line">        <span class="comment"># batch.c_char = (batch,seq_len,word_len) åä¸¤ä¸ªç»´åº¦å¯¹åº”question</span></span><br><span class="line">        <span class="comment"># c_char = (batch, seq_len, char_channel_size)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. Word Embedding Layer</span></span><br><span class="line">        c_word = self.word_emb(batch.c_word[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># batch.c_word[0] = (batch,seq_len) åä¸€ä¸ªç»´åº¦å¯¹åº”context</span></span><br><span class="line">        <span class="comment"># c_word = (batch, seq_len, word_dim) word_dimæ˜¯Gloveè¯å‘é‡ç»´åº¦</span></span><br><span class="line">        q_word = self.word_emb(batch.q_word[<span class="number">0</span>]) </span><br><span class="line">        <span class="comment"># batch.q_word[0] = (batch,seq_len) åä¸€ä¸ªç»´åº¦å¯¹åº”question</span></span><br><span class="line">        <span class="comment"># q_word = (batch, seq_len, word_dim)</span></span><br><span class="line">        c_lens = batch.c_word[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># c_lensï¼šä¸€ä¸ªbatchä¸­æ‰€æœ‰contextçš„æ ·æœ¬é•¿åº¦</span></span><br><span class="line">        q_lens = batch.q_word[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># q_lensï¼šä¸€ä¸ªbatchä¸­æ‰€æœ‰questionçš„æ ·æœ¬é•¿åº¦</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Highway network</span></span><br><span class="line">        c = highway_network(c_char, c_word)</span><br><span class="line">        <span class="comment"># c = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        q = highway_network(q_char, q_word)</span><br><span class="line">        <span class="comment"># q = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. Contextual Embedding Layer</span></span><br><span class="line">        c = self.context_LSTM((c, c_lens))[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># c = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        q = self.context_LSTM((q, q_lens))[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># q = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 4. Attention Flow Layer</span></span><br><span class="line">        g = att_flow_layer(c, q)</span><br><span class="line">        <span class="comment"># (batch, c_len, hidden_size * 8)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 5. Modeling Layer</span></span><br><span class="line">        m = self.modeling_LSTM2((self.modeling_LSTM1((g, c_lens))[<span class="number">0</span>], c_lens))[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># self.modeling_LSTM1((g, c_lens))[0] = (batch, c_len, hidden_size * 2) # 2å› ä¸ºæ˜¯åŒå‘</span></span><br><span class="line">        <span class="comment"># m = (batch, c_len, hidden_size * 2) 2å› ä¸ºæ˜¯åŒå‘</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 6. Output Layer</span></span><br><span class="line">        p1, p2 = output_layer(g, m, c_lens) <span class="comment"># é¢„æµ‹å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®</span></span><br><span class="line">        <span class="comment"># (batch, c_len), (batch, c_len)</span></span><br><span class="line">        <span class="keyword">return</span> p1, p2</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand((<span class="number">2</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line">print(x)</span><br><span class="line">y = x.select(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(len(data.WORD.vocab)) <span class="comment"># 108777ä¸ªå•è¯</span></span><br><span class="line">print(data.WORD.vocab.vectors.shape) <span class="comment"># è¯å‘é‡ç»´åº¦</span></span><br><span class="line"></span><br><span class="line">print(data.WORD.vocab.itos[:<span class="number">50</span>]) <span class="comment"># å‰50ä¸ªè¯é¢‘æœ€é«˜çš„å•è¯</span></span><br><span class="line">print(<span class="string">"------"</span>*<span class="number">10</span>)</span><br><span class="line">print(list(data.WORD.vocab.stoi.items())[<span class="number">0</span>:<span class="number">50</span>]) <span class="comment"># å¯¹åº”çš„ç´¢å¼•</span></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(len(data.CHAR.vocab)) <span class="comment"># 1307ä¸ªå•è¯</span></span><br><span class="line">print(data.CHAR.vocab.itos[:<span class="number">50</span>]) <span class="comment"># 108777ä¸ªå•è¯</span></span><br><span class="line">print(<span class="string">"------"</span>*<span class="number">10</span>)</span><br><span class="line">print(list(data.CHAR.vocab.stoi.items())[<span class="number">0</span>:<span class="number">50</span>]) <span class="comment"># å¯¹åº”çš„ç´¢å¼•</span></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">f"cuda:<span class="subst">&#123;args.gpu&#125;</span>"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">model = BiDAF(args, data.WORD.vocab.vectors).to(device)</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EMA</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mu)</span>:</span></span><br><span class="line">        <span class="comment"># mu = args.exp_decay_rate = 0.999</span></span><br><span class="line">        self.mu = mu</span><br><span class="line">        self.shadow = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">register</span><span class="params">(self, name, val)</span>:</span></span><br><span class="line">        <span class="comment"># name:å„ä¸ªå‚æ•°å±‚çš„åå­—, param.dataï¼›å‚æ•°å±‚çš„æ•°æ®</span></span><br><span class="line">        self.shadow[name] = val.clone() <span class="comment"># å»ºç«‹å­—å…¸</span></span><br><span class="line">        <span class="comment"># clone()å¾—åˆ°çš„Tensorä¸ä»…æ‹·è´äº†åŸå§‹çš„valueï¼Œè€Œä¸”ä¼šè®¡ç®—æ¢¯åº¦ä¼ æ’­ä¿¡æ¯ï¼Œcopy_()åªæ‹·è´æ•°å€¼</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.shadow[name]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, name, x)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> name <span class="keyword">in</span> self.shadow</span><br><span class="line">        new_average = (<span class="number">1.0</span> - self.mu) * x + self.mu * self.shadow[name]</span><br><span class="line">        self.shadow[name] = new_average.clone()</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(model, ema, args, data)</span>:</span></span><br><span class="line">    device = torch.device(<span class="string">f"cuda:<span class="subst">&#123;args.gpu&#125;</span>"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    answers = dict()</span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    backup_params = EMA(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">            backup_params.register(name, param.data) <span class="comment"># é‡æ–°å»ºç«‹å­—å…¸</span></span><br><span class="line">            param.data.copy_(ema.get(name))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.set_grad_enabled(<span class="literal">False</span>):</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> iter(data.dev_iter):</span><br><span class="line">            p1, p2 = model(batch)</span><br><span class="line">            print(p1.shape,p2.shape)</span><br><span class="line">            print(batch.s_idx,batch.e_idx)</span><br><span class="line">            batch_loss = criterion(p1, batch.s_idx<span class="number">-1</span>) + criterion(p2, batch.e_idx<span class="number">-1</span>)</span><br><span class="line">            print(<span class="string">"batch_loss"</span>,batch_loss)</span><br><span class="line">            print(<span class="string">"----"</span>*<span class="number">40</span>)</span><br><span class="line">            loss += batch_loss.item()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (batch, c_len, c_len)</span></span><br><span class="line">            batch_size, c_len = p1.size()</span><br><span class="line">            ls = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">            mask = (torch.ones(c_len, c_len) * float(<span class="string">'-inf'</span>)).to(device).tril(<span class="number">-1</span>).unsqueeze(<span class="number">0</span>).expand(batch_size, <span class="number">-1</span>, <span class="number">-1</span>)</span><br><span class="line">            score = (ls(p1).unsqueeze(<span class="number">2</span>) + ls(p2).unsqueeze(<span class="number">1</span>)) + mask</span><br><span class="line">            score, s_idx = score.max(dim=<span class="number">1</span>)</span><br><span class="line">            score, e_idx = score.max(dim=<span class="number">1</span>)</span><br><span class="line">            s_idx = torch.gather(s_idx, <span class="number">1</span>, e_idx.view(<span class="number">-1</span>, <span class="number">1</span>)).squeeze()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">                id = batch.id[i]</span><br><span class="line">                answer = batch.c_word[<span class="number">0</span>][i][s_idx[i]:e_idx[i]+<span class="number">1</span>]</span><br><span class="line">                answer = <span class="string">' '</span>.join([data.WORD.vocab.itos[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> answer])</span><br><span class="line">                answers[id] = answer</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                param.data.copy_(backup_params.get(name))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(args.prediction_file, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        print(json.dumps(answers), file=f)</span><br><span class="line"></span><br><span class="line">    results = evaluate.main(args)</span><br><span class="line">    <span class="keyword">return</span> loss, results[<span class="string">'exact_match'</span>], results[<span class="string">'f1'</span>]</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    print(name)</span><br><span class="line">    print(param.requires_grad)</span><br><span class="line">    print(param.data.shape)</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">strftime(&apos;%H:%M:%S&apos;, gmtime())</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">iterator = data.train_iter</span><br><span class="line">n= <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    print(<span class="string">"j="</span>,j)</span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(iterator):</span><br><span class="line">        print(<span class="string">"å½“å‰epoch"</span>,int(iterator.epoch))</span><br><span class="line">        print(<span class="string">"-----"</span>*<span class="number">10</span>)</span><br><span class="line">        print(i)</span><br><span class="line">        print(batch)</span><br><span class="line">        n+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> n&gt;<span class="number">3</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(args, data)</span>:</span></span><br><span class="line">    device = torch.device(<span class="string">f"cuda:<span class="subst">&#123;args.gpu&#125;</span>"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">    model = BiDAF(args, data.WORD.vocab.vectors).to(device) <span class="comment"># å®šä¹‰ä¸»æ¨¡å‹ç±»å®ä¾‹</span></span><br><span class="line"></span><br><span class="line">    ema = EMA(args.exp_decay_rate) <span class="comment"># args.exp_decay_rate = 0.999</span></span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters(): </span><br><span class="line">        <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">            ema.register(name, param.data) <span class="comment"># å‚æ•°åå­—å’Œå¯¹åº”çš„å‚æ•°æ•°æ®å½¢æˆå­—å…¸</span></span><br><span class="line">    parameters = filter(<span class="keyword">lambda</span> p: p.requires_grad, model.parameters())</span><br><span class="line">    <span class="comment"># p.requires_grad = True or False ä¿ç•™æœ‰æ¢¯åº¦çš„å‚æ•°</span></span><br><span class="line">    optimizer = optim.Adadelta(parameters, lr=args.learning_rate)</span><br><span class="line">    <span class="comment"># args.learning_rate = 0.5,ä¼˜åŒ–å™¨é€‰ç”¨Adadelta</span></span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    <span class="comment"># äº¤å‰ç†µæŸå¤±</span></span><br><span class="line"></span><br><span class="line">    writer = SummaryWriter(log_dir=<span class="string">'runs/'</span> + args.model_time)</span><br><span class="line">    <span class="comment"># args.model_time = strftime('%H:%M:%S', gmtime()) æ–‡ä»¶å¤¹å‘½åä¸ºå†™å…¥æ–‡ä»¶çš„å½“åœ°æ—¶é—´</span></span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line">    loss, last_epoch = <span class="number">0</span>, <span class="number">-1</span></span><br><span class="line">    max_dev_exact, max_dev_f1 = <span class="number">-1</span>, <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    iterator = data.train_iter</span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(iterator):</span><br><span class="line">        present_epoch = int(iterator.epoch) </span><br><span class="line">        <span class="comment">#print("å½“å‰epoch",present_epoch)# è¿™ä¸ªæˆ‘æ‰“å°äº†ä¸‹ï¼Œä¸€ç›´æ˜¯0ï¼Œè§‰å¾—æœ‰é—®é¢˜</span></span><br><span class="line">        <span class="keyword">if</span> present_epoch == args.epoch:</span><br><span class="line">            <span class="comment"># args.epoch=12</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> present_epoch &gt; last_epoch:</span><br><span class="line">            print(<span class="string">'epoch:'</span>, present_epoch + <span class="number">1</span>)</span><br><span class="line">        last_epoch = present_epoch</span><br><span class="line"></span><br><span class="line">        p1, p2 = model(batch)</span><br><span class="line">        <span class="comment"># (batch, c_len), (batch, c_len)</span></span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        batch_loss = criterion(p1, batch.s_idx) + criterion(p2, batch.e_idx)</span><br><span class="line">        <span class="comment"># æœ€åçš„ç›®æ ‡å‡½æ•°ï¼šbatch.s_idxæ˜¯ç­”æ¡ˆå¼€å§‹çš„ä½ç½®ï¼Œbatch.e_idxæ˜¯ç­”æ¡ˆç»“æŸçš„ä½ç½®</span></span><br><span class="line">        loss += batch_loss.item()</span><br><span class="line">        batch_loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                ema.update(name, param.data) <span class="comment"># æ›´æ–°è®­ç»ƒå®Œåçš„çš„å‚æ•°æ•°æ®</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % args.print_freq == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"i"</span>,i)</span><br><span class="line">            dev_loss, dev_exact, dev_f1 = test(model, ema, args, data)</span><br><span class="line">            c = (i + <span class="number">1</span>) // args.print_freq</span><br><span class="line"></span><br><span class="line">            writer.add_scalar(<span class="string">'loss/train'</span>, loss, c)</span><br><span class="line">            writer.add_scalar(<span class="string">'loss/dev'</span>, dev_loss, c)</span><br><span class="line">            writer.add_scalar(<span class="string">'exact_match/dev'</span>, dev_exact, c)</span><br><span class="line">            writer.add_scalar(<span class="string">'f1/dev'</span>, dev_f1, c)</span><br><span class="line">            print(<span class="string">f'train loss: <span class="subst">&#123;loss:<span class="number">.3</span>f&#125;</span> / dev loss: <span class="subst">&#123;dev_loss:<span class="number">.3</span>f&#125;</span>'</span></span><br><span class="line">                  <span class="string">f' / dev EM: <span class="subst">&#123;dev_exact:<span class="number">.3</span>f&#125;</span> / dev F1: <span class="subst">&#123;dev_f1:<span class="number">.3</span>f&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> dev_f1 &gt; max_dev_f1:</span><br><span class="line">                max_dev_f1 = dev_f1</span><br><span class="line">                max_dev_exact = dev_exact</span><br><span class="line">                best_model = copy.deepcopy(model)</span><br><span class="line"></span><br><span class="line">            loss = <span class="number">0</span></span><br><span class="line">            model.train()</span><br><span class="line"></span><br><span class="line">    writer.close()</span><br><span class="line"><span class="comment">#     print(f'max dev EM: &#123;max_dev_exact:.3f&#125; / max dev F1: &#123;max_dev_f1:.3f&#125;')</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> best_model</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(&apos;training start!&apos;)</span><br><span class="line">best_model = train(args, data)</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/pytorch/pytorch/issues/4144</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ä»£ç æ˜¯åœ¨github&lt;a href=&quot;https://github.com/galsang/BiDAF-pytorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;BiDAF-pytorch&lt;/a&gt;ä¸Šä¸‹è½½çš„ï¼Œæˆ‘æŠŠä»£ç å¼„æˆäº†ä¸‹é¢jupyter notebo
      
    
    </summary>
    
      <category term="NLP" scheme="http://mmyblog.cn/categories/NLP/"/>
    
    
      <category term="SQuAD-BiDAF" scheme="http://mmyblog.cn/tags/SQuAD-BiDAF/"/>
    
  </entry>
  
  <entry>
    <title>NLPä¸­çš„ConvNet</title>
    <link href="http://mmyblog.cn/2020/04/15/NLP%E4%B8%AD%E7%9A%84ConvNet/"/>
    <id>http://mmyblog.cn/2020/04/15/NLPä¸­çš„ConvNet/</id>
    <published>2020-04-15T00:21:14.000Z</published>
    <updated>2020-06-09T01:27:40.076Z</updated>
    
    <content type="html"><![CDATA[<p>â€‹        NLP/AIæ˜¯è¿‘å‡ å¹´æ¥é£é€Ÿå‘å±•çš„é¢†åŸŸï¼Œå¾ˆå¤šçš„æ¨¡å‹å’Œç®—æ³•åªèƒ½åœ¨è®ºæ–‡ã€è®²ä¹‰å’Œåšå®¢ä¸­æ‰¾åˆ°ï¼Œè€Œä¸ä¼šå‡ºç°åœ¨ä»»ä½•çš„æ•™ç§‘ä¹¦ä¸­ã€‚å‡¡æ˜¯è¯¾ç¨‹ä¸­æåˆ°çš„è®ºæ–‡ï¼Œå¤§å®¶éƒ½èƒ½å¤Ÿé˜…è¯»ä¸€éã€‚å¯¹äºé‡è¦çš„è®ºæ–‡ï¼ˆæˆ‘ä¼šç‰¹åˆ«æ ‡æ˜æˆ–è€…åœ¨è¯¾ä¸Šå¼ºè°ƒï¼Œä¾‹å¦‚BERT, transformerç­‰ï¼‰ï¼Œå»ºè®®è®¤çœŸé˜…è¯»ï¼Œææ¸…æ¥šæ¨¡å‹çš„ç»†èŠ‚ã€‚å…¶ä½™çš„è®ºæ–‡ï¼Œå»ºè®®è‡³å°‘èƒ½å¤Ÿé˜…è¯»ï¼Œäº†è§£è®ºæ–‡çš„åˆ›æ–°ç‚¹å’Œä¸­å¿ƒæ€æƒ³ã€‚</p><h3 id="å¦‚ä½•è¯»è®ºæ–‡ï¼Ÿ"><a href="#å¦‚ä½•è¯»è®ºæ–‡ï¼Ÿ" class="headerlink" title="å¦‚ä½•è¯»è®ºæ–‡ï¼Ÿ"></a>å¦‚ä½•è¯»è®ºæ–‡ï¼Ÿ</h3><p>å¯¹äºå¦‚ä½•è¯»è®ºæ–‡ï¼Œæ¯ä¸ªäººæœ‰è‡ªå·±ä¸åŒçš„æ–¹æ³•ã€‚æˆ‘çš„å»ºè®®æ˜¯ï¼š</p><ul><li><p>æœ€å¿«è¯»è®ºæ–‡çš„æ–¹æ³•ï¼šä¸Šå„å¤§ä¸­æ–‡ç½‘ç«™ï¼ˆçŸ¥ä¹ï¼ŒCSDNï¼Œå¾®ä¿¡å…¬ä¼—å·ç­‰ï¼‰å¯»æ‰¾è¯¥è®ºæ–‡çš„ä¸­æ–‡è§£è¯»ï¼Œå¤§éƒ¨åˆ†æœ‰åçš„è®ºæ–‡éƒ½ä¼šæœ‰å¾ˆå¤šçš„è§£è¯»æ–‡ç« ã€‚</p></li><li><p>è¯»è®ºæ–‡æ—¶å€™çš„é‡ç‚¹ç« èŠ‚ï¼šå¤§éƒ¨åˆ†NLPçš„è®ºæ–‡çš„ä¸»è¦ä¸¤ä¸ªç« èŠ‚æ˜¯ï¼ŒModel, Experimentsã€‚åŸºæœ¬ä¸Šçœ‹å®Œè¿™ä¸¤ä¸ªç« èŠ‚å°±äº†è§£äº†è®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³ã€‚å¦å¤–æˆ‘ä¹Ÿä¼šç‰¹åˆ«å…³æ³¨è®ºæ–‡ä½¿ç”¨çš„<strong>æ•°æ®</strong>ï¼Œå› ä¸ºè¿™äº›æ•°æ®æˆ‘ä»¬å¯èƒ½å¯ä»¥æ‹¿æ¥ç”¨åœ¨è‡ªå·±çš„é¡¹ç›®ä¸Šã€‚</p></li><li><p>å¦‚æœæƒ³è¦æ›´åŠ æ·±å…¥åœ°å­¦ä¹ è¯¥è®ºæ–‡çš„å†…å®¹ï¼Œå¯ä»¥ä¸Šç½‘å»å¯»æ‰¾ä¸è¯¥è®ºæ–‡ç›¸å…³çš„èµ„æ–™ï¼ŒåŒ…æ‹¬ä½œè€…çš„ä¸ªäººä¸»é¡µï¼Œä»–/å¥¹å‘å¸ƒçš„è®ºæ–‡slidesï¼Œè®ºæ–‡ä»£ç ç­‰ç­‰ã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œå¦‚æœä½ æƒ³è¦å¤ç°è®ºæ–‡çš„ç»“æœï¼Œä½†æ˜¯åœ¨ç½‘ä¸Šæ‰¾ä¸åˆ°ä»£ç ï¼Œä¸è¦æ€¥äºè‡ªå·±å®ç°ï¼Œå¯ä»¥å†™é‚®ä»¶ç»™è®ºæ–‡çš„ç¬¬ä¸€ä½œè€…ä¸é€šè®¯ä½œè€…ï¼ˆæœ€åä¸€ä½ï¼‰ï¼Œç¤¼è²Œåœ°è¯¢é—®å¯¹æ–¹æ˜¯å¦å¯ä»¥å°†æºç å’Œæ•°æ®æä¾›ç»™ä½ ï¼Œç†è®ºä¸Šè®ºæ–‡ä½œè€…æœ‰ä¹‰åŠ¡å…¬å¼€è‡ªå·±çš„ä»£ç å’Œæ•°æ®ã€‚å¦‚æœæ²¡æœ‰ä»£ç å¯ä»¥å…¬å¼€ï¼Œè¦ä¸ç„¶å¯èƒ½æ˜¯è®ºæ–‡å¤ªæ–°ï¼Œè¿˜æ²¡æœ‰å…¬å¼€ä»£ç ï¼Œè¦ä¸ç„¶å¯èƒ½æ˜¯è®ºæ–‡ä¸­æŸäº›éƒ¨åˆ†çš„å®ç°æœ‰å›°éš¾ï¼Œä¸é‚£ä¹ˆå®¹æ˜“å¤ç°ã€‚</p></li><li><p>å¦å¤–å¦‚æœä½ æƒ³è¦æ›´æ·±å…¥åœ°å­¦ä¹ è¿™ä¸ªè®ºæ–‡ç›¸å…³çš„é¢†åŸŸï¼Œå¯ä»¥è¯»ä¸€ä¸‹Related Workä¸­æåˆ°çš„ä¸€äº›æ–‡ç« ã€‚</p></li></ul><h1 id="NLPä¸­çš„-ConvNet-ç²¾é€‰è®ºæ–‡"><a href="#NLPä¸­çš„-ConvNet-ç²¾é€‰è®ºæ–‡" class="headerlink" title="NLPä¸­çš„ ConvNet ç²¾é€‰è®ºæ–‡"></a>NLPä¸­çš„ ConvNet ç²¾é€‰è®ºæ–‡</h1><p>MNIST</p><p>convolutional kernel: local feature detector</p><p>å›¾åƒï¼š</p><ul><li><p>å¹³ç§»ä¸å˜æ€§</p></li><li><p>pixel features</p></li></ul><p>Hinton</p><ul><li><p>Capsule Network</p></li><li><p>ConvNetçš„ç¼ºé™·ï¼š</p></li><li><p>æ²¡æœ‰å¤„ç†æ—‹è½¬ä¸å˜æ€§</p></li><li><p>å›¾ç‰‡å¤§å°å‘ç”Ÿæ”¹å˜</p></li></ul><p>æ–‡æœ¬</p><ul><li><p>ngram</p></li><li><p>ngram ä¹‹é—´çš„è”ç³» n-n-gram</p></li></ul><p>æ›¾ç»æœ‰ä¸€æ®µæ—¶é—´ç”±äº<strong>Yann Lecun</strong>åŠ å…¥Facebook AI Researchæ‹…ä»»Directorçš„å…³ç³»ï¼ŒFBæŠ•å…¥äº†å¾ˆå¤šçš„ç²¾åŠ›ç ”å‘æŠŠConvNetç”¨åœ¨Texté—®é¢˜ä¸Šã€‚ConvNetä¸»æ‰“çš„ä¸€ä¸ªå¼ºé¡¹å°±æ˜¯é€Ÿåº¦æ¯”RNNå¿«ï¼ŒEncoderå¯ä»¥å¹¶è¡Œã€‚åæ¥å¯èƒ½æ˜¯ç”±äºGoogleçš„Transformerå¼€å§‹ç»Ÿæ²»è¿™ä¸ªé¢†åŸŸï¼Œå¯¼è‡´å¤§å®¶æ…¢æ…¢åœ¨ConvNetä¸Šçš„å…³æ³¨åº¦è¶Šæ¥è¶Šå°ã€‚</p><p>transformer (BERT) å°±æ˜¯ filter size ä¸º 1 çš„ convolutional neural network ã€‚</p><p>ä¸è¿‡è¿™ä¸€ç³»åˆ—ä»¥ConvNetä¸ºæ ¸å¿ƒçš„NLPæ¨¡å‹ä¾ç„¶éå¸¸å€¼å¾—å­¦ä¹ ã€‚ConvNetçš„ä¸€ä¸ªé•¿å¤„åœ¨äºå®ƒå¯ä»¥å¾ˆè‡ªç„¶åœ°å¾—åˆ° <strong>ngram</strong> çš„è¡¨ç¤ºã€‚ç”±äºNLPæœ€è¿‘çš„è¿›å±•æ—¥æ–°æœˆå¼‚ï¼Œå¯èƒ½å‡ å¤©æˆ–è€…å‡ ä¸ªæœˆä¹‹ååˆæœ‰ä¸€ç³»åˆ—åŸºäºConvNetçš„æ¨¡å‹é‡ç™»SOTAï¼Œè°çŸ¥é“å‘¢ã€‚</p><p>å¯¹äºä¸äº†è§£ä»€ä¹ˆæ˜¯Convolutional Neural Networkçš„åŒå­¦ï¼Œå»ºè®®é˜…è¯»æ–¯å¦ç¦cs231çš„è¯¾ç¨‹èµ„æ–™ <a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="noopener">http://cs231n.github.io/convolutional-networks/</a> ç½‘ä¸Šçš„ä¸­æ–‡ç¿»è¯‘å¾ˆå¤šï¼Œä¾‹å¦‚ï¼š<a href="https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit</a></p><h2 id="Yoon-Kim-Convolutional-Neural-Networks-for-Sentence-Classification"><a href="#Yoon-Kim-Convolutional-Neural-Networks-for-Sentence-Classification" class="headerlink" title="Yoon Kim Convolutional Neural Networks for Sentence Classification"></a>Yoon Kim <a href="https://aclweb.org/anthology/D14-1181" target="_blank" rel="noopener">Convolutional Neural Networks for Sentence Classification</a></h2><p><a href="https://aclweb.org/anthology/D14-1181" target="_blank" rel="noopener">https://aclweb.org/anthology/D14-1181</a></p><p>è¿™ç¯‡æ–‡ç« é¦–æ¬¡æå‡ºäº†åœ¨textä¸Šä½¿ç”¨convolutional networkï¼Œå¹¶ä¸”å–å¾—äº†ä¸é”™çš„æ•ˆæœã€‚åç»­å¾ˆå¤šæŠŠConvNetç”¨åœ¨NLPä»»åŠ¡ä¸Šéƒ½æ˜¯åŸºäºè¿™ç¯‡è®ºæ–‡çš„æ¨¡å‹æ”¹è¿›ã€‚</p><h3 id="æ¨¡å‹æ¶æ„å›¾"><a href="#æ¨¡å‹æ¶æ„å›¾" class="headerlink" title="æ¨¡å‹æ¶æ„å›¾"></a>æ¨¡å‹æ¶æ„å›¾</h3><p><img src="https://uploader.shimo.im/f/bAD5TU2kjCQipLid.png!thumbnail" alt="img"></p><h3 id="embeddingå±‚"><a href="#embeddingå±‚" class="headerlink" title="embeddingå±‚"></a>embeddingå±‚</h3><p><img src="https://uploader.shimo.im/f/pOmG3eS8ntYi0dSQ.png!thumbnail" alt="img"></p><h3 id="convolutionå±‚"><a href="#convolutionå±‚" class="headerlink" title="convolutionå±‚"></a>convolutionå±‚</h3><p><img src="https://uploader.shimo.im/f/MlEd8ePXgDs7gaLg.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/T3pionftmyImwzPH.png!thumbnail" alt="img"></p><h3 id="Max-over-time-pooling"><a href="#Max-over-time-pooling" class="headerlink" title="Max over time pooling"></a>Max over time pooling</h3><p><img src="https://uploader.shimo.im/f/cM7DZvGSt3gNz8uL.png!thumbnail" alt="img"></p><h3 id="è¾“å‡ºå±‚"><a href="#è¾“å‡ºå±‚" class="headerlink" title="è¾“å‡ºå±‚"></a>è¾“å‡ºå±‚</h3><p>ä¸€ä¸ªaffine transformationåŠ ä¸Šdropout</p><p><img src="https://uploader.shimo.im/f/5ZrPtIuh6X4P9lQJ.png!thumbnail" alt="img"></p><h3 id="æ¨¡å‹çš„æ•ˆæœ"><a href="#æ¨¡å‹çš„æ•ˆæœ" class="headerlink" title="æ¨¡å‹çš„æ•ˆæœ"></a>æ¨¡å‹çš„æ•ˆæœ</h3><p>å¯ä»¥åª²ç¾å½“æ—¶çš„ä¼—å¤šä¼ ç»Ÿæ¨¡å‹ã€‚ä»ä»Šå¤©çš„çœ¼å…‰æ¥çœ‹è¿™ä¸ªæ¨¡å‹çš„æ€è·¯è¿˜æ˜¯æŒºç®€å•çš„ï¼Œä¸è¿‡å½“æ—¶å¤§å®¶å¼€å§‹æ¢ç´¢æŠŠCNNç”¨åˆ°texté—®é¢˜ä¸Šçš„æ—¶å€™ï¼Œè¿™ä¸€ç³»åˆ—æ¨¡å‹æ¶æ„çš„æƒ³æ³•è¿˜æ˜¯å¾ˆæ–°é¢–çš„ã€‚</p><p><img src="https://uploader.shimo.im/f/BxnIovJf5Pwv8RHZ.png!thumbnail" alt="img"></p><h3 id="æˆ‘ä»¬çš„ä»£ç å®ç°"><a href="#æˆ‘ä»¬çš„ä»£ç å®ç°" class="headerlink" title="æˆ‘ä»¬çš„ä»£ç å®ç°"></a>æˆ‘ä»¬çš„ä»£ç å®ç°</h3><p>ç”¨ConvNetåšæ–‡æœ¬åˆ†ç±»çš„éƒ¨åˆ†ä»£ç ã€‚æœ‰äº›éƒ¨åˆ†å¯èƒ½çš„å®ç°å¯èƒ½å’Œæ¨¡å‹æœ‰ä¸€å®šå‡ºå…¥ï¼Œä¸è¿‡æˆ‘çš„æ¨¡å‹å®ç°æ•ˆæœä¹Ÿå¾ˆä¸é”™ï¼Œä»…ä¾›å‚è€ƒã€‚</p><p><a href="https://github.com/ZeweiChu/PyTorch-Course/blob/master/notebooks/4.sentiment_with_mask.ipynb" target="_blank" rel="noopener">https://github.com/ZeweiChu/PyTorch-Course/blob/master/notebooks/4.sentiment_with_mask.ipynb</a></p><p>æ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥å‚è€ƒæ›´å¤šYoon Kimçš„å·¥ä½œ</p><p><a href="http://www.people.fas.harvard.edu/~yoonkim/" target="_blank" rel="noopener">http://www.people.fas.harvard.edu/~yoonkim/</a></p><p>Yoon Kimçš„å¯¼å¸ˆAlex Rush</p><p><a href="http://nlp.seas.harvard.edu/rush.html" target="_blank" rel="noopener">http://nlp.seas.harvard.edu/rush.html</a></p><p>ä»–ä»¬çš„ä¸€é¡¹å·¥ä½œOpenNMT-py</p><p><a href="https://github.com/OpenNMT/OpenNMT-py" target="_blank" rel="noopener">https://github.com/OpenNMT/OpenNMT-py</a></p><p>Alex Rushçš„ä¸€äº›ä¼˜ç§€å­¦ç”Ÿ</p><p>Sam Wiseman <a href="https://swiseman.github.io/" target="_blank" rel="noopener">https://swiseman.github.io/</a> ä»–åšäº†å¾ˆå¤šVAEçš„å·¥ä½œ</p><h2 id="Zhang-et-al-Character-level-Convolutional-Networks-for-Text-Classification"><a href="#Zhang-et-al-Character-level-Convolutional-Networks-for-Text-Classification" class="headerlink" title="Zhang et. al., Character-level Convolutional Networks for Text Classification "></a>Zhang et. al., <a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf" target="_blank" rel="noopener">Character-level Convolutional Networks for Text Classification </a></h2><p><a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf</a></p><p>è¿™ç¯‡æ–‡ç« åœ¨charå±‚é¢ä¸Šä½¿ç”¨ConvNetï¼Œå½“æ—¶åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šå–å¾—äº†SOTAçš„æ•ˆæœã€‚åæ¥äººä»¬ç»å¸¸æŠŠè¿™å¥—æ–¹æ³•ç”¨æ¥åšå•è¯è¡¨ç¤ºçš„å­¦ä¹ ï¼Œä¾‹å¦‚ELMoå°±æ˜¯ç”¨CharCNNæ¥encodeå•è¯çš„ã€‚</p><h3 id="å…³é”®Modules"><a href="#å…³é”®Modules" class="headerlink" title="å…³é”®Modules"></a>å…³é”®Modules</h3><p>Convolutional Module</p><p><img src="https://uploader.shimo.im/f/24t3sOep2m8g4ls6.png!thumbnail" alt="img"></p><p>kæ˜¯kernel sizeã€‚</p><p>max pooling</p><p><img src="https://uploader.shimo.im/f/NzpvIEElx3UVKJyo.png!thumbnail" alt="img"></p><h3 id="æ¨¡å‹æ¶æ„å›¾-1"><a href="#æ¨¡å‹æ¶æ„å›¾-1" class="headerlink" title="æ¨¡å‹æ¶æ„å›¾"></a>æ¨¡å‹æ¶æ„å›¾</h3><h2 id><a href="#" class="headerlink" title></a><img src="https://uploader.shimo.im/f/WDfJgndz6HMQ5CTF.png!thumbnail" alt="img"></h2><p>åœ¨ELMoä¸Šçš„character embedding</p><p><img src="https://uploader.shimo.im/f/3aLnMpCpyUUQSGcQ.png!thumbnail" alt="img"></p><h3 id="æ¨¡å‹ä»£ç "><a href="#æ¨¡å‹ä»£ç " class="headerlink" title="æ¨¡å‹ä»£ç "></a>æ¨¡å‹ä»£ç </h3><p><a href="https://github.com/srviest/char-cnn-text-classification-pytorch/blob/master/model.py" target="_blank" rel="noopener">https://github.com/srviest/char-cnn-text-classification-pytorch/blob/master/model.py</a></p><h2 id="Gehring-et-al-Convolutional-Sequence-to-Sequence-Learning"><a href="#Gehring-et-al-Convolutional-Sequence-to-Sequence-Learning" class="headerlink" title="Gehring et. al., Convolutional Sequence to Sequence Learning"></a>Gehring et. al., <a href="https://arxiv.org/pdf/1705.03122.pdf" target="_blank" rel="noopener">Convolutional Sequence to Sequence Learning</a></h2><p><a href="https://arxiv.org/pdf/1705.03122.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1705.03122.pdf</a></p><p>å‚è€ƒåšå®¢èµ„æ–™</p><p><a href="https://ycts.github.io/weeklypapers/convSeq2seq/" target="_blank" rel="noopener">https://ycts.github.io/weeklypapers/convSeq2seq/</a></p><p>ç”¨ConvNetåšSeq2Seqæ¨¡å‹ï¼Œå…¶å®è¿™ç¯‡æ–‡ç« ä¸­æœ‰å¾ˆå¤šTransformerçš„å½±å­ï¼Œå¹¶ä¸”æ¨¡å‹æ•ˆæœä¹Ÿå¾ˆå¥½ã€‚å¯èƒ½ç”±äºåŒæ—¶æœŸçš„Transformerå…‰èŠ’è¿‡äºè€€çœ¼ï¼Œæ©ç›–äº†è¿™ä¸€ç¯‡åŒæ ·éå¸¸é‡é‡çº§çš„æ–‡ç« ã€‚</p><p>æˆ‘çš„å»ºè®®æ˜¯ï¼Œè¿™ç¯‡æ–‡ç« å¯ä»¥ç®€è¦é˜…è¯»ï¼Œäº†è§£ConvNetå¯ä»¥æ€ä¹ˆæ ·è¢«è¿ç”¨åˆ°Text Modelingé—®é¢˜ä¸Šã€‚ç”±äºç°åœ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„ä¸»æµæ˜¯å„ç§Transformeræ¨¡å‹çš„å˜ç§ï¼Œä¸”Transformerçš„æ¨¡å‹ç›¸å¯¹æ›´ç®€æ´æ˜“æ‡‚ï¼Œæ‰€ä»¥å»ºè®®åŒå­¦ä»¬åœ¨åé¢èŠ±æ›´å¤šçš„æ—¶é—´åœ¨Transformerä¸Šã€‚æœ€è¿‘å¾ˆå¤šNLPçš„é¢è¯•éƒ½ä¼šé—®åˆ°ä¸€äº›ä¸Transformerå’ŒBERTç›¸å…³çš„é—®é¢˜ï¼Œå¯èƒ½å¾ˆå¤šäººä¸å¤ªäº†è§£è¿™ç¯‡Conv Seq2Seqçš„è®ºæ–‡ã€‚</p><h3 id="Positional-Embedddings"><a href="#Positional-Embedddings" class="headerlink" title="Positional Embedddings"></a>Positional Embedddings</h3><p><img src="https://uploader.shimo.im/f/3s1XVoEyWCA2091O.png!thumbnail" alt="img"></p><p>å¯¹æ¯ä¸ªå•è¯åˆ†åˆ«åšword embedding w_iå’Œpositional embedding p_iï¼Œç„¶åå•è¯çš„embeddingçš„w_i + p_iã€‚p_iæ˜¯æ¨¡å‹çš„å‚æ•°ï¼Œåœ¨è®­ç»ƒä¸­ä¼šè¢«æ›´æ–°ã€‚</p><p>å¦‚æœæ²¡æœ‰positional embeddingï¼ŒCNNæ˜¯æ— æ³•çŸ¥æ™“å•è¯çš„ä½ç½®ä¿¡æ¯çš„ã€‚å› ä¸ºä¸åŒäºLSTMï¼Œå¦‚æœæ²¡æœ‰postional embeddingï¼Œåœ¨CNN encoderä¸­çš„å•è¯ä½ç½®å…¶å®æ²¡æœ‰åŒºåˆ«ã€‚</p><h3 id="Convolutional-Block-Structure"><a href="#Convolutional-Block-Structure" class="headerlink" title="Convolutional Block Structure"></a>Convolutional Block Structure</h3><p>Encoderå’ŒDecoderç¬¬lå±‚çš„è¾“å…¥</p><p><img src="https://uploader.shimo.im/f/8RwAjCP390sIoG1A.png!thumbnail" alt="img"></p><p>æ¯ä¸€å±‚éƒ½åŒ…å«ä¸€ä¸ªä¸€ç»´Convolutionï¼Œä»¥åŠä¸€ä¸ªnon-linearityå•å…ƒï¼Œå…¶ä¸­conv block/layerçš„kernelå®½åº¦ä¸ºkï¼Œå…¶outputåŒ…å«kä¸ªè¾“å…¥å…ƒç´ çš„ä¿¡æ¯ã€‚å‚æ•°ä¸º</p><p><img src="https://uploader.shimo.im/f/8tCfiuW0gHgwBKAi.png!thumbnail" alt="img"></p><p>è¾“å‡ºä¸º</p><p><img src="https://uploader.shimo.im/f/kDZ1ulm65h8m0dOX.png!thumbnail" alt="img"></p><p>ç„¶åä½¿ç”¨ä¸€ä¸ªGated Linear Unitsä½œä¸ºnon-linearityã€‚</p><p><img src="https://uploader.shimo.im/f/MuogKhR9b48cABpI.png!thumbnail" alt="img"></p><p>encoderå’Œdecoderéƒ½æœ‰å¥½å¤šå±‚ï¼Œæ¯ä¸€å±‚éƒ½åŠ ä¸Šäº†residual connectionã€‚</p><p><img src="https://uploader.shimo.im/f/EdLJ369IWcQgqx0x.png!thumbnail" alt="img"></p><p>æˆ‘ä»¬åœ¨encoderæ¯ä¸€å±‚çš„å·¦å³ä¸¤è¾¹éƒ½æ·»åŠ paddingï¼Œè¿™æ ·å¯ä»¥ä¿è¯æ¯ä¸€å±‚ç»è¿‡convolutionä¹‹åè¾“å‡ºçš„é•¿åº¦å’ŒåŸæ¥ä¸€æ ·ã€‚decoderå’Œencoderç¨æœ‰ä¸åŒï¼Œå› ä¸ºæˆ‘ä»¬å¿…é¡»ä¿è¯æˆ‘ä»¬åœ¨decoderä¸€ä¸ªä½ç½®çš„å•è¯çš„æ—¶å€™æ²¡æœ‰çœ‹åˆ°è¿™ä¸ªä½ç½®åé¢çš„å•è¯ã€‚æ‰€ä»¥æˆ‘ä»¬çš„åšæ³•æ˜¯ï¼Œåœ¨decoderæ¯ä¸€å±‚å·¦å³ä¸¤è¾¹éƒ½åŠ ä¸Šk-1ä¸ªpaddingï¼Œåšå®Œconvä¹‹åæŠŠå³è¾¹çš„kä¸ªå•ä½ç§»é™¤ã€‚</p><p>æœ€åçš„ä¸€ä¸ªæ ‡å‡†å¥—è·¯æ˜¯æŠŠhidden stateåšä¸ªaffine transformationï¼Œç„¶åSoftmaxå˜æˆå•è¯è¡¨ä¸Šçš„ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒã€‚</p><p><img src="https://uploader.shimo.im/f/yFXfmhqvzzcU8D7D.png!thumbnail" alt="img"></p><h3 id="Multi-step-Attention"><a href="#Multi-step-Attention" class="headerlink" title="Multi-step Attention"></a>Multi-step Attention</h3><p>Decoderçš„æ¯ä¸€å±‚éƒ½æœ‰å•ç‹¬çš„Attentionã€‚</p><p><img src="https://uploader.shimo.im/f/uQLvQ1erpmAJfLlP.png!thumbnail" alt="img"></p><p>g_iæ˜¯å½“å‰å•è¯çš„embeddingï¼Œ</p><p><img src="https://uploader.shimo.im/f/eHzdNjYIZBM93TQN.png!thumbnail" alt="img"></p><p>ç„¶åæˆ‘ä»¬ç”¨è¿™ä¸ªæ–°é€ çš„ d_i^l å¯¹ encoder çš„æ¯ä¸ªä½ç½®åšattentionã€‚</p><p><img src="https://uploader.shimo.im/f/er1A7CMeiukGczjw.png!thumbnail" alt="img"></p><p>ç„¶åéå¸¸å¸¸è§„çš„ï¼Œç”¨attention scoreå¯¹encoder hidden statesåšåŠ æƒå¹³å‡ã€‚å”¯ä¸€ä¸åŒçš„æ˜¯ï¼Œè¿™é‡Œè¿˜ç›´æ¥åŠ ä¸Šäº†è¾“å…¥çš„embeddingã€‚</p><p><img src="https://uploader.shimo.im/f/BJXPoQQi9Xg4fJJa.png!thumbnail" alt="img"></p><p>ä½œè€…è¯´ä»–ä»¬å‘ç°ç›´æ¥åŠ ä¸Šè¿™ä¸ªè¯å‘é‡çš„embeddingè¿˜æ˜¯å¾ˆæœ‰ç”¨çš„ã€‚</p><h3 id="æ¨¡å‹æ¶æ„å›¾-2"><a href="#æ¨¡å‹æ¶æ„å›¾-2" class="headerlink" title="æ¨¡å‹æ¶æ„å›¾"></a>æ¨¡å‹æ¶æ„å›¾</h3><p><img src="https://uploader.shimo.im/f/r5pAK5SQWzQDyDFL.png!thumbnail" alt="img"></p><h3 id="Normalizationç­–ç•¥"><a href="#Normalizationç­–ç•¥" class="headerlink" title="Normalizationç­–ç•¥"></a>Normalizationç­–ç•¥</h3><p>ä¸ºäº†ä¿æŒæ¨¡å‹è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹ä¸­é—´çš„å‘é‡çš„varianceä¸è¦å¤ªå¤§ã€‚</p><ul><li>è¾“å‡º+residualä¹‹åä¹˜ä»¥\sqrt{5}ï¼Œè¿™æ ·å¯ä»¥è®©è¿™äº›vectoræ¯ä¸ªç»´åº¦çš„varianceå‡åŠã€‚å…¶å®å¾ˆå¤šæ—¶å€™è¿™äº›ç¡®ä¿æ¨¡å‹ç¨³å®šåº¦çš„ç»†èŠ‚æŒºå…³é”®çš„ï¼Œå¤§å®¶å¯èƒ½ä¹ŸçŸ¥é“transformerä¸­ä¹Ÿå¢åŠ äº†ä¸€äº›å‡å°‘varianceçš„æ–¹æ³•ã€‚å¦‚æœä¸æ˜¯è°ƒæ¨¡å‹ä¸“å®¶å°±ä¼šå¿½è§†è¿™äº›ç»†èŠ‚ï¼Œç„¶åæ¨¡å‹å°±è®­ç»ƒä¸å¥½äº†ã€‚</li></ul><p><img src="https://uploader.shimo.im/f/IMLzC3rtvxkqLmuo.png!thumbnail" alt="img"></p><p>è¿˜æœ‰æ›´å¤šçš„æ¨¡å‹å‚æ•°åˆå§‹åŒ–ç»†èŠ‚ï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥è‡ªå·±å»è®¤çœŸé˜…è¯»paperã€‚</p><h3 id="å®éªŒç»“æœ"><a href="#å®éªŒç»“æœ" class="headerlink" title="å®éªŒç»“æœ"></a>å®éªŒç»“æœ</h3><p><img src="https://uploader.shimo.im/f/aNGZg3EjGyw1aWu7.png!thumbnail" alt="img"></p><p>åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šè¶…è¶Šäº†GNMT (Google Neural Machine Translation)ï¼Œå…¶å®è¿™ä¸ªæ¯”è¾ƒèƒ½è¯´æ˜é—®é¢˜ï¼Œå› ä¸ºå½“æ—¶çš„GNMTæ˜¯State of the Artã€‚</p><p><img src="https://uploader.shimo.im/f/VMfU3If3biw5286r.png!thumbnail" alt="img"></p><p>ç„¶åä»–ä»¬è¿˜å±•ç¤ºäº†ConvS2Sçš„é€Ÿåº¦æ¯”GNMTæ›´å¿«ã€‚</p><p>æ€»ç»“æ¥è¯´ï¼ŒConvS2Så…¶å®æ˜¯ä¸€ç¯‡å¾ˆæœ‰ä»·å€¼çš„æ–‡ç« ï¼ŒDecoderçš„è®¾è®¡æ¯”è¾ƒç²¾è‡´ï¼Œ ä¸çŸ¥é“è¿™ç¯‡æ–‡ç« å¯¹åæ¥çš„Transformeräº§ç”Ÿäº†å¤šå°‘çš„å½±å“ï¼Œå½“ç„¶ä»–ä»¬å¯ä»¥è¯´æ˜¯åŒæ—¶æœŸçš„ä½œå“ã€‚</p><h3 id="ä»£ç "><a href="#ä»£ç " class="headerlink" title="ä»£ç "></a>ä»£ç </h3><p>ä¸»è¦ä»£ç åœ¨Fairseqçš„ä¸‹é¢è¿™ä¸ªæ–‡ä»¶ä¸­</p><p><a href="https://github.com/ZeweiChu/fairseq/blob/master/fairseq/models/fconv.py" target="_blank" rel="noopener">https://github.com/ZeweiChu/fairseq/blob/master/fairseq/models/fconv.py</a></p><p>Fairseqæ˜¯ä¸€ä¸ªå€¼å¾—å…³æ³¨ä¸€æ³¢çš„å·¥å…·åŒ…ï¼Œç”±Facebookå¼€å‘ï¼Œä¸»è¦å¼€å‘è€…æœ‰ </p><ul><li>Myle Ott <a href="https://myleott.com/" target="_blank" rel="noopener">https://myleott.com/</a></li></ul><h1 id="å…³äºæ–‡æœ¬åˆ†ç±»çš„æ›´å¤šå‚è€ƒèµ„æ–™"><a href="#å…³äºæ–‡æœ¬åˆ†ç±»çš„æ›´å¤šå‚è€ƒèµ„æ–™" class="headerlink" title="å…³äºæ–‡æœ¬åˆ†ç±»çš„æ›´å¤šå‚è€ƒèµ„æ–™"></a>å…³äºæ–‡æœ¬åˆ†ç±»çš„æ›´å¤šå‚è€ƒèµ„æ–™</h1><p>åŸºäºæ·±åº¦å­¦ä¹ çš„æ–‡æœ¬åˆ†ç±»</p><p><a href="https://zhuanlan.zhihu.com/p/34212945" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34212945</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;â€‹        NLP/AIæ˜¯è¿‘å‡ å¹´æ¥é£é€Ÿå‘å±•çš„é¢†åŸŸï¼Œå¾ˆå¤šçš„æ¨¡å‹å’Œç®—æ³•åªèƒ½åœ¨è®ºæ–‡ã€è®²ä¹‰å’Œåšå®¢ä¸­æ‰¾åˆ°ï¼Œè€Œä¸ä¼šå‡ºç°åœ¨ä»»ä½•çš„æ•™ç§‘ä¹¦ä¸­ã€‚å‡¡æ˜¯è¯¾ç¨‹ä¸­æåˆ°çš„è®ºæ–‡ï¼Œå¤§å®¶éƒ½èƒ½å¤Ÿé˜…è¯»ä¸€éã€‚å¯¹äºé‡è¦çš„è®ºæ–‡ï¼ˆæˆ‘ä¼šç‰¹åˆ«æ ‡æ˜æˆ–è€…åœ¨è¯¾ä¸Šå¼ºè°ƒï¼Œä¾‹å¦‚BERT, transformerç­‰ï¼‰ï¼Œå»ºè®®è®¤çœŸé˜…è¯»ï¼Œ
      
    
    </summary>
    
      <category term="NLP" scheme="http://mmyblog.cn/categories/NLP/"/>
    
    
      <category term="ConvNet" scheme="http://mmyblog.cn/tags/ConvNet/"/>
    
  </entry>
  
  <entry>
    <title>seq2seq</title>
    <link href="http://mmyblog.cn/2020/04/13/seq2seq/"/>
    <id>http://mmyblog.cn/2020/04/13/seq2seq/</id>
    <published>2020-04-12T23:04:08.000Z</published>
    <updated>2020-06-09T01:29:18.742Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Seq2Seq-Attention"><a href="#Seq2Seq-Attention" class="headerlink" title="Seq2Seq, Attention"></a>Seq2Seq, Attention</h1><p>åœ¨è¿™ä»½notebookå½“ä¸­ï¼Œæˆ‘ä»¬ä¼š(å°½å¯èƒ½)å¤ç°Luongçš„attentionæ¨¡å‹</p><p>ç”±äºæˆ‘ä»¬çš„æ•°æ®é›†éå¸¸å°ï¼Œåªæœ‰ä¸€ä¸‡å¤šä¸ªå¥å­çš„è®­ç»ƒæ•°æ®ï¼Œæ‰€ä»¥è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æ•ˆæœå¹¶ä¸å¥½ã€‚å¦‚æœå¤§å®¶æƒ³è®­ç»ƒä¸€ä¸ªå¥½ä¸€ç‚¹çš„æ¨¡å‹ï¼Œå¯ä»¥å‚è€ƒä¸‹é¢çš„èµ„æ–™ã€‚</p><h2 id="æ›´å¤šé˜…è¯»"><a href="#æ›´å¤šé˜…è¯»" class="headerlink" title="æ›´å¤šé˜…è¯»"></a>æ›´å¤šé˜…è¯»</h2><h4 id="è¯¾ä»¶"><a href="#è¯¾ä»¶" class="headerlink" title="è¯¾ä»¶"></a>è¯¾ä»¶</h4><ul><li><a href="http://cs224d.stanford.edu/lectures/CS224d-Lecture15.pdf" target="_blank" rel="noopener">cs224d</a></li></ul><h4 id="è®ºæ–‡"><a href="#è®ºæ–‡" class="headerlink" title="è®ºæ–‡"></a>è®ºæ–‡</h4><ul><li><a href="https://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></li><li><a href="https://arxiv.org/abs/1508.04025?context=cs" target="_blank" rel="noopener">Effective Approaches to Attention-based Neural Machine Translation</a></li><li><a href="https://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a></li></ul><h4 id="PyTorchä»£ç "><a href="#PyTorchä»£ç " class="headerlink" title="PyTorchä»£ç "></a>PyTorchä»£ç </h4><ul><li><a href="https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb" target="_blank" rel="noopener">seq2seq-tutorial</a></li><li><a href="https://github.com/bentrevett/pytorch-seq2seq" target="_blank" rel="noopener">Tutorial from Ben Trevett</a></li><li><a href="https://github.com/IBM/pytorch-seq2seq" target="_blank" rel="noopener">IBM seq2seq</a></li><li><a href="https://github.com/OpenNMT/OpenNMT-py" target="_blank" rel="noopener">OpenNMT-py</a></li></ul><h4 id="æ›´å¤šå…³äºMachine-Translation"><a href="#æ›´å¤šå…³äºMachine-Translation" class="headerlink" title="æ›´å¤šå…³äºMachine Translation"></a>æ›´å¤šå…³äºMachine Translation</h4><ul><li><a href="https://www.coursera.org/lecture/nlp-sequence-models/beam-search-4EtHZ" target="_blank" rel="noopener">Beam Search</a></li><li>Pointer network æ–‡æœ¬æ‘˜è¦</li><li>Copy Mechanism æ–‡æœ¬æ‘˜è¦</li><li>Converage Loss </li><li>ConvSeq2Seq</li><li>Transformer</li><li>Tensor2Tensor</li></ul><h4 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h4><ul><li>å»ºè®®åŒå­¦å°è¯•å¯¹ä¸­æ–‡è¿›è¡Œåˆ†è¯</li></ul><h4 id="NER"><a href="#NER" class="headerlink" title="NER"></a>NER</h4><ul><li><a href="https://github.com/allenai/allennlp/tree/master/allennlp" target="_blank" rel="noopener">https://github.com/allenai/allennlp/tree/master/allennlp</a></li></ul><p>In [137]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter <span class="comment">#è®¡æ•°å™¨</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> nltk</span><br></pre></td></tr></table></figure><p>è¯»å…¥ä¸­è‹±æ–‡æ•°æ®</p><ul><li>è‹±æ–‡æˆ‘ä»¬ä½¿ç”¨nltkçš„word tokenizeræ¥åˆ†è¯ï¼Œå¹¶ä¸”ä½¿ç”¨å°å†™å­—æ¯</li><li>ä¸­æ–‡æˆ‘ä»¬ç›´æ¥ä½¿ç”¨å•ä¸ªæ±‰å­—ä½œä¸ºåŸºæœ¬å•å…ƒ</li></ul><p>In [138]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(in_file)</span>:</span></span><br><span class="line">    cn = []</span><br><span class="line">    en = []</span><br><span class="line">    num_examples = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> open(in_file, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            <span class="comment">#print(line) #Anyone can do that.ä»»ä½•äººéƒ½å¯ä»¥åšåˆ°ã€‚</span></span><br><span class="line">            line = line.strip().split(<span class="string">"\t"</span>) <span class="comment">#åˆ†è¯åç”¨é€—å·éš”å¼€</span></span><br><span class="line">            <span class="comment">#print(line) #['Anyone can do that.', 'ä»»ä½•äººéƒ½å¯ä»¥åšåˆ°ã€‚']</span></span><br><span class="line">            en.append([<span class="string">"BOS"</span>] + nltk.word_tokenize(line[<span class="number">0</span>].lower()) + [<span class="string">"EOS"</span>])</span><br><span class="line">            <span class="comment">#BOS:beginning of sequence EOS:end of</span></span><br><span class="line">            <span class="comment"># split chinese sentence into characters</span></span><br><span class="line">            cn.append([<span class="string">"BOS"</span>] + [c <span class="keyword">for</span> c <span class="keyword">in</span> line[<span class="number">1</span>]] + [<span class="string">"EOS"</span>])</span><br><span class="line">            <span class="comment">#ä¸­æ–‡ä¸€ä¸ªä¸€ä¸ªå­—åˆ†è¯ï¼Œå¯ä»¥å°è¯•ç”¨åˆ†è¯å™¨åˆ†è¯</span></span><br><span class="line">    <span class="keyword">return</span> en, cn</span><br><span class="line"></span><br><span class="line">train_file = <span class="string">"nmt/en-cn/train.txt"</span></span><br><span class="line">dev_file = <span class="string">"nmt/en-cn/dev.txt"</span></span><br><span class="line">train_en, train_cn = load_data(train_file)</span><br><span class="line">dev_en, dev_cn = load_data(dev_file)</span><br></pre></td></tr></table></figure><p>In [0]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(train_en[:10])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[&apos;BOS&apos;, &apos;anyone&apos;, &apos;can&apos;, &apos;do&apos;, &apos;that&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;how&apos;, &apos;about&apos;, &apos;another&apos;, &apos;piece&apos;, &apos;of&apos;, &apos;cake&apos;, &apos;?&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;she&apos;, &apos;married&apos;, &apos;him&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;i&apos;, &apos;do&apos;, &quot;n&apos;t&quot;, &apos;like&apos;, &apos;learning&apos;, &apos;irregular&apos;, &apos;verbs&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;it&apos;, &quot;&apos;s&quot;, &apos;a&apos;, &apos;whole&apos;, &apos;new&apos;, &apos;ball&apos;, &apos;game&apos;, &apos;for&apos;, &apos;me&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;he&apos;, &quot;&apos;s&quot;, &apos;sleeping&apos;, &apos;like&apos;, &apos;a&apos;, &apos;baby&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;he&apos;, &apos;can&apos;, &apos;play&apos;, &apos;both&apos;, &apos;tennis&apos;, &apos;and&apos;, &apos;baseball&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;we&apos;, &apos;should&apos;, &apos;cancel&apos;, &apos;the&apos;, &apos;hike&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;he&apos;, &apos;is&apos;, &apos;good&apos;, &apos;at&apos;, &apos;dealing&apos;, &apos;with&apos;, &apos;children&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;she&apos;, &apos;will&apos;, &apos;do&apos;, &apos;her&apos;, &apos;best&apos;, &apos;to&apos;, &apos;be&apos;, &apos;here&apos;, &apos;on&apos;, &apos;time&apos;, &apos;.&apos;, &apos;EOS&apos;]]</span><br></pre></td></tr></table></figure><p>In [0]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(train_cn[:10])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[&apos;BOS&apos;, &apos;ä»»&apos;, &apos;ä½•&apos;, &apos;äºº&apos;, &apos;éƒ½&apos;, &apos;å¯&apos;, &apos;ä»¥&apos;, &apos;åš&apos;, &apos;åˆ°&apos;, &apos;ã€‚&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;è¦&apos;, &apos;ä¸&apos;, &apos;è¦&apos;, &apos;å†&apos;, &apos;ä¾†&apos;, &apos;ä¸€&apos;, &apos;å¡Š&apos;, &apos;è›‹&apos;, &apos;ç³•&apos;, &apos;ï¼Ÿ&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;å¥¹&apos;, &apos;å«&apos;, &apos;ç»™&apos;, &apos;äº†&apos;, &apos;ä»–&apos;, &apos;ã€‚&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;æˆ‘&apos;, &apos;ä¸&apos;, &apos;å–œ&apos;, &apos;æ¬¢&apos;, &apos;å­¦&apos;, &apos;ä¹ &apos;, &apos;ä¸&apos;, &apos;è§„&apos;, &apos;åˆ™&apos;, &apos;åŠ¨&apos;, &apos;è¯&apos;, &apos;ã€‚&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;é€™&apos;, &apos;å°&apos;, &apos;æˆ‘&apos;, &apos;ä¾†&apos;, &apos;èªª&apos;, &apos;æ˜¯&apos;, &apos;å€‹&apos;, &apos;å…¨&apos;, &apos;æ–°&apos;, &apos;çš„&apos;, &apos;çƒ&apos;, &apos;é¡&apos;, &apos;éŠ&apos;, &apos;æˆ²&apos;, &apos;ã€‚&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;ä»–&apos;, &apos;æ­£&apos;, &apos;ç¡&apos;, &apos;ç€&apos;, &apos;ï¼Œ&apos;, &apos;åƒ&apos;, &apos;ä¸ª&apos;, &apos;å©´&apos;, &apos;å„¿&apos;, &apos;ä¸€&apos;, &apos;æ ·&apos;, &apos;ã€‚&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;ä»–&apos;, &apos;æ—¢&apos;, &apos;ä¼š&apos;, &apos;æ‰“&apos;, &apos;ç½‘&apos;, &apos;çƒ&apos;, &apos;ï¼Œ&apos;, &apos;åˆ&apos;, &apos;ä¼š&apos;, &apos;æ‰“&apos;, &apos;æ£’&apos;, &apos;çƒ&apos;, &apos;ã€‚&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;æˆ‘&apos;, &apos;å€‘&apos;, &apos;æ‡‰&apos;, &apos;è©²&apos;, &apos;å–&apos;, &apos;æ¶ˆ&apos;, &apos;é€™&apos;, &apos;æ¬¡&apos;, &apos;é &apos;, &apos;è¶³&apos;, &apos;ã€‚&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;ä»–&apos;, &apos;æ“…&apos;, &apos;é•·&apos;, &apos;æ‡‰&apos;, &apos;ä»˜&apos;, &apos;å°&apos;, &apos;å­©&apos;, &apos;å­&apos;, &apos;ã€‚&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;å¥¹&apos;, &apos;ä¼š&apos;, &apos;å°½&apos;, &apos;é‡&apos;, &apos;æŒ‰&apos;, &apos;æ—¶&apos;, &apos;èµ¶&apos;, &apos;æ¥&apos;, &apos;çš„&apos;, &apos;ã€‚&apos;, &apos;EOS&apos;]]</span><br></pre></td></tr></table></figure><p>In [0]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>æ„å»ºå•è¯è¡¨</p><p>In [139]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">UNK_IDX = <span class="number">0</span></span><br><span class="line">PAD_IDX = <span class="number">1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dict</span><span class="params">(sentences, max_words=<span class="number">50000</span>)</span>:</span></span><br><span class="line">    word_count = Counter()</span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> sentence:</span><br><span class="line">            word_count[s] += <span class="number">1</span>  <span class="comment">#word_countè¿™é‡Œåº”è¯¥æ˜¯ä¸ªå­—å…¸</span></span><br><span class="line">    ls = word_count.most_common(max_words) </span><br><span class="line">    <span class="comment">#æŒ‰æ¯ä¸ªå•è¯æ•°é‡æ’åºå‰50000ä¸ª,è¿™ä¸ªæ•°å­—è‡ªå·±å®šçš„ï¼Œä¸é‡å¤å•è¯æ•°æ²¡æœ‰50000</span></span><br><span class="line">    print(len(ls)) <span class="comment">#train_enï¼š5491</span></span><br><span class="line">    total_words = len(ls) + <span class="number">2</span></span><br><span class="line">    <span class="comment">#åŠ çš„2æ˜¯ç•™ç»™"unk"å’Œ"pad"</span></span><br><span class="line">    <span class="comment">#ls = [('BOS', 14533), ('EOS', 14533), ('.', 12521), ('i', 4045), .......</span></span><br><span class="line">    word_dict = &#123;w[<span class="number">0</span>]: index+<span class="number">2</span> <span class="keyword">for</span> index, w <span class="keyword">in</span> enumerate(ls)&#125;</span><br><span class="line">    <span class="comment">#åŠ çš„2æ˜¯ç•™ç»™"unk"å’Œ"pad",è½¬æ¢æˆå­—å…¸æ ¼å¼ã€‚</span></span><br><span class="line">    word_dict[<span class="string">"UNK"</span>] = UNK_IDX</span><br><span class="line">    word_dict[<span class="string">"PAD"</span>] = PAD_IDX</span><br><span class="line">    <span class="keyword">return</span> word_dict, total_words</span><br><span class="line"></span><br><span class="line">en_dict, en_total_words = build_dict(train_en)</span><br><span class="line">cn_dict, cn_total_words = build_dict(train_cn)</span><br><span class="line">inv_en_dict = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> en_dict.items()&#125;</span><br><span class="line"><span class="comment">#en_dict.items()æŠŠå­—å…¸è½¬æ¢æˆå¯è¿­ä»£å¯¹è±¡ï¼Œå–å‡ºé”®å€¼ï¼Œå¹¶è°ƒæ¢é”®å€¼çš„ä½ç½®ã€‚</span></span><br><span class="line">inv_cn_dict = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> cn_dict.items()&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">5491</span><br><span class="line">3193</span><br></pre></td></tr></table></figure><p>In [1]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># print(en_dict)</span><br><span class="line"># print(en_total_words)</span><br></pre></td></tr></table></figure><p>In [3]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(cn_dict)</span><br><span class="line">print(cn_total_words)</span><br></pre></td></tr></table></figure><p>In [4]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(inv_en_dict)</span><br></pre></td></tr></table></figure><p>In [5]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(inv_cn_dict)</span><br></pre></td></tr></table></figure><p>æŠŠå•è¯å…¨éƒ¨è½¬å˜æˆæ•°å­—</p><p>In [140]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        Encode the sequences. </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    length = len(en_sentences)</span><br><span class="line">    <span class="comment">#en_sentences=[['BOS', 'anyone', 'can', 'do', 'that', '.', 'EOS'],....</span></span><br><span class="line">    </span><br><span class="line">    out_en_sentences = [[en_dict.get(w, <span class="number">0</span>) <span class="keyword">for</span> w <span class="keyword">in</span> sent] <span class="keyword">for</span> sent <span class="keyword">in</span> en_sentences]</span><br><span class="line">    <span class="comment">#out_en_sentences=[[2, 328, 43, 14, 28, 4, 3], ....</span></span><br><span class="line">    <span class="comment">#.get(w, 0)ï¼Œè¿”å›wå¯¹åº”çš„å€¼ï¼Œæ²¡æœ‰å°±ä¸º0.å› é¢˜åº“æ¯”è¾ƒå°ï¼Œè¿™é‡Œæ‰€æœ‰çš„å•è¯å‘é‡éƒ½æœ‰éé›¶ç´¢å¼•ã€‚</span></span><br><span class="line">    </span><br><span class="line"> </span><br><span class="line">    out_cn_sentences = [[cn_dict.get(w, <span class="number">0</span>) <span class="keyword">for</span> w <span class="keyword">in</span> sent] <span class="keyword">for</span> sent <span class="keyword">in</span> cn_sentences]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sort sentences by english lengths</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">len_argsort</span><span class="params">(seq)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> sorted(range(len(seq)), key=<span class="keyword">lambda</span> x: len(seq[x]))</span><br><span class="line">      <span class="comment">#sorted()æ’åº,keyå‚æ•°å¯ä»¥è‡ªå®šä¹‰è§„åˆ™ï¼ŒæŒ‰seq[x]çš„é•¿åº¦æ’åºï¼Œseq[0]ä¸ºç¬¬ä¸€å¥è¯é•¿åº¦</span></span><br><span class="line">       </span><br><span class="line">    <span class="comment"># æŠŠä¸­æ–‡å’Œè‹±æ–‡æŒ‰ç…§åŒæ ·çš„é¡ºåºæ’åº</span></span><br><span class="line">    <span class="keyword">if</span> sort_by_len:</span><br><span class="line">        sorted_index = len_argsort(out_en_sentences)</span><br><span class="line">    <span class="comment">#print(sorted_index)</span></span><br><span class="line">    <span class="comment">#sorted_index=[63, 1544, 1917, 2650, 3998, 6240, 6294, 6703, ....</span></span><br><span class="line">     <span class="comment">#å‰é¢çš„ç´¢å¼•éƒ½æ˜¯æœ€çŸ­å¥å­çš„ç´¢å¼•</span></span><br><span class="line">      </span><br><span class="line">        out_en_sentences = [out_en_sentences[i] <span class="keyword">for</span> i <span class="keyword">in</span> sorted_index]</span><br><span class="line">     <span class="comment">#print(out_en_sentences)</span></span><br><span class="line">     <span class="comment">#out_en_sentences=[[2, 475, 4, 3], [2, 1318, 126, 3], [2, 1707, 126, 3], ......</span></span><br><span class="line">     </span><br><span class="line">        out_cn_sentences = [out_cn_sentences[i] <span class="keyword">for</span> i <span class="keyword">in</span> sorted_index]</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> out_en_sentences, out_cn_sentences</span><br><span class="line"></span><br><span class="line">train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)</span><br><span class="line">dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)</span><br></pre></td></tr></table></figure><p>In [6]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">k=10000</span><br><span class="line">print(&quot; &quot;.join([inv_cn_dict[i] for i in train_cn[k]])) #é€šè¿‡invå­—å…¸è·å–å•è¯</span><br><span class="line">print(&quot; &quot;.join([inv_en_dict[i] for i in train_en[k]]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BOS ä»– æ¥ è¿™ é‡Œ çš„ ç›® çš„ æ˜¯ ä»€ ä¹ˆ ï¼Ÿ EOS</span><br><span class="line">BOS for what purpose did he come here ? EOS</span><br></pre></td></tr></table></figure><p>æŠŠå…¨éƒ¨å¥å­åˆ†æˆbatch</p><p>In [0]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(np.arange(0, 100, 15))</span><br><span class="line">print(np.arange(0, 15))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ 0 15 30 45 60 75 90]</span><br><span class="line">[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]</span><br></pre></td></tr></table></figure><p>In [141]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_minibatches</span><span class="params">(n, minibatch_size, shuffle=True)</span>:</span></span><br><span class="line">    idx_list = np.arange(<span class="number">0</span>, n, minibatch_size) <span class="comment"># [0, 1, ..., n-1]</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        np.random.shuffle(idx_list) <span class="comment">#æ‰“ä¹±æ•°æ®</span></span><br><span class="line">    minibatches = []</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> idx_list:</span><br><span class="line">        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))</span><br><span class="line">        <span class="comment">#æ‰€æœ‰batchæ”¾åœ¨ä¸€ä¸ªå¤§åˆ—è¡¨é‡Œ</span></span><br><span class="line">    <span class="keyword">return</span> minibatches</span><br></pre></td></tr></table></figure><p>In [10]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_minibatches(<span class="number">100</span>,<span class="number">15</span>) <span class="comment">#éšæœºæ‰“ä¹±çš„</span></span><br></pre></td></tr></table></figure><p>Out[10]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[array([75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]),</span><br><span class="line"> array([45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]),</span><br><span class="line"> array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]),</span><br><span class="line"> array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),</span><br><span class="line"> array([15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),</span><br><span class="line"> array([60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]),</span><br><span class="line"> array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99])]</span><br></pre></td></tr></table></figure><p>In [142]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_data</span><span class="params">(seqs)</span>:</span></span><br><span class="line"><span class="comment">#seqs=[[2, 12, 167, 23, 114, 5, 27, 1755, 4, 3], ........</span></span><br><span class="line">    lengths = [len(seq) <span class="keyword">for</span> seq <span class="keyword">in</span> seqs]<span class="comment">#æ¯ä¸ªbatché‡Œè¯­å¥çš„é•¿åº¦ç»Ÿè®¡å‡ºæ¥</span></span><br><span class="line">    n_samples = len(seqs) <span class="comment">#ä¸€ä¸ªbatchæœ‰å¤šå°‘è¯­å¥</span></span><br><span class="line">    max_len = np.max(lengths) <span class="comment">#å–å‡ºæœ€é•¿çš„çš„è¯­å¥é•¿åº¦ï¼Œåé¢ç”¨è¿™ä¸ªåšpaddingåŸºå‡†</span></span><br><span class="line">    x = np.zeros((n_samples, max_len)).astype(<span class="string">'int32'</span>)</span><br><span class="line">    <span class="comment">#å…ˆåˆå§‹åŒ–å…¨é›¶çŸ©é˜µï¼Œåé¢ä¾æ¬¡èµ‹å€¼</span></span><br><span class="line">    <span class="comment">#print(x.shape) #64*æœ€å¤§å¥å­é•¿åº¦</span></span><br><span class="line">    </span><br><span class="line">    x_lengths = np.array(lengths).astype(<span class="string">"int32"</span>)</span><br><span class="line">    <span class="comment">#print(x_lengths) </span></span><br><span class="line"><span class="comment">#è¿™é‡Œçœ‹ä¸‹é¢çš„è¾“å…¥è¯­å¥å‘ç°è‹±æ–‡å¥å­é•¿åº¦éƒ½ä¸€æ ·ï¼Œä¸­æ–‡å¥å­é•¿çŸ­ä¸ä¸€ã€‚</span></span><br><span class="line"><span class="comment">#è¯´æ˜è‹±æ–‡å¥å­æ˜¯ç‰¹å¾ï¼Œä¸­æ–‡å¥å­æ˜¯æ ‡ç­¾ã€‚</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, seq <span class="keyword">in</span> enumerate(seqs):</span><br><span class="line">      <span class="comment">#å–å‡ºä¸€ä¸ªbatchçš„æ¯æ¡è¯­å¥å’Œå¯¹åº”çš„ç´¢å¼•</span></span><br><span class="line">        x[idx, :lengths[idx]] = seq</span><br><span class="line">        <span class="comment">#æ¯æ¡è¯­å¥æŒ‰è¡Œèµ‹å€¼ç»™xï¼Œxä¼šæœ‰ä¸€äº›é›¶å€¼æ²¡æœ‰è¢«èµ‹å€¼ã€‚</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> x, x_lengths <span class="comment">#x_mask</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_examples</span><span class="params">(en_sentences, cn_sentences, batch_size)</span>:</span></span><br><span class="line">    minibatches = get_minibatches(len(en_sentences), batch_size)</span><br><span class="line">    all_ex = []</span><br><span class="line">    <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line">        mb_en_sentences = [en_sentences[t] <span class="keyword">for</span> t <span class="keyword">in</span> minibatch]</span><br><span class="line"><span class="comment">#æŒ‰æ‰“ä¹±çš„batchåºå·åˆ†æ•°æ®ï¼Œæ‰“ä¹±åªæ˜¯batchæ‰“ä¹±ï¼Œä¸€ä¸ªbataché‡Œé¢çš„è¯­å¥è¿˜æ˜¯é¡ºåºçš„ã€‚</span></span><br><span class="line">        <span class="comment">#print(mb_en_sentences)</span></span><br><span class="line">        </span><br><span class="line">        mb_cn_sentences = [cn_sentences[t] <span class="keyword">for</span> t <span class="keyword">in</span> minibatch]</span><br><span class="line">        mb_x, mb_x_len = prepare_data(mb_en_sentences)</span><br><span class="line">        <span class="comment">#è¿”å›çš„ç»´åº¦ä¸ºï¼šmb_x=(64 * æœ€å¤§å¥å­é•¿åº¦ï¼‰,mb_x_len=æœ€å¤§å¥å­é•¿åº¦</span></span><br><span class="line">        mb_y, mb_y_len = prepare_data(mb_cn_sentences)</span><br><span class="line">        </span><br><span class="line">        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))</span><br><span class="line">  <span class="comment">#è¿™é‡ŒæŠŠæ‰€æœ‰batchæ•°æ®é›†åˆåˆ°ä¸€èµ·ã€‚</span></span><br><span class="line">  <span class="comment">#ä¾æ¬¡ä¸ºè‹±æ–‡å¥å­ï¼Œè‹±æ–‡é•¿åº¦ï¼Œä¸­æ–‡å¥å­ç¿»è¯‘ï¼Œä¸­æ–‡å¥å­é•¿åº¦ï¼Œè¿™å››ä¸ªæ”¾åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­</span></span><br><span class="line">  <span class="comment">#ä¸€ä¸ªåˆ—è¡¨ä¸ºä¸€ä¸ªbatchçš„æ•°æ®ï¼Œæ‰€æœ‰batchç»„æˆä¸€ä¸ªå¤§åˆ—è¡¨æ•°æ®</span></span><br><span class="line">  </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> all_ex</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_data = gen_examples(train_en, train_cn, batch_size)</span><br><span class="line">random.shuffle(train_data)</span><br><span class="line">dev_data = gen_examples(dev_en, dev_cn, batch_size)</span><br></pre></td></tr></table></figure><p>In [28]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[0]</span><br></pre></td></tr></table></figure><p>Out[28]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">(array([[   2,   12,  707,   23,    7,  295,    4,    3],</span><br><span class="line">        [   2,   12,  120, 1207,  517,  604,    4,    3],</span><br><span class="line">        [   2,    8,   90,  433,   64, 1470,  126,    3],</span><br><span class="line">        [   2,   12,  144,   46,    9,   94,    4,    3],</span><br><span class="line">        [   2,   25,   10,    9,  535,  639,    4,    3],</span><br><span class="line">        [   2,   25,   10,   64,  377, 2512,    4,    3],</span><br><span class="line">        [   2,   12,   43,  309,    9,   96,    4,    3],</span><br><span class="line">        [   2,   43,  328, 1475,   25,  469,   11,    3],</span><br><span class="line">        [   2,   82, 1043,   34, 1991, 2514,    4,    3],</span><br><span class="line">        [   2,    5,   54,    7,  181, 1694,    4,    3],</span><br><span class="line">        [   2,   30,   51,  472,    6,  294,   11,    3],</span><br><span class="line">        [   2,    5,  241,   16,   65,  551,    4,    3],</span><br><span class="line">        [   2,   14,    8,   36, 2516,  680,   11,    3],</span><br><span class="line">        [   2,    8,   30,    9,   66,  333,    4,    3],</span><br><span class="line">        [   2,   12,   10,   34,   40,  777,    4,    3],</span><br><span class="line">        [   2,   29,   54,    9,  138, 1633,    4,    3],</span><br><span class="line">        [   2,   43,    8,  309,    9,   96,   11,    3],</span><br><span class="line">        [   2,   47,   12,   39,   59,  190,   11,    3],</span><br><span class="line">        [   2,   29,   85,   14,  150,  221,    4,    3],</span><br><span class="line">        [   2,   12,   70,   37,   36,  242,    4,    3],</span><br><span class="line">        [   2,    5,  239,   64, 2521, 1696,    4,    3],</span><br><span class="line">        [   2,    5,   14,   13,   36,  314,    4,    3],</span><br><span class="line">        [   2,    5,  234,    7,   45,   44,    4,    3],</span><br><span class="line">        [   2,    5,   76,  226,   17,  621,    4,    3],</span><br><span class="line">        [   2,   29,  180,    9,  269,  266,    4,    3],</span><br><span class="line">        [   2,   85,    5,   22,    6,  708,   11,    3],</span><br><span class="line">        [   2,    6,  788,   48,   37,  889,    4,    3],</span><br><span class="line">        [   2,    8,   63,  124,   45,   95,    4,    3],</span><br><span class="line">        [   2,  921,   10,   21,  640,  350,    4,    3],</span><br><span class="line">        [   2,   52,   10,    6,  296,   44,   11,    3],</span><br><span class="line">        [   2,  681,   10,  190,   24,  146,   11,    3],</span><br><span class="line">        [   2,   19, 1480,  838,    7,  596,    4,    3],</span><br><span class="line">        [   2,   29,   90,  472, 2036,  132,    4,    3],</span><br><span class="line">        [   2,    8,   90,    9,   66,  645,    4,    3],</span><br><span class="line">        [   2,    5,  192,  257,    7,  684,    4,    3],</span><br><span class="line">        [   2,    5,   68,   36,  384, 1686,    4,    3],</span><br><span class="line">        [   2,   12,   10,  120,   38,   23,    4,    3],</span><br><span class="line">        [   2,   18,   47,  965,  106,  112,    4,    3],</span><br><span class="line">        [   2,    8,   30,   37,    9,  250,    4,    3],</span><br><span class="line">        [   2,   31,   20,  129,   20,  900,   11,    3],</span><br><span class="line">        [   2,   29,  519,  118, 2044, 1313,    4,    3],</span><br><span class="line">        [   2,   29,   22,    6,  294,  229,    4,    3],</span><br><span class="line">        [   2,   25,  189, 1056,  335,  151,    4,    3],</span><br><span class="line">        [   2,    8,   67,   89,   57,  887,    4,    3],</span><br><span class="line">        [   2,   41,    8,   72,   59,  362,   11,    3],</span><br><span class="line">        [   2,   51,  923, 2534,   26,  364,    4,    3],</span><br><span class="line">        [   2,   22,    8, 1209,  914,  834,   11,    3],</span><br><span class="line">        [   2,   19,   48,    9, 1127,  847,    4,    3],</span><br><span class="line">        [   2,   25,  224,   70,   13,  425,    4,    3],</span><br><span class="line">        [   2,   19,  949,   62, 1112,  657,    4,    3],</span><br><span class="line">        [   2,   87,   10,    6,  751,  443,   11,    3],</span><br><span class="line">        [   2,   19,  144,   99,    9,  539,    4,    3],</span><br><span class="line">        [   2,   19,  599,  242,  117,  103,    4,    3],</span><br><span class="line">        [   2,   14,    8,   22,    9,  386,   11,    3],</span><br><span class="line">        [   2,   16,   20,   60,    7,   45,    4,    3],</span><br><span class="line">        [   2,   25,  145,  133,   10, 1974,    4,    3],</span><br><span class="line">        [   2,   25,   10,  426,   17,  343,    4,    3],</span><br><span class="line">        [   2,    5,   22,  239,    6,  461,    4,    3],</span><br><span class="line">        [   2,   14,   13,    8,  162,  242,   11,    3],</span><br><span class="line">        [   2,    8,   67,   13,  159,   59,    4,    3],</span><br><span class="line">        [   2,  140, 3452, 1220,   33,  601,    4,    3],</span><br><span class="line">        [   2,    5,   79, 1937,   35,  232,    4,    3],</span><br><span class="line">        [   2,   18, 1612,   35,  779,  926,    4,    3],</span><br><span class="line">        [   2,   12,  197,  599,    6,  632,    4,    3]], dtype=int32),</span><br><span class="line"> array([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,</span><br><span class="line">        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,</span><br><span class="line">        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8],</span><br><span class="line">       dtype=int32),</span><br><span class="line"> array([[  2,   9, 793, ...,   0,   0,   0],</span><br><span class="line">        [  2,   9, 504, ...,   0,   0,   0],</span><br><span class="line">        [  2,   8, 114, ...,   0,   0,   0],</span><br><span class="line">        ...,</span><br><span class="line">        [  2,   5, 154, ...,   0,   0,   0],</span><br><span class="line">        [  2, 214, 171, ..., 838,   4,   3],</span><br><span class="line">        [  2,   9,  74, ...,   0,   0,   0]], dtype=int32),</span><br><span class="line"> array([10, 12,  9, 10,  8, 10,  7, 13, 17,  8, 11, 10, 11,  9,  9, 12,  8,</span><br><span class="line">        12, 10,  9, 14,  9,  9,  6,  9, 10,  9, 10, 13, 11, 14, 13, 14,  8,</span><br><span class="line">         8, 10, 10,  9,  8,  7, 14, 12, 13, 13, 13, 12, 13,  8, 11, 11, 10,</span><br><span class="line">        12, 10,  9,  6, 10,  8, 11,  9, 11, 10, 12, 21,  9], dtype=int32))</span><br></pre></td></tr></table></figure><h3 id="æ²¡æœ‰Attentionçš„ç‰ˆæœ¬"><a href="#æ²¡æœ‰Attentionçš„ç‰ˆæœ¬" class="headerlink" title="æ²¡æœ‰Attentionçš„ç‰ˆæœ¬"></a>æ²¡æœ‰Attentionçš„ç‰ˆæœ¬</h3><p>ä¸‹é¢æ˜¯ä¸€ä¸ªæ›´ç®€å•çš„æ²¡æœ‰Attentionçš„encoder decoderæ¨¡å‹</p><p>In [143]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PlainEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, hidden_size, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        <span class="comment">#ä»¥è‹±æ–‡ä¸ºä¾‹ï¼Œvocab_size=5493, hidden_size=100, dropout=0.2</span></span><br><span class="line">        super(PlainEncoder, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, hidden_size)</span><br><span class="line">        <span class="comment">#è¿™é‡Œçš„hidden_sizeä¸ºembedding_dimï¼šä¸€ä¸ªå•è¯çš„ç»´åº¦ </span></span><br><span class="line">        <span class="comment">#torch.nn.Embedding(num_embeddings, embedding_dim, .....)</span></span><br><span class="line">        <span class="comment">#è¿™é‡Œçš„hidden_size = 100</span></span><br><span class="line">        </span><br><span class="line">        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=<span class="literal">True</span>)      </span><br><span class="line">        <span class="comment">#ç¬¬ä¸€ä¸ªå‚æ•°ä¸ºinput_size ï¼šè¾“å…¥ç‰¹å¾æ•°é‡</span></span><br><span class="line">        <span class="comment">#ç¬¬äºŒä¸ªå‚æ•°ä¸ºhidden_size ï¼šéšè—å±‚ç‰¹å¾æ•°é‡</span></span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, lengths)</span>:</span> </span><br><span class="line">        <span class="comment">#xæ˜¯è¾“å…¥çš„batchçš„æ‰€æœ‰å•è¯ï¼Œlengthsï¼šbatché‡Œæ¯ä¸ªå¥å­çš„é•¿åº¦</span></span><br><span class="line">        <span class="comment">#å› ä¸ºéœ€è¦æŠŠæœ€åä¸€ä¸ªhidden stateå–å‡ºæ¥ï¼Œéœ€è¦çŸ¥é“é•¿åº¦ï¼Œå› ä¸ºå¥å­é•¿åº¦ä¸ä¸€æ ·</span></span><br><span class="line">        <span class="comment">##print(x.shape,lengths),x.sahpe = torch.Size([64, 10])</span></span><br><span class="line">        <span class="comment"># lengths= =tensor([10, 10, 10, ..... 10, 10, 10])</span></span><br><span class="line">        </span><br><span class="line">        sorted_len, sorted_idx = lengths.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">#æŒ‰ç…§é•¿åº¦æ’åºï¼Œdescending=Trueé•¿çš„åœ¨å‰ã€‚</span></span><br><span class="line">        <span class="comment">#è¿”å›ä¸¤ä¸ªå‚æ•°ï¼Œå¥å­é•¿åº¦å’Œæœªæ’åºå‰çš„ç´¢å¼•</span></span><br><span class="line">        <span class="comment"># sorted_idx=tensor([41, 40, 46, 45,...... 19, 18, 63])</span></span><br><span class="line">        <span class="comment"># sorted_len=tensor([10, 10, 10, ..... 10, 10, 10])</span></span><br><span class="line">        </span><br><span class="line">        x_sorted = x[sorted_idx.long()] <span class="comment">#å¥å­ç”¨æ–°çš„idxï¼ŒæŒ‰é•¿åº¦æ’å¥½åºäº†</span></span><br><span class="line">        </span><br><span class="line">        embedded = self.dropout(self.embed(x_sorted))</span><br><span class="line">        <span class="comment">#print(embedded.shape)=torch.Size([64, 10, 100])</span></span><br><span class="line">        <span class="comment">#tensor([[[-0.6312, -0.9863, -0.3123,  ..., -0.7384,  0.9230, -0.4311],....</span></span><br><span class="line"></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">#è¿™ä¸ªå‡½æ•°å°±æ˜¯ç”¨æ¥å¤„ç†ä¸åŒé•¿åº¦çš„å¥å­çš„ï¼Œhttps: // www.cnblogs.com / sbj123456789 / p / 9834018. html</span></span><br><span class="line"></span><br><span class="line">        packed_out, hid = self.rnn(packed_embedded)</span><br><span class="line">        <span class="comment">#hid.shape = torch.Size([1, 64, 100])</span></span><br><span class="line">        </span><br><span class="line">        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">#out.shape = torch.Size([64, 10, 100]),</span></span><br><span class="line"></span><br><span class="line">        _, original_idx = sorted_idx.sort(<span class="number">0</span>, descending=<span class="literal">False</span>)</span><br><span class="line">        out = out[original_idx.long()].contiguous()</span><br><span class="line">        hid = hid[:, original_idx.long()].contiguous()</span><br><span class="line">        <span class="comment">#out.shape = torch.Size([64, 10, 100])</span></span><br><span class="line">        <span class="comment">#hid.shape = torch.Size([1, 64, 100])</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> out, hid[[<span class="number">-1</span>]] <span class="comment">#æœ‰æ—¶å€™num_layerså±‚æ•°å¤šï¼Œéœ€è¦å–å‡ºæœ€åä¸€å±‚</span></span><br></pre></td></tr></table></figure><p>In [124]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PlainDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, hidden_size, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        super(PlainDecoder, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, hidden_size)</span><br><span class="line">        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.out = nn.Linear(hidden_size, vocab_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, y, y_lengths, hid)</span>:</span></span><br><span class="line">        <span class="comment">#print(y.shape)=torch.Size([64, 12])</span></span><br><span class="line">        <span class="comment">#print(hid.shape)=torch.Size([1, 64, 100])</span></span><br><span class="line">        <span class="comment">#ä¸­æ–‡çš„yå’Œy_lengths</span></span><br><span class="line">        sorted_len, sorted_idx = y_lengths.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">        y_sorted = y[sorted_idx.long()]</span><br><span class="line">        hid = hid[:, sorted_idx.long()] <span class="comment">#éšè—å±‚ä¹Ÿè¦æ’åº</span></span><br><span class="line"></span><br><span class="line">        y_sorted = self.dropout(self.embed(y_sorted)) </span><br><span class="line">        <span class="comment"># batch_size, output_length, embed_size</span></span><br><span class="line"></span><br><span class="line">        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=<span class="literal">True</span>)</span><br><span class="line">        out, hid = self.rnn(packed_seq, hid) <span class="comment">#åŠ ä¸Šéšè—å±‚</span></span><br><span class="line">        <span class="comment">#print(hid.shape)=torch.Size([1, 64, 100])</span></span><br><span class="line">        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=<span class="literal">True</span>)</span><br><span class="line">        _, original_idx = sorted_idx.sort(<span class="number">0</span>, descending=<span class="literal">False</span>)</span><br><span class="line">        output_seq = unpacked[original_idx.long()].contiguous()</span><br><span class="line">        <span class="comment">#print(output_seq.shape)=torch.Size([64, 12, 100])</span></span><br><span class="line">        hid = hid[:, original_idx.long()].contiguous()</span><br><span class="line">        <span class="comment">#print(hid.shape)=torch.Size([1, 64, 100])</span></span><br><span class="line">        output = F.log_softmax(self.out(output_seq), <span class="number">-1</span>)</span><br><span class="line">        <span class="comment">#print(output.shape)=torch.Size([64, 12, 3195])</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, hid</span><br></pre></td></tr></table></figure><p>In [144]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PlainSeq2Seq</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder)</span>:</span></span><br><span class="line">        <span class="comment">#encoderæ˜¯ä¸Šé¢PlainEncoderçš„å®ä¾‹</span></span><br><span class="line">        <span class="comment">#decoderæ˜¯ä¸Šé¢PlainDecoderçš„å®ä¾‹</span></span><br><span class="line">        super(PlainSeq2Seq, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">       </span><br><span class="line">    <span class="comment">#æŠŠä¸¤ä¸ªæ¨¡å‹ä¸²èµ·æ¥ </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, x_lengths, y, y_lengths)</span>:</span></span><br><span class="line">        encoder_out, hid = self.encoder(x, x_lengths)</span><br><span class="line">        <span class="comment">#self.encoder(x, x_lengths)è°ƒç”¨PlainEncoderé‡Œé¢forwardçš„æ–¹æ³•</span></span><br><span class="line">        <span class="comment">#è¿”å›forwardçš„outå’Œhid</span></span><br><span class="line">        </span><br><span class="line">        output, hid = self.decoder(y=y,y_lengths=y_lengths,hid=hid)</span><br><span class="line">        <span class="comment">#self.dencoder()è°ƒç”¨PlainDecoderé‡Œé¢forwardçš„æ–¹æ³•</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">translate</span><span class="params">(self, x, x_lengths, y, max_length=<span class="number">10</span>)</span>:</span></span><br><span class="line">        <span class="comment">#xæ˜¯ä¸€ä¸ªå¥å­ï¼Œç”¨æ•°å€¼è¡¨ç¤º</span></span><br><span class="line">        <span class="comment">#yæ˜¯å¥å­çš„é•¿åº¦</span></span><br><span class="line">        <span class="comment">#yæ˜¯â€œbosâ€çš„æ•°å€¼ç´¢å¼•=2</span></span><br><span class="line">        </span><br><span class="line">        encoder_out, hid = self.encoder(x, x_lengths)</span><br><span class="line">        preds = []</span><br><span class="line">        batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">        attns = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(max_length):</span><br><span class="line">            output, hid = self.decoder(y=y,</span><br><span class="line">                    y_lengths=torch.ones(batch_size).long().to(y.device),</span><br><span class="line">                    hid=hid) </span><br><span class="line">            </span><br><span class="line"><span class="comment">#åˆšå¼€å§‹å¾ªç¯bosä½œä¸ºæ¨¡å‹çš„é¦–ä¸ªè¾“å…¥å•è¯ï¼Œåç»­æ›´æ–°yï¼Œä¸‹ä¸ªé¢„æµ‹å•è¯çš„è¾“å…¥æ˜¯ä¸Šä¸ªè¾“å‡ºå•è¯</span></span><br><span class="line">            y = output.max(<span class="number">2</span>)[<span class="number">1</span>].view(batch_size, <span class="number">1</span>)</span><br><span class="line">            preds.append(y)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> torch.cat(preds, <span class="number">1</span>), <span class="literal">None</span></span><br></pre></td></tr></table></figure><p>In [145]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">hidden_size = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#ä¼ å…¥ä¸­æ–‡å’Œè‹±æ–‡å‚æ•°</span></span><br><span class="line">encoder = PlainEncoder(vocab_size=en_total_words,</span><br><span class="line">                      hidden_size=hidden_size,</span><br><span class="line">                      dropout=dropout)</span><br><span class="line">decoder = PlainDecoder(vocab_size=cn_total_words,</span><br><span class="line">                      hidden_size=hidden_size,</span><br><span class="line">                      dropout=dropout)</span><br><span class="line">model = PlainSeq2Seq(encoder, decoder)</span><br></pre></td></tr></table></figure><p>In [146]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># masked cross entropy loss</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LanguageModelCriterion</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LanguageModelCriterion, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, target, mask)</span>:</span></span><br><span class="line">        <span class="comment">#target=tensor([[5,108,8,4,3,0,0,0,0,0,0,0],....</span></span><br><span class="line">        <span class="comment">#  mask=tensor([[1,1 ,1,1,1,0,0,0,0,0,0,0],.....</span></span><br><span class="line">        <span class="comment">#print(input.shape,target.shape,mask.shape)</span></span><br><span class="line">        <span class="comment">#torch.Size([64, 12, 3195]) torch.Size([64, 12]) torch.Size([64, 12])</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># input: (batch_size * seq_len) * vocab_size</span></span><br><span class="line">        input = input.contiguous().view(<span class="number">-1</span>, input.size(<span class="number">2</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># target: batch_size * 1=768*1</span></span><br><span class="line">        target = target.contiguous().view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        mask = mask.contiguous().view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment">#print(-input.gather(1, target))</span></span><br><span class="line">        output = -input.gather(<span class="number">1</span>, target) * mask</span><br><span class="line"><span class="comment">#è¿™é‡Œç®—å¾—å°±æ˜¯äº¤å‰ç†µæŸå¤±ï¼Œå‰é¢å·²ç»ç®—äº†F.log_softmax</span></span><br><span class="line"><span class="comment">#.gatherçš„ä½œç”¨https://blog.csdn.net/edogawachia/article/details/80515038</span></span><br><span class="line"><span class="comment">#output.shape=torch.Size([768, 1])</span></span><br><span class="line"><span class="comment">#maskä½œç”¨æ˜¯æŠŠpaddingä¸º0çš„åœ°æ–¹é‡ç½®ä¸ºé›¶ï¼Œå› ä¸ºinput.gatheræ—¶ï¼Œä¸º0çš„åœ°æ–¹ä¸æ˜¯é›¶äº†</span></span><br><span class="line">        </span><br><span class="line">        output = torch.sum(output) / torch.sum(mask)</span><br><span class="line">        <span class="comment">#å‡å€¼æŸå¤±</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>In [147]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = model.to(device)</span><br><span class="line">loss_fn = LanguageModelCriterion().to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters())</span><br></pre></td></tr></table></figure><p>pythonIn [151]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, data, num_epochs=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        total_num_words = total_loss = <span class="number">0.</span></span><br><span class="line">        <span class="keyword">for</span> it, (mb_x, mb_x_len, mb_y, mb_y_len) <span class="keyword">in</span> enumerate(data):</span><br><span class="line">            <span class="comment">#ï¼ˆè‹±æ–‡batchï¼Œè‹±æ–‡é•¿åº¦ï¼Œä¸­æ–‡batchï¼Œä¸­æ–‡é•¿åº¦ï¼‰</span></span><br><span class="line">            </span><br><span class="line">            mb_x = torch.from_numpy(mb_x).to(device).long()</span><br><span class="line">            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#å‰n-1ä¸ªå•è¯ä½œä¸ºè¾“å…¥ï¼Œån-1ä¸ªå•è¯ä½œä¸ºè¾“å‡ºï¼Œå› ä¸ºè¾“å…¥çš„å‰ä¸€ä¸ªå•è¯è¦é¢„æµ‹åä¸€ä¸ªå•è¯</span></span><br><span class="line">            mb_input = torch.from_numpy(mb_y[:, :<span class="number">-1</span>]).to(device).long()</span><br><span class="line">            mb_output = torch.from_numpy(mb_y[:, <span class="number">1</span>:]).to(device).long()</span><br><span class="line">            <span class="comment">#</span></span><br><span class="line">            mb_y_len = torch.from_numpy(mb_y_len<span class="number">-1</span>).to(device).long()</span><br><span class="line">            <span class="comment">#è¾“å…¥è¾“å‡ºçš„é•¿åº¦éƒ½å‡ä¸€ã€‚</span></span><br><span class="line">            </span><br><span class="line">            mb_y_len[mb_y_len&lt;=<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)</span><br><span class="line">            <span class="comment">#è¿”å›çš„æ˜¯ç±»PlainSeq2Seqé‡Œforwardå‡½æ•°çš„ä¸¤ä¸ªè¿”å›å€¼</span></span><br><span class="line">            </span><br><span class="line">            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[<span class="literal">None</span>, :] &lt; mb_y_len[:, <span class="literal">None</span>]</span><br><span class="line"><span class="comment">#mb_out_mask=tensor([[1, 1, 1,  ..., 0, 0, 0],[1, 1, 1,  ..., 0, 0, 0],</span></span><br><span class="line"><span class="comment">#mb_out_mask.shape= (64*19),è¿™å¥ä»£ç å’±ä¸æ‡‚ï¼Œè¿™ä¸ªmaskå°±æ˜¯paddingçš„ä½ç½®è®¾ç½®ä¸º0ï¼Œå…¶ä»–è®¾ç½®ä¸º1</span></span><br><span class="line"><span class="comment">#mb_out_maskå°±æ˜¯LanguageModelCriterionçš„ä¼ å…¥å‚æ•°maskã€‚</span></span><br><span class="line"></span><br><span class="line">            mb_out_mask = mb_out_mask.float()</span><br><span class="line">            </span><br><span class="line">            loss = loss_fn(mb_pred, mb_output, mb_out_mask)</span><br><span class="line">            </span><br><span class="line">            num_words = torch.sum(mb_y_len).item()</span><br><span class="line">            <span class="comment">#ä¸€ä¸ªbatché‡Œå¤šå°‘ä¸ªå•è¯</span></span><br><span class="line">            </span><br><span class="line">            total_loss += loss.item() * num_words</span><br><span class="line">            <span class="comment">#æ€»æŸå¤±ï¼Œlossè®¡ç®—çš„æ˜¯å‡å€¼æŸå¤±ï¼Œæ¯ä¸ªå•è¯éƒ½æ˜¯éƒ½æœ‰æŸå¤±ï¼Œæ‰€ä»¥ä¹˜ä»¥å•è¯æ•°</span></span><br><span class="line">            </span><br><span class="line">            total_num_words += num_words</span><br><span class="line">            <span class="comment">#æ€»å•è¯æ•°</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># æ›´æ–°æ¨¡å‹</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">5.</span>)</span><br><span class="line">            <span class="comment">#ä¸ºäº†é˜²æ­¢æ¢¯åº¦è¿‡å¤§ï¼Œè®¾ç½®æ¢¯åº¦çš„é˜ˆå€¼</span></span><br><span class="line">            </span><br><span class="line">            optimizer.step()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> it % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"Epoch"</span>, epoch, <span class="string">"iteration"</span>, it, <span class="string">"loss"</span>, loss.item())</span><br><span class="line"></span><br><span class="line">                </span><br><span class="line">        print(<span class="string">"Epoch"</span>, epoch, <span class="string">"Training loss"</span>, total_loss/total_num_words)</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            evaluate(model, dev_data) <span class="comment">#è¯„ä¼°æ¨¡å‹</span></span><br><span class="line">train(model, train_data, num_epochs=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0 iteration 0 loss 4.277793884277344</span><br><span class="line">Epoch 0 iteration 100 loss 3.5520756244659424</span><br><span class="line">Epoch 0 iteration 200 loss 3.483494997024536</span><br><span class="line">Epoch 0 Training loss 3.6435126089915557</span><br><span class="line">Evaluation loss 3.698509503997669</span><br><span class="line">Epoch 1 iteration 0 loss 4.158623218536377</span><br><span class="line">Epoch 1 iteration 100 loss 3.412541389465332</span><br><span class="line">Epoch 1 iteration 200 loss 3.3976175785064697</span><br><span class="line">Epoch 1 Training loss 3.5087569079050698</span><br></pre></td></tr></table></figure><p>In [135]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, data)</span>:</span></span><br><span class="line">    model.eval()</span><br><span class="line">    total_num_words = total_loss = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():<span class="comment">#ä¸éœ€è¦æ›´æ–°æ¨¡å‹ï¼Œä¸éœ€è¦æ¢¯åº¦</span></span><br><span class="line">        <span class="keyword">for</span> it, (mb_x, mb_x_len, mb_y, mb_y_len) <span class="keyword">in</span> enumerate(data):</span><br><span class="line">            mb_x = torch.from_numpy(mb_x).to(device).long()</span><br><span class="line">            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()</span><br><span class="line">            mb_input = torch.from_numpy(mb_y[:, :<span class="number">-1</span>]).to(device).long()</span><br><span class="line">            mb_output = torch.from_numpy(mb_y[:, <span class="number">1</span>:]).to(device).long()</span><br><span class="line">            mb_y_len = torch.from_numpy(mb_y_len<span class="number">-1</span>).to(device).long()</span><br><span class="line">            mb_y_len[mb_y_len&lt;=<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)</span><br><span class="line"></span><br><span class="line">            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[<span class="literal">None</span>, :] &lt; mb_y_len[:, <span class="literal">None</span>]</span><br><span class="line">            mb_out_mask = mb_out_mask.float()</span><br><span class="line"></span><br><span class="line">            loss = loss_fn(mb_pred, mb_output, mb_out_mask)</span><br><span class="line"></span><br><span class="line">            num_words = torch.sum(mb_y_len).item()</span><br><span class="line">            total_loss += loss.item() * num_words</span><br><span class="line">            total_num_words += num_words</span><br><span class="line">    print(<span class="string">"Evaluation loss"</span>, total_loss/total_num_words)</span><br></pre></td></tr></table></figure><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#ç¿»è¯‘ä¸ªå¥å­çœ‹çœ‹ç»“æœå’‹æ ·</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">translate_dev</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="comment">#éšä¾¿å–å‡ºå¥å­</span></span><br><span class="line">    en_sent = <span class="string">" "</span>.join([inv_en_dict[w] <span class="keyword">for</span> w <span class="keyword">in</span> dev_en[i]])</span><br><span class="line">    print(en_sent)</span><br><span class="line">    cn_sent = <span class="string">" "</span>.join([inv_cn_dict[w] <span class="keyword">for</span> w <span class="keyword">in</span> dev_cn[i]])</span><br><span class="line">    print(<span class="string">""</span>.join(cn_sent))</span><br><span class="line"></span><br><span class="line">    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(<span class="number">1</span>, <span class="number">-1</span>)).long().to(device)</span><br><span class="line">    <span class="comment">#æŠŠå¥å­å‡ç»´ï¼Œå¹¶è½¬æ¢æˆtensor</span></span><br><span class="line">    </span><br><span class="line">    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)</span><br><span class="line">    <span class="comment">#å–å‡ºå¥å­é•¿åº¦ï¼Œå¹¶è½¬æ¢æˆtensor</span></span><br><span class="line">    </span><br><span class="line">    bos = torch.Tensor([[cn_dict[<span class="string">"BOS"</span>]]]).long().to(device)</span><br><span class="line">    <span class="comment">#bos=tensor([[2]])</span></span><br><span class="line"></span><br><span class="line">    translation, attn = model.translate(mb_x, mb_x_len, bos)</span><br><span class="line">    <span class="comment">#è¿™é‡Œä¼ å…¥bosä½œä¸ºé¦–ä¸ªå•è¯çš„è¾“å…¥</span></span><br><span class="line">    <span class="comment">#translation=tensor([[ 8,  6, 11, 25, 22, 57, 10,  5,  6,  4]])</span></span><br><span class="line">    </span><br><span class="line">    translation = [inv_cn_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> translation.data.cpu().numpy().reshape(<span class="number">-1</span>)]</span><br><span class="line">    trans = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> translation:</span><br><span class="line">        <span class="keyword">if</span> word != <span class="string">"EOS"</span>: <span class="comment"># æŠŠæ•°å€¼å˜æˆå•è¯å½¢å¼</span></span><br><span class="line">            trans.append(word) <span class="comment">#</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    print(<span class="string">""</span>.join(trans))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>,<span class="number">120</span>):</span><br><span class="line">    translate_dev(i)</span><br><span class="line">    print()</span><br></pre></td></tr></table></figure><p>æ•°æ®å…¨éƒ¨å¤„ç†å®Œæˆï¼Œç°åœ¨æˆ‘ä»¬å¼€å§‹æ„å»ºseq2seqæ¨¡å‹</p><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><ul><li>Encoderæ¨¡å‹çš„ä»»åŠ¡æ˜¯æŠŠè¾“å…¥æ–‡å­—ä¼ å…¥embeddingå±‚å’ŒGRUå±‚ï¼Œè½¬æ¢æˆä¸€äº›hidden statesä½œä¸ºåç»­çš„context vectors</li></ul><h2 id="ä¸‹é¢çš„æ³¨é‡Šæˆ‘å…ˆæŠŠåŸç†æ‹æ¸…æ¥šå§"><a href="#ä¸‹é¢çš„æ³¨é‡Šæˆ‘å…ˆæŠŠåŸç†æ‹æ¸…æ¥šå§" class="headerlink" title="ä¸‹é¢çš„æ³¨é‡Šæˆ‘å…ˆæŠŠåŸç†æ‹æ¸…æ¥šå§"></a>ä¸‹é¢çš„æ³¨é‡Šæˆ‘å…ˆæŠŠåŸç†æ‹æ¸…æ¥šå§</h2><p>In [0]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        </span><br><span class="line">        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=<span class="literal">True</span>, bidirectional=<span class="literal">True</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.fc = nn.Linear(enc_hidden_size * <span class="number">2</span>, dec_hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, lengths)</span>:</span></span><br><span class="line">        sorted_len, sorted_idx = lengths.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">        x_sorted = x[sorted_idx.long()]</span><br><span class="line">        embedded = self.dropout(self.embed(x_sorted))</span><br><span class="line">        </span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=<span class="literal">True</span>)</span><br><span class="line">        packed_out, hid = self.rnn(packed_embedded)</span><br><span class="line">        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=<span class="literal">True</span>)</span><br><span class="line">        _, original_idx = sorted_idx.sort(<span class="number">0</span>, descending=<span class="literal">False</span>)</span><br><span class="line">        out = out[original_idx.long()].contiguous()</span><br><span class="line">        hid = hid[:, original_idx.long()].contiguous()</span><br><span class="line">        </span><br><span class="line">        hid = torch.cat([hid[<span class="number">-2</span>], hid[<span class="number">-1</span>]], dim=<span class="number">1</span>)</span><br><span class="line">        hid = torch.tanh(self.fc(hid)).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out, hid</span><br></pre></td></tr></table></figure><h4 id="Luong-Attention"><a href="#Luong-Attention" class="headerlink" title="Luong Attention"></a>Luong Attention</h4><ul><li>æ ¹æ®context vectorså’Œå½“å‰çš„è¾“å‡ºhidden statesï¼Œè®¡ç®—è¾“å‡º</li></ul><p>In [0]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, enc_hidden_size, dec_hidden_size)</span>:</span></span><br><span class="line">        super(Attention, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.enc_hidden_size = enc_hidden_size</span><br><span class="line">        self.dec_hidden_size = dec_hidden_size</span><br><span class="line"></span><br><span class="line">        self.linear_in = nn.Linear(enc_hidden_size*<span class="number">2</span>, dec_hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.linear_out = nn.Linear(enc_hidden_size*<span class="number">2</span> + dec_hidden_size, dec_hidden_size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, output, context, mask)</span>:</span></span><br><span class="line">        <span class="comment"># output: batch_size, output_len, dec_hidden_size</span></span><br><span class="line">        <span class="comment"># context: batch_size, context_len, 2*enc_hidden_size</span></span><br><span class="line">    </span><br><span class="line">        batch_size = output.size(<span class="number">0</span>)</span><br><span class="line">        output_len = output.size(<span class="number">1</span>)</span><br><span class="line">        input_len = context.size(<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        context_in = self.linear_in(context.view(batch_size*input_len, <span class="number">-1</span>)).view(                </span><br><span class="line">            batch_size, input_len, <span class="number">-1</span>) <span class="comment"># batch_size, context_len, dec_hidden_size</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># context_in.transpose(1,2): batch_size, dec_hidden_size, context_len </span></span><br><span class="line">        <span class="comment"># output: batch_size, output_len, dec_hidden_size</span></span><br><span class="line">        attn = torch.bmm(output, context_in.transpose(<span class="number">1</span>,<span class="number">2</span>)) </span><br><span class="line">        <span class="comment"># batch_size, output_len, context_len</span></span><br><span class="line"></span><br><span class="line">        attn.data.masked_fill(mask, <span class="number">-1e6</span>)</span><br><span class="line"></span><br><span class="line">        attn = F.softmax(attn, dim=<span class="number">2</span>) </span><br><span class="line">        <span class="comment"># batch_size, output_len, context_len</span></span><br><span class="line"></span><br><span class="line">        context = torch.bmm(attn, context) </span><br><span class="line">        <span class="comment"># batch_size, output_len, enc_hidden_size</span></span><br><span class="line">        </span><br><span class="line">        output = torch.cat((context, output), dim=<span class="number">2</span>) <span class="comment"># batch_size, output_len, hidden_size*2</span></span><br><span class="line"></span><br><span class="line">        output = output.view(batch_size*output_len, <span class="number">-1</span>)</span><br><span class="line">        output = torch.tanh(self.linear_out(output))</span><br><span class="line">        output = output.view(batch_size, output_len, <span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> output, attn</span><br></pre></td></tr></table></figure><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><ul><li>decoderä¼šæ ¹æ®å·²ç»ç¿»è¯‘çš„å¥å­å†…å®¹ï¼Œå’Œcontext vectorsï¼Œæ¥å†³å®šä¸‹ä¸€ä¸ªè¾“å‡ºçš„å•è¯</li></ul><p>In [0]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.attention = Attention(enc_hidden_size, dec_hidden_size)</span><br><span class="line">        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.out = nn.Linear(dec_hidden_size, vocab_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_mask</span><span class="params">(self, x_len, y_len)</span>:</span></span><br><span class="line">        <span class="comment"># a mask of shape x_len * y_len</span></span><br><span class="line">        device = x_len.device</span><br><span class="line">        max_x_len = x_len.max()</span><br><span class="line">        max_y_len = y_len.max()</span><br><span class="line">        x_mask = torch.arange(max_x_len, device=x_len.device)[<span class="literal">None</span>, :] &lt; x_len[:, <span class="literal">None</span>]</span><br><span class="line">        y_mask = torch.arange(max_y_len, device=x_len.device)[<span class="literal">None</span>, :] &lt; y_len[:, <span class="literal">None</span>]</span><br><span class="line">        mask = (<span class="number">1</span> - x_mask[:, :, <span class="literal">None</span>] * y_mask[:, <span class="literal">None</span>, :]).byte()</span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, ctx, ctx_lengths, y, y_lengths, hid)</span>:</span></span><br><span class="line">        sorted_len, sorted_idx = y_lengths.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">        y_sorted = y[sorted_idx.long()]</span><br><span class="line">        hid = hid[:, sorted_idx.long()]</span><br><span class="line">        </span><br><span class="line">        y_sorted = self.dropout(self.embed(y_sorted)) <span class="comment"># batch_size, output_length, embed_size</span></span><br><span class="line"></span><br><span class="line">        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=<span class="literal">True</span>)</span><br><span class="line">        out, hid = self.rnn(packed_seq, hid)</span><br><span class="line">        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=<span class="literal">True</span>)</span><br><span class="line">        _, original_idx = sorted_idx.sort(<span class="number">0</span>, descending=<span class="literal">False</span>)</span><br><span class="line">        output_seq = unpacked[original_idx.long()].contiguous()</span><br><span class="line">        hid = hid[:, original_idx.long()].contiguous()</span><br><span class="line"></span><br><span class="line">        mask = self.create_mask(y_lengths, ctx_lengths)</span><br><span class="line"></span><br><span class="line">        output, attn = self.attention(output_seq, ctx, mask)</span><br><span class="line">        output = F.log_softmax(self.out(output), <span class="number">-1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, hid, attn</span><br></pre></td></tr></table></figure><h4 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h4><ul><li>æœ€åæˆ‘ä»¬æ„å»ºSeq2Seqæ¨¡å‹æŠŠencoder, attention, decoderä¸²åˆ°ä¸€èµ·</li></ul><p>In [0]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2Seq</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder)</span>:</span></span><br><span class="line">        super(Seq2Seq, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, x_lengths, y, y_lengths)</span>:</span></span><br><span class="line">        encoder_out, hid = self.encoder(x, x_lengths)</span><br><span class="line">        output, hid, attn = self.decoder(ctx=encoder_out, </span><br><span class="line">                    ctx_lengths=x_lengths,</span><br><span class="line">                    y=y,</span><br><span class="line">                    y_lengths=y_lengths,</span><br><span class="line">                    hid=hid)</span><br><span class="line">        <span class="keyword">return</span> output, attn</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">translate</span><span class="params">(self, x, x_lengths, y, max_length=<span class="number">100</span>)</span>:</span></span><br><span class="line">        encoder_out, hid = self.encoder(x, x_lengths)</span><br><span class="line">        preds = []</span><br><span class="line">        batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">        attns = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(max_length):</span><br><span class="line">            output, hid, attn = self.decoder(ctx=encoder_out, </span><br><span class="line">                    ctx_lengths=x_lengths,</span><br><span class="line">                    y=y,</span><br><span class="line">                    y_lengths=torch.ones(batch_size).long().to(y.device),</span><br><span class="line">                    hid=hid)</span><br><span class="line">            y = output.max(<span class="number">2</span>)[<span class="number">1</span>].view(batch_size, <span class="number">1</span>)</span><br><span class="line">            preds.append(y)</span><br><span class="line">            attns.append(attn)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(preds, <span class="number">1</span>), torch.cat(attns, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>è®­ç»ƒ</p><p>In [0]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">embed_size = hidden_size = <span class="number">100</span></span><br><span class="line">encoder = Encoder(vocab_size=en_total_words,</span><br><span class="line">                       embed_size=embed_size,</span><br><span class="line">                      enc_hidden_size=hidden_size,</span><br><span class="line">                       dec_hidden_size=hidden_size,</span><br><span class="line">                      dropout=dropout)</span><br><span class="line">decoder = Decoder(vocab_size=cn_total_words,</span><br><span class="line">                      embed_size=embed_size,</span><br><span class="line">                      enc_hidden_size=hidden_size,</span><br><span class="line">                       dec_hidden_size=hidden_size,</span><br><span class="line">                      dropout=dropout)</span><br><span class="line">model = Seq2Seq(encoder, decoder)</span><br><span class="line">model = model.to(device)</span><br><span class="line">loss_fn = LanguageModelCriterion().to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters())</span><br></pre></td></tr></table></figure><p>In [2]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(model, train_data, num_epochs=30)</span><br></pre></td></tr></table></figure><p>In [0]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for i in range(100,120):</span><br><span class="line">    translate_dev(i)</span><br><span class="line">    print()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">BOS you have nice skin . EOS</span><br><span class="line">BOS ä½  çš„ çš® è†š çœŸ å¥½ ã€‚ EOS</span><br><span class="line">ä½ å¥½å®³æ€•ã€‚</span><br><span class="line"></span><br><span class="line">BOS you &apos;re UNK correct . EOS</span><br><span class="line">BOS ä½  éƒ¨ åˆ† æ­£ ç¡® ã€‚ EOS</span><br><span class="line">ä½ æ˜¯å…¨å­çš„å£°éŸ³ã€‚</span><br><span class="line"></span><br><span class="line">BOS everyone admired his courage . EOS</span><br><span class="line">BOS æ¯ å€‹ äºº éƒ½ ä½© æœ ä»– çš„ å‹‡ æ°£ ã€‚ EOS</span><br><span class="line">ä»–çš„è¢‹å­æ˜¯ä»–çš„å‹‡æ°£ã€‚</span><br><span class="line"></span><br><span class="line">BOS what time is it ? EOS</span><br><span class="line">BOS å‡  ç‚¹ äº† ï¼Ÿ EOS</span><br><span class="line">å¤šå°‘æ—¶é—´æ˜¯ä»€ä¹ˆï¼Ÿ</span><br><span class="line"></span><br><span class="line">BOS i &apos;m free tonight . EOS</span><br><span class="line">BOS æˆ‘ ä»Š æ™š æœ‰ ç©º ã€‚ EOS</span><br><span class="line">æˆ‘ä»Šæ™šæœ‰ç©ºã€‚</span><br><span class="line"></span><br><span class="line">BOS here is your book . EOS</span><br><span class="line">BOS é€™ æ˜¯ ä½  çš„ æ›¸ ã€‚ EOS</span><br><span class="line">è¿™å„¿æ˜¯ä½ çš„ä¹¦ã€‚</span><br><span class="line"></span><br><span class="line">BOS they are at lunch . EOS</span><br><span class="line">BOS ä»– ä»¬ åœ¨ åƒ åˆ é¥­ ã€‚ EOS</span><br><span class="line">ä»–ä»¬åœ¨åˆé¤ã€‚</span><br><span class="line"></span><br><span class="line">BOS this chair is UNK . EOS</span><br><span class="line">BOS é€™ æŠŠ æ¤… å­ å¾ˆ UNK ã€‚ EOS</span><br><span class="line">é€™äº›èŠ±ä¸€ä¸‹æ˜¯æ­£åœ¨çš„ã€‚</span><br><span class="line"></span><br><span class="line">BOS it &apos;s pretty heavy . EOS</span><br><span class="line">BOS å®ƒ çœŸ é‡ ã€‚ EOS</span><br><span class="line">å®ƒå¾ˆç¾çš„è„šã€‚</span><br><span class="line"></span><br><span class="line">BOS many attended his funeral . EOS</span><br><span class="line">BOS å¾ˆ å¤š äºº éƒ½ å‚ åŠ  äº† ä»– çš„ è‘¬ ç¤¼ ã€‚ EOS</span><br><span class="line">å¤šå¤šè¡›å¹´è½»åœ°äº†ä»–ã€‚</span><br><span class="line"></span><br><span class="line">BOS training will be provided . EOS</span><br><span class="line">BOS ä¼š æœ‰ è®­ ç»ƒ ã€‚ EOS</span><br><span class="line">åˆ«å°†è¢«ä»˜éŒ¢ã€‚</span><br><span class="line"></span><br><span class="line">BOS someone is watching you . EOS</span><br><span class="line">BOS æœ‰ äºº åœ¨ çœ‹ è‘— ä½  ã€‚ EOS</span><br><span class="line">æœ‰äººçœ‹ä½ ã€‚</span><br><span class="line"></span><br><span class="line">BOS i slapped his face . EOS</span><br><span class="line">BOS æˆ‘ æ‘‘ äº† ä»– çš„ è‡‰ ã€‚ EOS</span><br><span class="line">æˆ‘æŠŠä»–çš„è‡‰æŠ±æ­‰ã€‚</span><br><span class="line"></span><br><span class="line">BOS i like UNK music . EOS</span><br><span class="line">BOS æˆ‘ å–œ æ­¡ æµ è¡Œ éŸ³ æ¨‚ ã€‚ EOS</span><br><span class="line">æˆ‘å–œæ­¡éŸ³æ¨‚ã€‚</span><br><span class="line"></span><br><span class="line">BOS tom had no children . EOS</span><br><span class="line">BOS T o m æ²’ æœ‰ å­© å­ ã€‚ EOS</span><br><span class="line">æ±¤å§†æ²¡æœ‰ç…§é¡§å­©å­ã€‚</span><br><span class="line"></span><br><span class="line">BOS please lock the door . EOS</span><br><span class="line">BOS è«‹ æŠŠ é–€ é– ä¸Š ã€‚ EOS</span><br><span class="line">è¯·æŠŠé–€é–‹é–€ã€‚</span><br><span class="line"></span><br><span class="line">BOS tom has calmed down . EOS</span><br><span class="line">BOS æ±¤ å§† å†· é™ ä¸‹ æ¥ äº† ã€‚ EOS</span><br><span class="line">æ±¤å§†åœ¨åšäº†ã€‚</span><br><span class="line"></span><br><span class="line">BOS please speak more loudly . EOS</span><br><span class="line">BOS è«‹ èªª å¤§ è² ä¸€ é» å…’ ã€‚ EOS</span><br><span class="line">è«‹èªªæ›´å¤šã€‚</span><br><span class="line"></span><br><span class="line">BOS keep next sunday free . EOS</span><br><span class="line">BOS æŠŠ ä¸‹ å‘¨ æ—¥ ç©º å‡º æ¥ ã€‚ EOS</span><br><span class="line">ç¹¼çºŒä¸‹é€±ä¸€ä¸‹ä¸€æ­¥ã€‚</span><br><span class="line"></span><br><span class="line">BOS i made a mistake . EOS</span><br><span class="line">BOS æˆ‘ çŠ¯ äº† ä¸€ å€‹ éŒ¯ ã€‚ EOS</span><br><span class="line">æˆ‘åšäº†ä¸€ä»¶äº‹ã€‚</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Seq2Seq-Attention&quot;&gt;&lt;a href=&quot;#Seq2Seq-Attention&quot; class=&quot;headerlink&quot; title=&quot;Seq2Seq, Attention&quot;&gt;&lt;/a&gt;Seq2Seq, Attention&lt;/h1&gt;&lt;p&gt;åœ¨è¿™ä»½noteb
      
    
    </summary>
    
      <category term="NLP" scheme="http://mmyblog.cn/categories/NLP/"/>
    
    
      <category term="Seq2Seq" scheme="http://mmyblog.cn/tags/Seq2Seq/"/>
    
      <category term="Attention" scheme="http://mmyblog.cn/tags/Attention/"/>
    
  </entry>
  
  <entry>
    <title>æœºå™¨ç¿»è¯‘ä¸æ–‡æœ¬æ‘˜è¦</title>
    <link href="http://mmyblog.cn/2020/04/09/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/"/>
    <id>http://mmyblog.cn/2020/04/09/æœºå™¨ç¿»è¯‘ä¸æ–‡æœ¬æ‘˜è¦/</id>
    <published>2020-04-09T00:37:53.000Z</published>
    <updated>2020-06-09T00:59:04.592Z</updated>
    
    <content type="html"><![CDATA[<h1 id="æœºå™¨ç¿»è¯‘"><a href="#æœºå™¨ç¿»è¯‘" class="headerlink" title="æœºå™¨ç¿»è¯‘"></a>æœºå™¨ç¿»è¯‘</h1><p><img src="https://uploader.shimo.im/f/brdWUlzu4FsL8Owh.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/L7hZdGTE2Rw7LaK7.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/khdZXF2WzdUjtnfh.png!thumbnail" alt="img"></p><h1 id><a href="#" class="headerlink" title></a><img src="https://uploader.shimo.im/f/OA9BcNUkI4USlE4s.png!thumbnail" alt="img"></h1><p>ç°åœ¨çš„<strong>æœºå™¨ç¿»è¯‘æ¨¡å‹éƒ½æ˜¯ç”±æ•°æ®é©±åŠ¨</strong>çš„ã€‚ä»€ä¹ˆæ•°æ®ï¼Ÿ</p><ul><li><p>æ–°é—»</p></li><li><p>å…¬å¸ç½‘é¡µ</p></li><li><p>æ³•å¾‹/ä¸“åˆ©æ–‡ä»¶ï¼Œè”åˆå›½documents</p></li><li><p>ç”µå½±/ç”µè§†å­—å¹•</p></li></ul><p>IBM fire a linguist, their machine translation system improves by 1%</p><p>Parallel Data</p><ul><li><p>æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨åŒè¯­çš„ï¼Œæœ‰å¯¹åº”å…³ç³»çš„æ•°æ®</p></li><li><p>å¤§éƒ¨åˆ†æ•°æ®éƒ½æ˜¯ç”±æ–‡æ¡£çº§åˆ«çš„</p></li></ul><p>å¦‚ä½•<strong>è¯„ä¼°</strong>ç¿»è¯‘æ¨¡å‹ï¼Ÿ</p><ul><li><p><strong>äººå·¥è¯„ä¼°</strong>æœ€å¥½ï¼Œä½†æ˜¯éå¸¸<strong>è´¹æ—¶è´¹åŠ›</strong></p></li><li><p>è¿˜æœ‰å“ªäº›é—®é¢˜éœ€è¦äººç±»è¯„ä¼°ï¼Ÿ</p></li><li><p>éœ€è¦ä¸€äº›è‡ªåŠ¨è¯„ä¼°çš„æ‰‹æ®µ</p></li><li><p><strong>BLUE</strong> (Bilingual Evaluation Understudy), Papineni et al. (2002)</p></li><li><p>è®¡ç®—ç³»ç»Ÿç”Ÿæˆç¿»è¯‘ä¸äººç±»å‚è€ƒç¿»è¯‘ä¹‹é—´çš„n-gram overlap</p></li><li><p>BLEU scoreä¸<strong>äººç±»è¯„æµ‹çš„ç›¸å…³åº¦éå¸¸é«˜</strong></p></li><li><p><a href="https://www.aclweb.org/anthology/P02-1040.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/P02-1040.pdf</a></p></li><li><p>precision based metric</p></li><li><p>è‡ªåŠ¨è¯„ä¼°ä¾ç„¶æ˜¯ä¸€ä¸ª<strong>æœ‰ä»·å€¼çš„ç ”ç©¶é—®é¢˜</strong></p></li></ul><p>precision: åœ¨æˆ‘ç¿»è¯‘çš„å•è¯å½“ä¸­ï¼Œæœ‰å“ªäº›å•è¯æ˜¯æ­£ç¡®çš„ã€‚</p><p>unigram, bigram, trigram, 4-gram precision </p><p><strong>BLEU-4</strong>: average of the 4 kinds of grams</p><p><strong>BLEU-3</strong></p><p>ç»Ÿè®¡å­¦ç¿»è¯‘æ¨¡å‹</p><p><img src="https://uploader.shimo.im/f/phryZdcQGH8Z5i5V.png!thumbnail" alt="img"></p><p>Encoder-decoder æ¨¡å‹</p><p>xï¼šè‹±æ–‡</p><p><strong>yï¼šä¸­æ–‡</strong></p><p>P(y|x) x: noisy input</p><p><img src="https://uploader.shimo.im/f/1zBNjrukMK8ennr8.png!thumbnail" alt="img"></p><p>P(y|x) = P(x, y) / P(x) = P(x|y)P(y) / P(x)</p><p>argmax_y P(y|x) = <strong>argmax_y P(x|y)P(y)</strong></p><p><strong>P(x|y)</strong> </p><p><strong>P(y)</strong></p><h2 id="Encoder-Decoder-Model"><a href="#Encoder-Decoder-Model" class="headerlink" title="Encoder-Decoder Model"></a>Encoder-Decoder Model</h2><p><img src="https://uploader.shimo.im/f/fSgtSMHGwlsMwqR8.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/NJXXeu6kA4QJzzj3.png!thumbnail" alt="img"></p><p>RNN(x) â€“&gt; c (<strong>cèƒ½å¤Ÿå®Œå…¨åŒ…å«æ•´ä¸ªå¥å­çš„ä¿¡æ¯?</strong>ï¼‰</p><p>RNN(c) â€“&gt; y (cä½œä¸ºè¾“å…¥è¿›å…¥æ¯ä¸€ä¸ªdecoding step)</p><p>è®­ç»ƒæ–¹å¼æ˜¯ä»€ä¹ˆï¼ŸæŸå¤±å‡½æ•°æ˜¯ä»€ä¹ˆï¼Ÿ</p><ul><li><p>cross entropy lossï¼Œ ä½œä¸šä¸€ä¸­çš„contextæ¨¡å‹</p></li><li><p>SGD, Adam</p></li></ul><p>GRU</p><p><a href="https://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1406.1078.pdf</a></p><p><img src="https://uploader.shimo.im/f/UBhRdKWsAvEKpbz0.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/eF9pRfBLFSQi5NHd.png!thumbnail" alt="img"></p><h2 id="Attentionæœºåˆ¶"><a href="#Attentionæœºåˆ¶" class="headerlink" title="Attentionæœºåˆ¶"></a>Attentionæœºåˆ¶</h2><p><img src="https://uploader.shimo.im/f/CkL5KNLrUQE2tzH4.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/JQtrWTdEgiURNKTX.png!thumbnail" alt="img"></p><p>å›¾ç‰‡æ¥è‡ª Bahdanau et al., Neural Machine Translation by Jointly Learning to Align and Translate <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1409.0473.pdf</a></p><h2 id="-1"><a href="#-1" class="headerlink" title></a><img src="https://uploader.shimo.im/f/dWsWHO9MF20QJ2kI.png!thumbnail" alt="img"></h2><p><img src="https://uploader.shimo.im/f/KRbuH9pTLpoNLHN7.png!thumbnail" alt="img"></p><p>å›¾ç‰‡æ¥è‡ªLuong et al., Effective Approaches to Attention-based Neural Machine Translation</p><p><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1508.04025.pdf</a></p><p>Google Neural Machine Translation</p><p><a href="https://arxiv.org/pdf/1609.08144.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1609.08144.pdf</a></p><p><img src="https://uploader.shimo.im/f/en9dH9PnTeoPDMMv.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/W8BSXh4U2Kc8ZKjL.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/vpGOGoKHN5AQvKiB.png!thumbnail" alt="img"></p><h2 id="Zero-shot-NMT"><a href="#Zero-shot-NMT" class="headerlink" title="Zero-shot NMT"></a>Zero-shot NMT</h2><p><img src="https://uploader.shimo.im/f/I5SzyIfYl6sfFUoA.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/v9nmM5q6jDoyq7ZR.png!thumbnail" alt="img"></p><h2 id="Transformeræ¨¡å‹"><a href="#Transformeræ¨¡å‹" class="headerlink" title="Transformeræ¨¡å‹"></a>Transformeræ¨¡å‹</h2><p><a href="https://shimo.im/docs/gPwkqCXrkJyRW89V" target="_blank" rel="noopener">https://shimo.im/docs/gPwkqCXrkJyRW89V</a></p><p>è¿™ä¸ªæ¨¡å‹éå¸¸é‡è¦</p><p>æ¨¡å‹ x â€“&gt; encoder decoder model â€“&gt; \hat{y}</p><p>cross entropy loss (\hat{y}, y)</p><p>è®­ç»ƒ P(y_i | x, <strong>y_1, â€¦, y_{i-1}</strong>) è®­ç»ƒçš„æ—¶å€™ï¼Œæˆ‘ä»¬çŸ¥é“y_1 â€¦ y_{i-1}</p><p>åœ¨é¢„æµ‹çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¸çŸ¥é“y_1 â€¦ y_{i-1}</p><p>æ€ä¹ˆæ ·ç»Ÿä¸€è®­ç»ƒå’Œæµ‹è¯•</p><h2 id="Model-Inference"><a href="#Model-Inference" class="headerlink" title="Model Inference"></a>Model Inference</h2><p>åœ¨å„ç±»æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå…¶å®æ–‡æœ¬çš„ç”Ÿæˆä¸è®­ç»ƒæ˜¯ä¸¤ç§ä¸åŒçš„æƒ…å½¢ã€‚åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å‡è®¾æ¨¡å‹åœ¨ç”Ÿæˆä¸‹ä¸€ä¸ªå•è¯çš„æ—¶å€™çŸ¥é“æ‰€æœ‰ä¹‹å‰çš„å•è¯ï¼ˆgroud truthï¼‰ã€‚ç„¶è€Œåœ¨çœŸæ­£ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„æ—¶å€™ï¼Œæ¯ä¸€æ­¥ç”Ÿæˆçš„æ–‡æœ¬éƒ½æ¥è‡ªäºæ¨¡å‹æœ¬èº«ã€‚è¿™å…¶ä¸­è®­ç»ƒå’Œé¢„æµ‹çš„ä¸åŒå¯¼è‡´äº†æ¨¡å‹çš„æ•ˆæœå¯èƒ½ä¼šå¾ˆå·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œäººä»¬å‘æ˜äº†å„ç§æå‡æ¨¡å‹é¢„æµ‹æ°´å¹³çš„æ–¹æ³•ï¼Œä¾‹å¦‚Beam Searchã€‚</p><p><strong>Beam Search</strong></p><p>Kyunghyun Cho Lecture Notes Page 94-96 <a href="https://arxiv.org/pdf/1511.07916.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1511.07916.pdf</a></p><p>Encoder(æˆ‘å–œæ¬¢è‡ªç„¶è¯­è¨€å¤„ç†) â€“&gt; c</p><p>Decoder(c) â€“&gt; y_1</p><p>Decoder(c, y_1) â€“&gt; y_2</p><p>Decoder(c, y_1, y_2) â€“&gt; y_3</p><p>â€¦..</p><p>EOS</p><p>argmax_y P(y|x) </p><p>greedy search</p><p>argmax y_1</p><p>Beam æ¨ªæ¢</p><p>â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”</p><p>ä¸€ç§å›ºå®šå®½åº¦çš„è£…ç½®</p><p>â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”</p><p>åœ¨åç»­çš„è¯¾ç¨‹ä¸­æˆ‘ä»¬è¿˜ä¼šä»‹ç»ä¸€äº›åˆ«çš„æ–¹æ³•ç”¨äºç”Ÿæˆæ–‡æœ¬ã€‚</p><p>ç¾å›½æ€»ç»Ÿå’Œä¸­å›½ä¸»å¸­æ‰“ç”µè¯</p><p>â€“&gt; K = æ— ç©·å¤§ |V|^seq_len</p><p>American, U.S. , United</p><p>â€¦.</p><p>decoding step: K</p><p>K x |V| â€“&gt; K</p><p>K x |V| â€“&gt; K</p><h2 id="å¼€æºé¡¹ç›®"><a href="#å¼€æºé¡¹ç›®" class="headerlink" title="å¼€æºé¡¹ç›®"></a>å¼€æºé¡¹ç›®</h2><p>FairSeq <a href="https://github.com/pytorch/fairseq" target="_blank" rel="noopener">https://github.com/pytorch/fairseq</a></p><p>Tensor2Tensor <a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener">https://github.com/tensorflow/tensor2tensor</a></p><p>Trax <a href="https://github.com/google/trax" target="_blank" rel="noopener">https://github.com/google/trax</a></p><h1 id="æ–‡æœ¬æ‘˜è¦"><a href="#æ–‡æœ¬æ‘˜è¦" class="headerlink" title="æ–‡æœ¬æ‘˜è¦"></a>æ–‡æœ¬æ‘˜è¦</h1><p>æ–‡æœ¬æ‘˜è¦è¿™ä¸ªä»»åŠ¡å®šä¹‰éå¸¸ç®€å•ï¼Œç»™å®šä¸€æ®µé•¿æ–‡ç« ï¼Œæˆ‘ä»¬å¸Œæœ›ç”Ÿæˆä¸€æ®µæ¯”è¾ƒç²¾ç®€çš„æ–‡æœ¬æ‘˜è¦ï¼Œå¯ä»¥è¦†ç›–æ•´ç¯‡æ–‡ç« çš„ä¿¡æ¯ã€‚</p><p>æ–‡æœ¬æ‘˜è¦æŒ‰ç…§ä»»åŠ¡çš„å®šä¹‰å¤§è‡´å¯ä»¥åˆ†ä¸ºä¸¤ç±»ã€‚</p><ul><li><p>æŠ½å–å¼ï¼šç»™å®šä¸€ä¸ªåŒ…å«å¤šä¸ªå¥å­çš„é•¿æ–‡æœ¬ï¼Œé€‰æ‹©å…¶ä¸­çš„ä¸€äº›å¥å­ä½œä¸ºçŸ­æ–‡æœ¬ã€‚è¿™æœ¬è´¨ä¸Šæ˜¯ä¸ªåˆ†ç±»é—®é¢˜ï¼Œä¹Ÿå°±æ˜¯åˆ¤æ–­å“ªäº›å¥å­éœ€è¦ä¿ç•™ï¼Œå“ªäº›å¥å­éœ€è¦ä¸¢å¼ƒã€‚<strong>äºŒåˆ†ç±»ä»»åŠ¡</strong></p></li><li><p>ç”Ÿæˆå¼ï¼šä¸æŠ½å–å¼æ–‡æœ¬æ‘˜è¦ä¸åŒï¼Œè¿™é‡Œæˆ‘ä»¬ä¸ä»…ä»…æ˜¯å¸Œæœ›é€‰å‡ºä¸€äº›å¥å­ï¼Œè€Œæ˜¯å¸Œæœ›èƒ½å¤Ÿæ€»ç»“å½’çº³æ–‡æœ¬çš„ä¿¡æ¯ï¼Œç”¨è‡ªå·±çš„è¯å¤è¿°ä¸€éã€‚<strong>ç›´æ¥ä¸Štransformeræ¨¡å‹</strong></p></li></ul><p>gold standard</p><p>è¯„ä¼°æ‰‹æ®µ: <strong>ROUGE</strong></p><p>ROUGEè¯„ä¼°çš„æ˜¯ç³»ç»Ÿç”Ÿæˆæ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬ä¹‹é—´ n-gram overlap çš„ recallã€‚</p><p><strong>Candidate</strong> Summary</p><p>the cat was found under the bed</p><p><strong>Reference</strong> Summary</p><p>the cat was under the bed</p><p>é’ˆå¯¹è¿™ä¸€ä¸ªä¾‹å­ï¼ŒROUGE-1åˆ†æ•°ä¸º1ï¼Œ ROUGE-2ä¸º4/5ã€‚</p><p>s: the cat was found under the bed</p><p>p: <strong>the cat was under the bed</strong></p><p>ROUGE-Lï¼ŒåŸºäº longest common subsequenceçš„F1 score</p><p>ä¾‹å¦‚ä¸Šé¢è¿™ä¸ªæ¡ˆä¾‹ LCS  = 6</p><p>P = 6/7 </p><p>R = 6/6</p><p>F1 = 2 / (6/6 + 7/6 )  = 12/13</p><p>harmoic mean</p><p><img src="https://uploader.shimo.im/f/oNU6qvIsXX8cK7Ta.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/65Raa96bt7kl0ldv.png!thumbnail" alt="img"></p><p><a href="https://arxiv.org/pdf/1908.08345.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1908.08345.pdf</a></p><p><img src="https://uploader.shimo.im/f/8YQylm0VFkgRXqzU.png!thumbnail" alt="img"></p><p><img src="https://uploader.shimo.im/f/vC7ZiGFfsBInlMtQ.png!thumbnail" alt="img"></p><p>ä¸ŠæœŸå­¦å‘˜çš„åšå®¢</p><p><a href="https://blog.csdn.net/Chen_Meng_/article/details/103756716" target="_blank" rel="noopener">https://blog.csdn.net/Chen_Meng_/article/details/103756716</a></p><p>CopyNet</p><p><a href="https://arxiv.org/pdf/1603.06393.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.06393.pdf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;æœºå™¨ç¿»è¯‘&quot;&gt;&lt;a href=&quot;#æœºå™¨ç¿»è¯‘&quot; class=&quot;headerlink&quot; title=&quot;æœºå™¨ç¿»è¯‘&quot;&gt;&lt;/a&gt;æœºå™¨ç¿»è¯‘&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://uploader.shimo.im/f/brdWUlzu4FsL8Owh.png!thu
      
    
    </summary>
    
      <category term="NLP" scheme="http://mmyblog.cn/categories/NLP/"/>
    
    
      <category term="GRU" scheme="http://mmyblog.cn/tags/GRU/"/>
    
      <category term="BLUE" scheme="http://mmyblog.cn/tags/BLUE/"/>
    
  </entry>
  
</feed>
