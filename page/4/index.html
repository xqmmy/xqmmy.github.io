<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no">
<meta name="author" content="MingmingYe">


    
    


<meta property="og:type" content="website">
<meta property="og:title" content="Stay hungry, Stay foolish.">
<meta property="og:url" content="http://mmyblog.cn/page/4/index.html">
<meta property="og:site_name" content="Stay hungry, Stay foolish.">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Stay hungry, Stay foolish.">

<link rel="apple-touch-icon" href="/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="Stay hungry, Stay foolish." type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">


    <style> .article { opacity: 0;} </style>


<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>Stay hungry, Stay foolish.</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: true
    }
</script>


    <script>
        yiliaConfig.jquery_ui = [true, "//cdn.bootcss.com/jqueryui/1.10.4/jquery-ui.min.js", "//cdn.bootcss.com/jqueryui/1.10.4/css/jquery-ui.min.css"];
    </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






</head></html>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/deep.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">MingmingYe</a></h1>
        </hgroup>

        
        <p class="header-subtitle">当你的才华撑不起你的野心时，只有静下心来好好学习！纵使命运注定是个打酱油的，也要打一瓶与别人不一样的酱油！</p>
        

        
            <form id="search-form">
            <input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false" />
            <i class="fa fa-times" onclick="resetSearch()"></i>
            </form>
            <div id="local-search-result"></div>
            <p class='no-result'>No results found <i class='fa fa-spinner fa-pulse'></i></p>
        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:878759487@qq.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AUC/">AUC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention/">Attention</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/">BERT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BLUE/">BLUE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Beam-search/">Beam search</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CRF/">CRF</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ConvNet/">ConvNet</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ELMo/">ELMo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GPT/">GPT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GRU/">GRU</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gradient-Clipping/">Gradient Clipping</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LR/">LR</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LSTM/">LSTM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linear/">Linear</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Parsing/">Parsing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PyTorch/">PyTorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/QA/">QA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN-LSTM/">RNN/LSTM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ROC/">ROC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recursive-Neural-Networks/">Recursive Neural Networks</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQuAD-BiDAF/">SQuAD-BiDAF</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Seq2Seq/">Seq2Seq</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TorchText/">TorchText</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/">Transformer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer-XL/">Transformer-XL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/XLNet/">XLNet</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/boosting/">boosting</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cbow/">cbow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hierarchical-softmax/">hierarchical softmax</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/inference/">inference</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jiaba/">jiaba</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jieba/">jieba</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mumpy/">mumpy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/negative-sampling/">negative sampling</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pandas/">pandas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pyspark/">pyspark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/seaborn/">seaborn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/skip-gram/">skip-gram</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/softmax/">softmax</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/word-embedding/">word-embedding</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/wxBot/">wxBot</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/中文分词/">中文分词</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/优化方法/">优化方法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/决策树/">决策树</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/图神经网络/">图神经网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/微积分/">微积分</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/朴素贝叶斯/">朴素贝叶斯</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/概率/">概率</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/模型调优/">模型调优</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/正则表达式/">正则表达式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习速查表/">深度学习速查表</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/特征工程/">特征工程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/线性代数/">线性代数</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/统计/">统计</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/聊天机器人/">聊天机器人</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/聚类/">聚类</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/语言模型/">语言模型</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/贝叶斯分类器/">贝叶斯分类器</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/降维/">降维</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/集成学习/">集成学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/面试/">面试</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://mmyblog.cn/">mmy</a>
                    
                    </div>
                </section>
                

                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">MingmingYe</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/deep.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">MingmingYe</a></h1>
            </hgroup>
            
            <p class="header-subtitle">当你的才华撑不起你的野心时，只有静下心来好好学习！纵使命运注定是个打酱油的，也要打一瓶与别人不一样的酱油！</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:878759487@qq.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap">
  
    <article id="post-用朴素贝叶斯完成语种检测" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/11/08/用朴素贝叶斯完成语种检测/" class="article-date">
      <time datetime="2019-11-08T10:46:50.000Z" itemprop="datePublished">2019-11-08</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/11/08/用朴素贝叶斯完成语种检测/">用朴素贝叶斯完成语种检测</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="用朴素贝叶斯完成语种检测"><a href="#用朴素贝叶斯完成语种检测" class="headerlink" title="用朴素贝叶斯完成语种检测"></a>用朴素贝叶斯完成语种检测</h1><p>我们试试用朴素贝叶斯完成一个语种检测的分类器，说起来，用朴素贝叶斯完成这个任务，其实准确度还不错。</p>
<p><img src="data:image/png;base64,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" alt="image.png"></p>
<h2 id="查看数据，并划分训练集和验证集"><a href="#查看数据，并划分训练集和验证集" class="headerlink" title="查看数据，并划分训练集和验证集"></a>查看数据，并划分训练集和验证集</h2><p>In [87]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">in_f = open(<span class="string">'data.csv'</span>)</span><br><span class="line">lines = in_f.readlines() <span class="comment"># 一次读取整个文件，每行的内容放在一个字符串变量中作为列表的一个元素。</span></span><br><span class="line">in_f.close()</span><br><span class="line">dataset = [(line.strip()[:<span class="number">-3</span>], line.strip()[<span class="number">-2</span>:]) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"><span class="comment"># 把句子和语言分开，这里.strip()把\n去掉了。</span></span><br><span class="line">print(lines[<span class="number">0</span>]) <span class="comment"># 打印第一个句子</span></span><br><span class="line">print(dataset[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 december wereld aids dag voorlichting in zuidafrika over bieten taboes en optimisme,nl</span><br><span class="line"></span><br><span class="line">(&apos;1 december wereld aids dag voorlichting in zuidafrika over bieten taboes en optimisme&apos;, &apos;nl&apos;)</span><br></pre></td></tr></table></figure>

<p>In [88]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x, y = zip(*dataset) <span class="comment"># 解压缩</span></span><br><span class="line">x, y = list(x),list(y) </span><br><span class="line">print(x[<span class="number">0</span>],y[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">"-----"</span>*<span class="number">10</span>)</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 默认0.25的比例</span></span><br><span class="line"></span><br><span class="line">print(len(x_train),x_train[<span class="number">0</span>],y_train[<span class="number">0</span>])</span><br><span class="line">print(len(x_test),x_test[<span class="number">0</span>],y_test[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1 december wereld aids dag voorlichting in zuidafrika over bieten taboes en optimisme nl</span><br><span class="line">--------------------------------------------------</span><br><span class="line">6799 io non ho ipad ma mi sa che è fatta un po meglio sfrutta meglio la superficie it</span><br><span class="line">2267 ook jullie bedankt voor jullie steun nl</span><br></pre></td></tr></table></figure>

<h2 id="数据去燥，设置正则化规则"><a href="#数据去燥，设置正则化规则" class="headerlink" title="数据去燥，设置正则化规则"></a>数据去燥，设置正则化规则</h2><p>In [89]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="comment">#因为是twitter的数据，有时候会有一些@ # http等字符。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_noise</span><span class="params">(document)</span>:</span></span><br><span class="line">    noise_pattern = re.compile(<span class="string">"|"</span>.join([<span class="string">"http\S+"</span>, <span class="string">"\@\w+"</span>, <span class="string">"\#\w+"</span>]))</span><br><span class="line">    <span class="comment"># 关于正则表达式的使用方法，看第一节课</span></span><br><span class="line">    </span><br><span class="line">    clean_text = re.sub(noise_pattern, <span class="string">""</span>, document)</span><br><span class="line">    <span class="comment"># 使用空白""替换document中每一个匹配noise_pattern规则的子串。</span></span><br><span class="line">    <span class="keyword">return</span> clean_text.strip()</span><br><span class="line"></span><br><span class="line"><span class="comment">#举例</span></span><br><span class="line">remove_noise(<span class="string">"Trump images are now more popular than cat gifs. @trump a #trends http://www.trumptrends.html"</span>)</span><br><span class="line"><span class="comment"># http\S+ 匹配的是http://www.trumptrends.html</span></span><br><span class="line"><span class="comment"># \@\w+ 匹配的是@trump</span></span><br><span class="line"><span class="comment"># \#\w+ 匹配的是#trends</span></span><br></pre></td></tr></table></figure>

<p>Out[89]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;Trump images are now more popular than cat gifs.  a&apos;</span><br></pre></td></tr></table></figure>

<h2 id="统计词频，并选取1000个特征作为输入的特征维度"><a href="#统计词频，并选取1000个特征作为输入的特征维度" class="headerlink" title="统计词频，并选取1000个特征作为输入的特征维度"></a>统计词频，并选取1000个特征作为输入的特征维度</h2><p>In [94]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="comment"># 关于CountVectorizer参数可以看这个：https://blog.csdn.net/weixin_38278334/article/details/82320307</span></span><br><span class="line"></span><br><span class="line">vec = CountVectorizer(</span><br><span class="line">    lowercase=<span class="literal">True</span>,     <span class="comment"># 将所有字符变成小写</span></span><br><span class="line">    analyzer=<span class="string">'char_wb'</span>, <span class="comment"># 按ngram_range：1～2个字符划分特征</span></span><br><span class="line">    ngram_range=(<span class="number">1</span>,<span class="number">2</span>),  <span class="comment"># 考虑两个单词的的关联顺序</span></span><br><span class="line">    max_features=<span class="number">1000</span>,  <span class="comment"># 选取出现次数最多的1000个特征</span></span><br><span class="line">    preprocessor=remove_noise  <span class="comment"># 调用前面去燥的函数，这里是说计数前需要先做这一步</span></span><br><span class="line">)</span><br><span class="line">vec.fit(x_train)</span><br><span class="line">print(vec.transform(x_train).toarray()) <span class="comment"># 训练集</span></span><br><span class="line">print(vec.transform(x_train).toarray().shape) <span class="comment"># 训练集维度</span></span><br><span class="line">vec.get_feature_names()[:<span class="number">50</span>] <span class="comment"># 看下选取的1000个特征前100个长啥样，可以看到按1～2个字符不等划分特征。</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[[34  0  0 ...  0  0  0]</span><br><span class="line"> [22  0  0 ...  0  0  0]</span><br><span class="line"> [18  0  0 ...  0  0  0]</span><br><span class="line"> ...</span><br><span class="line"> [32  0  0 ...  0  0  0]</span><br><span class="line"> [18  0  0 ...  0  0  0]</span><br><span class="line"> [ 8  0  0 ...  0  0  0]]</span><br><span class="line">(6799, 1000)</span><br></pre></td></tr></table></figure>

<p>Out[94]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">[&apos; &apos;,</span><br><span class="line"> &apos; 0&apos;,</span><br><span class="line"> &apos; 1&apos;,</span><br><span class="line"> &apos; 2&apos;,</span><br><span class="line"> &apos; 3&apos;,</span><br><span class="line"> &apos; 4&apos;,</span><br><span class="line"> &apos; 5&apos;,</span><br><span class="line"> &apos; 6&apos;,</span><br><span class="line"> &apos; 7&apos;,</span><br><span class="line"> &apos; 8&apos;,</span><br><span class="line"> &apos; 9&apos;,</span><br><span class="line"> &apos; a&apos;,</span><br><span class="line"> &apos; b&apos;,</span><br><span class="line"> &apos; c&apos;,</span><br><span class="line"> &apos; d&apos;,</span><br><span class="line"> &apos; e&apos;,</span><br><span class="line"> &apos; f&apos;,</span><br><span class="line"> &apos; g&apos;,</span><br><span class="line"> &apos; h&apos;,</span><br><span class="line"> &apos; i&apos;,</span><br><span class="line"> &apos; j&apos;,</span><br><span class="line"> &apos; k&apos;,</span><br><span class="line"> &apos; l&apos;,</span><br><span class="line"> &apos; m&apos;,</span><br><span class="line"> &apos; n&apos;,</span><br><span class="line"> &apos; o&apos;,</span><br><span class="line"> &apos; p&apos;,</span><br><span class="line"> &apos; q&apos;,</span><br><span class="line"> &apos; r&apos;,</span><br><span class="line"> &apos; s&apos;,</span><br><span class="line"> &apos; t&apos;,</span><br><span class="line"> &apos; u&apos;,</span><br><span class="line"> &apos; v&apos;,</span><br><span class="line"> &apos; w&apos;,</span><br><span class="line"> &apos; x&apos;,</span><br><span class="line"> &apos; y&apos;,</span><br><span class="line"> &apos; z&apos;,</span><br><span class="line"> &apos; ä&apos;,</span><br><span class="line"> &apos; è&apos;,</span><br><span class="line"> &apos; é&apos;,</span><br><span class="line"> &apos; ê&apos;,</span><br><span class="line"> &apos; ú&apos;,</span><br><span class="line"> &apos; ü&apos;,</span><br><span class="line"> &apos;0&apos;,</span><br><span class="line"> &apos;0 &apos;,</span><br><span class="line"> &apos;00&apos;,</span><br><span class="line"> &apos;01&apos;,</span><br><span class="line"> &apos;02&apos;,</span><br><span class="line"> &apos;04&apos;,</span><br><span class="line"> &apos;05&apos;]</span><br></pre></td></tr></table></figure>

<h2 id="训练及预测"><a href="#训练及预测" class="headerlink" title="训练及预测"></a>训练及预测</h2><p>In [91]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line">classifier = MultinomialNB()</span><br><span class="line">classifier.fit(vec.transform(x_train), y_train)</span><br><span class="line"><span class="comment">#classifier.fit(vec.transform(x_train).toarray(), y_train) 也可以</span></span><br><span class="line">print(classifier.score(vec.transform(x_test), y_test))</span><br><span class="line"><span class="comment"># 打印准确率</span></span><br><span class="line">print(classifier.predict(vec.transform([<span class="string">'This is an English sentence'</span>])))</span><br><span class="line"><span class="comment"># 测试下</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0.9770621967357741</span><br><span class="line">[&apos;en&apos;]</span><br></pre></td></tr></table></figure>

<h2 id="重新封装下"><a href="#重新封装下" class="headerlink" title="重新封装下"></a>重新封装下</h2><p>In [92]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LanguageDetector</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, classifier=MultinomialNB<span class="params">()</span>)</span>:</span></span><br><span class="line">        self.classifier = classifier</span><br><span class="line">        self.vectorizer = CountVectorizer(ngram_range=(<span class="number">1</span>,<span class="number">2</span>), max_features=<span class="number">1000</span>, preprocessor=self._remove_noise)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_remove_noise</span><span class="params">(self, document)</span>:</span></span><br><span class="line">        noise_pattern = re.compile(<span class="string">"|"</span>.join([<span class="string">"http\S+"</span>, <span class="string">"\@\w+"</span>, <span class="string">"\#\w+"</span>]))</span><br><span class="line">        clean_text = re.sub(noise_pattern, <span class="string">""</span>, document)</span><br><span class="line">        <span class="keyword">return</span> clean_text</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">features</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.vectorizer.transform(X)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        self.vectorizer.fit(X)</span><br><span class="line">        self.classifier.fit(self.features(X), y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.classifier.predict(self.features([x]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.classifier.score(self.features(X), y)</span><br></pre></td></tr></table></figure>

<p>In [93]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">in_f = open(<span class="string">'data.csv'</span>)</span><br><span class="line">lines = in_f.readlines()</span><br><span class="line">in_f.close()</span><br><span class="line">dataset = [(line.strip()[:<span class="number">-3</span>], line.strip()[<span class="number">-2</span>:]) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">x, y = zip(*dataset)</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">language_detector = LanguageDetector()</span><br><span class="line">language_detector.fit(x_train, y_train)</span><br><span class="line">print(language_detector.predict(<span class="string">'This is an English sentence'</span>))</span><br><span class="line">print(language_detector.score(x_test, y_test))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&apos;en&apos;]</span><br><span class="line">0.9770621967357741</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>


      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/朴素贝叶斯/">朴素贝叶斯</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-深度学习速查表" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/09/01/深度学习速查表/" class="article-date">
      <time datetime="2019-09-01T14:53:18.000Z" itemprop="datePublished">2019-09-01</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/01/深度学习速查表/">深度学习速查表</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>请参考</p>
<ul>
<li><a href="https://github.com/fengdu78/deeplearning_ai_books" target="_blank" rel="noopener">吴恩达老师的深度学习课程笔记及资源</a></li>
<li><a href="https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning" target="_blank" rel="noopener">CS229深度学习速查表</a></li>
<li><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks" target="_blank" rel="noopener">CS230深度学习速查表</a></li>
</ul>
<h2 id="深度学习各种模型与知识速查表"><a href="#深度学习各种模型与知识速查表" class="headerlink" title="深度学习各种模型与知识速查表"></a>深度学习各种模型与知识速查表</h2><p><img src="/blog_picture/dl-0001.jpg" alt="avatar"><br><img src="/blog_picture/dl-0002.jpg" alt="avatar"><br><img src="/blog_picture/dl-0003.jpg" alt="avatar"><br><img src="/blog_picture/dl-0004.jpg" alt="avatar"><br><img src="/blog_picture/dl-0005.jpg" alt="avatar"><br><img src="/blog_picture/dl-0006.jpg" alt="avatar"><br><img src="/blog_picture/dl-0007.jpg" alt="avatar"><br><img src="/blog_picture/dl-0008.jpg" alt="avatar"><br><img src="/blog_picture/dl-0009.jpg" alt="avatar"><br><img src="/blog_picture/dl-0010.jpg" alt="avatar"><br><img src="/blog_picture/dl-0011.jpg" alt="avatar"><br><img src="/blog_picture/dl-0012.jpg" alt="avatar"><br><img src="/blog_picture/dl-0013.jpg" alt="avatar"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习速查表/">深度学习速查表</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-模型调优" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/09/01/模型调优/" class="article-date">
      <time datetime="2019-09-01T14:52:00.000Z" itemprop="datePublished">2019-09-01</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/01/模型调优/">模型调优</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>参考阅读材料：</p>
<ul>
<li><a href="https://www.zhihu.com/question/34470160" target="_blank" rel="noopener">机器学习各种算法怎么调参?</a></li>
<li><a href="https://blog.csdn.net/mozhizun/article/details/60966354" target="_blank" rel="noopener">机器学习模型应用以及模型优化的一些思路</a></li>
<li><a href="https://blog.csdn.net/mozhizun/article/details/71438821" target="_blank" rel="noopener">机器学习模型优化中常见问题和解决思路</a></li>
<li><a href="https://blog.csdn.net/bitcarmanlee/article/details/71753056" target="_blank" rel="noopener">机器学习中模型优化不得不思考的几个问题</a></li>
<li><a href="https://www.jianshu.com/p/9fe84a7a5ba8" target="_blank" rel="noopener">机器学习中模型优化的两个问题</a></li>
</ul>
<h2 id="模型选择与调优"><a href="#模型选择与调优" class="headerlink" title="模型选择与调优"></a>模型选择与调优</h2><h3 id="不同模型的选择"><a href="#不同模型的选择" class="headerlink" title="不同模型的选择"></a>不同模型的选择</h3><p><img src="/blog_picture/model_tuning1.png" alt="avatar"></p>
<h3 id="模型超参数的选择"><a href="#模型超参数的选择" class="headerlink" title="模型超参数的选择"></a>模型超参数的选择</h3><p><img src="/blog_picture/model_tuning2.png" alt="avatar"></p>
<h3 id="评估方法-超参数产出方法"><a href="#评估方法-超参数产出方法" class="headerlink" title="评估方法+超参数产出方法"></a>评估方法+超参数产出方法</h3><p><img src="/blog_picture/model_tuning4.png" alt="avatar"><br><img src="/blog_picture/model_tuning5.png" alt="avatar"><br><img src="/blog_picture/model_tuning6.png" alt="avatar"></p>
<h3 id="模型状态"><a href="#模型状态" class="headerlink" title="模型状态"></a>模型状态</h3><p><img src="/blog_picture/model_tuning3.png" alt="avatar"><br><img src="/blog_picture/model_tuning7.png" alt="avatar"><br><img src="/blog_picture/model_tuning8.png" alt="avatar"><br><img src="/blog_picture/model_tuning9.png" alt="avatar"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/模型调优/">模型调优</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-集成学习与boosting模型" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/09/01/集成学习与boosting模型/" class="article-date">
      <time datetime="2019-09-01T14:44:32.000Z" itemprop="datePublished">2019-09-01</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/01/集成学习与boosting模型/">集成学习与boosting模型</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="机器学习中的集成学习"><a href="#机器学习中的集成学习" class="headerlink" title="机器学习中的集成学习"></a>机器学习中的集成学习</h2><p>顾名思义，集成学习（ensemble learning）指的是将多个学习器进行有效地结合，组建一个“学习器委员会”，其中每个学习器担任委员会成员并行使投票表决权，使得委员会最后的决定更能够四方造福普度众生<del>…</del>，即其泛化性能要能优于其中任何一个学习器。</p>
<h3 id="个体与集成"><a href="#个体与集成" class="headerlink" title="个体与集成"></a>个体与集成</h3><p>集成学习的基本结构为：先产生一组个体学习器，再使用某种策略将它们结合在一起。集成模型如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0c15683.png" alt="1.png"></p>
<p>在上图的集成模型中，若个体学习器都属于同一类别，例如都是决策树或都是神经网络，则称该集成为同质的（homogeneous）;若个体学习器包含多种类型的学习算法，例如既有决策树又有神经网络，则称该集成为异质的（heterogenous）。</p>
<blockquote>
<p><strong>同质集成</strong>：个体学习器称为“基学习器”（base learner），对应的学习算法为“基学习算法”（base learning algorithm）。</p>
</blockquote>
<blockquote>
<p><strong>异质集成</strong>：个体学习器称为“组件学习器”（component learner）或直称为“个体学习器”。</p>
</blockquote>
<p>上面我们已经提到要让集成起来的泛化性能比单个学习器都要好，虽说团结力量大但也有木桶短板理论调皮捣蛋，那如何做到呢？这就引出了集成学习的两个重要概念：<strong>准确性</strong>和<strong>多样性</strong>（diversity）。准确性指的是个体学习器不能太差，要有一定的准确度；多样性则是个体学习器之间的输出要具有差异性。通过下面的这三个例子可以很容易看出这一点，准确度较高，差异度也较高，可以较好地提升集成性能。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0d23e13.png" alt="2.png"></p>
<p>现在考虑二分类的简单情形，假设基分类器之间相互独立（能提供较高的差异度），且错误率相等为 ε，则可以将集成器的预测看做一个伯努利实验，易知当所有基分类器中不足一半预测正确的情况下，集成器预测错误，所以集成器的错误率可以计算为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0cce0bb.png" alt="3.png"></p>
<p>此时，集成器错误率随着基分类器的个数的增加呈指数下降，但前提是基分类器之间相互独立，在实际情形中显然是不可能的，假设训练有A和B两个分类器，对于某个测试样本，显然满足：P（A=1 | B=1）&gt; P（A=1），因为A和B为了解决相同的问题而训练，因此在预测新样本时存在着很大的联系。因此，<strong>个体学习器的“准确性”和“差异性”本身就是一对矛盾的变量</strong>，准确性高意味着牺牲多样性，所以产生“<strong>好而不同</strong>”的个体学习器正是集成学习研究的核心。现阶段有三种主流的集成学习方法：Boosting、Bagging以及随机森林（Random Forest），接下来将进行逐一介绍。</p>
<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>Boosting是一种串行的工作机制，即个体学习器的训练存在依赖关系，必须一步一步序列化进行。其基本思想是：增加前一个基学习器在训练训练过程中预测错误样本的权重，使得后续基学习器更加关注这些打标错误的训练样本，尽可能纠正这些错误，一直向下串行直至产生需要的T个基学习器，Boosting最终对这T个学习器进行加权结合，产生学习器委员会。</p>
<p>Boosting族算法最著名、使用最为广泛的就是AdaBoost，因此下面主要是对AdaBoost算法进行介绍。AdaBoost使用的是<strong>指数损失函数</strong>，因此AdaBoost的权值与样本分布的更新都是围绕着最小化指数损失函数进行的。看到这里回想一下之前的机器学习算法，<strong>不难发现机器学习的大部分带参模型只是改变了最优化目标中的损失函数</strong>：如果是Square loss，那就是最小二乘了；如果是Hinge Loss，那就是著名的SVM了；如果是log-Loss，那就是Logistic Regression了。</p>
<p>定义基学习器的集成为加权结合，则有：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0ca2ca5.png" alt="4.png"></p>
<p>AdaBoost算法的指数损失函数定义为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0d10461.png" alt="5.png"></p>
<p>具体说来，整个Adaboost 迭代算法分为3步：</p>
<ul>
<li>初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。</li>
<li>训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。</li>
<li>将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。</li>
</ul>
<p>整个AdaBoost的算法流程如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0d7c057.png" alt="6.png"></p>
<p>可以看出：<strong>AdaBoost的核心步骤就是计算基学习器权重和样本权重分布</strong>，那为何是上述的计算公式呢？这就涉及到了我们之前为什么说大部分带参机器学习算法只是改变了损失函数，就是因为<strong>大部分模型的参数都是通过最优化损失函数（可能还加个规则项）而计算（梯度下降，坐标下降等）得到</strong>，这里正是通过最优化指数损失函数从而得到这两个参数的计算公式，具体的推导过程此处不进行展开。</p>
<p>Boosting算法要求基学习器能对特定分布的数据进行学习，即每次都更新样本分布权重，这里书上提到了两种方法：“重赋权法”（re-weighting）和“重采样法”（re-sampling），书上的解释有些晦涩，这里进行展开一下：</p>
<blockquote>
<p><strong>重赋权法</strong> : 对每个样本附加一个权重，这时涉及到样本属性与标签的计算，都需要乘上一个权值。<br><strong>重采样法</strong> : 对于一些无法接受带权样本的及学习算法，适合用“重采样法”进行处理。方法大致过程是，根据各个样本的权重，对训练数据进行重采样，初始时样本权重一样，每个样本被采样到的概率一致，每次从N个原始的训练样本中按照权重有放回采样N个样本作为训练集，然后计算训练集错误率，然后调整权重，重复采样，集成多个基学习器。</p>
</blockquote>
<p>从偏差-方差分解来看：Boosting算法主要关注于降低偏差，每轮的迭代都关注于训练过程中预测错误的样本，将弱学习提升为强学习器。从AdaBoost的算法流程来看，标准的AdaBoost只适用于二分类问题。在此，当选为数据挖掘十大算法之一的AdaBoost介绍到这里，能够当选正是说明这个算法十分婀娜多姿，背后的数学证明和推导充分证明了这一点，限于篇幅不再继续展开。</p>
<h3 id="Bagging与Random-Forest"><a href="#Bagging与Random-Forest" class="headerlink" title="Bagging与Random Forest"></a>Bagging与Random Forest</h3><p>相比之下，Bagging与随机森林算法就简洁了许多，上面已经提到产生“好而不同”的个体学习器是集成学习研究的核心，即在保证基学习器准确性的同时增加基学习器之间的多样性。而这两种算法的基本思（tao）想（lu）都是通过“自助采样”的方法来增加多样性。</p>
<h4 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h4><p>Bagging是一种并行式的集成学习方法，即基学习器的训练之间没有前后顺序可以同时进行，Bagging使用“有放回”采样的方式选取训练集，对于包含m个样本的训练集，进行m次有放回的随机采样操作，从而得到m个样本的采样集，这样训练集中有接近36.8%的样本没有被采到。按照相同的方式重复进行，我们就可以采集到T个包含m个样本的数据集，从而训练出T个基学习器，最终对这T个基学习器的输出进行结合。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0ce62fc.png" alt="7.png"></p>
<p>Bagging算法的流程如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0d0e761.png" alt="8.png"></p>
<p>可以看出Bagging主要通过<strong>样本的扰动</strong>来增加基学习器之间的多样性，因此Bagging的基学习器应为那些对训练集十分敏感的不稳定学习算法，例如：神经网络与决策树等。从偏差-方差分解来看，Bagging算法主要关注于降低方差，即通过多次重复训练提高稳定性。不同于AdaBoost的是，Bagging可以十分简单地移植到多分类、回归等问题。总的说起来则是：<strong>AdaBoost关注于降低偏差，而Bagging关注于降低方差。</strong></p>
<h4 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h4><p>随机森林（Random Forest）是Bagging的一个拓展体，它的基学习器固定为决策树，多棵树也就组成了森林，而“随机”则在于选择划分属性的随机，随机森林在训练基学习器时，也采用有放回采样的方式添加样本扰动，同时它还引入了一种<strong>属性扰动</strong>，即在基决策树的训练过程中，在选择划分属性时，RF先从候选属性集中随机挑选出一个包含K个属性的子集，再从这个子集中选择最优划分属性，一般推荐K=log2（d）。</p>
<p>这样随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，从而进一步提升了基学习器之间的差异度。相比决策树的Bagging集成，随机森林的起始性能较差（由于属性扰动，基决策树的准确度有所下降），但随着基学习器数目的增多，随机森林往往会收敛到更低的泛化误差。同时不同于Bagging中决策树从所有属性集中选择最优划分属性，随机森林只在属性集的一个子集中选择划分属性，因此训练效率更高。<br><img src="../img/random-forest.png" alt><br><img src="https://i.loli.net/2018/10/18/5bc84d0d7a4fd.png" alt="9.png"></p>
<h3 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h3><p>结合策略指的是在训练好基学习器后，如何将这些基学习器的输出结合起来产生集成模型的最终输出，下面将介绍一些常用的结合策略：</p>
<h4 id="平均法（回归问题）"><a href="#平均法（回归问题）" class="headerlink" title="平均法（回归问题）"></a>平均法（回归问题）</h4><p><img src="https://i.loli.net/2018/10/18/5bc84d0d07983.png" alt="10.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de1b74ff.png" alt="11.png"></p>
<p>易知简单平均法是加权平均法的一种特例，加权平均法可以认为是集成学习研究的基本出发点。由于各个基学习器的权值在训练中得出，<strong>一般而言，在个体学习器性能相差较大时宜使用加权平均法，在个体学习器性能相差较小时宜使用简单平均法</strong>。</p>
<h4 id="投票法（分类问题）"><a href="#投票法（分类问题）" class="headerlink" title="投票法（分类问题）"></a>投票法（分类问题）</h4><p><img src="https://i.loli.net/2018/10/18/5bc84de2629c4.png" alt="12.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de25a74b.png" alt="13.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de1bacc4.png" alt="14.png"></p>
<p>绝对多数投票法（majority voting）提供了拒绝选项，这在可靠性要求很高的学习任务中是一个很好的机制。同时，对于分类任务，各个基学习器的输出值有两种类型，分别为类标记和类概率。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de2768c1.png" alt="15.png"></p>
<p>一些在产生类别标记的同时也生成置信度的学习器，置信度可转化为类概率使用，<strong>一般基于类概率进行结合往往比基于类标记进行结合的效果更好</strong>，需要注意的是对于异质集成，其类概率不能直接进行比较，此时需要将类概率转化为类标记输出，然后再投票。</p>
<h4 id="学习法"><a href="#学习法" class="headerlink" title="学习法"></a>学习法</h4><p>学习法是一种更高级的结合策略，即学习出一种“投票”的学习器，Stacking是学习法的典型代表。Stacking的基本思想是：首先训练出T个基学习器，对于一个样本它们会产生T个输出，将这T个基学习器的输出与该样本的真实标记作为新的样本，m个样本就会产生一个m<em>T的样本集，来训练一个新的“投票”学习器。投票学习器的输入属性与学习算法对Stacking集成的泛化性能有很大的影响，书中已经提到：*</em>投票学习器采用类概率作为输入属性，选用多响应线性回归（MLR）一般会产生较好的效果**。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de25cbaf.png" alt="16.png"></p>
<h4 id="多样性（diversity）"><a href="#多样性（diversity）" class="headerlink" title="多样性（diversity）"></a>多样性（diversity）</h4><p>在集成学习中，基学习器之间的多样性是影响集成器泛化性能的重要因素。因此增加多样性对于集成学习研究十分重要，一般的思路是在学习过程中引入随机性，常见的做法主要是对数据样本、输入属性、输出表示、算法参数进行扰动。</p>
<blockquote>
<p><strong>数据样本扰动</strong>，即利用具有差异的数据集来训练不同的基学习器。例如：有放回自助采样法，但此类做法只对那些不稳定学习算法十分有效，例如：决策树和神经网络等，训练集的稍微改变能导致学习器的显著变动。<br><strong>输入属性扰动</strong>，即随机选取原空间的一个子空间来训练基学习器。例如：随机森林，从初始属性集中抽取子集，再基于每个子集来训练基学习器。但若训练集只包含少量属性，则不宜使用属性扰动。<br><strong>输出表示扰动</strong>，此类做法可对训练样本的类标稍作变动，或对基学习器的输出进行转化。<br><strong>算法参数扰动</strong>，通过随机设置不同的参数，例如：神经网络中，随机初始化权重与随机设置隐含层节点数。</p>
</blockquote>
<h2 id="机器学习中的Boosting模型：GBDT-vs-Xgboost-vs-LightGBM"><a href="#机器学习中的Boosting模型：GBDT-vs-Xgboost-vs-LightGBM" class="headerlink" title="机器学习中的Boosting模型：GBDT vs Xgboost vs LightGBM"></a>机器学习中的Boosting模型：GBDT vs Xgboost vs LightGBM</h2><p>见资料<strong>GBDT_wepon.pdf</strong></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/boosting/">boosting</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/集成学习/">集成学习</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-聚类与降维" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/09/01/聚类与降维/" class="article-date">
      <time datetime="2019-09-01T14:40:06.000Z" itemprop="datePublished">2019-09-01</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/01/聚类与降维/">聚类与降维</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="机器学习中的聚类算法"><a href="#机器学习中的聚类算法" class="headerlink" title="机器学习中的聚类算法"></a>机器学习中的聚类算法</h2><p>聚类是一种经典的<strong>无监督学习</strong>方法，<strong>无监督学习的目标是通过对无标记训练样本的学习，发掘和揭示数据集本身潜在的结构与规律</strong>，即不依赖于训练数据集的类标记信息。聚类则是试图将数据集的样本划分为若干个互不相交的类簇，从而每个簇对应一个潜在的类别。</p>
<p>聚类直观上来说是将相似的样本聚在一起，从而形成一个<strong>类簇（cluster）</strong>。那首先的问题是如何来<strong>度量相似性</strong>（similarity measure）呢？这便是<strong>距离度量</strong>，在生活中我们说差别小则相似，对应到多维样本，每个样本可以对应于高维空间中的一个数据点，若它们的距离相近，我们便可以称它们相似。那接着如何来评价聚类结果的好坏呢？这便是<strong>性能度量</strong>，性能度量为评价聚类结果的好坏提供了一系列有效性指标。</p>
<h3 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h3><p>谈及距离度量，最熟悉的莫过于欧式距离了，从年头一直用到年尾的距离计算公式：即对应属性之间相减的平方和再开根号。度量距离还有其它的很多经典方法，通常它们需要满足一些基本性质：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed4c0390.png" alt="1.png"></p>
<p>最常用的距离度量方法是<strong>“闵可夫斯基距离”（Minkowski distance)</strong>：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed49e31f.png" alt="2.png"></p>
<p>当p=1时，闵可夫斯基距离即<strong>曼哈顿距离（Manhattan distance）</strong>：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed49c31f.png" alt="3.png"></p>
<p>当p=2时，闵可夫斯基距离即<strong>欧氏距离（Euclidean distance）</strong>：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed497613.png" alt="4.png"></p>
<p>我们知道属性分为两种：<strong>连续属性</strong>和<strong>离散属性</strong>（有限个取值）。对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；而对于离散值的属性，需要作下面进一步的处理：</p>
<blockquote>
<p>若属性值之间<strong>存在序关系</strong>，则可以将其转化为连续值，例如：身高属性“高”“中等”“矮”，可转化为{1, 0.5, 0}。<br>若属性值之间<strong>不存在序关系</strong>，则通常将其转化为向量的形式，例如：性别属性“男”“女”，可转化为{（1,0），（0,1）}。</p>
</blockquote>
<p>在进行距离度量时，易知<strong>连续属性和存在序关系的离散属性都可以直接参与计算</strong>，因为它们都可以反映一种程度，我们称其为“<strong>有序属性</strong>”；而对于不存在序关系的离散属性，我们称其为：“<strong>无序属性</strong>”，显然无序属性再使用闵可夫斯基距离就行不通了。</p>
<p><strong>对于无序属性，我们一般采用VDM进行距离的计算</strong>，例如：对于离散属性的两个取值a和b，定义：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed4e9560.png" alt="5.png"></p>
<p>于是，在计算两个样本之间的距离时，我们可以将闵可夫斯基距离和VDM混合在一起进行计算：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed507bc7.png" alt="6.png"></p>
<p>若我们定义的距离计算方法是用来度量相似性，例如下面将要讨论的聚类问题，即距离越小，相似性越大，反之距离越大，相似性越小。这时距离的度量方法并不一定需要满足前面所说的四个基本性质，这样的方法称为：<strong>非度量距离（non-metric distance）</strong>。</p>
<h3 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h3><p>由于聚类算法不依赖于样本的真实类标，就不能像监督学习的分类那般，通过计算分对分错（即精确度或错误率）来评价学习器的好坏或作为学习过程中的优化目标。一般聚类有两类性能度量指标：<strong>外部指标</strong>和<strong>内部指标</strong>。</p>
<h4 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h4><p>即将聚类结果与某个参考模型的结果进行比较，<strong>以参考模型的输出作为标准，来评价聚类好坏</strong>。假设聚类给出的结果为λ，参考模型给出的结果是λ*，则我们将样本进行两两配对，定义：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed59160e.png" alt="7.png"></p>
<p>显然a和b代表着聚类结果好坏的正能量，b和c则表示参考结果和聚类结果相矛盾，基于这四个值可以导出以下常用的外部评价指标：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed587438.png" alt="8.png"></p>
<h4 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h4><p>内部指标即不依赖任何外部模型，直接对聚类的结果进行评估，聚类的目的是想将那些相似的样本尽可能聚在一起，不相似的样本尽可能分开，直观来说：<strong>簇内高内聚紧紧抱团，簇间低耦合老死不相往来</strong>。定义：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed581852.png" alt="9.png"></p>
<p>基于上面的四个距离，可以导出下面这些常用的内部评价指标：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84ed582854.png" alt="10.png"></p>
<h3 id="原型聚类"><a href="#原型聚类" class="headerlink" title="原型聚类"></a>原型聚类</h3><p>原型聚类即“<strong>基于原型的聚类</strong>”（prototype-based clustering），原型表示模板的意思，就是通过参考一个模板向量或模板分布的方式来完成聚类的过程，常见的K-Means便是基于簇中心来实现聚类，混合高斯聚类则是基于簇分布来实现聚类。</p>
<h4 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h4><p>K-Means的思想十分简单，<strong>首先随机指定类中心，根据样本与类中心的远近划分类簇，接着重新计算类中心，迭代直至收敛</strong>。但是其中迭代的过程并不是主观地想象得出，事实上，若将样本的类别看做为“隐变量”（latent variable），类中心看作样本的分布参数，这一过程正是通过<strong>EM算法</strong>的两步走策略而计算出，其根本的目的是为了最小化平方误差函数E：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb82b5d3.png" alt="11.png"></p>
<p>K-Means的算法流程如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb9c0817.png" alt="12.png"></p>
<p><img src="../img/K-Means.png" alt></p>
<h4 id="高斯混合聚类"><a href="#高斯混合聚类" class="headerlink" title="高斯混合聚类"></a>高斯混合聚类</h4><p>现在可以看出K-Means与LVQ都试图以类中心作为原型指导聚类，高斯混合聚类则采用高斯分布来描述原型。现假设<strong>每个类簇中的样本都服从一个多维高斯分布，那么空间中的样本可以看作由k个多维高斯分布混合而成</strong>。</p>
<p>对于多维高斯分布，其概率密度函数如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb870d98.png" alt="14.png"></p>
<p>其中u表示均值向量，∑表示协方差矩阵，可以看出一个多维高斯分布完全由这两个参数所确定。接着定义高斯混合分布为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb876794.png" alt="15.png"></p>
<p>α称为混合系数，这样空间中样本的采集过程则可以抽象为：<strong>（1）先选择一个类簇（高斯分布），（2）再根据对应高斯分布的密度函数进行采样</strong>，这时候贝叶斯公式又能大展身手了：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb9191d9.png" alt="16.png"></p>
<p>此时只需要选择PM最大时的类簇并将该样本划分到其中，看到这里很容易发现：这和那个传说中的贝叶斯分类不是神似吗，都是通过贝叶斯公式展开，然后计算类先验概率和类条件概率。但遗憾的是：<strong>这里没有真实类标信息，对于类条件概率，并不能像贝叶斯分类那样通过最大似然法美好地计算出来</strong>，因为这里的样本可能属于所有的类簇，这里的似然函数变为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb871d4a.png" alt="17.png"></p>
<p>可以看出：简单的最大似然法根本无法求出所有的参数，这样PM也就没法计算。<strong>这里就要召唤出之前的EM大法，首先对高斯分布的参数及混合系数进行随机初始化，计算出各个PM（即γji，第i个样本属于j类），再最大化似然函数（即LL（D）分别对α、u和∑求偏导 ），对参数进行迭代更新</strong>。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb8a6f32.png" alt="18.png"></p>
<p>高斯混合聚类的算法流程如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb9c4fa4.png" alt="19.png"></p>
<h4 id="密度聚类"><a href="#密度聚类" class="headerlink" title="密度聚类"></a>密度聚类</h4><p>密度聚类则是基于密度的聚类，它从样本分布的角度来考察样本之间的可连接性，并基于可连接性（密度可达）不断拓展疆域（类簇）。其中最著名的便是<strong>DBSCAN</strong>算法，首先定义以下概念：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84fb9bd69c.png" alt="20.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc8509f8d619.png" alt="21.png"></p>
<p>简单来理解DBSCAN便是：<strong>找出一个核心对象所有密度可达的样本集合形成簇</strong>。首先从数据集中任选一个核心对象A，找出所有A密度可达的样本集合，将这些样本形成一个密度相连的类簇，直到所有的核心对象都遍历完。DBSCAN算法的流程如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc8509feb587.png" alt="22.png"></p>
<h4 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h4><p>层次聚类是一种基于树形结构的聚类方法，常用的是<strong>自底向上</strong>的结合策略（<strong>AGNES算法</strong>）。假设有N个待聚类的样本，其基本步骤是：</p>
<ul>
<li>1.初始化–&gt;把每个样本归为一类，计算每两个类之间的距离，也就是样本与样本之间的相似度；</li>
<li>2.寻找各个类之间最近的两个类，把他们归为一类（这样类的总数就少了一个）；</li>
<li>3.重新计算新生成的这个<strong>类与各个旧类之间的相似度</strong>；</li>
<li>4.重复2和3直到所有样本点都归为一类，结束。</li>
</ul>
<p>可以看出其中最关键的一步就是<strong>计算两个类簇的相似度</strong>，这里有多种度量方法：</p>
<pre><code>* 单链接（single-linkage）:取类间最小距离。</code></pre><p><img src="https://i.loli.net/2018/10/18/5bc8509ebb022.png" alt="23.png"></p>
<pre><code>* 全链接（complete-linkage）:取类间最大距离</code></pre><p><img src="https://i.loli.net/2018/10/18/5bc8509eb2b30.png" alt="24.png"></p>
<pre><code>* 均链接（average-linkage）:取类间两两的平均距离</code></pre><p><img src="https://i.loli.net/2018/10/18/5bc8509f089a7.png" alt="25.png"></p>
<p>很容易看出：<strong>单链接的包容性极强，稍微有点暧昧就当做是自己人了，全链接则是坚持到底，只要存在缺点就坚决不合并，均连接则是从全局出发顾全大局</strong>。层次聚类法的算法流程如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc8509f9d4a0.png" alt="26.png"><br><img src="/blog_picture/clustering.png" alt="avatar"></p>
<h2 id="机器学习中的PCA降维"><a href="#机器学习中的PCA降维" class="headerlink" title="机器学习中的PCA降维"></a>机器学习中的PCA降维</h2><p>资料from<a href="http://blog.codinglabs.org/articles/pca-tutorial.html" target="_blank" rel="noopener">PCA的数学原理</a></p>
<p><img src="/blog_picture/PCA_.jpg" alt="avatar"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/聚类/">聚类</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/降维/">降维</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-贝叶斯分类器" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/09/01/贝叶斯分类器/" class="article-date">
      <time datetime="2019-09-01T14:37:35.000Z" itemprop="datePublished">2019-09-01</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/01/贝叶斯分类器/">贝叶斯分类器</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>参考阅读材料：</p>
<ul>
<li><a href="https://blog.csdn.net/han_xiaoyang/article/details/50616559" target="_blank" rel="noopener">NLP系列(2)_用朴素贝叶斯进行文本分类(上)</a></li>
<li><a href="https://blog.csdn.net/han_xiaoyang/article/details/50629587" target="_blank" rel="noopener">NLP系列(3)_用朴素贝叶斯进行文本分类(下)</a></li>
</ul>
<p>朴素贝叶斯</p>
<ul>
<li>贝叶斯公式 + 条件独立假设</li>
<li>平滑算法</li>
</ul>
<h2 id="机器学习中的贝叶斯分类器"><a href="#机器学习中的贝叶斯分类器" class="headerlink" title="机器学习中的贝叶斯分类器"></a>机器学习中的贝叶斯分类器</h2><p>贝叶斯分类器是一种概率框架下的统计学习分类器，对分类任务而言，假设在相关概率都已知的情况下，贝叶斯分类器考虑如何基于这些概率为样本判定最优的类标。在开始介绍贝叶斯决策论之前，我们首先来回顾下概率论委员会常委–贝叶斯公式。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd7a2575.png" alt="1.png"></p>
<h3 id="贝叶斯决策论"><a href="#贝叶斯决策论" class="headerlink" title="贝叶斯决策论"></a>贝叶斯决策论</h3><p>若将上述定义中样本空间的划分Bi看做为类标，A看做为一个新的样本，则很容易将条件概率理解为样本A是类别Bi的概率。在机器学习训练模型的过程中，往往我们都试图去优化一个风险函数，因此在概率框架下我们也可以为贝叶斯定义“<strong>条件风险</strong>”（conditional risk）。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd15db94.png" alt="2.png"></p>
<p>我们的任务就是寻找一个判定准则最小化所有样本的条件风险总和，因此就有了<strong>贝叶斯判定准则</strong>（Bayes decision rule）:为最小化总体风险，只需在每个样本上选择那个使得条件风险最小的类标。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd308600.png" alt="3.png"></p>
<p>若损失函数λ取0-1损失，则有：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd37c502.png" alt="4.png"></p>
<p>即对于每个样本x，选择其后验概率P（c | x）最大所对应的类标，能使得总体风险函数最小，从而将原问题转化为估计后验概率P（c | x）。一般这里有两种策略来对后验概率进行估计：</p>
<pre><code>* 判别式模型：直接对 P（c | x）进行建模求解。例我们前面所介绍的决策树、神经网络、SVM都是属于判别式模型。
* 生成式模型：通过先对联合分布P（x,c）建模，从而进一步求解 P（c | x）。</code></pre><p>贝叶斯分类器就属于生成式模型，基于贝叶斯公式对后验概率P（c | x） 进行一项神奇的变换，巴拉拉能量…. P（c | x）变身：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd501ad3.png" alt="5.png"></p>
<p>对于给定的样本x，P（x）与类标无关，P（c）称为类先验概率，p（x | c ）称为类条件概率。这时估计后验概率P（c | x）就变成为估计类先验概率和类条件概率的问题。对于先验概率和后验概率，在看这章之前也是模糊了我好久，这里普及一下它们的基本概念。</p>
<pre><code>* 先验概率： 根据以往经验和分析得到的概率。
* 后验概率：后验概率是基于新的信息，修正原来的先验概率后所获得的更接近实际情况的概率估计。</code></pre><p>实际上先验概率就是在没有任何结果出来的情况下估计的概率，而后验概率则是在有一定依据后的重新估计，直观意义上后验概率就是条件概率。下面直接上Wiki上的一个例子，简单粗暴快速完事…</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd799610.png" alt="6.png"></p>
<p>回归正题，对于类先验概率P（c），p（c）就是样本空间中各类样本所占的比例，根据大数定理（当样本足够多时，频率趋于稳定等于其概率），这样当训练样本充足时，p(c)可以使用各类出现的频率来代替。因此只剩下类条件概率p（x | c ），它表达的意思是在类别c中出现x的概率，它涉及到属性的联合概率问题，若只有一个离散属性还好，当属性多时采用频率估计起来就十分困难，因此这里一般采用极大似然法进行估计。</p>
<h3 id="极大似然法"><a href="#极大似然法" class="headerlink" title="极大似然法"></a>极大似然法</h3><p>极大似然估计（Maximum Likelihood Estimation，简称MLE），是一种根据数据采样来估计概率分布的经典方法。常用的策略是先假定总体具有某种确定的概率分布，再基于训练样本对概率分布的参数进行估计。运用到类条件概率p（x | c ）中，假设p（x | c ）服从一个参数为θ的分布，问题就变为根据已知的训练样本来估计θ。极大似然法的核心思想就是：估计出的参数使得已知样本出现的概率最大，即使得训练数据的似然最大。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd70fb73.png" alt="7.png"></p>
<p>所以，贝叶斯分类器的训练过程就是参数估计。总结最大似然法估计参数的过程，一般分为以下四个步骤：</p>
<pre><code>* 1.写出似然函数；
* 2.对似然函数取对数，并整理；
* 3.求导数，令偏导数为0，得到似然方程组；
* 4.解似然方程组，得到所有参数即为所求。</code></pre><p>例如：假设样本属性都是连续值，p（x | c ）服从一个多维高斯分布，则通过MLE计算出的参数刚好分别为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd705729.png" alt="8.png"></p>
<p>上述结果看起来十分合乎实际，但是采用最大似然法估计参数的效果很大程度上依赖于作出的假设是否合理，是否符合潜在的真实数据分布。这就需要大量的经验知识，搞统计越来越值钱也是这个道理，大牛们掐指一算比我们搬砖几天更有效果。</p>
<h3 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h3><p>不难看出：原始的贝叶斯分类器最大的问题在于联合概率密度函数的估计，首先需要根据经验来假设联合概率分布，其次当属性很多时，训练样本往往覆盖不够，参数的估计会出现很大的偏差。为了避免这个问题，朴素贝叶斯分类器（naive Bayes classifier）采用了“属性条件独立性假设”，即样本数据的所有属性之间相互独立。这样类条件概率p（x | c ）可以改写为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd55e102.png" alt="9.png"></p>
<p>这样，为每个样本估计类条件概率变成为每个样本的每个属性估计类条件概率。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fd6678cd.png" alt="10.png"></p>
<p>相比原始贝叶斯分类器，朴素贝叶斯分类器基于单个的属性计算类条件概率更加容易操作，需要注意的是：若某个属性值在训练集中和某个类别没有一起出现过，这样会抹掉其它的属性信息，因为该样本的类条件概率被计算为0。因此在估计概率值时，常常用进行平滑（smoothing）处理，拉普拉斯修正（Laplacian correction）就是其中的一种经典方法，具体计算方法如下：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc83fe54aaed.png" alt="11.png"></p>
<p>当训练集越大时，拉普拉斯修正引入的影响越来越小。对于贝叶斯分类器，模型的训练就是参数估计，因此可以事先将所有的概率储存好，当有新样本需要判定时，直接查表计算即可。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/贝叶斯分类器/">贝叶斯分类器</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-决策树与随机森林" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/09/01/决策树与随机森林/" class="article-date">
      <time datetime="2019-09-01T12:25:50.000Z" itemprop="datePublished">2019-09-01</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/01/决策树与随机森林/">决策树与随机森林</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="机器学习中的决策树模型"><a href="#机器学习中的决策树模型" class="headerlink" title="机器学习中的决策树模型"></a>机器学习中的决策树模型</h2><ul>
<li>① 树模型不用做scaling</li>
<li>② 树模型不太需要做离散化</li>
<li>③ 用Xgboost等工具库，是不需要做缺失值填充</li>
<li>④ 树模型是非线性模型，有非线性的表达能力</li>
</ul>
<h3 id="决策树基本概念"><a href="#决策树基本概念" class="headerlink" title="决策树基本概念"></a>决策树基本概念</h3><ul>
<li>决策时是一种树形结构，其中每个内部节点表示在一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别</li>
<li>决策树学习是以实例为基础的归纳学习</li>
<li>决策树学习采用的是自顶向下的归纳方法，其基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子节点处的熵值为0，此时每个叶子节点中的实例都属于同一类。</li>
</ul>
<p>顾名思义，决策树是基于树结构来进行决策的，，在网上看到一个例子十分有趣，放在这里正好合适。现想象一位捉急的母亲想要给自己的女娃介绍一个男朋友，于是有了下面的对话：</p>
<hr>
<pre><code>女儿：多大年纪了？
母亲：26。
女儿：长的帅不帅？
母亲：挺帅的。
女儿：收入高不？
母亲：不算很高，中等情况。
女儿：是公务员不？
母亲：是，在税务局上班呢。
女儿：那好，我去见见。</code></pre><hr>
<p>这个女孩的挑剔过程就是一个典型的决策树，即相当于通过年龄、长相、收入和是否公务员将男童鞋分为两个类别：见和不见。假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么使用下图就能很好地表示女孩的决策逻辑（即一颗决策树）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec84a77.png" alt="1.png"></p>
<p>在上图的决策树中，决策过程的每一次判定都是对某一属性的“测试”，决策最终结论则对应最终的判定结果。一般一颗决策树包含：一个根节点、若干个内部节点和若干个叶子节点，易知：</p>
<pre><code>* 每个非叶节点表示一个特征属性测试。
* 每个分支代表这个特征属性在某个值域上的输出。
* 每个叶子节点存放一个类别。
* 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。</code></pre><h3 id="决策树的构造"><a href="#决策树的构造" class="headerlink" title="决策树的构造"></a>决策树的构造</h3><p>决策树的构造是一个递归的过程，有三种情形会导致递归返回：(1) 当前结点包含的样本全属于同一类别，这时直接将该节点标记为叶节点，并设为相应的类别；(2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别；(3) 当前结点包含的样本集合为空，不能划分，这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。算法的基本流程如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ecc27fe.png" alt="2.png"></p>
<p>可以看出：决策树学习的关键在于如何选择划分属性，不同的划分属性得出不同的分支结构，从而影响整颗决策树的性能。属性划分的目标是让各个划分出来的子节点尽可能地“纯”，即属于同一类别。因此下面便是介绍量化纯度的具体方法，决策树最常用的算法有三种：ID3，C4.5和CART。##</p>
<h4 id="决策树学习算法的特点"><a href="#决策树学习算法的特点" class="headerlink" title="决策树学习算法的特点"></a>决策树学习算法的特点</h4><p>决策树学习算法最大的优点是，它可以自学习。在学习的过程中，不需要使用者了解过多背景知识，只需要对训练实例进行较好的标注，就能进行学习。显然，属于有监督学习。从一类无序、无规则的事物中推理出决策树表示分类的规则。</p>
<h4 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h4><p>ID3算法使用信息增益为准则来选择划分属性，“信息熵”(information entropy)是度量样本结合纯度的常用指标，假定当前样本集合D中第k类样本所占比例为pk，则样本集合D的信息熵定义为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec515a5.png" alt="3.png"></p>
<p>假定通过属性划分样本集D，产生了V个分支节点，v表示其中第v个分支节点，易知：分支节点包含的样本数越多，表示该分支节点的影响力越大。故可以计算出划分后相比原始数据集D获得的“信息增益”（information gain）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec3e067.png" alt="4.png"></p>
<p>Ent(D)划分前的信息增益 -划分后的信息增益</p>
<p>DV/D表示第V个分支的权重，样本越多越重要</p>
<p>信息增益越大，表示使用该属性划分样本集D的效果越好，因此ID3算法在递归过程中，每次选择最大信息增益的属性作为当前的划分属性。</p>
<h4 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h4><p>ID3算法存在一个问题，就是偏向于取值数目较多的属性，例如：如果存在一个唯一标识，这样样本集D将会被划分为|D|个分支，每个分支只有一个样本，这样划分后的信息熵为零，十分纯净，但是对分类毫无用处。因此C4.5算法使用了“增益率”（gain ratio）来选择划分属性，来避免这个问题带来的困扰。首先使用ID3算法计算出信息增益高于平均水平的候选属性，接着C4.5计算这些候选属性的增益率，增益率定义为：</p>
<p>启发式：先从候选划分属性中找出信息增益高于平均水平的，再从中选取增益率最高的</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec69647.png" alt="5.png"></p>
<h4 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h4><p>CART决策树使用“基尼指数”（Gini index）来选择划分属性，基尼指数反映的是从样本集D中随机抽取两个样本，其类别标记不一致的概率，因此Gini(D)越小越好，数据集D的纯度越高。基尼指数定义如下：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec5a2ff.png" alt="6.png"></p>
<p>进而，使用属性α划分后的基尼指数为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec62eaf.png" alt="7.png"></p>
<p>二分类视角看CART</p>
<ul>
<li>每一个产生分支的过程是一个二分类过程</li>
<li>这个过程叫作“决策树桩”</li>
<li>一棵CART是由许多决策树桩拼接起来的</li>
<li>决策树桩是只有一层的决策树</li>
</ul>
<h4 id="三种不同的决策树"><a href="#三种不同的决策树" class="headerlink" title="三种不同的决策树"></a>三种不同的决策树</h4><ul>
<li>ID3:取值多的属性，更容易使数据更纯，其信息增益更大；训练得到的是一棵庞大且深度浅的树：不合理</li>
<li>C4.5:采用信息增益率替代信息增益</li>
<li>CART：以基尼系数替代熵；最小化不纯度，而不是最大化信息增益</li>
</ul>
<h4 id="剪枝处理"><a href="#剪枝处理" class="headerlink" title="剪枝处理"></a>剪枝处理</h4><p>从决策树的构造流程中我们可以直观地看出：不管怎么样的训练集，决策树总是能很好地将各个类别分离开来，这时就会遇到之前提到过的问题：过拟合（overfitting），即太依赖于训练样本。剪枝（pruning）则是决策树算法对付过拟合的主要手段，剪枝的策略有两种如下：</p>
<pre><code>* 预剪枝（prepruning）：在构造的过程中先评估，再考虑是否分支。
* 后剪枝（post-pruning）：在构造好一颗完整的决策树后，自底向上，评估分支的必要性。</code></pre><p>评估指的是性能度量，即决策树的泛化性能。之前提到：可以使用测试集作为学习器泛化性能的近似，因此可以将数据集划分为训练集和测试集。预剪枝表示在构造数的过程中，对一个节点考虑是否分支时，首先计算决策树不分支时在测试集上的性能，再计算分支之后的性能，若分支对性能没有提升，则选择不分支（即剪枝）。后剪枝则表示在构造好一颗完整的决策树后，从最下面的节点开始，考虑该节点分支对模型的性能是否有提升，若无则剪枝，即将该节点标记为叶子节点，类别标记为其包含样本最多的类别。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec80d34.png" alt="8.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec9e330.png" alt="9.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec9d497.png" alt="10.png"></p>
<p>上图分别表示不剪枝处理的决策树、预剪枝决策树和后剪枝决策树。预剪枝处理使得决策树的很多分支被剪掉，因此大大降低了训练时间开销，同时降低了过拟合的风险，但另一方面由于剪枝同时剪掉了当前节点后续子节点的分支，因此预剪枝“贪心”的本质阻止了分支的展开，在一定程度上带来了欠拟合的风险。而后剪枝则通常保留了更多的分支，因此采用后剪枝策略的决策树性能往往优于预剪枝，但其自底向上遍历了所有节点，并计算性能，训练时间开销相比预剪枝大大提升。</p>
<h4 id="连续值与缺失值处理"><a href="#连续值与缺失值处理" class="headerlink" title="连续值与缺失值处理"></a>连续值与缺失值处理</h4><p>对于连续值的属性，若每个取值作为一个分支则显得不可行，因此需要进行离散化处理，常用的方法为二分法，基本思想为：给定样本集D与连续属性α，二分法试图找到一个划分点t将样本集D在属性α上分为≤t与＞t。</p>
<pre><code>* 首先将α的所有取值按升序排列，所有相邻属性的均值作为候选划分点（n-1个，n为α所有的取值数目）。
* 计算每一个划分点划分集合D（即划分为两个分支）后的信息增益。
* 选择最大信息增益的划分点作为最优划分点。</code></pre><p><img src="https://i.loli.net/2018/10/17/5bc72a0968fad.png" alt="11.png"></p>
<p>现实中常会遇到不完整的样本，即某些属性值缺失。有时若简单采取剔除，则会造成大量的信息浪费，因此在属性值缺失的情况下需要解决两个问题：（1）如何选择划分属性。（2）给定划分属性，若某样本在该属性上缺失值，如何划分到具体的分支上。假定为样本集中的每一个样本都赋予一个权重，根节点中的权重初始化为1，则定义：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72a098f3be.png" alt="12.png"></p>
<p>对于（1）：通过在样本集D中选取在属性α上没有缺失值的样本子集，计算在该样本子集上的信息增益，最终的信息增益等于该样本子集划分后信息增益乘以样本子集占样本集的比重。即：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72a096ccc3.png" alt="13.png"></p>
<p>对于（2）：若该样本子集在属性α上的值缺失，则将该样本以不同的权重（即每个分支所含样本比例）划入到所有分支节点中。该样本在分支节点中的权重变为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72a093ed3c.png" alt="14.png"></p>
<h4 id="Bootstraping"><a href="#Bootstraping" class="headerlink" title="Bootstraping"></a>Bootstraping</h4><p>称为自助法，它是一种有放回的抽样方法</p>
<p>####Bagging的策略</p>
<ul>
<li>bootstrap aggregation</li>
<li>从样本集中重采样(有重复的)选出n个样本</li>
<li>在所有属性上，对这n个样本建立分类器(ID3、C4.5、CART、SVM、Logistic回归等)</li>
<li>重复以上两步m次，即获得了m个分类器</li>
<li>将数据放在这m个分类器上，最后根据这m个分类器的投票结果，决定数据属于哪一类</li>
</ul>
<h4 id="OOB数据"><a href="#OOB数据" class="headerlink" title="OOB数据"></a>OOB数据</h4><p>可以发现，Bootstrap每次约有36.79%的样本不会出现在Bootstrap所采集的样本集合中，将未参与模型训练的数据称为袋外数据(out of bag)。它可以用于取代测试集用于误差估计。得到的模型参数是无偏估计。</p>
<h4 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h4><p>随机森林在bagging基础上做了修改</p>
<ul>
<li>从样本集中用bootstrap采样选出n个样本</li>
<li>从所有属性中随机选择k个属性，选择最佳分割属性作为节点建立CART决策树</li>
<li>重复以上两步m次，即建立m课CART决策树</li>
<li>这m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类</li>
</ul>
<h4 id="随机森林-bagging和决策树的关系"><a href="#随机森林-bagging和决策树的关系" class="headerlink" title="随机森林/bagging和决策树的关系"></a>随机森林/bagging和决策树的关系</h4><ul>
<li>当然可以使用决策树作为基本分类器</li>
<li>但也可以使用SVM、Logistics回归等其他分类，习惯上，这些分类器组成的“总分类器”，仍然叫做随机森林</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/决策树/">决策树</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-机器学习逻辑回归与softmax" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/08/29/机器学习逻辑回归与softmax/" class="article-date">
      <time datetime="2019-08-29T08:39:02.000Z" itemprop="datePublished">2019-08-29</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/29/机器学习逻辑回归与softmax/">机器学习逻辑回归与softmax</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="机器学习逻辑回归与softmax"><a href="#机器学习逻辑回归与softmax" class="headerlink" title="机器学习逻辑回归与softmax"></a>机器学习逻辑回归与softmax</h1><h2 id="机器学习中的线性模型"><a href="#机器学习中的线性模型" class="headerlink" title="机器学习中的线性模型"></a>机器学习中的线性模型</h2><p>谈及线性模型，其实我们很早就已经与它打过交道，还记得高中数学必修3课本中那个顽皮的“最小二乘法”吗？这就是线性模型的经典算法之一：根据给定的（x，y）点对，求出一条与这些点拟合效果最好的直线y=ax+b，之前我们利用下面的公式便可以计算出拟合直线的系数a,b（3.1中给出了具体的计算过程），从而对于一个新的x，可以预测它所对应的y值。前面我们提到：在机器学习的术语中，当预测值为连续值时，称为“回归问题”，离散值时为“分类问题”。本篇先从线性回归任务开始，接着讨论分类和多分类问题。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b068e48.png" alt="1.png"></p>
<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>线性回归问题就是试图学到一个线性模型尽可能准确地预测新样本的输出值，例如：通过历年的人口数据预测2017年人口数量。在这类问题中，往往我们会先得到一系列的有标记数据，例如：2000–&gt;13亿…2016–&gt;15亿，这时输入的属性只有一个，即年份；也有输入多属性的情形，假设我们预测一个人的收入，这时输入的属性值就不止一个了，例如：（学历，年龄，性别，颜值，身高，体重）–&gt;15k。</p>
<p>有时这些输入的属性值并不能直接被我们的学习模型所用，需要进行相应的处理，对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；对于离散值的属性，可作下面的处理：</p>
<ul>
<li><p>若属性值之间存在“序关系”，则可以将其转化为连续值，例如：身高属性分为“高”“中等”“矮”，可转化为数值：{1， 0.5， 0}。</p>
</li>
<li><p>若属性值之间不存在“序关系”，则通常将其转化为向量的形式，例如：性别属性分为“男”“女”，可转化为二维向量：{（1，0），（0，1）}。</p>
</li>
</ul>
<p>（1）当输入属性只有一个的时候，就是最简单的情形，也就是我们高中时最熟悉的“最小二乘法”（Euclidean distance），首先计算出每个样本预测值与真实值之间的误差并求和，通过最小化均方误差MSE，使用求偏导等于零的方法计算出拟合直线y=wx+b的两个参数w和b，计算过程如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0ccec4.png" alt="2.png"></p>
<p>（2）当输入属性有多个的时候，例如对于一个样本有d个属性{（x1,x2…xd）,y}，则y=wx+b需要写成：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72567b8bcd.png" alt="0.png"></p>
<p>通常对于多元问题，常常使用矩阵的形式来表示数据。在本问题中，将具有m个样本的数据集表示成矩阵X，将系数w与b合并成一个列向量，这样每个样本的预测值以及所有样本的均方误差最小化就可以写成下面的形式：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0ad8f7.png" alt="3.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0af652.png" alt="4.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b090543.png" alt="5.png"></p>
<p><img src="/blog_picture/line04.jpg" alt="avatar"></p>
<p><img src="/blog_picture/line05.jpg" alt="avatar"></p>
<p><img src="/blog_picture/line06.jpg" alt="avatar"></p>
<p><img src="/blog_picture/line07.jpg" alt="avatar"></p>
<h3 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h3><p>同样地，我们使用最小二乘法对w和b进行估计，令均方误差的求导等于0，需要注意的是，当一个矩阵的行列式不等于0时，我们才可能对其求逆，因此对于下式，我们需要考虑矩阵（X的转置*X）的行列式是否为0，若不为0，则可以求出其解，若为0，则需要使用其它的方法进行计算，书中提到了引入正则化，此处不进行深入。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0cde33.png" alt="6.png"></p>
<p>然而现实任务中当特征数量大于样本数时，XTX不满秩，此时θ有多个解；而且当数据量大时，求矩阵的逆非常耗时；对于不可逆矩阵（特征之间不相互独立），这种正规方程方法是不能用的。所以，还可以采用梯度下降法，利用迭代的方式求解θ。</p>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>梯度下降法是按下面的流程进行的：<br>1）首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。<br>2）改变θ的值，使得θ按梯度下降的方向进行减少。</p>
<p><img src="/blog_picture/line01.jpg" alt="avatar"></p>
<p>对于只有两维属性的样本，J(θ)即J(θ0,θ1)的等高线图</p>
<p><img src="/blog_picture/line02.jpg" alt="avatar"></p>
<p><img src="/blog_picture/line03.jpg" alt="avatar"></p>
<p>迭代更新的方式有多种</p>
<ul>
<li>批量梯度下降（batch gradient descent），也就是是梯度下降法最原始的形式，对全部的训练数据求得误差后再对θ<br>进行更新，优点是每步都趋向全局最优解；缺点是对于大量数据，由于每步要计算整体数据，训练过程慢；</li>
<li>随机梯度下降（stochastic gradient descent），每一步随机选择一个样本对θ<br>进行更新，优点是训练速度快；缺点是每次的前进方向不好确定，容易陷入局部最优；</li>
<li>微型批量梯度下降（mini-batch gradient descent），每步选择一小批数据进行批量梯度下降更新θ<br>，属于批量梯度下降和随机梯度下降的一种折中，非常适合并行处理。</li>
</ul>
<p>另一方面，有时像上面这种原始的线性回归可能并不能满足需求，例如：y值并不是线性变化，而是在指数尺度上变化。这时我们可以采用线性模型来逼近y的衍生物，例如lny，这时衍生的线性模型如下所示，实际上就是相当于将指数曲线投影在一条直线上，如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b103cbf.png" alt="7.png"></p>
<p>更一般地，考虑所有y的衍生物的情形，就得到了“广义的线性模型”（generalized linear model），其中，g（*）称为联系函数（link function）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0a2841.png" alt="8.png"></p>
<h3 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a>对数几率回归</h3><p>回归就是通过输入的属性值得到一个预测值，利用上述广义线性模型的特征，是否可以通过一个联系函数，将预测值转化为离散值从而进行分类呢？线性几率回归正是研究这样的问题。对数几率引入了一个对数几率函数（logistic function）,将预测值投影到0-1之间，从而将线性回归问题转化为二分类问题。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0c7748.png" alt="9.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0a655d.png" alt="10.png"></p>
<p>若将y看做样本为正例的概率，（1-y）看做样本为反例的概率，则上式实际上使用线性回归模型的预测结果器逼近真实标记的对数几率。因此这个模型称为“对数几率回归”（logistic regression），也有一些书籍称之为“逻辑回归”。下面使用最大似然估计的方法来计算出w和b两个参数的取值，下面只列出求解的思路，不列出具体的计算过程。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b824f0c.png" alt="11.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b817961.png" alt="12.png"></p>
<h3 id="线性判别分析"><a href="#线性判别分析" class="headerlink" title="线性判别分析"></a>线性判别分析</h3><p>线性判别分析（Linear Discriminant Analysis，简称LDA）,其基本思想是：将训练样本投影到一条直线上，使得同类的样例尽可能近，不同类的样例尽可能远。如图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b863ebb.png" alt="13.png"><img src="https://i.loli.net/2018/10/17/5bc723b85bfa9.png" alt="14.png"></p>
<p>想让同类样本点的投影点尽可能接近，不同类样本点投影之间尽可能远，即：让各类的协方差之和尽可能小，不用类之间中心的距离尽可能大。基于这样的考虑，LDA定义了两个散度矩阵。</p>
<ul>
<li>类内散度矩阵（within-class scatter matrix）</li>
</ul>
<p><img src="https://i.loli.net/2018/10/17/5bc723b8156e1.png" alt="15.png"></p>
<ul>
<li>类间散度矩阵(between-class scaltter matrix)</li>
</ul>
<p><img src="https://i.loli.net/2018/10/17/5bc723b7e9db3.png" alt="16.png"></p>
<p>因此得到了LDA的最大化目标：“广义瑞利商”（generalized Rayleigh quotient）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b7e8a61.png" alt="17.png"></p>
<p>从而分类问题转化为最优化求解w的问题，当求解出w后，对新的样本进行分类时，只需将该样本点投影到这条直线上，根据与各个类别的中心值进行比较，从而判定出新样本与哪个类别距离最近。求解w的方法如下所示，使用的方法为λ乘子。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b83d5e0.png" alt="18.png"></p>
<p>若将w看做一个投影矩阵，类似PCA的思想，则LDA可将样本投影到N-1维空间（N为类簇数），投影的过程使用了类别信息（标记信息），因此LDA也常被视为一种经典的监督降维技术。    </p>
<h3 id="回归与欠-过拟合"><a href="#回归与欠-过拟合" class="headerlink" title="回归与欠/过拟合"></a>回归与欠/过拟合</h3><p><img src="/blog_picture/line08.jpg" alt="avatar">    </p>
<h3 id="线性回归与正则化"><a href="#线性回归与正则化" class="headerlink" title="线性回归与正则化"></a>线性回归与正则化</h3><p><img src="/blog_picture/line09.jpg" alt="avatar">     </p>
<h3 id="多分类学习"><a href="#多分类学习" class="headerlink" title="多分类学习"></a>多分类学习</h3><p>现实中我们经常遇到不只两个类别的分类问题，即多分类问题，在这种情形下，我们常常运用“拆分”的策略，通过多个二分类学习器来解决多分类问题，即将多分类问题拆解为多个二分类问题，训练出多个二分类学习器，最后将多个分类结果进行集成得出结论。最为经典的拆分策略有三种：“一对一”（OvO）、“一对其余”（OvR）和“多对多”（MvM），核心思想与示意图如下所示。</p>
<ul>
<li><p>OvO：给定数据集D，假定其中有N个真实类别，将这N个类别进行两两配对（一个正类/一个反类），从而产生N（N-1）/2个二分类学习器，在测试阶段，将新样本放入所有的二分类学习器中测试，得出N（N-1）个结果，最终通过投票产生最终的分类结果。</p>
</li>
<li><p>OvM：给定数据集D，假定其中有N个真实类别，每次取出一个类作为正类，剩余的所有类别作为一个新的反类，从而产生N个二分类学习器，在测试阶段，得出N个结果，若仅有一个学习器预测为正类，则对应的类标作为最终分类结果。</p>
</li>
<li><p>MvM：给定数据集D，假定其中有N个真实类别，每次取若干个类作为正类，若干个类作为反类（通过ECOC码给出，编码），若进行了M次划分，则生成了M个二分类学习器，在测试阶段（解码），得出M个结果组成一个新的码，最终通过计算海明/欧式距离选择距离最小的类别作为最终分类结果。</p>
</li>
</ul>
<p><img src="https://i.loli.net/2018/10/17/5bc723b862bfb.png" alt="19.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b8300d5.png" alt="20.png"></p>
<h3 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h3><p>类别不平衡（class-imbanlance）就是指分类问题中不同类别的训练样本相差悬殊的情况，例如正例有900个，而反例只有100个，这个时候我们就需要进行相应的处理来平衡这个问题。常见的做法有三种：</p>
<ol>
<li>在训练样本较多的类别中进行“欠采样”（undersampling）,比如从正例中采出100个，常见的算法有：EasyEnsemble。</li>
<li>在训练样本较少的类别中进行“过采样”（oversampling）,例如通过对反例中的数据进行插值，来产生额外的反例，常见的算法有SMOTE。</li>
<li>直接基于原数据集进行学习，对预测值进行“再缩放”处理。其中再缩放也是代价敏感学习的基础。<img src="https://i.loli.net/2018/10/17/5bc726fe87ae2.png" alt="21.png"></li>
</ol>
<h3 id="LR应用经验"><a href="#LR应用经验" class="headerlink" title="LR应用经验"></a>LR应用经验</h3><p>LR实现简单高效易解释，计算速度快，易并行，在大规模数据情况下非常适用，更适合于应对数值型和标称型数据，主要适合解决线性可分的问题，但容易欠拟合，大多数情况下需要手动进行特征工程，构建组合特征，分类精度不高。</p>
<p>LR直接对分类可能性进行建模，无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题<br>LR能以概率的形式输出，而非知识0，1判定，对许多利用概率辅助决策的任务很有用<br>对率函数任意阶可导，具有很好的数学性质，许多现有的数值优化算法都可以用来求最优解，训练速度快<br>适用情景：LR是很多分类算法的基础组件，它的好处是输出值自然地落在0到1之间，并且有概率意义。因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。虽然效果一般，却胜在模型清晰，背后的概率学经得住推敲。它拟合出来的参数就代表了每一个特征(feature)对结果的影响。也是一个理解数据的好工具。</p>
<p>应用上： </p>
<ul>
<li>CTR预估，推荐系统的learning to rank，各种分类场景 </li>
<li>某搜索引擎厂的广告CTR预估基线版是LR </li>
<li>某电商搜索排序基线版是LR </li>
<li>某新闻app排序基线版是LR</li>
</ul>
<p>大规模工业实时数据，需要可解释性的金融数据，需要快速部署低耗时数据<br>LR就是简单，可解释，速度快，消耗资源少，分布式性能好</p>
<p>ADMM-LR:用ADMM求解LogisticRegression的优化方法称作ADMM_LR。ADMM算法是一种求解约束问题的最优化方法，它适用广泛。相比于SGD，ADMM在精度要求不高的情况下，在少数迭代轮数时就达到一个合理的精度，但是收敛到很精确的解则需要很多次迭代。</p>
<p><img src="/blog_picture/line10.jpg" alt="avatar">   </p>
<p><img src="/blog_picture/line11.jpg" alt="avatar">   </p>
<p><img src="/blog_picture/line12.jpg" alt="avatar">   </p>
<h2 id="LR多分类推广-Softmax回归"><a href="#LR多分类推广-Softmax回归" class="headerlink" title="LR多分类推广 - Softmax回归"></a>LR多分类推广 - Softmax回归</h2><p>LR是一个传统的二分类模型，它也可以用于多分类任务，其基本思想是：将多分类任务拆分成若干个二分类任务，然后对每个二分类任务训练一个模型，最后将多个模型的结果进行集成以获得最终的分类结果。一般来说，可以采取的拆分策略有：</p>
<h3 id="one-vs-one策略"><a href="#one-vs-one策略" class="headerlink" title="one vs one策略"></a>one vs one策略</h3><p>　　假设我们有N个类别，该策略基本思想就是不同类别两两之间训练一个分类器，这时我们一共会训练出<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171104175530607-1392543504.png" alt="img">种不同的分类器。在预测时，我们将样本提交给所有的分类器，一共会获得N(N-1)个结果，最终结果通过<strong>投票</strong>产生。</p>
<h3 id="one-vs-all策略"><a href="#one-vs-all策略" class="headerlink" title="one vs all策略"></a>one vs all策略</h3><p>　　该策略基本思想就是将第i种类型的所有样本作为正例，将剩下的所有样本作为负例，进行训练得到一个分类器。这样我们就一共可以得到N个分类器。在预测时，我们将样本提交给所有的分类器，一共会获得N个结果，我们<strong>选择其中概率值最大</strong>的那个作为最终分类结果。 <img src="https://images2017.cnblogs.com/blog/1251096/201710/1251096-20171021171313943-1199609768.png" alt="img"></p>
<h2 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h2><p>　　softmax是LR在多分类的推广。与LR一样，同属于广义线性模型。什么是Softmax函数？假设我们有一个数组A，<img src="https://images2017.cnblogs.com/blog/1251096/201710/1251096-20171021164616881-992414484.png" alt="img">表示的是数组A中的第i个元素，那么这个元素的Softmax值就是</p>
<p>　　　　　　　　　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201710/1251096-20171021165228865-866731732.png" alt="img"></p>
<p>也就是说，是该元素的指数，与所有元素指数和的比值。那么 softmax回归模型的假设函数又是怎么样的呢？</p>
<p>　　　　　　　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105100956795-1587348606.png" alt="img"></p>
<p>由上式很明显可以得出，假设函数的分母其实就是对概率分布进行了归一化，使得所有类别的概率之和为1；也可以看出LR其实就是K=2时的Softmax。在参数获得上，我们可以采用one vs all策略获得K个不同的训练数据集进行训练，进而针对每一类别都会得到一组参数向量<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102153701-629755133.png" alt="img">。当测试样本特征向量<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102416560-1451219507.png" alt="img">输入时，我们先用假设函数针对每一个类别<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102622045-1005416234.png" alt="img">估算出概率值<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105102706623-369597312.png" alt="img">。因此我们的假设函数将要输出一个K维的向量（向量元素和为1）来表示K个类别的估计概率，我们选择其中得分最大的类别作为该输入的预测类别。Softmax看起来和one vs all 的LR很像，它们最大的不同在与Softmax得到的K个类别的得分和为1，而one vs all的LR并不是。</p>
<h3 id="softmax的代价函数"><a href="#softmax的代价函数" class="headerlink" title="softmax的代价函数"></a>softmax的代价函数</h3><p>　　类似于LR，其似然函数我们采用对数似然，故：</p>
<p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105113747779-692061991.png" alt="img"></p>
<p>加入<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171104162333248-539020480.png" alt="img">正则项的损失函数为：</p>
<p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105114132232-517057992.png" alt="img"></p>
<p>此处的<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105110553560-1026190635.png" alt="img">为符号函数。对于其参数的求解过程，我们依然采用梯度下降法。</p>
<h3 id="softmax的梯度的求解"><a href="#softmax的梯度的求解" class="headerlink" title="softmax的梯度的求解"></a>softmax的梯度的求解</h3><p>　　正则化项的求导很简单，就等于<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105120226607-495282914.png" alt="img">，下面我们主要讨论没有加正则项的损失函数的梯度求解，即</p>
<p>　　　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105113747779-692061991.png" alt="img"></p>
<p>的导数（梯度）。为了使得求解过程看起来简便、易于理解，我们仅仅只对于一个样本（x,y）情况（SGD）进行讨论，</p>
<p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105161912482-1607069737.png" alt="img"></p>
<p>此时，我们令</p>
<p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105162625888-1575402902.png" alt="img"></p>
<p>可以得到</p>
<p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105163457810-492161690.png" alt="img"></p>
<p>故：</p>
<p><img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105170233232-810575386.png" alt="img"></p>
<p>所以，正则化之后的损失函数的梯度为</p>
<p>　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171105171748341-1281292385.png" alt="img"></p>
<p>然后通过梯度下降法最小化 <img src="http://ufldl.stanford.edu/wiki/images/math/c/e/0/ce027336c1cb3c0cd461406c81369ebf.png" alt="\textstyle J(\theta)">，我们就能实现一个可用的 softmax 回归模型了。</p>
<h3 id="多分类LR与Softmax回归"><a href="#多分类LR与Softmax回归" class="headerlink" title="多分类LR与Softmax回归"></a>多分类LR与Softmax回归</h3><p>　　有了多分类的处理方法，那么我们什么时候该用多分类LR？什么时候要用softmax呢？</p>
<p>总的来说，若待分类的<strong>类别互斥</strong>，我们就使用Softmax方法；若待分类的<strong>类别有相交</strong>，我们则要选用多分类LR，然后投票表决。</p>
<h2 id="Softmax分类器"><a href="#Softmax分类器" class="headerlink" title="Softmax分类器"></a>Softmax分类器</h2><p>SVM是最常用的两个分类器之一，而另一个就是<strong>Softmax分类器，</strong>它的损失函数与SVM的损失函数不同。对于学习过二元逻辑回归分类器的读者来说，Softmax分类器就可以理解为逻辑回归分类器面对多个分类的一般化归纳。SVM将输出<img src="https://www.zhihu.com/equation?tex=f%28x_i%2CW%29" alt="[公式]">作为每个分类的评分（因为无定标，所以难以直接解释）。与SVM不同，Softmax的输出（归一化的分类概率）更加直观，并且从概率上可以解释，这一点后文会讨论。在Softmax分类器中，函数映射<img src="https://www.zhihu.com/equation?tex=f%28x_i%3BW%29%3DWx_i" alt="[公式]">保持不变，但将这些评分值视为每个分类的未归一化的对数概率，并且将<em>折叶损失（hinge loss）</em>替换为<strong>交叉熵损失</strong>（<strong>cross-entropy loss）</strong>。公式如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+Li%3D-log%28%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%29" alt="[公式]"> 或等价的 <img src="https://www.zhihu.com/equation?tex=L_i%3D-f_%7By_i%7D%2Blog%28%5Csum_je%5E%7Bf_j%7D%29" alt="[公式]"></p>
<p>在上式中，使用<img src="https://www.zhihu.com/equation?tex=f_j" alt="[公式]">来表示分类评分向量<img src="https://www.zhihu.com/equation?tex=f" alt="[公式]">中的第j个元素。和之前一样，整个数据集的损失值是数据集中所有样本数据的损失值<img src="https://www.zhihu.com/equation?tex=L_i" alt="[公式]">的均值与正则化损失<img src="https://www.zhihu.com/equation?tex=R%28W%29" alt="[公式]">之和。其中函数<img src="https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D" alt="[公式]">被称作<strong>softmax 函数</strong>：其输入值是一个向量，向量中元素为任意实数的评分值（<img src="https://www.zhihu.com/equation?tex=z" alt="[公式]">中的），函数对其进行压缩，输出一个向量，其中每个元素值在0到1之间，且所有元素之和为1。所以，包含softmax函数的完整交叉熵损失看起唬人，实际上还是比较容易理解的。</p>
<p><strong>信息理论视角</strong>：在“真实”分布<img src="https://www.zhihu.com/equation?tex=p" alt="[公式]">和估计分布<img src="https://www.zhihu.com/equation?tex=q" alt="[公式]">之间的<em>交叉熵</em>定义如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29" alt="[公式]"></p>
<p><strong>*译者注</strong>：Kullback-Leibler差异（Kullback-Leibler Divergence）也叫做相对熵（Relative Entropy），它衡量的是相同事件空间里的两个概率分布的差异情况。*</p>
<p><strong>概率论解释</strong>：先看下面的公式：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28y_i%7Cx_i%2CW%29%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D" alt="[公式]"></p>
<p><strong>实操事项：数值稳定。</strong>编程实现softmax函数计算的时候，中间项<img src="https://www.zhihu.com/equation?tex=e%5E%7Bf_%7By_i%7D%7D" alt="[公式]">和<img src="https://www.zhihu.com/equation?tex=%5Csum_j+e%5E%7Bf_j%7D" alt="[公式]">因为存在指数函数，所以数值可能非常大。除以大数值可能导致数值计算的不稳定，所以学会使用归一化技巧非常重要。如果在分式的分子和分母都乘以一个常数<img src="https://www.zhihu.com/equation?tex=C" alt="[公式]">，并把它变换到求和之中，就能得到一个从数学上等价的公式：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7BCe%5E%7Bf_%7By_i%7D%7D%7D%7BC%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%2BlogC%7D%7D%7B%5Csum_je%5E%7Bf_j%2BlogC%7D%7D" alt="[公式]"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">f = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>]) <span class="comment"># 例子中有3个分类，每个评分的数值都很大</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 不妙：数值问题，可能导致数值爆炸</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 那么将f中的值平移到最大值为0：</span></span><br><span class="line">f -= np.max(f) <span class="comment"># f becomes [-666, -333, 0]</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 现在OK了，将给出正确结果</span></span><br></pre></td></tr></table></figure>

<p><strong>让人迷惑的命名规则</strong>：精确地说，SVM分类器使用的是<em>折叶损失（hinge loss）</em>，有时候又被称为<em>最大边界损失（max-margin loss）</em>。Softmax分类器使用的是<em>交叉熵损失（corss-entropy loss）</em>。Softmax分类器的命名是从<em>softmax函数</em>那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。注意从技术上说“softmax损失（softmax loss）”是没有意义的，因为softmax只是一个压缩数值的函数。但是在这个说法常常被用来做简称。</p>
<h2 id="SVM和Softmax的比较"><a href="#SVM和Softmax的比较" class="headerlink" title="SVM和Softmax的比较"></a>SVM和Softmax的比较</h2><p>下图有助于区分这 Softmax和SVM这两种分类器：</p>
<p>————————————————————————————————————————</p>
<p><img src="https://pic1.zhimg.com/80/a90ce9e0ff533f3efee4747305382064_hd.png" alt="img"></p>
<p>针对一个数据点，SVM和Softmax分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量<strong>f</strong>（本节中是通过矩阵乘来实现）。不同之处在于对<strong>f</strong>中分值的解释：SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别2）的分值比其他分类的分值高出至少一个边界值。Softmax分类器将这些数值看做是每个分类没有归一化的<strong>对数概率</strong>，鼓励正确分类的归一化的对数概率变高，其余的变低。SVM的最终的损失值是1.58，Softmax的最终的损失值是0.452，但要注意这两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。</p>
<p>————————————————————————————————————————</p>
<p><strong>Softmax分类器为每个分类提供了“可能性”</strong>：SVM的计算是无标定的，而且难以针对所有分类的评分值给出直观解释。Softmax分类器则不同，它允许我们计算出对于所有分类标签的可能性。举个例子，针对给出的图像，SVM分类器可能给你的是一个[12.5, 0.6, -23.0]对应分类“猫”，“狗”，“船”。而softmax分类器可以计算出这三个标签的”可能性“是[0.9, 0.09, 0.01]，这就让你能看出对于不同分类准确性的把握。为什么我们要在”可能性“上面打引号呢？这是因为可能性分布的集中或离散程度是由正则化参数λ直接决定的，λ是你能直接控制的一个输入参数。举个例子，假设3个分类的原始分数是[1, -2, 0]，那么softmax函数就会计算：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5B1%2C-2%2C0%5D%5Cto%5Be%5E1%2Ce%5E%7B-2%7D%2Ce%5E0%5D%3D%5B2.71%2C0.14%2C1%5D%5Cto%5B0.7%2C0.04%2C0.26%5D" alt="[公式]"></p>
<p>现在，如果正则化参数λ更大，那么权重W就会被惩罚的更多，然后他的权重数值就会更小。这样算出来的分数也会更小，假设小了一半吧[0.5, -1, 0]，那么softmax函数的计算就是：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5B0.5%2C-1%2C0%5D%5Cto%5Be%5E%7B0.5%7D%2Ce%5E%7B-1%7D%2Ce%5E0%5D%3D%5B1.65%2C0.73%2C1%5D%5Cto%5B0.55%2C0.12%2C0.33%5D" alt="[公式]"></p>
<p>现在看起来，概率的分布就更加分散了。还有，随着正则化参数λ不断增强，权重数值会越来越小，最后输出的概率会接近于均匀分布。这就是说，softmax分类器算出来的概率最好是看成一种对于分类正确性的自信。和SVM一样，数字间相互比较得出的大小顺序是可以解释的，但其绝对值则难以直观解释<strong>。</strong></p>
<p><strong>在实际使用中，SVM和Softmax经常是相似的</strong>：通常说来，两种分类器的表现差别很小，不同的人对于哪个分类器更好有不同的看法。相对于Softmax分类器，SVM更加“局部目标化（local objective）”，这既可以看做是一个特性，也可以看做是一个劣势。考虑一个评分是[10, -2, 3]的数据，其中第一个分类是正确的。那么一个SVM（<img src="https://www.zhihu.com/equation?tex=%5CDelta+%3D1" alt="[公式]">）会看到正确分类相较于不正确分类，已经得到了比边界值还要高的分数，它就会认为损失值是0。SVM对于数字个体的细节是不关心的：如果分数是[10, -100, -100]或者[10, 9, 9]，对于SVM来说没设么不同，只要满足超过边界值等于1，那么损失值就等于0。</p>
<p>对于softmax分类器，情况则不同。对于[10, 9, 9]来说，计算出的损失值就远远高于[10, -100, -100]的。换句话来说，softmax分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。但是，SVM只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。这可以被看做是SVM的一种特性。举例说来，一个汽车的分类器应该把他的大量精力放在如何分辨小轿车和大卡车上，而不应该纠结于如何与青蛙进行区分，因为区分青蛙得到的评分已经足够低了。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LR/">LR</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/softmax/">softmax</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-文本分类问题" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/08/24/文本分类问题/" class="article-date">
      <time datetime="2019-08-24T08:41:33.000Z" itemprop="datePublished">2019-08-24</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/24/文本分类问题/">文本分类问题</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="文本分类问题"><a href="#文本分类问题" class="headerlink" title="文本分类问题"></a>文本分类问题</h1><p>下面我们来看一个文本分类问题，经典的新闻主题分类，用朴素贝叶斯怎么做。</p>
<p>In [193]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">seg_list = jieba.cut(<span class="string">"他来到东海识别区"</span>,cut_all=<span class="literal">False</span>)</span><br><span class="line">print(<span class="string">"/"</span> .join(seg_list) )</span><br><span class="line">jieba.add_word(<span class="string">'东海识别区'</span>)</span><br><span class="line">seg_list = jieba.cut(<span class="string">"他来到东海识别区"</span>,cut_all=<span class="literal">False</span>)</span><br><span class="line">print(<span class="string">"/ "</span> .join(seg_list) )</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">他/来到/东海/识别区</span><br><span class="line">他/ 来到/ 东海识别区</span><br></pre></td></tr></table></figure>

<p>In [175]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> jieba  <span class="comment">#处理中文</span></span><br><span class="line"><span class="keyword">import</span> nltk  <span class="comment">#处理英文</span></span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> pl</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> nltk</span><br></pre></td></tr></table></figure>

<h2 id="文本处理¶"><a href="#文本处理¶" class="headerlink" title="文本处理¶"></a>文本处理<a href="file:///Users/mmy/Downloads/朴素贝叶斯新闻分类.html#文本处理" target="_blank" rel="noopener">¶</a></h2><p>1、把训练样本划分为训练集和测试集</p>
<p>2、统计了词频，按词频降序生成词袋</p>
<p>In [137]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文本处理，也就是样本生成过程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_processing</span><span class="params">(folder_path, test_size=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">    folder_list = os.listdir(folder_path)</span><br><span class="line">    <span class="comment"># os.listdir 方法用于返回路径下包含的文件或文件夹的名字的列表</span></span><br><span class="line">    <span class="comment"># folder_list = ['C000008', 'C000014', 'C000013', 'C000022', 'C000023', 'C000024', 'C000010', 'C000020', 'C000016']</span></span><br><span class="line">    </span><br><span class="line">    data_list = []</span><br><span class="line">    class_list = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历文件夹，每个文件夹里是一个新闻的类别</span></span><br><span class="line">    <span class="keyword">for</span> folder <span class="keyword">in</span> folder_list:</span><br><span class="line">        new_folder_path = os.path.join(folder_path, folder)</span><br><span class="line">        <span class="comment"># os.path.join就是把两个路径拼接</span></span><br><span class="line">        <span class="comment"># new_folder_path = ./Database/SogouC/Sample/C000008</span></span><br><span class="line">        </span><br><span class="line">        files = os.listdir(new_folder_path)</span><br><span class="line">        <span class="comment"># 读取new_folder_path路径下的文件名</span></span><br><span class="line">        <span class="comment"># ['15.txt', '14.txt', '16.txt', '17.txt', '13.txt', '12.txt', '10.txt', '11.txt', '19.txt', '18.txt']</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 读取文件</span></span><br><span class="line">        j = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">            <span class="keyword">if</span> j &gt; <span class="number">100</span>: </span><br><span class="line">            <span class="comment"># 怕内存爆掉，只取100个样本文件，你可以注释掉取完，</span></span><br><span class="line">            <span class="comment"># 这里每个类别下只有10个样本，没事</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">with</span> open(os.path.join(new_folder_path, file), <span class="string">'r'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">                raw = fp.read()</span><br><span class="line">                <span class="comment"># read() 返回值为str，每次读取整个文件，将文件所有内容放到一个字符串变量中</span></span><br><span class="line">                <span class="comment"># readline() 返回值为str，每次只读取一行,每行的内容放在一个字符串变量中</span></span><br><span class="line">                <span class="comment"># readlines() 返回值为list，一次读取整个文件，每行的内容放在一个字符串变量中作为列表的一个元素。</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">## 是的，随处可见的jieba中文分词</span></span><br><span class="line">            jieba.enable_parallel(<span class="number">4</span>) <span class="comment"># 开启并行分词模式，参数为并行进程数，不支持windows</span></span><br><span class="line">            word_cut = jieba.cut(raw, cut_all=<span class="literal">False</span>) <span class="comment"># 精确模式，返回的结构是一个可迭代的genertor</span></span><br><span class="line">            word_list = list(word_cut) <span class="comment"># genertor转化为list，每个词unicode格式</span></span><br><span class="line">            jieba.disable_parallel() <span class="comment"># 关闭并行分词模式</span></span><br><span class="line">            </span><br><span class="line">            data_list.append(word_list) <span class="comment">#训练集list</span></span><br><span class="line">            <span class="comment">#class_list.append(folder.decode('utf-8')) #类别,str.decode会报错</span></span><br><span class="line">            class_list.append(folder) <span class="comment">#训练集的标签类别</span></span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">    <span class="comment">## 下面手动粗暴地划分训练集和测试集</span></span><br><span class="line">    data_class_list = zip(data_list, class_list) <span class="comment"># zip 函数返回一个zip对象</span></span><br><span class="line">    data_class_list = list(data_class_list) <span class="comment"># 需要用list转换成列表</span></span><br><span class="line">    </span><br><span class="line">    random.shuffle(data_class_list) <span class="comment"># shuffle随机打乱样本</span></span><br><span class="line">    index = int(len(data_class_list)*test_size)+<span class="number">1</span></span><br><span class="line">    train_list = data_class_list[index:]</span><br><span class="line">    test_list = data_class_list[:index]</span><br><span class="line">    </span><br><span class="line">    train_data_list, train_class_list = zip(*train_list) </span><br><span class="line">    <span class="comment"># 解压缩,文本和类别分开返回的是元组格式，可以在用list转换</span></span><br><span class="line">    test_data_list, test_class_list = zip(*test_list) </span><br><span class="line">    <span class="comment">#以上划分训练集和测试集其实可以用sklearn自带的部分做</span></span><br><span class="line">    <span class="comment">#train_data_list, test_data_list, train_class_list, test_class_list = \</span></span><br><span class="line">    <span class="comment">#sklearn.model_selection.train_test_split(data_list, class_list, test_size=test_size) </span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">   </span><br><span class="line">    <span class="comment"># 统计词频放入all_words_dict</span></span><br><span class="line">    all_words_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> word_list <span class="keyword">in</span> train_data_list:</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> word_list:</span><br><span class="line">            <span class="comment">#if all_words_dict.has_key(word):已删除此方法</span></span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> all_words_dict:</span><br><span class="line">                all_words_dict[word] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                all_words_dict[word] = <span class="number">1</span></span><br><span class="line">    <span class="comment"># all_words_dict=&#123;'\n': 1257, '\u3000': 1986, '有': 175, '江湖': 1,.....&#125;</span></span><br><span class="line">    <span class="comment"># 同样以上有现成的统计词频的API可以调用</span></span><br><span class="line">    <span class="comment"># from collections import Counter </span></span><br><span class="line">    <span class="comment"># Counter(train_data_list)</span></span><br><span class="line"></span><br><span class="line">    all_words_tuple_list = sorted(all_words_dict.items(), key=<span class="keyword">lambda</span> f:f[<span class="number">1</span>], reverse=<span class="literal">True</span>) </span><br><span class="line">    <span class="comment"># 内建函数sorted第一个参数需为list，all_words_dict.items()转化为列表，键和值为元组</span></span><br><span class="line">    <span class="comment"># key函数代表按元组的的词频排序，并降序返回结果</span></span><br><span class="line">    <span class="comment"># all_words_tuple_list = [('，', 3424), ('的', 2527), ('\u3000', 1734), ('。', 1482),.....]</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#all_words_list = list(zip(*all_words_tuple_list)[0]) 报错，需要修改</span></span><br><span class="line">    all_words_list,_ = zip(*all_words_tuple_list) <span class="comment"># 解压缩</span></span><br><span class="line">    all_words_list = list(all_words_list)</span><br><span class="line">    <span class="comment"># all_words_list = ['，', '的', '\u3000', '。', '\n', '在', ' ', '、', '了', '“',.....]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> all_words_list, train_data_list, test_data_list, train_class_list, test_class_list</span><br></pre></td></tr></table></figure>

<p>In [138]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (<span class="string">"start"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 文本预处理</span></span><br><span class="line">folder_path = <span class="string">'./Database/SogouC/Sample'</span></span><br><span class="line">all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = \</span><br><span class="line">text_processing(folder_path, test_size=<span class="number">0.2</span>)</span><br><span class="line">print(len(all_words_list)) <span class="comment"># 9748个不重复单词</span></span><br><span class="line">print(all_words_list[:<span class="number">100</span>])</span><br><span class="line">print(len(train_data_list)) <span class="comment"># 71个训练集样本</span></span><br><span class="line">print(len(test_data_list))  <span class="comment"># 19个测试集样本</span></span><br><span class="line">print(len(train_class_list))  <span class="comment"># 71个训练集标签</span></span><br><span class="line">print(len(test_class_list))  <span class="comment"># 19个测试集标签</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">start</span><br><span class="line">9481</span><br><span class="line">[&apos;，&apos;, &apos;的&apos;, &apos;\u3000&apos;, &apos;。&apos;, &apos;\n&apos;, &apos; &apos;, &apos;;&apos;, &apos;&amp;&apos;, &apos;nbsp&apos;, &apos;、&apos;, &apos;在&apos;, &apos;了&apos;, &apos;“&apos;, &apos;是&apos;, &apos;”&apos;, &apos;\x00&apos;, &apos;：&apos;, &apos;和&apos;, &apos;中国&apos;, &apos;有&apos;, &apos;也&apos;, &apos;我&apos;, &apos;对&apos;, &apos;就&apos;, &apos;将&apos;, &apos;—&apos;, &apos;上&apos;, &apos;这&apos;, &apos;游客&apos;, &apos;都&apos;, &apos;旅游&apos;, &apos;中&apos;, &apos;不&apos;, &apos;为&apos;, &apos;要&apos;, &apos;与&apos;, &apos;年&apos;, &apos;等&apos;, &apos;而&apos;, &apos;；&apos;, &apos;可以&apos;, &apos;月&apos;, &apos;（&apos;, &apos;）&apos;, &apos;导弹&apos;, &apos;大陆&apos;, &apos;一个&apos;, &apos;从&apos;, &apos;人&apos;, &apos;3&apos;, &apos;到&apos;, &apos;但&apos;, &apos;你&apos;, &apos;公司&apos;, &apos;说&apos;, &apos;火炮&apos;, &apos;日&apos;, &apos;(&apos;, &apos;)&apos;, &apos;他&apos;, &apos;考生&apos;, &apos;台军&apos;, &apos;认为&apos;, &apos;北京&apos;, &apos;时&apos;, &apos;多&apos;, &apos;还&apos;, &apos;个&apos;, &apos;1&apos;, &apos;.&apos;, &apos;能&apos;, &apos;《&apos;, &apos;》&apos;, &apos;已经&apos;, &apos;解放军&apos;, &apos;一种&apos;, &apos;会&apos;, &apos;时间&apos;, &apos;自己&apos;, &apos;来&apos;, &apos;新&apos;, &apos;各种&apos;, &apos;大&apos;, &apos;０&apos;, &apos;5&apos;, &apos;进行&apos;, &apos;市场&apos;, &apos;主要&apos;, &apos;我们&apos;, &apos;以&apos;, &apos;后&apos;, &apos;美国&apos;, &apos;五一&apos;, &apos;让&apos;, &apos;支付&apos;, &apos;黄金周&apos;, &apos;增长&apos;, &apos;并&apos;, &apos;成为&apos;, &apos;最&apos;]</span><br><span class="line">71</span><br><span class="line">19</span><br><span class="line">71</span><br><span class="line">19</span><br></pre></td></tr></table></figure>

<h2 id="停用词文件去重"><a href="#停用词文件去重" class="headerlink" title="停用词文件去重"></a>停用词文件去重</h2><p>这个停用词文件不是很官方，所以需要清洗下</p>
<p>In [140]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 粗暴的词去重</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_word_set</span><span class="params">(words_file)</span>:</span></span><br><span class="line">    words_set = set() <span class="comment"># 集合格式</span></span><br><span class="line">    <span class="keyword">with</span> open(words_file, <span class="string">'r'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fp.readlines(): <span class="comment"># 循环取出每一行</span></span><br><span class="line">            word = line.strip()</span><br><span class="line">            <span class="comment"># line.strip() 当()为空时，默认删除空白符（包括'\n','\r','\t',' ')</span></span><br><span class="line">            <span class="keyword">if</span> len(word)&gt;<span class="number">0</span> <span class="keyword">and</span> word <span class="keyword">not</span> <span class="keyword">in</span> words_set: <span class="comment"># 去重</span></span><br><span class="line">                words_set.add(word)</span><br><span class="line">    <span class="keyword">return</span> words_set</span><br></pre></td></tr></table></figure>

<p>In [145]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成stopwords_set</span></span><br><span class="line">stopwords_file = <span class="string">'./stopwords_cn.txt'</span> <span class="comment"># 停用词列表文件</span></span><br><span class="line">stopwords_set = make_word_set(stopwords_file) <span class="comment"># 首先停用词去重</span></span><br><span class="line">print(len(stopwords_set))</span><br><span class="line">print(stopwords_set)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">428</span><br><span class="line">&#123;&apos;得了&apos;, &apos;还是&apos;, &apos;所在&apos;, &apos;为此&apos;, &apos;如同下&apos;, &apos;并且&apos;, &apos;许多&apos;, &apos;但&apos;, &apos;不尽然&apos;, &apos;无&apos;, &apos;却&apos;, &apos;所&apos;, &apos;据此&apos;, &apos;分别&apos;, &apos;向&apos;, &apos;遵照&apos;, &apos;多会&apos;, &apos;而后&apos;, &apos;如下&apos;, &apos;再有&apos;, &apos;的确&apos;, &apos;此外&apos;, &apos;距&apos;, &apos;而已&apos;, &apos;何处&apos;, &apos;在于&apos;, &apos;说来&apos;, &apos;不料&apos;, &apos;且&apos;, &apos;于&apos;, &apos;亦&apos;, &apos;不单&apos;, &apos;而是&apos;, &apos;本人&apos;, &apos;正如&apos;, &apos;前者&apos;, &apos;别&apos;, &apos;才能&apos;, &apos;啦&apos;, &apos;只因&apos;, &apos;受到&apos;, &apos;甚至于&apos;, &apos;另一方面&apos;, &apos;此&apos;, &apos;只有&apos;, &apos;可是&apos;, &apos;您&apos;, &apos;别的&apos;, &apos;别处&apos;, &apos;些&apos;, &apos;或&apos;, &apos;不论&apos;, &apos;这会&apos;, &apos;其它&apos;, &apos;况且&apos;, &apos;有&apos;, &apos;此次&apos;, &apos;因此&apos;, &apos;去&apos;, &apos;毋宁&apos;, &apos;它们&apos;, &apos;根据&apos;, &apos;基于&apos;, &apos;当地&apos;, &apos;依据&apos;, &apos;然而&apos;, &apos;虽然&apos;, &apos;因为&apos;, &apos;从而&apos;, &apos;对比&apos;, &apos;怎&apos;, &apos;以上&apos;, &apos;诸如&apos;, &apos;倘若&apos;, &apos;一些&apos;, &apos;否则&apos;, &apos;所以&apos;, &apos;那边&apos;, &apos;每&apos;, &apos;并非&apos;, &apos;之&apos;, &apos;另&apos;, &apos;简言之&apos;, &apos;只限&apos;, &apos;连同&apos;, &apos;反之&apos;, &apos;这般&apos;, &apos;几&apos;, &apos;往&apos;, &apos;是&apos;, &apos;既&apos;, &apos;各自&apos;, &apos;该&apos;, &apos;因之&apos;, &apos;得&apos;, &apos;来&apos;, &apos;不然&apos;, &apos;么&apos;, &apos;甚至&apos;, &apos;为&apos;, &apos;处在&apos;, &apos;全部&apos;, &apos;如上&apos;, &apos;按照&apos;, &apos;加之&apos;, &apos;介于&apos;, &apos;正巧&apos;, &apos;好&apos;, &apos;似的&apos;, &apos;不过&apos;, &apos;有些&apos;, &apos;那样&apos;, &apos;此地&apos;, &apos;凡是&apos;, &apos;每当&apos;, &apos;不是&apos;, &apos;万一&apos;, &apos;由于&apos;, &apos;曾&apos;, &apos;而&apos;, &apos;照着&apos;, &apos;依照&apos;, &apos;彼时&apos;, &apos;就要&apos;, &apos;然后&apos;, &apos;以为&apos;, &apos;只需&apos;, &apos;为何&apos;, &apos;谁&apos;, &apos;到&apos;, &apos;不仅仅&apos;, &apos;既然&apos;, &apos;当然&apos;, &apos;起&apos;, &apos;嗡&apos;, &apos;此间&apos;, &apos;果然&apos;, &apos;与否&apos;, &apos;随&apos;, &apos;因&apos;, &apos;值此&apos;, &apos;即便&apos;, &apos;格里斯&apos;, &apos;趁着&apos;, &apos;要不然&apos;, &apos;那个&apos;, &apos;至于&apos;, &apos;乃&apos;, &apos;随后&apos;, &apos;有时&apos;, &apos;何况&apos;, &apos;当&apos;, &apos;使&apos;, &apos;只是&apos;, &apos;为着&apos;, &apos;可见&apos;, &apos;即使&apos;, &apos;以来&apos;, &apos;着&apos;, &apos;吧&apos;, &apos;截至&apos;, &apos;个&apos;, &apos;为什么&apos;, &apos;用&apos;, &apos;除外&apos;, &apos;他们&apos;, &apos;随时&apos;, &apos;这些&apos;, &apos;还&apos;, &apos;了&apos;, &apos;还有&apos;, &apos;啥&apos;, &apos;甚而&apos;, &apos;他&apos;, &apos;但是&apos;, &apos;并不&apos;, &apos;某某&apos;, &apos;如果说&apos;, &apos;从&apos;, &apos;各&apos;, &apos;别人&apos;, &apos;趁&apos;, &apos;这里&apos;, &apos;别说&apos;, &apos;以&apos;, &apos;不光&apos;, &apos;其次&apos;, &apos;就算&apos;, &apos;沿着&apos;, &apos;如果&apos;, &apos;大家&apos;, &apos;朝着&apos;, &apos;正是&apos;, &apos;对待&apos;, &apos;自己&apos;, &apos;不尽&apos;, &apos;虽&apos;, &apos;有关&apos;, &apos;替代&apos;, &apos;哟&apos;, &apos;对于&apos;, &apos;以及&apos;, &apos;这儿&apos;, &apos;加以&apos;, &apos;一&apos;, &apos;不外乎&apos;, &apos;尽管如此&apos;, &apos;个别&apos;, &apos;再则&apos;, &apos;哪&apos;, &apos;她&apos;, &apos;除此&apos;, &apos;也&apos;, &apos;自身&apos;, &apos;用来&apos;, &apos;不&apos;, &apos;什么&apos;, &apos;人们&apos;, &apos;便于&apos;, &apos;又&apos;, &apos;此处&apos;, &apos;非但&apos;, &apos;如何&apos;, &apos;哇&apos;, &apos;那里&apos;, &apos;只消&apos;, &apos;既是&apos;, &apos;凭&apos;, &apos;的&apos;, &apos;故而&apos;, &apos;打&apos;, &apos;嘻嘻&apos;, &apos;对方&apos;, &apos;谁人&apos;, &apos;乃至&apos;, &apos;以至&apos;, &apos;再&apos;, &apos;什么样&apos;, &apos;何&apos;, &apos;任何&apos;, &apos;最&apos;, &apos;由此&apos;, &apos;而且&apos;, &apos;自&apos;, &apos;直到&apos;, &apos;为了&apos;, &apos;固然&apos;, &apos;除了&apos;, &apos;假如&apos;, &apos;人&apos;, &apos;才是&apos;, &apos;据&apos;, &apos;的话&apos;, &apos;来自&apos;, &apos;有的&apos;, &apos;我们&apos;, &apos;从此&apos;, &apos;关于&apos;, &apos;向着&apos;, &apos;那么&apos;, &apos;给&apos;, &apos;或者&apos;, &apos;某个&apos;, &apos;等等&apos;, &apos;光是&apos;, &apos;怎么样&apos;, &apos;嘿嘿&apos;, &apos;如此&apos;, &apos;只&apos;, &apos;多少&apos;, &apos;来说&apos;, &apos;那般&apos;, &apos;哪个&apos;, &apos;那时&apos;, &apos;首先&apos;, &apos;赖以&apos;, &apos;这边&apos;, &apos;我&apos;, &apos;于是&apos;, &apos;另外&apos;, &apos;她们&apos;, &apos;已&apos;, &apos;不如&apos;, &apos;哪儿&apos;, &apos;及&apos;, &apos;及至&apos;, &apos;很&apos;, &apos;多么&apos;, &apos;哪些&apos;, &apos;又及&apos;, &apos;其&apos;, &apos;还要&apos;, &apos;既往&apos;, &apos;以致&apos;, &apos;或者说&apos;, &apos;如是&apos;, &apos;不仅&apos;, &apos;为止&apos;, &apos;本着&apos;, &apos;鉴于&apos;, &apos;什么的&apos;, &apos;而外&apos;, &apos;譬如&apos;, &apos;那儿&apos;, &apos;咱们&apos;, &apos;只要&apos;, &apos;凭借&apos;, &apos;后者&apos;, &apos;则&apos;, &apos;比如&apos;, &apos;一切&apos;, &apos;个人&apos;, &apos;何以&apos;, &apos;那&apos;, &apos;咱&apos;, &apos;上&apos;, &apos;在&apos;, &apos;如若&apos;, &apos;他人&apos;, &apos;一旦&apos;, &apos;哪怕&apos;, &apos;这样&apos;, &apos;只限于&apos;, &apos;仍&apos;, &apos;之所以&apos;, &apos;所有&apos;, &apos;或是&apos;, &apos;诸位&apos;, &apos;总之&apos;, &apos;怎么办&apos;, &apos;其中&apos;, &apos;怎么&apos;, &apos;若&apos;, &apos;作为&apos;, &apos;怎样&apos;, &apos;本身&apos;, &apos;凡&apos;, &apos;连带&apos;, &apos;由&apos;, &apos;不管&apos;, &apos;不但&apos;, &apos;只怕&apos;, &apos;和&apos;, &apos;看&apos;, &apos;同&apos;, &apos;把&apos;, &apos;宁可&apos;, &apos;那些&apos;, &apos;彼此&apos;, &apos;不只&apos;, &apos;唯有&apos;, &apos;继而&apos;, &apos;呵呵&apos;, &apos;正值&apos;, &apos;至&apos;, &apos;你们&apos;, &apos;下&apos;, &apos;跟&apos;, &apos;针对&apos;, &apos;并&apos;, &apos;以免&apos;, &apos;不至于&apos;, &apos;经过&apos;, &apos;你&apos;, &apos;就是&apos;, &apos;虽说&apos;, &apos;小&apos;, &apos;与其&apos;, &apos;至今&apos;, &apos;一来&apos;, &apos;让&apos;, &apos;们&apos;, &apos;即&apos;, &apos;诸&apos;, &apos;要不&apos;, &apos;沿&apos;, &apos;出来&apos;, &apos;两者&apos;, &apos;此时&apos;, &apos;遵循&apos;, &apos;如&apos;, &apos;这&apos;, &apos;这么&apos;, &apos;出于&apos;, &apos;较之&apos;, &apos;比&apos;, &apos;嘛&apos;, &apos;某些&apos;, &apos;以便&apos;, &apos;可以&apos;, &apos;若非&apos;, &apos;各位&apos;, &apos;今&apos;, &apos;逐步&apos;, &apos;这个&apos;, &apos;它&apos;, &apos;例如&apos;, &apos;其他&apos;, &apos;反而&apos;, &apos;就是说&apos;, &apos;随着&apos;, &apos;致&apos;, &apos;同时&apos;, &apos;可&apos;, &apos;被&apos;, &apos;接着&apos;, &apos;靠&apos;, &apos;除非&apos;, &apos;某&apos;, &apos;后&apos;, &apos;尔&apos;, &apos;其余&apos;, &apos;与&apos;, &apos;全体&apos;, &apos;仍旧&apos;, &apos;进而&apos;, &apos;儿&apos;, &apos;自从&apos;, &apos;开外&apos;, &apos;拿&apos;, &apos;要是&apos;, &apos;无论&apos;, &apos;要么&apos;, &apos;若是&apos;, &apos;因而&apos;, &apos;本地&apos;, &apos;尽管&apos;, &apos;何时&apos;&#125;</span><br></pre></td></tr></table></figure>

<h2 id="词袋中选取有代表性的特征词"><a href="#词袋中选取有代表性的特征词" class="headerlink" title="词袋中选取有代表性的特征词"></a>词袋中选取有代表性的特征词</h2><p>第一步生成的词袋里有很多通用的、无意义的词语，需要去掉。<br>有代表性的词语很大概率是一些对最终类别区分有作用的词语。并且后面这些词语会作为特征作为模型的输入。</p>
<p>In [125]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">words_dict</span><span class="params">(all_words_list, deleteN, stopwords_set=set<span class="params">()</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 选取特征词</span></span><br><span class="line">    feature_words = []</span><br><span class="line">    n = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(deleteN, len(all_words_list), <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 循环时从第20个开始，也就是舍弃前20个词语</span></span><br><span class="line">        <span class="keyword">if</span> n &gt; <span class="number">1000</span>: <span class="comment"># feature_words的维度1000</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> all_words_list[t].isdigit() <span class="keyword">and</span> \</span><br><span class="line">        all_words_list[t] <span class="keyword">not</span> <span class="keyword">in</span> stopwords_set <span class="keyword">and</span> \</span><br><span class="line">        <span class="number">1</span>&lt;len(all_words_list[t])&lt;<span class="number">5</span>:</span><br><span class="line">        <span class="comment"># isdigit() 方法检测字符串是否只由数字组成,返回True和False</span></span><br><span class="line">        <span class="comment"># 满足三个条件：不是数字；不在停用词表；长度2～4</span></span><br><span class="line">            feature_words.append(all_words_list[t])</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> feature_words</span><br></pre></td></tr></table></figure>

<p>In [149]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">deleteN = <span class="number">20</span> </span><br><span class="line"><span class="comment"># 删除前20个词语,可以调整这个数值</span></span><br><span class="line"><span class="comment"># 越靠前的词语出现的越频繁，有可能所有类别中都出现很多次，这类词语是可以去掉的。</span></span><br><span class="line">feature_words = words_dict(all_words_list, deleteN, stopwords_set)=</span><br><span class="line">print(feature_words[:<span class="number">100</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&apos;游客&apos;, &apos;旅游&apos;, &apos;导弹&apos;, &apos;大陆&apos;, &apos;一个&apos;, &apos;公司&apos;, &apos;火炮&apos;, &apos;考生&apos;, &apos;台军&apos;, &apos;认为&apos;, &apos;北京&apos;, &apos;已经&apos;, &apos;解放军&apos;, &apos;一种&apos;, &apos;时间&apos;, &apos;各种&apos;, &apos;进行&apos;, &apos;市场&apos;, &apos;主要&apos;, &apos;美国&apos;, &apos;五一&apos;, &apos;支付&apos;, &apos;黄金周&apos;, &apos;增长&apos;, &apos;成为&apos;, &apos;复习&apos;, &apos;很多&apos;, &apos;目前&apos;, &apos;没有&apos;, &apos;记者&apos;, &apos;问题&apos;, &apos;分析&apos;, &apos;远程&apos;, &apos;万人次&apos;, &apos;射程&apos;, &apos;接待&apos;, &apos;基础&apos;, &apos;部分&apos;, &apos;部署&apos;, &apos;作战&apos;, &apos;一定&apos;, &apos;选择&apos;, &apos;辅导班&apos;, &apos;考试&apos;, &apos;词汇&apos;, &apos;技术&apos;, &apos;比赛&apos;, &apos;文章&apos;, &apos;完全&apos;, &apos;可能&apos;, &apos;收入&apos;, &apos;工作&apos;, &apos;时候&apos;, &apos;今年&apos;, &apos;表示&apos;, &apos;期间&apos;, &apos;企业&apos;, &apos;VS&apos;, &apos;能力&apos;, &apos;达到&apos;, &apos;毕业生&apos;, &apos;上海&apos;, &apos;表现&apos;, &apos;影响&apos;, &apos;比较&apos;, &apos;人数&apos;, &apos;用户&apos;, &apos;相对&apos;, &apos;专家&apos;, &apos;服务&apos;, &apos;重要&apos;, &apos;拥有&apos;, &apos;需要&apos;, &apos;训练&apos;, &apos;开始&apos;, &apos;销售&apos;, &apos;通过&apos;, &apos;阵地&apos;, &apos;资料&apos;, &apos;情况&apos;, &apos;要求&apos;, &apos;阅读&apos;, &apos;老师&apos;, &apos;新浪&apos;, &apos;坦克&apos;, &apos;网络&apos;, &apos;军事&apos;, &apos;英语&apos;, &apos;项目&apos;, &apos;历史&apos;, &apos;设计&apos;, &apos;几乎&apos;, &apos;这是&apos;, &apos;写作&apos;, &apos;日本&apos;, &apos;考古&apos;, &apos;不同&apos;, &apos;提高&apos;, &apos;活动&apos;, &apos;公里&apos;]</span><br></pre></td></tr></table></figure>

<h2 id="训练和测试集生成固定长度的词向量特征"><a href="#训练和测试集生成固定长度的词向量特征" class="headerlink" title="训练和测试集生成固定长度的词向量特征"></a>训练和测试集生成固定长度的词向量特征</h2><p>这步为后面数据输入进贝叶斯模型训练做准备。</p>
<p>因为文本长度不一，所以每个样本需要固定好维度，才能喂给模型训练。</p>
<p>In [153]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文本特征</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_features</span><span class="params">(train_data_list, test_data_list, feature_words, flag=<span class="string">'nltk'</span>)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">text_features</span><span class="params">(text, feature_words)</span>:</span> <span class="comment"># text的定义在下面</span></span><br><span class="line">        text_words = set(text) <span class="comment"># 样本去重</span></span><br><span class="line">        <span class="comment">## -----------------------------------------------------------------------------------</span></span><br><span class="line">        <span class="keyword">if</span> flag == <span class="string">'nltk'</span>:</span><br><span class="line">            <span class="comment">## nltk特征 dict</span></span><br><span class="line">            features = &#123;word:<span class="number">1</span> <span class="keyword">if</span> word <span class="keyword">in</span> text_words <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> word <span class="keyword">in</span> feature_words&#125;</span><br><span class="line"><span class="comment"># 遍历每个样本词语，凡是样本的词语出现在1000个特征词里，就记录下来，保存为字典格式，键为词语，值为1，否则值为0。</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">elif</span> flag == <span class="string">'sklearn'</span>:</span><br><span class="line">            <span class="comment">## sklearn特征 list</span></span><br><span class="line">            features = [<span class="number">1</span> <span class="keyword">if</span> word <span class="keyword">in</span> text_words <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> word <span class="keyword">in</span> feature_words] </span><br><span class="line"><span class="comment"># 同上，遍历每个样本词语，结果不是字典，出现即为1，不出现为0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            features = []</span><br><span class="line">        <span class="comment">## -----------------------------------------------------------------------------------</span></span><br><span class="line">        <span class="keyword">return</span> features</span><br><span class="line">    train_feature_list = [text_features(text, feature_words) <span class="keyword">for</span> text <span class="keyword">in</span> train_data_list]</span><br><span class="line">    <span class="comment"># text为每一个训练的样本，返回值是二维列表</span></span><br><span class="line">    </span><br><span class="line">    test_feature_list = [text_features(text, feature_words) <span class="keyword">for</span> text <span class="keyword">in</span> test_data_list]</span><br><span class="line">    <span class="comment"># train为每一个测试样本，返回值是二维列表</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> train_feature_list, test_feature_list</span><br></pre></td></tr></table></figure>

<p>In [169]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">flag = <span class="string">'sklearn'</span></span><br><span class="line">train_feature_list, test_feature_list = \</span><br><span class="line">text_features(train_data_list, test_data_list, feature_words, flag)</span><br><span class="line">print(len(train_feature_list)) <span class="comment">#</span></span><br><span class="line">print(len(test_feature_list))  <span class="comment"># </span></span><br><span class="line">print(len(test_feature_list[<span class="number">5</span>])) <span class="comment"># 每个样本的维度都是1000 </span></span><br><span class="line">print(test_feature_list[<span class="number">5</span>][<span class="number">0</span>:<span class="number">100</span>]) <span class="comment"># 打印测试集的第5个样本的前100个值</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">71</span><br><span class="line">19</span><br><span class="line">1000</span><br><span class="line">[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</span><br></pre></td></tr></table></figure>

<h2 id="贝叶斯模型开始训练和预测"><a href="#贝叶斯模型开始训练和预测" class="headerlink" title="贝叶斯模型开始训练和预测"></a>贝叶斯模型开始训练和预测</h2><p>In [176]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分类，同时输出准确率等</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_classifier</span><span class="params">(train_feature_list, test_feature_list, </span></span></span><br><span class="line"><span class="function"><span class="params">                    train_class_list, test_class_list, flag=<span class="string">'nltk'</span>)</span>:</span></span><br><span class="line">    <span class="comment">## -----------------------------------------------------------------------------------</span></span><br><span class="line">    <span class="keyword">if</span> flag == <span class="string">'nltk'</span>:</span><br><span class="line">        <span class="comment">## 使用nltk分类器</span></span><br><span class="line">        train_flist = zip(train_feature_list, train_class_list)</span><br><span class="line">        train_flist = list(train_flist) </span><br><span class="line">        test_flist = zip(test_feature_list, test_class_list)</span><br><span class="line">        train_flist = list(test_flist) </span><br><span class="line">        classifier = nltk.classify.NaiveBayesClassifier.train(train_flist)</span><br><span class="line">        test_accuracy = nltk.classify.accuracy(classifier, test_flist)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">elif</span> flag == <span class="string">'sklearn'</span>:</span><br><span class="line">        <span class="comment">## sklearn分类器</span></span><br><span class="line">        classifier = MultinomialNB().fit(train_feature_list, train_class_list)</span><br><span class="line">        <span class="comment"># MultinomialNB()的使用方法和参数见：https://www.cnblogs.com/pinard/p/6074222.html</span></span><br><span class="line">        </span><br><span class="line">        test_accuracy = classifier.score(test_feature_list, test_class_list)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        test_accuracy = []</span><br><span class="line">    <span class="keyword">return</span> test_accuracy</span><br></pre></td></tr></table></figure>

<p>In [177]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">flag=&apos;sklearn&apos;</span><br><span class="line">test_accuracy = text_classifier(train_feature_list, test_feature_list, </span><br><span class="line">                                    train_class_list, test_class_list, flag)</span><br><span class="line">print(test_accuracy)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.7368421052631579</span><br></pre></td></tr></table></figure>

<h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>这步调参，查看不同的deleteNs对模型效果的影响</p>
<p>In [179]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (<span class="string">"start"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 文本预处理</span></span><br><span class="line">folder_path = <span class="string">'./Database/SogouC/Sample'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = text_processing(folder_path, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成stopwords_set</span></span><br><span class="line">stopwords_file = <span class="string">'./stopwords_cn.txt'</span></span><br><span class="line">stopwords_set = make_word_set(stopwords_file)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 文本特征提取和分类</span></span><br><span class="line"><span class="comment"># flag = 'nltk'</span></span><br><span class="line">flag = <span class="string">'sklearn'</span></span><br><span class="line">deleteNs = range(<span class="number">0</span>, <span class="number">1000</span>, <span class="number">20</span>)</span><br><span class="line">test_accuracy_list = []</span><br><span class="line"><span class="keyword">for</span> deleteN <span class="keyword">in</span> deleteNs:</span><br><span class="line">    <span class="comment"># feature_words = words_dict(all_words_list, deleteN)</span></span><br><span class="line">    feature_words = words_dict(all_words_list, deleteN, stopwords_set)</span><br><span class="line">    train_feature_list, test_feature_list = text_features(train_data_list, test_data_list, feature_words, flag)</span><br><span class="line">    test_accuracy = text_classifier(train_feature_list, test_feature_list, train_class_list, test_class_list, flag)</span><br><span class="line">    test_accuracy_list.append(test_accuracy)</span><br><span class="line"><span class="keyword">print</span> (test_accuracy_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果评价</span></span><br><span class="line"><span class="comment">#plt.figure()</span></span><br><span class="line">plt.plot(deleteNs, test_accuracy_list)</span><br><span class="line">plt.title(<span class="string">'Relationship of deleteNs and test_accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'deleteNs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'test_accuracy'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#plt.savefig('result.png')</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"finished"</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start</span><br><span class="line">[0.6842105263157895, 0.6842105263157895, 0.7368421052631579, 0.7368421052631579, 0.7368421052631579, 0.7368421052631579, 0.7368421052631579, 0.6842105263157895, 0.6842105263157895, 0.6842105263157895, 0.6842105263157895, 0.6842105263157895, 0.6842105263157895, 0.7368421052631579, 0.7368421052631579, 0.7894736842105263, 0.7368421052631579, 0.7368421052631579, 0.7368421052631579, 0.7368421052631579, 0.7368421052631579, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.7368421052631579, 0.6842105263157895, 0.6842105263157895, 0.6842105263157895, 0.7368421052631579, 0.7368421052631579, 0.6842105263157895, 0.7368421052631579, 0.6842105263157895, 0.6842105263157895, 0.7368421052631579, 0.6842105263157895, 0.631578947368421, 0.6842105263157895, 0.6842105263157895, 0.7368421052631579, 0.7894736842105263, 0.7894736842105263, 0.7368421052631579, 0.7368421052631579, 0.7894736842105263]</span><br></pre></td></tr></table></figure>

<p><img src="data:image/png;base64,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" alt="img"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">finished</span><br></pre></td></tr></table></figure>


      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/jiaba/">jiaba</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-机器学习基本概念" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/08/23/机器学习基本概念/" class="article-date">
      <time datetime="2019-08-23T09:01:11.000Z" itemprop="datePublished">2019-08-23</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/23/机器学习基本概念/">机器学习基本概念</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="机器学习基本概念"><a href="#机器学习基本概念" class="headerlink" title="机器学习基本概念"></a>机器学习基本概念</h2><p><strong>1  机器学习</strong></p>
<p><strong>1.1 机器学习的定义</strong></p>
<p>正如我们根据过去的经验来判断明天的天气，吃货们希望从购买经验中挑选一个好瓜，那能不能让计算机帮助人类来实现这个呢？机器学习正是这样的一门学科，人的“经验”对应计算机中的“数据”，让计算机来学习这些经验数据，生成一个算法模型，在面对新的情况中，计算机便能作出有效的判断，这便是机器学习。</p>
<p>另一本经典教材的作者Mitchell给出了一个形式化的定义，假设：</p>
<ul>
<li>P：计算机程序在某任务类T上的性能。</li>
<li>T：计算机程序希望实现的任务类。</li>
<li>E：表示经验，即历史的数据集。</li>
</ul>
<p>若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习。</p>
<p><strong>1.2 机器学习的一些基本术语</strong><br><img src="../img/ml_concepts.png" alt><br>假设我们收集了一批西瓜的数据，例如：（色泽=青绿;根蒂=蜷缩;敲声=浊响)， (色泽=乌黑;根蒂=稍蜷;敲声=沉闷)， (色泽=浅自;根蒂=硬挺;敲声=清脆)……每对括号内是一个西瓜的记录，定义：     </p>
<ul>
<li><p>所有记录的集合为：数据集。</p>
</li>
<li><p>每一条记录为：一个实例（instance）或样本（sample）。</p>
</li>
<li><p>例如：色泽或敲声，单个的特点为特征（feature）或属性（attribute）。</p>
</li>
<li><p>对于一条记录，如果在坐标轴上表示，每个西瓜都可以用坐标轴中的一个点表示，一个点也是一个向量，例如（青绿，蜷缩，浊响），即每个西瓜为：一个特征向量（feature vector）。</p>
</li>
<li><p>一个样本的特征数为：维数（dimensionality），该西瓜的例子维数为3，当维数非常大时，也就是现在说的“维数灾难”。</p>
<p> 计算机程序学习经验数据生成算法模型的过程中，每一条记录称为一个“训练样本”，同时在训练好模型后，我们希望使用新的样本来测试模型的效果，则每一个新的样本称为一个“测试样本”。定义：    </p>
</li>
<li><p>所有训练样本的集合为：训练集（trainning set），[特殊]。</p>
</li>
<li><p>所有测试样本的集合为：测试集（test set），[一般]。  </p>
</li>
<li><p>机器学习出来的模型适用于新样本的能力为：泛化能力（generalization），即从特殊到一般。</p>
<p> 西瓜的例子中，我们是想计算机通过学习西瓜的特征数据，训练出一个决策模型，来判断一个新的西瓜是否是好瓜。可以得知我们预测的是：西瓜是好是坏，即好瓜与差瓜两种，是离散值。同样地，也有通过历年的人口数据，来预测未来的人口数量，人口数量则是连续值。定义：    </p>
</li>
<li><p>预测值为离散值的问题为：分类（classification）。</p>
</li>
<li><p>预测值为连续值的问题为：回归（regression）。</p>
<p> 我们预测西瓜是否是好瓜的过程中，很明显对于训练集中的西瓜，我们事先已经知道了该瓜是否是好瓜，学习器通过学习这些好瓜或差瓜的特征，从而总结出规律，即训练集中的西瓜我们都做了标记，称为标记信息。但也有没有标记信息的情形，例如：我们想将一堆西瓜根据特征分成两个小堆，使得某一堆的西瓜尽可能相似，即都是好瓜或差瓜，对于这种问题，我们事先并不知道西瓜的好坏，样本没有标记信息。定义：    </p>
</li>
<li><p>训练数据有标记信息的学习任务为：监督学习（supervised learning），容易知道上面所描述的分类和回归都是监督学习的范畴。</p>
</li>
<li><p>训练数据没有标记信息的学习任务为：无监督学习（unsupervised learning），常见的有聚类和关联规则。</p>
</li>
</ul>
<p><strong>2  模型的评估与选择</strong></p>
<p><strong>2.1 误差与过拟合</strong></p>
<p>我们将学习器对样本的实际预测结果与样本的真实值之间的差异成为：误差（error）。定义：    </p>
<ul>
<li>在训练集上的误差称为训练误差（training error）或经验误差（empirical error）。</li>
<li>在测试集上的误差称为测试误差（test error）。</li>
<li>学习器在所有新样本上的误差称为泛化误差（generalization error）。</li>
</ul>
<p>显然，我们希望得到的是在新样本上表现得很好的学习器，即泛化误差小的学习器。因此，我们应该让学习器尽可能地从训练集中学出普适性的“一般特征”，这样在遇到新样本时才能做出正确的判别。然而，当学习器把训练集学得“太好”的时候，即把一些训练样本的自身特点当做了普遍特征；同时也有学习能力不足的情况，即训练集的基本特征都没有学习出来。我们定义：</p>
<ul>
<li>学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了，称为：过拟合（overfitting）。</li>
<li>学习能太差，训练样本的一般性质尚未学好，称为：欠拟合（underfitting）。</li>
</ul>
<p>可以得知：在过拟合问题中，训练误差十分小，但测试误差教大；在欠拟合问题中，训练误差和测试误差都比较大。目前，欠拟合问题比较容易克服，例如增加迭代次数等，但过拟合问题还没有十分好的解决方案，过拟合是机器学习面临的关键障碍。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7181172996.png" alt></p>
<p><strong>2.2 评估方法</strong></p>
<p>在现实任务中，我们往往有多种算法可供选择，那么我们应该选择哪一个算法才是最适合的呢？如上所述，我们希望得到的是泛化误差小的学习器，理想的解决方案是对模型的泛化误差进行评估，然后选择泛化误差最小的那个学习器。但是，泛化误差指的是模型在所有新样本上的适用能力，我们无法直接获得泛化误差。</p>
<p>因此，通常我们采用一个“测试集”来测试学习器对新样本的判别能力，然后以“测试集”上的“测试误差”作为“泛化误差”的近似。显然：我们选取的测试集应尽可能与训练集互斥，下面用一个小故事来解释why：</p>
<p>假设老师出了10 道习题供同学们练习，考试时老师又用同样的这10道题作为试题，可能有的童鞋只会做这10 道题却能得高分，很明显：这个考试成绩并不能有效地反映出真实水平。回到我们的问题上来，我们希望得到泛化性能好的模型，好比希望同学们课程学得好并获得了对所学知识”举一反三”的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试。显然，若测试样本被用作训练了，则得到的将是过于”乐观”的估计结果。</p>
<p><strong>2.3 训练集与测试集的划分方法</strong></p>
<p>如上所述：我们希望用一个“测试集”的“测试误差”来作为“泛化误差”的近似，因此我们需要对初始数据集进行有效划分，划分出互斥的“训练集”和“测试集”。下面介绍几种常用的划分方法：</p>
<p><strong>2.3.1 留出法</strong></p>
<p>将数据集D划分为两个互斥的集合，一个作为训练集S，一个作为测试集T，满足D=S∪T且S∩T=∅，常见的划分为：大约2/3-4/5的样本用作训练，剩下的用作测试。需要注意的是：训练/测试集的划分要尽可能保持数据分布的一致性，以避免由于分布的差异引入额外的偏差，常见的做法是采取分层抽样。同时，由于划分的随机性，单次的留出法结果往往不够稳定，一般要采用若干次随机划分，重复实验取平均值的做法。</p>
<p><strong>2.3.2 交叉验证法</strong></p>
<p>将数据集D划分为k个大小相同的互斥子集，满足D=D1∪D2∪…∪Dk，Di∩Dj=∅（i≠j），同样地尽可能保持数据分布的一致性，即采用分层抽样的方法获得这些子集。交叉验证法的思想是：每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就有K种训练集/测试集划分的情况，从而可进行k次训练和测试，最终返回k次测试结果的均值。交叉验证法也称“k折交叉验证”，k最常用的取值是10，下图给出了10折交叉验证的示意图。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc718115d224.png" alt></p>
<p>与留出法类似，将数据集D划分为K个子集的过程具有随机性，因此K折交叉验证通常也要重复p次，称为p次k折交叉验证，常见的是10次10折交叉验证，即进行了100次训练/测试。特殊地当划分的k个子集的每个子集中只有一个样本时，称为“留一法”，显然，留一法的评估结果比较准确，但对计算机的消耗也是巨大的。</p>
<p><strong>2.3.3 自助法</strong></p>
<p>我们希望评估的是用整个D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。“自助法”正是解决了这样的问题。</p>
<p>自助法的基本思想是：给定包含m个样本的数据集D，每次随机从D 中挑选一个样本，将其拷贝放入D’，然后再将该样本放回初始数据集D 中，使得该样本在下次采样时仍有可能被采到。重复执行m 次，就可以得到了包含m个样本的数据集D’。可以得知在m次采样中，样本始终不被采到的概率取极限为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71811246dd.png" alt></p>
<p>这样，通过自助采样，初始样本集D中大约有36.8%的样本没有出现在D’中，于是可以将D’作为训练集，D-D’作为测试集。自助法在数据集较小，难以有效划分训练集/测试集时很有用，但由于自助法产生的数据集（随机抽样）改变了初始数据集的分布，因此引入了估计偏差。在初始数据集足够时，留出法和交叉验证法更加常用。</p>
<p><strong>2.4 调参</strong></p>
<p>大多数学习算法都有些参数(parameter) 需要设定，参数配置不同，学得模型的性能往往有显著差别，这就是通常所说的”参数调节”或简称”调参” (parameter tuning)。</p>
<p>学习算法的很多参数是在实数范围内取值，因此，对每种参数取值都训练出模型来是不可行的。常用的做法是：对每个参数选定一个范围和步长λ，这样使得学习的过程变得可行。例如：假定算法有3 个参数，每个参数仅考虑5 个候选值，这样对每一组训练/测试集就有5<em>5</em>5= 125 个模型需考察，由此可见：拿下一个参数（即经验值）对于算法人员来说是有多么的happy。</p>
<p>最后需要注意的是：当选定好模型和调参完成后，我们需要使用初始的数据集D重新训练模型，即让最初划分出来用于评估的测试集也被模型学习，增强模型的学习效果。用上面考试的例子来比喻：就像高中时大家每次考试完，要将考卷的题目消化掉（大多数题目都还是之前没有见过的吧？），这样即使考差了也能开心的玩耍了~。</p>
<p><strong>3  机器学习评估与度量指标</strong></p>
<p>这里的内容主要包括：性能度量、比较检验和偏差与方差。在上一个notebook中，我们解决了评估学习器泛化性能的方法，即用测试集的“测试误差”作为“泛化误差”的近似，当我们划分好训练/测试集后，那如何计算“测试误差”呢？这就是性能度量，例如：均方差，错误率等，即“测试误差”的一个评价标准。有了评估方法和性能度量，就可以计算出学习器的“测试误差”，但由于“测试误差”受到很多因素的影响，例如：算法随机性或测试集本身的选择，那如何对两个或多个学习器的性能度量结果做比较呢？这就是比较检验。最后偏差与方差是解释学习器泛化性能的一种重要工具。</p>
<p><strong>3.1 性能度量</strong></p>
<p>性能度量（performance measure）是衡量模型泛化能力的评价标准，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。本节除2.5.1外，其它主要介绍分类模型的性能度量。</p>
<p><strong>3.1.1 最常见的性能度量</strong></p>
<p>在回归任务中，即预测连续值的问题，最常用的性能度量是“均方误差”（mean squared error）,很多的经典算法都是采用了MSE作为评价函数，想必大家都十分熟悉。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf76276.png" alt="1.png"></p>
<p>在分类任务中，即预测离散值的问题，最常用的是错误率和精度，错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例，易知：错误率+精度=1。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf4c704.png" alt="2.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf6fb84.png" alt="3.png"></p>
<p><strong>3.1.2 查准率/查全率/F1</strong></p>
<p>错误率和精度虽然常用，但不能满足所有的需求，例如：在推荐系统中，我们只关心推送给用户的内容用户是否感兴趣（即查准率），或者说所有用户感兴趣的内容我们推送出来了多少（即查全率）。因此，使用查准/查全率更适合描述这类问题。对于二分类问题，分类结果混淆矩阵与查准/查全率定义如下：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf885a4.png" alt="4.png"></p>
<p>初次接触时，FN与FP很难正确的理解，按照惯性思维容易把FN理解成：False-&gt;Negtive，即将错的预测为错的，这样FN和TN就反了，后来找到一张图，描述得很详细，为方便理解，把这张图也贴在了下边：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf871a6.png" alt="5.png"></p>
<p>正如天下没有免费的午餐，查准率和查全率是一对矛盾的度量。例如我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容，这样就漏掉了一些用户感兴趣的内容，查全率就低了；如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上，宁可错杀一千，不可放过一个，这样查准率就很低了。</p>
<p>“P-R曲线”正是描述查准/查全率变化的曲线，P-R曲线定义如下：根据学习器的预测结果（一般为一个实值或概率）对测试样本进行排序，将最可能是“正例”的样本排在前面，最不可能是“正例”的排在后面，按此顺序逐个把样本作为“正例”进行预测，每次计算出当前的P值和R值，如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71dafc4411.png" alt="6.png"></p>
<p>P-R曲线如何评估呢？若一个学习器A的P-R曲线被另一个学习器B的P-R曲线完全包住，则称：B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的，所以衍生出了“平衡点”（Break-Event Point，简称BEP），即当P=R时的取值，平衡点的取值越高，性能更优。</p>
<p>P和R指标有时会出现矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的加权调和平均，即：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf40ff6.png" alt="7.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf75407.png" alt="8.png"></p>
<p>特别地，当β=1时，也就是常见的F1度量，是P和R的调和平均，当F1较高时，模型的性能越好。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf20885.png" alt="9.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf4b90a.png" alt="10.png"></p>
<p>有时候我们会有多个二分类混淆矩阵，例如：多次训练或者在多个数据集上训练，那么估算全局性能的方法有两种，分为宏观和微观。简单理解，宏观就是先算出每个混淆矩阵的P值和R值，然后取得平均P值macro-P和平均R值macro-R，在算出Fβ或F1，而微观则是计算出混淆矩阵的平均TP、FP、TN、FN，接着进行计算P、R，进而求出Fβ或F1。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed70230e.png" alt="11.png"></p>
<p><strong>3.1.3 ROC与AUC</strong></p>
<p>如上所述：学习器对测试样本的评估结果一般为一个实值或概率，设定一个阈值，大于阈值为正例，小于阈值为负例，因此这个实值的好坏直接决定了学习器的泛化性能，若将这些实值排序，则排序的好坏决定了学习器的性能高低。ROC曲线正是从这个角度出发来研究学习器的泛化性能，ROC曲线与P-R曲线十分类似，都是按照排序的顺序逐一按照正例预测，不同的是ROC曲线以“真正例率”（True Positive Rate，简称TPR）为横轴，纵轴为“假正例率”（False Positive Rate，简称FPR），ROC偏重研究基于测试样本评估值的排序好坏。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed6bee91.png" alt="12.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed75cefe.png" alt="13.png"></p>
<p>简单分析图像，可以得知：当FN=0时，TN也必须0，反之也成立，我们可以画一个队列，试着使用不同的截断点（即阈值）去分割队列，来分析曲线的形状，（0,0）表示将所有的样本预测为负例，（1,1）则表示将所有的样本预测为正例，（0,1）表示正例全部出现在负例之前的理想情况，（1,0）则表示负例全部出现在正例之前的最差情况。限于篇幅，这里不再论述。</p>
<p>现实中的任务通常都是有限个测试样本，因此只能绘制出近似ROC曲线。绘制方法：首先根据测试样本的评估值对测试样本排序，接着按照以下规则进行绘制。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed740a24.png" alt="14.png"></p>
<p>同样地，进行模型的性能比较时，若一个学习器A的ROC曲线被另一个学习器B的ROC曲线完全包住，则称B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。ROC曲线下的面积定义为AUC（Area Uder ROC Curve），不同于P-R的是，这里的AUC是可估算的，即AOC曲线下每一个小矩形的面积之和。易知：AUC越大，证明排序的质量越好，AUC为1时，证明所有正例排在了负例的前面，AUC为0时，所有的负例排在了正例的前面。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed6e2c57.png" alt="15.png"></p>
<p><strong>3.1.4 代价敏感错误率与代价曲线</strong></p>
<p>上面的方法中，将学习器的犯错同等对待，但在现实生活中，将正例预测成假例与将假例预测成正例的代价常常是不一样的，例如：将无疾病–&gt;有疾病只是增多了检查，但有疾病–&gt;无疾病却是增加了生命危险。以二分类为例，由此引入了“代价矩阵”（cost matrix）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed6ed582.png" alt="16.png"></p>
<p>在非均等错误代价下，我们希望的是最小化“总体代价”，这样“代价敏感”的错误率（2.5.1节介绍）为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed70bebe.png" alt="17.png"></p>
<p>同样对于ROC曲线，在非均等错误代价下，演变成了“代价曲线”，代价曲线横轴是取值在[0,1]之间的正例概率代价，式中p表示正例的概率，纵轴是取值为[0,1]的归一化代价。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed6e952e.png" alt="18.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed6eee7b.png" alt="19.png"></p>
<p>代价曲线的绘制很简单：设ROC曲线上一点的坐标为(TPR，FPR) ，则可相应计算出FNR，然后在代价平面上绘制一条从(0，FPR) 到(1，FNR) 的线段，线段下的面积即表示了该条件下的期望总体代价；如此将ROC 曲线土的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价，如图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed716e0d.png" alt="20.png"></p>
<p><strong>4  机器学习指标ROC与AUC</strong></p>
<p>AUC是一种模型分类指标，且仅仅是二分类模型的评价指标。AUC是Area Under Curve的简称，那么Curve就是ROC（Receiver Operating Characteristic），翻译为”接受者操作特性曲线”。</p>
<h3 id="ROC"><a href="#ROC" class="headerlink" title="ROC"></a>ROC</h3><p>曲线由两个变量TPR和FPR组成，这个组合以FPR对TPR，即是以代价(costs)对收益(benefits)。</p>
<ul>
<li><p>x轴为假阳性率（FPR）：在所有的负样本中，分类器预测错误的比例</p>
<p>$$FPR = \frac {FP}{FP+TN}$$</p>
</li>
<li><p>y轴为真阳性率（TPR）：在所有的正样本中，分类器预测正确的比例（等于Recall）</p>
<p>$$TPR = \frac {TP}{TP+FN}$$</p>
</li>
</ul>
<p>为了更好地理解ROC曲线，我们使用具体的实例来说明：</p>
<p>如在医学诊断中,判断有病的样本。那么尽量把有病的揪出来是主要任务，也就是第一个指标TPR，要越高越好。而把没病的样本误诊为有病的，也就是第二个指标FPR，要越低越好。</p>
<p>不难发现,这两个指标之间是相互制约的。如果某个医生对于有病的症状比较敏感，稍微的小症状都判断为有病,那么他的第一个指标应该会很高，但是第二个指标也就相应地变高。最极端的情况下,他把所有的样本都看做有病,那么第一个指标达到1,第二个指标也为1。</p>
<p>我们以FPR为横轴,TPR为纵轴,得到如下ROC空间。</p>
<img src="/blog_picture/1.7.png" width="60%">

<p>我们可以看出，左上角的点(TPR=1，FPR=0)，为完美分类，也就是这个医生医术高明，诊断全对。点A(TPR&gt;FPR),医生A的判断大体是正确的。中线上的点B(TPR=FPR),也就是医生B全都是蒙的，蒙对一半，蒙错一半；下半平面的点C(TPR&lt;FPR)，这个医生说你有病，那么你很可能没有病，医生C的话我们要反着听，为真庸医。上图中一个阈值，得到一个点。现在我们需要一个独立于阈值的评价指标来衡量这个医生的医术如何，也就是遍历所有的阈值,得到ROC曲线。</p>
<p>假设如下就是某个医生的诊断统计图，直线代表阈值。通过改变不同的阈值$1.0 \rightarrow 0$，从而绘制出ROC曲线。下图为未得病人群（蓝色）和得病人群（红色）的模型输出概率分布图（横坐标表示模型输出概率，纵坐标表示概率对应的人群的数量）。阈值为1时，不管你什么症状，医生均未诊断出疾病（预测值都为N），此时FPR=TPR=0，位于左下。阈值为0时，不管你什么症状，医生都诊断结果都是得病（预测值都为P），此时FPR=TPR=1，位于右上。</p>
<img src="/blog_picture/1.8.png" width="50%">


<p>曲线距离左上角越近,证明分类器效果越好。</p>
<img src="/blog_picture/1.9.png" width="60%">

<p>如上，是三条ROC曲线，在0.23处取一条直线。那么，在同样的低FPR=0.23的情况下，红色分类器得到更高的PTR。也就表明，ROC越往左上，分类器效果越好。我们用一个标量值AUC来量化它。</p>
<h3 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h3><p><strong>AUC定义：</strong></p>
<p>AUC值为ROC曲线所覆盖的区域面积，显然，AUC越大，分类器分类效果越好。</p>
<p>AUC = 1，是完美分类器。绝大多数预测的场合，不存在完美分类器。</p>
<p>0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。</p>
<p>AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。</p>
<p>AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。</p>
<p>注：对于AUC小于0.5的模型，我们可以考虑取反（模型预测为positive，那我们就取negtive），这样就可以保证模型的性能不可能比随机猜测差。</p>
<p>以下为ROC曲线和AUC值的实例：</p>
<img src="/blog_picture/1.12.png" width="70%">

<p><strong>AUC的物理意义</strong></p>
<p>AUC的物理意义正样本的预测结果大于负样本的预测结果的概率。所以AUC反应的是分类器对样本的排序能力。  </p>
<p>另外值得注意的是，AUC对样本类别是否均衡并不敏感，这也是不均衡样本通常用AUC评价分类器性能的一个原因。</p>
<p>下面从一个小例子解释AUC的含义：小明一家四口，小明5岁，姐姐10岁，爸爸35岁，妈妈33岁建立一个逻辑回归分类器，来预测小明家人为成年人概率，假设分类器已经对小明的家人做过预测，得到每个人为成人的概率。</p>
<ol>
<li>AUC更多的是关注对计算概率的排序，关注的是概率值的相对大小，与阈值和概率值的绝对大小没有关系</li>
</ol>
<p>例子中并不关注小明是不是成人，而关注的是，预测为成人的概率的排序。</p>
<p><strong>问题⑪：</strong>以下为三种模型的输出结果，求三种模型的AUC。</p>
<table>
<thead>
<tr>
<th></th>
<th>小明</th>
<th>姐姐</th>
<th>妈妈</th>
<th>爸爸</th>
</tr>
</thead>
<tbody><tr>
<td>a</td>
<td>0.12</td>
<td>0.35</td>
<td>0.76</td>
<td>0.85</td>
</tr>
<tr>
<td>b</td>
<td>0.12</td>
<td>0.35</td>
<td>0.44</td>
<td>0.49</td>
</tr>
<tr>
<td>c</td>
<td>0.52</td>
<td>0.65</td>
<td>0.76</td>
<td>0.85</td>
</tr>
</tbody></table>
<p>AUC只与概率的相对大小（概率排序）有关，和绝对大小没关系。由于三个模型概率排序的前两位都是未成年人（小明，姐姐），后两位都是成年人（妈妈，爸爸），因此三个模型的AUC都等于。</p>
<ol>
<li><p>AUC只关注正负样本之间的排序，并不关心正样本内部，或者负样本内部的排序。这也体现了AUC的本质：任意个正样本的概率都大于负样本的概率的能力  </p>
<p>例子中AUC只需要保证（小明和姐姐）（爸爸和妈妈），小明和姐姐在前2个排序，爸爸和妈妈在后2个排序，而不会考虑小明和姐姐谁在前，或者爸爸和妈妈谁在前。</p>
<p><strong>问题⑫：</strong>以下已经对分类器输出概率从小到大进行了排列，哪些情况的AUC等于1， 情况的AUC为0（其中背景色表示True value，红色表示成年人，蓝色表示未成年人）。</p>
<img src="/blog_picture/1.10.png" width="70%">

<p>D 模型, E模型和F模型的AUC值为1，C模型的AUC值为0（爸妈为成年人的概率小于小明和姐姐，显然这个模型预测反了）。</p>
</li>
</ol>
<p><strong>AUC的计算：</strong></p>
<ul>
<li><p>法1：AUC为ROC曲线下的面积，那我们直接计算面积可得。面积为一个个小的梯形面积（曲线）之和。计算的精度与阈值的精度有关。</p>
</li>
<li><p>法2：根据AUC的物理意义，我们计算正样本预测结果大于负样本预测结果的概率。取n1*n0(n1为正样本数，n0为负样本数)个二元组，比较score（预测结果），最后得到AUC。时间复杂度为O(N*M)。</p>
</li>
<li><p>法3：我们首先把所有样本按照score排序，依次用rank表示他们，如最大score的样本，rank=n (n=n0+n1，其中n0为负样本个数，n1为正样本个数)，其次为n-1。那么对于正样本中rank最大的样本，rank_max，有n1-1个其他正样本比他score小,那么就有(rank_max-1)-(n1-1)个负样本比他score小。其次为(rank_second-1)-(n1-2)。最后我们得到正样本大于负样本的概率为</p>
<p><img src="/blog_picture/auc.jpg" alt="avatar"></p>
</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AUC/">AUC</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ROC/">ROC</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/3/">&laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/5/">Next &raquo;</a>
    </nav>
  
</div>
      <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2019-2020 MingmingYe
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>

    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>