<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no">
<meta name="author" content="MingmingYe">


    
    


<meta property="og:type" content="website">
<meta property="og:title" content="Stay hungry, Stay foolish.">
<meta property="og:url" content="http://mmyblog.cn/page/3/index.html">
<meta property="og:site_name" content="Stay hungry, Stay foolish.">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Stay hungry, Stay foolish.">

<link rel="apple-touch-icon" href="/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="Stay hungry, Stay foolish." type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">


    <style> .article { opacity: 0;} </style>


<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>Stay hungry, Stay foolish.</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: true
    }
</script>


    <script>
        yiliaConfig.jquery_ui = [true, "//cdn.bootcss.com/jqueryui/1.10.4/jquery-ui.min.js", "//cdn.bootcss.com/jqueryui/1.10.4/css/jquery-ui.min.css"];
    </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






</head></html>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/deep.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">MingmingYe</a></h1>
        </hgroup>

        
        <p class="header-subtitle">当你的才华撑不起你的野心时，只有静下心来好好学习！纵使命运注定是个打酱油的，也要打一瓶与别人不一样的酱油！</p>
        

        
            <form id="search-form">
            <input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false" />
            <i class="fa fa-times" onclick="resetSearch()"></i>
            </form>
            <div id="local-search-result"></div>
            <p class='no-result'>No results found <i class='fa fa-spinner fa-pulse'></i></p>
        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:878759487@qq.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AUC/">AUC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention/">Attention</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/">BERT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BLUE/">BLUE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Beam-search/">Beam search</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CRF/">CRF</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ConvNet/">ConvNet</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ELMo/">ELMo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GPT/">GPT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GRU/">GRU</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gradient-Clipping/">Gradient Clipping</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LR/">LR</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LSTM/">LSTM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linear/">Linear</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Parsing/">Parsing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PyTorch/">PyTorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/QA/">QA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN-LSTM/">RNN/LSTM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ROC/">ROC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recursive-Neural-Networks/">Recursive Neural Networks</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQuAD-BiDAF/">SQuAD-BiDAF</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Seq2Seq/">Seq2Seq</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TorchText/">TorchText</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/">Transformer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer-XL/">Transformer-XL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/XLNet/">XLNet</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/boosting/">boosting</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cbow/">cbow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hierarchical-softmax/">hierarchical softmax</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/inference/">inference</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jiaba/">jiaba</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jieba/">jieba</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mumpy/">mumpy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/negative-sampling/">negative sampling</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pandas/">pandas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pyspark/">pyspark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/seaborn/">seaborn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/skip-gram/">skip-gram</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/softmax/">softmax</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/word-embedding/">word-embedding</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/wxBot/">wxBot</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/中文分词/">中文分词</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/优化方法/">优化方法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/决策树/">决策树</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/图神经网络/">图神经网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/微积分/">微积分</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/朴素贝叶斯/">朴素贝叶斯</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/概率/">概率</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/模型调优/">模型调优</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/正则表达式/">正则表达式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习速查表/">深度学习速查表</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/特征工程/">特征工程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/线性代数/">线性代数</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/统计/">统计</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/聊天机器人/">聊天机器人</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/聚类/">聚类</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/语言模型/">语言模型</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/贝叶斯分类器/">贝叶斯分类器</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/降维/">降维</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/集成学习/">集成学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/面试/">面试</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://mmyblog.cn/">mmy</a>
                    
                    </div>
                </section>
                

                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">MingmingYe</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/deep.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">MingmingYe</a></h1>
            </hgroup>
            
            <p class="header-subtitle">当你的才华撑不起你的野心时，只有静下心来好好学习！纵使命运注定是个打酱油的，也要打一瓶与别人不一样的酱油！</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:878759487@qq.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap">
  
    <article id="post-sentiment情感分析代码注释" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/03/28/sentiment情感分析代码注释/" class="article-date">
      <time datetime="2020-03-28T11:04:14.000Z" itemprop="datePublished">2020-03-28</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/28/sentiment情感分析代码注释/">sentiment情感分析代码注释</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="情感分析"><a href="#情感分析" class="headerlink" title="情感分析"></a>情感分析</h2><h3 id="第一步：导入豆瓣电影数据集，只有训练集和测试集"><a href="#第一步：导入豆瓣电影数据集，只有训练集和测试集" class="headerlink" title="第一步：导入豆瓣电影数据集，只有训练集和测试集"></a>第一步：导入豆瓣电影数据集，只有训练集和测试集</h3><ul>
<li><p>TorchText中的一个重要概念是<code>Field</code>。<code>Field</code>决定了你的数据会被怎样处理。在我们的情感分类任务中，我们所需要接触到的数据有文本字符串和两种情感，”pos”或者”neg”。</p>
</li>
<li><p><code>Field</code>的参数制定了数据会被怎样处理。</p>
</li>
<li><p>我们使用<code>TEXT</code> field来定义如何处理电影评论，使用<code>LABEL</code> field来处理两个情感类别。</p>
</li>
<li><p>我们的<code>TEXT</code> field带有<code>tokenize=&#39;spacy&#39;</code>，这表示我们会用<a href="https://spacy.io/" target="_blank" rel="noopener">spaCy</a> tokenizer来tokenize英文句子。如果我们不特别声明<code>tokenize</code>这个参数，那么默认的分词方法是使用空格。</p>
</li>
<li><p>安装spaCy</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install -U spacy</span><br><span class="line">python -m spacy download en</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>LABEL</code>由<code>LabelField</code>定义。这是一种特别的用来处理label的<code>Field</code>。我们后面会解释dtype。</p>
</li>
<li><p>更多关于<code>Fields</code>，参见<a href="https://github.com/pytorch/text/blob/master/torchtext/data/field.py" target="_blank" rel="noopener">https://github.com/pytorch/text/blob/master/torchtext/data/field.py</a></p>
</li>
<li><p>和之前一样，我们会设定random seeds使实验可以复现。</p>
</li>
<li><p>TorchText支持很多常见的自然语言处理数据集。</p>
</li>
<li><p>下面的代码会自动下载IMDb数据集，然后分成train/test两个<code>torchtext.datasets</code>类别。数据被前面的<code>Fields</code>处理。IMDb数据集一共有50000电影评论，每个评论都被标注为正面的或负面的。</p>
</li>
</ul>
<p><strong>先了解下Spacy库：<a href="https://juejin.im/post/5971a4b9f265da6c42353332?utm_source=gold_browser_extension%5D" target="_blank" rel="noopener">spaCy介绍和使用教程</a></strong><br><strong>再了解下torchtext库：<a href="https://blog.csdn.net/u012436149/article/details/79310176" target="_blank" rel="noopener">torchtext介绍和使用教程</a>：这个新手必看，不看下面代码听不懂</strong></p>
<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!ls</span><br></pre></td></tr></table></figure>

<p>In [4]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1234</span></span><br><span class="line"></span><br><span class="line">torch.manual_seed(SEED) <span class="comment">#为CPU设置随机种子</span></span><br><span class="line">torch.cuda.manual_seed(SEED)<span class="comment">#为GPU设置随机种子</span></span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span>  <span class="comment">#在程序刚开始加这条语句可以提升一点训练速度，没什么额外开销。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#首先，我们要创建两个Field 对象：这两个对象包含了我们打算如何预处理文本数据的信息。</span></span><br><span class="line">TEXT = data.Field(tokenize=<span class="string">'spacy'</span>)</span><br><span class="line"><span class="comment">#torchtext.data.Field : 用来定义字段的处理方法（文本字段，标签字段）</span></span><br><span class="line"><span class="comment"># spaCy:英语分词器,类似于NLTK库，如果没有传递tokenize参数，则默认只是在空格上拆分字符串。</span></span><br><span class="line">LABEL = data.LabelField(dtype=torch.float)</span><br><span class="line"><span class="comment">#LabelField是Field类的一个特殊子集，专门用于处理标签。</span></span><br></pre></td></tr></table></figure>

<p>In [2]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> datasets</span><br><span class="line">train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)</span><br><span class="line"><span class="comment"># 加载豆瓣电影评论数据集</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">downloading aclImdb_v1.tar.gz</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aclImdb_v1.tar.gz: <span class="number">100</span>%|██████████| <span class="number">84.1</span>M/<span class="number">84.1</span>M [<span class="number">00</span>:<span class="number">03</span>&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">22.8</span>MB/s]</span><br></pre></td></tr></table></figure>

<p>In [3]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(vars(train_data.examples[<span class="number">0</span>])) <span class="comment">#可以查看数据集长啥样子</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&apos;text&apos;: [&apos;This&apos;, &apos;movie&apos;, &apos;is&apos;, &apos;visually&apos;, &apos;stunning&apos;, &apos;.&apos;, &apos;Who&apos;, &apos;cares&apos;, &apos;if&apos;, &apos;she&apos;, &apos;can&apos;, &apos;act&apos;, &apos;or&apos;, &apos;not&apos;, &apos;.&apos;, &apos;Each&apos;, &apos;scene&apos;, &apos;is&apos;, &apos;a&apos;, &apos;work&apos;, &apos;of&apos;, &apos;art&apos;, &apos;composed&apos;, &apos;and&apos;, &apos;captured&apos;, &apos;by&apos;, &apos;John&apos;, &apos;Derek&apos;, &apos;.&apos;, &apos;The&apos;, &apos;locations&apos;, &apos;,&apos;, &apos;set&apos;, &apos;designs&apos;, &apos;,&apos;, &apos;and&apos;, &apos;costumes&apos;, &apos;function&apos;, &apos;perfectly&apos;, &apos;to&apos;, &apos;convey&apos;, &apos;what&apos;, &apos;is&apos;, &apos;found&apos;, &apos;in&apos;, &apos;a&apos;, &apos;love&apos;, &apos;story&apos;, &apos;comprised&apos;, &apos;of&apos;, &apos;beauty&apos;, &apos;,&apos;, &apos;youth&apos;, &apos;and&apos;, &apos;wealth&apos;, &apos;.&apos;, &apos;In&apos;, &apos;some&apos;, &apos;ways&apos;, &apos;I&apos;, &apos;would&apos;, &apos;like&apos;, &apos;to&apos;, &apos;see&apos;, &apos;this&apos;, &apos;movie&apos;, &apos;as&apos;, &apos;a&apos;, &apos;tribute&apos;, &apos;to&apos;, &apos;John&apos;, &apos;and&apos;, &apos;Bo&apos;, &apos;Derek&apos;, &quot;&apos;s&quot;, &apos;story&apos;, &apos;.&apos;, &apos;And&apos;, &apos;...&apos;, &apos;this&apos;, &apos;commentary&apos;, &apos;would&apos;, &apos;not&apos;, &apos;be&apos;, &apos;complete&apos;, &apos;without&apos;, &apos;mentioning&apos;, &apos;Anthony&apos;, &apos;Quinn&apos;, &quot;&apos;s&quot;, &apos;role&apos;, &apos;as&apos;, &apos;father&apos;, &apos;,&apos;, &apos;mentor&apos;, &apos;,&apos;, &apos;lover&apos;, &apos;,&apos;, &apos;and&apos;, &apos;his&apos;, &apos;portrayal&apos;, &apos;of&apos;, &apos;a&apos;, &apos;man&apos;, &apos;,&apos;, &apos;of&apos;, &apos;men&apos;, &apos;,&apos;, &apos;lost&apos;, &apos;to&apos;, &apos;a&apos;, &apos;bygone&apos;, &apos;era&apos;, &apos;when&apos;, &apos;men&apos;, &apos;were&apos;, &apos;men&apos;, &apos;.&apos;, &apos;There&apos;, &apos;are&apos;, &apos;some&apos;, &apos;of&apos;, &apos;us&apos;, &apos;who&apos;, &apos;find&apos;, &apos;value&apos;, &apos;in&apos;, &apos;strength&apos;, &apos;and&apos;, &apos;direction&apos;, &apos;wrapped&apos;, &apos;in&apos;, &apos;a&apos;, &apos;confidence&apos;, &apos;that&apos;, &apos;contributes&apos;, &apos;to&apos;, &apos;a&apos;, &apos;sense&apos;, &apos;of&apos;, &apos;confidence&apos;, &apos;,&apos;, &apos;containment&apos;, &apos;,&apos;, &apos;and&apos;, &apos;security&apos;, &apos;.&apos;, &apos;Yes&apos;, &apos;,&apos;, &apos;they&apos;, &apos;do&apos;, &apos;not&apos;, &apos;make&apos;, &apos;men&apos;, &apos;like&apos;, &apos;that&apos;, &apos;anymore&apos;, &apos;!&apos;, &apos;But&apos;, &apos;,&apos;, &apos;then&apos;, &apos;how&apos;, &apos;often&apos;, &apos;do&apos;, &apos;you&apos;, &apos;find&apos;, &apos;women&apos;, &apos;who&apos;, &apos;are&apos;, &apos;made&apos;, &apos;like&apos;, &apos;Bo&apos;, &apos;Derek&apos;, &apos;.&apos;], &apos;label&apos;: &apos;pos&apos;&#125;</span><br></pre></td></tr></table></figure>

<h2 id="第二步：训练集划分为训练集和验证集"><a href="#第二步：训练集划分为训练集和验证集" class="headerlink" title="第二步：训练集划分为训练集和验证集"></a>第二步：训练集划分为训练集和验证集</h2><ul>
<li>由于我们现在只有train/test这两个分类，所以我们需要创建一个新的validation set。我们可以使用<code>.split()</code>创建新的分类。</li>
<li>默认的数据分割是 70、30，如果我们声明<code>split_ratio</code>，可以改变split之间的比例，<code>split_ratio=0.8</code>表示80%的数据是训练集，20%是验证集。</li>
<li>我们还声明<code>random_state</code>这个参数，确保我们每次分割的数据集都是一样的。</li>
</ul>
<p>In [4]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">train_data, valid_data = train_data.split(random_state=random.seed(SEED)) <span class="comment">#默认split_ratio=0.7</span></span><br></pre></td></tr></table></figure>

<p>In [5]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f'Number of training examples: <span class="subst">&#123;len(train_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Number of validation examples: <span class="subst">&#123;len(valid_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Number of testing examples: <span class="subst">&#123;len(test_data)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Number of training examples: <span class="number">17500</span></span><br><span class="line">Number of validation examples: <span class="number">7500</span></span><br><span class="line">Number of testing examples: <span class="number">25000</span></span><br></pre></td></tr></table></figure>

<h2 id="第三步：用训练集建立vocabulary，就是把每个单词一一映射到一个数字。"><a href="#第三步：用训练集建立vocabulary，就是把每个单词一一映射到一个数字。" class="headerlink" title="第三步：用训练集建立vocabulary，就是把每个单词一一映射到一个数字。"></a>第三步：用训练集建立vocabulary，就是把每个单词一一映射到一个数字。</h2><ul>
<li>下一步我们需要创建 <em>vocabulary</em> 。<em>vocabulary</em> 就是把每个单词一一映射到一个数字。<img src="file:///Users/mmy/Downloads/assets/sentiment5.png" alt="img"></li>
<li>我们使用最常见的25k个单词来构建我们的单词表，用<code>max_size</code>这个参数可以做到这一点。</li>
<li>所有其他的单词都用<code>&lt;unk&gt;</code>来表示。</li>
</ul>
<p>In [6]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TEXT.build_vocab(train_data, max_size=25000)</span></span><br><span class="line"><span class="comment"># LABEL.build_vocab(train_data)</span></span><br><span class="line">TEXT.build_vocab(train_data, max_size=<span class="number">25000</span>, vectors=<span class="string">"glove.6B.100d"</span>, unk_init=torch.Tensor.normal_)</span><br><span class="line"><span class="comment">#从预训练的词向量（vectors） 中，将当前(corpus语料库)词汇表的词向量抽取出来，构成当前 corpus 的 Vocab（词汇表）。</span></span><br><span class="line"><span class="comment">#预训练的 vectors 来自glove模型，每个单词有100维。glove模型训练的词向量参数来自很大的语料库，</span></span><br><span class="line"><span class="comment">#而我们的电影评论的语料库小一点，所以词向量需要更新，glove的词向量适合用做初始化参数。</span></span><br><span class="line">LABEL.build_vocab(train_data)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.vector_cache/glove<span class="number">.6</span>B.zip: <span class="number">862</span>MB [<span class="number">00</span>:<span class="number">23</span>, <span class="number">36.0</span>MB/s]                               </span><br><span class="line"><span class="number">100</span>%|█████████▉| <span class="number">399597</span>/<span class="number">400000</span> [<span class="number">00</span>:<span class="number">25</span>&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">16569.01</span>it/s]</span><br></pre></td></tr></table></figure>

<p>In [7]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f"Unique tokens in TEXT vocabulary: <span class="subst">&#123;len(TEXT.vocab)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Unique tokens in LABEL vocabulary: <span class="subst">&#123;len(LABEL.vocab)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Unique tokens in TEXT vocabulary: 25002</span><br><span class="line">Unique tokens in LABEL vocabulary: 2</span><br></pre></td></tr></table></figure>

<p>In [8]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(list(LABEL.vocab.stoi.items())) <span class="comment"># 只有两个类别值</span></span><br><span class="line">print(list(TEXT.vocab.stoi.items())[:<span class="number">20</span>])</span><br><span class="line"><span class="comment">#语料库单词频率越高，索引越靠前。前两个默认为unk和pad。</span></span><br><span class="line">print(<span class="string">"------"</span>*<span class="number">10</span>)</span><br><span class="line">print(TEXT.vocab.freqs.most_common(<span class="number">20</span>))</span><br><span class="line"><span class="comment"># 这里可以看到unk和pad没有计数</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;neg&apos;, 0), (&apos;pos&apos;, 1)]</span><br><span class="line">[(&apos;&lt;unk&gt;&apos;, 0), (&apos;&lt;pad&gt;&apos;, 1), (&apos;the&apos;, 2), (&apos;,&apos;, 3), (&apos;.&apos;, 4), (&apos;a&apos;, 5), (&apos;and&apos;, 6), (&apos;of&apos;, 7), (&apos;to&apos;, 8), (&apos;is&apos;, 9), (&apos;in&apos;, 10), (&apos;I&apos;, 11), (&apos;it&apos;, 12), (&apos;that&apos;, 13), (&apos;&quot;&apos;, 14), (&quot;&apos;s&quot;, 15), (&apos;this&apos;, 16), (&apos;-&apos;, 17), (&apos;/&gt;&lt;br&apos;, 18), (&apos;was&apos;, 19)]</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">[(&apos;the&apos;, 201815), (&apos;,&apos;, 192511), (&apos;.&apos;, 165127), (&apos;a&apos;, 109096), (&apos;and&apos;, 108875), (&apos;of&apos;, 100402), (&apos;to&apos;, 93905), (&apos;is&apos;, 76001), (&apos;in&apos;, 61097), (&apos;I&apos;, 54439), (&apos;it&apos;, 53649), (&apos;that&apos;, 49325), (&apos;&quot;&apos;, 44431), (&quot;&apos;s&quot;, 43359), (&apos;this&apos;, 42423), (&apos;-&apos;, 37142), (&apos;/&gt;&lt;br&apos;, 35613), (&apos;was&apos;, 34947), (&apos;as&apos;, 30412), (&apos;movie&apos;, 29873)]</span><br></pre></td></tr></table></figure>

<p>In [9]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(TEXT.vocab.itos[:10]) #查看TEXT单词表</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&apos;&lt;unk&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;the&apos;, &apos;,&apos;, &apos;.&apos;, &apos;a&apos;, &apos;and&apos;, &apos;of&apos;, &apos;to&apos;, &apos;is&apos;]</span><br></pre></td></tr></table></figure>

<h2 id="第四步：创建iterators，每个itartion都会返回一个batch的样本。"><a href="#第四步：创建iterators，每个itartion都会返回一个batch的样本。" class="headerlink" title="第四步：创建iterators，每个itartion都会返回一个batch的样本。"></a>第四步：创建iterators，每个itartion都会返回一个batch的样本。</h2><ul>
<li>最后一步数据的准备是创建iterators。每个itartion都会返回一个batch的examples。</li>
<li>我们会使用<code>BucketIterator</code>。<code>BucketIterator</code>会把长度差不多的句子放到同一个batch中，确保每个batch中不出现太多的padding。</li>
<li>严格来说，我们这份notebook中的模型代码都有一个问题，也就是我们把<code>&lt;pad&gt;</code>也当做了模型的输入进行训练。更好的做法是在模型中把由<code>&lt;pad&gt;</code>产生的输出给消除掉。在这节课中我们简单处理，直接把<code>&lt;pad&gt;</code>也用作模型输入了。由于<code>&lt;pad&gt;</code>数量不多，模型的效果也不差。</li>
<li>如果我们有GPU，还可以指定每个iteration返回的tensor都在GPU上。</li>
</ul>
<p>In [11]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#相当于把样本划分batch，把相等长度的单词尽可能的划分到一个batch，不够长的就用padding。</span></span><br><span class="line">train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(</span><br><span class="line">    (train_data, valid_data, test_data), </span><br><span class="line">    batch_size=BATCH_SIZE,</span><br><span class="line">    device=device)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Iterator：标准迭代器</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">BucketIerator：相比于标准迭代器，会将类似长度的样本当做一批来处理，</span></span><br><span class="line"><span class="string">因为在文本处理中经常会需要将每一批样本长度补齐为当前批中最长序列的长度，</span></span><br><span class="line"><span class="string">因此当样本长度差别较大时，使用BucketIerator可以带来填充效率的提高。</span></span><br><span class="line"><span class="string">除此之外，我们还可以在Field中通过fix_length参数来对样本进行截断补齐操作。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">BPTTIterator: 基于BPTT(基于时间的反向传播算法)的迭代器，一般用于语言模型中。</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<p>Out[11]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;\nIterator：标准迭代器\n\nBucketIerator：相比于标准迭代器，会将类似长度的样本当做一批来处理，\n因为在文本处理中经常会需要将每一批样本长度补齐为当前批中最长序列的长度，\n因此当样本长度差别较大时，使用BucketIerator可以带来填充效率的提高。\n除此之外，我们还可以在Field中通过fix_length参数来对样本进行截断补齐操作。\n\nBPTTIterator: 基于BPTT(基于时间的反向传播算法)的迭代器，一般用于语言模型中。\n&apos;</span><br></pre></td></tr></table></figure>

<p>In [12]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(next(iter(train_iterator)).label.shape)</span><br><span class="line">print(next(iter(train_iterator)).text.shape)<span class="comment"># </span></span><br><span class="line"><span class="comment"># 多运行一次可以发现一条评论的单词长度会变</span></span><br><span class="line"><span class="comment"># 下面text的维度983*64，983为一条评论的单词长度</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">64</span>])</span><br><span class="line">torch.Size([<span class="number">983</span>, <span class="number">64</span>])</span><br></pre></td></tr></table></figure>

<p>In [13]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取出一句评论</span></span><br><span class="line">batch = next(iter(train_iterator))</span><br><span class="line">print(batch.text.shape) </span><br><span class="line">print([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.text[:,<span class="number">0</span>]])</span><br><span class="line"><span class="comment"># 可以看到这句话的长度是1077，最后面有很多pad</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([1077, 64])</span><br><span class="line">[&apos;It&apos;, &apos;was&apos;, &apos;interesting&apos;, &apos;to&apos;, &apos;see&apos;, &apos;how&apos;, &apos;accurate&apos;, &apos;the&apos;, &apos;writing&apos;, &apos;was&apos;, &apos;on&apos;, &apos;the&apos;, &apos;geek&apos;, &apos;buzz&apos;, &apos;words&apos;, &apos;,&apos;, &apos;yet&apos;, &apos;very&apos;, &apos;naive&apos;, &apos;on&apos;, &apos;the&apos;, &apos;corporate&apos;, &apos;world&apos;, &apos;.&apos;, &apos;The&apos;, &apos;Justice&apos;, &apos;Department&apos;, &apos;would&apos;, &apos;catch&apos;, &apos;more&apos;, &apos;of&apos;, &apos;the&apos;, &apos;big&apos;, &apos;&lt;unk&gt;&apos;, &apos;giants&apos;, &apos;if&apos;, &apos;they&apos;, &apos;did&apos;, &apos;such&apos;, &apos;naive&apos;, &apos;things&apos;, &apos;to&apos;, &apos;win&apos;, &apos;.&apos;, &apos;The&apos;, &apos;real&apos;, &apos;corporate&apos;, &apos;world&apos;, &apos;is&apos;, &apos;much&apos;, &apos;more&apos;, &apos;subtle&apos;, &apos;and&apos;, &apos;interesting&apos;, &apos;,&apos;, &apos;yet&apos;, &apos;every&apos;, &apos;bit&apos;, &apos;as&apos;, &apos;sinister&apos;, &apos;.&apos;, &apos;I&apos;, &apos;seriously&apos;, &apos;doubt&apos;, &apos;ANY&apos;, &apos;&lt;unk&gt;&apos;, &apos;would&apos;, &apos;actually&apos;, &apos;kill&apos;, &apos;someone&apos;, &apos;directly&apos;, &apos;;&apos;, &apos;even&apos;, &apos;the&apos;, &apos;&lt;unk&gt;&apos;, &apos;is&apos;, &apos;more&apos;, &apos;&lt;unk&gt;&apos;, &apos;these&apos;, &apos;days&apos;, &apos;.&apos;, &apos;In&apos;, &apos;the&apos;, &apos;real&apos;, &apos;world&apos;, &apos;,&apos;, &apos;they&apos;, &apos;do&apos;, &apos;kill&apos;, &apos;people&apos;, &apos;with&apos;, &apos;&lt;unk&gt;&apos;, &apos;,&apos;, &apos;pollution&apos;, &apos;,&apos;, &apos;&lt;unk&gt;&apos;, &apos;,&apos;, &apos;&lt;unk&gt;&apos;, &apos;,&apos;, &apos;etc&apos;, &apos;.&apos;, &apos;This&apos;, &apos;movie&apos;, &apos;must&apos;, &apos;have&apos;, &apos;been&apos;, &apos;developed&apos;, &apos;by&apos;, &apos;some&apos;, &apos;garage&apos;, &apos;geeks&apos;, &apos;,&apos;, &apos;I&apos;, &apos;think&apos;, &apos;,&apos;, &apos;and&apos;, &apos;the&apos;, &apos;studios&apos;, &apos;did&apos;, &quot;n&apos;t&quot;, &apos;know&apos;, &apos;the&apos;, &apos;difference&apos;, &apos;.&apos;, &apos;They&apos;, &apos;just&apos;, &apos;wanted&apos;, &apos;something&apos;, &apos;to&apos;, &apos;capitalize&apos;, &apos;on&apos;, &apos;the&apos;, &apos;Microsoft&apos;, &apos;&lt;unk&gt;&apos;, &apos;case&apos;, &apos;in&apos;, &apos;the&apos;, &apos;news&apos;, &apos;.&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;&lt;pad&gt;&apos;]</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="第五步：创建Word-Averaging模型"><a href="#第五步：创建Word-Averaging模型" class="headerlink" title="第五步：创建Word Averaging模型"></a>第五步：创建Word Averaging模型</h2><h3 id="Word-Averaging模型"><a href="#Word-Averaging模型" class="headerlink" title="Word Averaging模型"></a>Word Averaging模型</h3><ul>
<li>我们首先介绍一个简单的Word Averaging模型。这个模型非常简单，我们把每个单词都通过<code>Embedding</code>层投射成word embedding vector，然后把一句话中的所有word vector做个平均，就是整个句子的vector表示了。接下来把这个sentence vector传入一个<code>Linear</code>层，做分类即可。</li>
</ul>
<p><img src="file:///Users/mmy/Downloads/assets/sentiment8.png" alt="img"></p>
<ul>
<li>我们使用<a href="https://pytorch.org/docs/stable/nn.html?highlight=avg_pool2d#torch.nn.functional.avg_pool2d" target="_blank" rel="noopener"><code>avg_pool2d</code></a>来做average pooling。我们的目标是把sentence length那个维度平均成1，然后保留embedding这个维度。</li>
</ul>
<p><img src="file:///Users/mmy/Downloads/assets/sentiment9.png" alt="img"></p>
<ul>
<li><code>avg_pool2d</code>的kernel size是 (<code>embedded.shape[1]</code>, 1)，所以句子长度的那个维度会被压扁。</li>
</ul>
<p><img src="file:///Users/mmy/Downloads/assets/sentiment10.png" alt="img"></p>
<p><img src="file:///Users/mmy/Downloads/assets/sentiment11.png" alt="img"></p>
<p>In [5]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordAVGModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, output_dim, pad_idx)</span>:</span></span><br><span class="line">        <span class="comment">#初始化参数，</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">        <span class="comment">#vocab_size=词汇表长度=25002，embedding_dim=每个单词的维度=100</span></span><br><span class="line">        <span class="comment">#padding_idx：如果提供的话，这里如果遇到padding的单词就用0填充。</span></span><br><span class="line">        </span><br><span class="line">        self.fc = nn.Linear(embedding_dim, output_dim)</span><br><span class="line">        <span class="comment">#output_dim输出的维度，一个数就可以了，=1</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="comment"># text.shape = (seq_len,batch_size)</span></span><br><span class="line">        <span class="comment"># text下面会指定，为一个batch的数据，seq_len为一条评论的单词长度</span></span><br><span class="line">        embedded = self.embedding(text) </span><br><span class="line">        <span class="comment"># embedded = [seq_len, batch_size, embedding_dim] </span></span><br><span class="line">        embedded = embedded.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>) </span><br><span class="line">        <span class="comment"># [batch_size, seq_len, embedding_dim]更换顺序</span></span><br><span class="line">        </span><br><span class="line">        pooled = F.avg_pool2d(embedded, (embedded.shape[<span class="number">1</span>], <span class="number">1</span>)).squeeze(<span class="number">1</span>) </span><br><span class="line">        <span class="comment"># [batch size, embedding_dim] 把单词长度的维度压扁为1，并降维</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.fc(pooled)  </span><br><span class="line">        <span class="comment">#（batch size, embedding_dim）*（embedding_dim, output_dim）=（batch size,output_dim）</span></span><br></pre></td></tr></table></figure>

<p>In [6]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = len(TEXT.vocab) <span class="comment">#25002</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">OUTPUT_DIM = <span class="number">1</span> <span class="comment"># 大于某个值是正，小于是负</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] </span><br><span class="line"><span class="comment"># TEXT.pad_token = pad</span></span><br><span class="line"><span class="comment"># PAD_IDX = 1 为pad的索引</span></span><br><span class="line"></span><br><span class="line">model = WordAVGModel(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">---------------------------------------------------------------------------</span><br><span class="line">AttributeError                            Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-6</span>-d9889c88c56d&gt; <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">----&gt; 1 INPUT_DIM = len(TEXT.vocab) #25002</span><br><span class="line">      <span class="number">2</span> EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">      <span class="number">3</span> OUTPUT_DIM = <span class="number">1</span> <span class="comment"># 大于某个值是正，小于是负</span></span><br><span class="line">      <span class="number">4</span> PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</span><br><span class="line">      <span class="number">5</span> <span class="comment"># TEXT.pad_token = pad</span></span><br><span class="line"></span><br><span class="line">AttributeError: <span class="string">'Field'</span> object has no attribute <span class="string">'vocab'</span></span><br></pre></td></tr></table></figure>

<p>In [16]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TEXT.pad_token</span><br></pre></td></tr></table></figure>

<p>Out[16]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;&lt;pad&gt;&apos;</span><br></pre></td></tr></table></figure>

<p>In [17]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_parameters</span><span class="params">(model)</span>:</span> <span class="comment">#统计参数，可以不用管</span></span><br><span class="line">    <span class="keyword">return</span> sum(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br><span class="line"><span class="comment"># &#123;&#125;大括号里调用了函数</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The model has 2,500,301 trainable parameters</span><br></pre></td></tr></table></figure>

<h2 id="第六步：初始化参数"><a href="#第六步：初始化参数" class="headerlink" title="第六步：初始化参数"></a>第六步：初始化参数</h2><p>In [18]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把模型参数初始化成glove的向量参数</span></span><br><span class="line">pretrained_embeddings = TEXT.vocab.vectors  <span class="comment"># 取出glove embedding词向量的参数</span></span><br><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings) <span class="comment">#遇到_的语句直接替换，不需要另外赋值=</span></span><br><span class="line"><span class="comment">#把上面vectors="glove.6B.100d"取出的词向量作为初始化参数，数量为25000*100个参数</span></span><br></pre></td></tr></table></figure>

<p>Out[18]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],</span><br><span class="line">        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],</span><br><span class="line">        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],</span><br><span class="line">        ...,</span><br><span class="line">        [-0.1419,  0.0282,  0.2185,  ..., -0.1100, -0.1250,  0.0282],</span><br><span class="line">        [-0.3326, -0.9215,  0.9239,  ...,  0.5057, -1.2898,  0.1782],</span><br><span class="line">        [-0.8304,  0.3732,  0.0726,  ..., -0.0122,  0.2313, -0.2783]])</span><br></pre></td></tr></table></figure>

<p>In [19]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token] <span class="comment"># UNK_IDX=0</span></span><br><span class="line"></span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM) <span class="comment">#</span></span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line"><span class="comment">#词汇表25002个单词，前两个unk和pad也需要初始化成EMBEDDING_DIM维的向量</span></span><br></pre></td></tr></table></figure>

<h2 id="第七步：训练模型"><a href="#第七步：训练模型" class="headerlink" title="第七步：训练模型"></a>第七步：训练模型</h2><p>In [20]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.parameters()) <span class="comment">#定义优化器</span></span><br><span class="line">criterion = nn.BCEWithLogitsLoss()  <span class="comment">#定义损失函数，这个BCEWithLogitsLoss特殊情况，二分类损失函数</span></span><br><span class="line"><span class="comment"># nn.BCEWithLogitsLoss()看这个：https://blog.csdn.net/qq_22210253/article/details/85222093</span></span><br><span class="line">model = model.to(device) <span class="comment">#送到gpu上去</span></span><br><span class="line">criterion = criterion.to(device) <span class="comment">#送到gpu上去</span></span><br></pre></td></tr></table></figure>

<p>计算预测的准确率</p>
<p>In [21]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_accuracy</span><span class="params">(preds, y)</span>:</span> <span class="comment">#计算准确率</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#round predictions to the closest integer</span></span><br><span class="line">    rounded_preds = torch.round(torch.sigmoid(preds))</span><br><span class="line">    <span class="comment">#.round函数：四舍五入</span></span><br><span class="line">    </span><br><span class="line">    correct = (rounded_preds == y).float() <span class="comment">#convert into float for division </span></span><br><span class="line">    acc = correct.sum()/len(correct)</span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure>

<p>In [22]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, iterator, optimizer, criterion)</span>:</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line">    total_len = <span class="number">0</span></span><br><span class="line">    model.train() <span class="comment">#model.train()代表了训练模式</span></span><br><span class="line">    <span class="comment">#这步一定要加，是为了区分model训练和测试的模式的。</span></span><br><span class="line">    <span class="comment">#有时候训练时会用到dropout、归一化等方法，但是测试的时候不能用dropout等方法。</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> iterator: <span class="comment">#iterator为train_iterator</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment">#加这步防止梯度叠加</span></span><br><span class="line">        </span><br><span class="line">        predictions = model(batch.text).squeeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#batch.text 就是上面forward函数的参数text</span></span><br><span class="line">        <span class="comment"># squeeze(1)压缩维度，不然跟batch.label维度对不上</span></span><br><span class="line">        </span><br><span class="line">        loss = criterion(predictions, batch.label)</span><br><span class="line">        acc = binary_accuracy(predictions, batch.label)</span><br><span class="line">        <span class="comment"># 每次迭代都计算一边准确率</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        loss.backward() <span class="comment">#反向传播</span></span><br><span class="line">        optimizer.step() <span class="comment">#梯度下降</span></span><br><span class="line">        </span><br><span class="line">        epoch_loss += loss.item() * len(batch.label)</span><br><span class="line">        <span class="comment">#二分类损失函数loss因为已经平均化了，这里需要乘以len(batch.label)，</span></span><br><span class="line">        <span class="comment">#得到一个batch的损失，累加得到所有样本损失。</span></span><br><span class="line">        </span><br><span class="line">        epoch_acc += acc.item() * len(batch.label)</span><br><span class="line">        <span class="comment">#（acc.item()：一个batch的正确率） *batch数 = 正确数</span></span><br><span class="line">        <span class="comment"># 累加得到所有训练样本正确数。</span></span><br><span class="line">        </span><br><span class="line">        total_len += len(batch.label)</span><br><span class="line">        <span class="comment">#计算train_iterator所有样本的数量，不出意外应该是17500</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> epoch_loss / total_len, epoch_acc / total_len</span><br><span class="line">    <span class="comment">#epoch_loss / total_len ：train_iterator所有batch的平均损失</span></span><br><span class="line">    <span class="comment">#epoch_acc / total_len ：train_iterator所有batch的平均正确率</span></span><br></pre></td></tr></table></figure>

<p>In [23]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, iterator, criterion)</span>:</span></span><br><span class="line">     </span><br><span class="line">    </span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line">    total_len = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    model.eval()</span><br><span class="line">    <span class="comment">#转换成测试模式，冻结dropout层或其他层。</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> iterator: </span><br><span class="line">            <span class="comment">#iterator为valid_iterator</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">#没有反向传播和梯度下降</span></span><br><span class="line">            predictions = model(batch.text).squeeze(<span class="number">1</span>)</span><br><span class="line">            loss = criterion(predictions, batch.label)</span><br><span class="line">            acc = binary_accuracy(predictions, batch.label)</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            epoch_loss += loss.item() * len(batch.label)</span><br><span class="line">            epoch_acc += acc.item() * len(batch.label)</span><br><span class="line">            total_len += len(batch.label)</span><br><span class="line">    model.train() <span class="comment">#调回训练模式   </span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> epoch_loss / total_len, epoch_acc / total_len</span><br></pre></td></tr></table></figure>

<p>In [24]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">epoch_time</span><span class="params">(start_time, end_time)</span>:</span>  <span class="comment">#查看每个epoch的时间</span></span><br><span class="line">    elapsed_time = end_time - start_time</span><br><span class="line">    elapsed_mins = int(elapsed_time / <span class="number">60</span>)</span><br><span class="line">    elapsed_secs = int(elapsed_time - (elapsed_mins * <span class="number">60</span>))</span><br><span class="line">    <span class="keyword">return</span> elapsed_mins, elapsed_secs</span><br></pre></td></tr></table></figure>

<h2 id="第八步：查看模型运行结果"><a href="#第八步：查看模型运行结果" class="headerlink" title="第八步：查看模型运行结果"></a>第八步：查看模型运行结果</h2><p>In [25]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同上，这里用的kaggleGPU跑的，花了2分钟。</span></span><br><span class="line">N_EPOCHS = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>) <span class="comment">#无穷大</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    </span><br><span class="line">    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">    <span class="comment"># 得到训练集每个epoch的平均损失和准确率</span></span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line">    <span class="comment"># 得到验证集每个epoch的平均损失和准确率，这个model里传入的参数是训练完的参数</span></span><br><span class="line">    </span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss: <span class="comment">#只要模型效果变好，就存模型</span></span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'wordavg-model.pt'</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 01 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.684 | Train Acc: 58.78%</span><br><span class="line">	 Val. Loss: 0.617 |  Val. Acc: 72.51%</span><br><span class="line">Epoch: 02 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.642 | Train Acc: 72.62%</span><br><span class="line">	 Val. Loss: 0.504 |  Val. Acc: 76.65%</span><br><span class="line">Epoch: 03 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.569 | Train Acc: 78.81%</span><br><span class="line">	 Val. Loss: 0.439 |  Val. Acc: 81.07%</span><br><span class="line">Epoch: 04 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.497 | Train Acc: 82.97%</span><br><span class="line">	 Val. Loss: 0.404 |  Val. Acc: 84.03%</span><br><span class="line">Epoch: 05 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.435 | Train Acc: 85.95%</span><br><span class="line">	 Val. Loss: 0.400 |  Val. Acc: 85.69%</span><br><span class="line">Epoch: 06 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.388 | Train Acc: 87.73%</span><br><span class="line">	 Val. Loss: 0.412 |  Val. Acc: 86.80%</span><br><span class="line">Epoch: 07 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.349 | Train Acc: 88.83%</span><br><span class="line">	 Val. Loss: 0.425 |  Val. Acc: 87.64%</span><br><span class="line">Epoch: 08 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.319 | Train Acc: 89.84%</span><br><span class="line">	 Val. Loss: 0.446 |  Val. Acc: 87.83%</span><br><span class="line">Epoch: 09 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.293 | Train Acc: 90.54%</span><br><span class="line">	 Val. Loss: 0.464 |  Val. Acc: 88.25%</span><br><span class="line">Epoch: 10 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.272 | Train Acc: 91.19%</span><br><span class="line">	 Val. Loss: 0.480 |  Val. Acc: 88.68%</span><br><span class="line">Epoch: 11 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.254 | Train Acc: 91.82%</span><br><span class="line">	 Val. Loss: 0.498 |  Val. Acc: 88.87%</span><br><span class="line">Epoch: 12 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.238 | Train Acc: 92.53%</span><br><span class="line">	 Val. Loss: 0.517 |  Val. Acc: 89.01%</span><br><span class="line">Epoch: 13 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.222 | Train Acc: 93.03%</span><br><span class="line">	 Val. Loss: 0.532 |  Val. Acc: 89.25%</span><br><span class="line">Epoch: 14 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.210 | Train Acc: 93.47%</span><br><span class="line">	 Val. Loss: 0.547 |  Val. Acc: 89.44%</span><br><span class="line">Epoch: 15 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.198 | Train Acc: 93.95%</span><br><span class="line">	 Val. Loss: 0.564 |  Val. Acc: 89.49%</span><br><span class="line">Epoch: 16 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.186 | Train Acc: 94.31%</span><br><span class="line">	 Val. Loss: 0.582 |  Val. Acc: 89.68%</span><br><span class="line">Epoch: 17 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.175 | Train Acc: 94.74%</span><br><span class="line">	 Val. Loss: 0.596 |  Val. Acc: 89.69%</span><br><span class="line">Epoch: 18 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.166 | Train Acc: 95.09%</span><br><span class="line">	 Val. Loss: 0.615 |  Val. Acc: 89.95%</span><br><span class="line">Epoch: 19 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.156 | Train Acc: 95.36%</span><br><span class="line">	 Val. Loss: 0.631 |  Val. Acc: 89.91%</span><br><span class="line">Epoch: 20 | Epoch Time: 0m 5s</span><br><span class="line">	Train Loss: 0.147 | Train Acc: 95.75%</span><br><span class="line">	 Val. Loss: 0.647 |  Val. Acc: 90.07%</span><br></pre></td></tr></table></figure>

<h2 id="第九步：预测结果"><a href="#第九步：预测结果" class="headerlink" title="第九步：预测结果"></a>第九步：预测结果</h2><p>In [26]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!ls</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__notebook_source__.ipynb  wordavg-model.pt</span><br></pre></td></tr></table></figure>

<p>In [55]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kaggle上下载模型文件到本地，运行下面代码，点击输出的链接就行</span></span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> HTML</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_download_link</span><span class="params">(title = <span class="string">"Download model file"</span>, filename = <span class="string">"CNN-model.pt"</span>)</span>:</span>  </span><br><span class="line">    html = <span class="string">'&lt;a href=&#123;filename&#125;&gt;&#123;title&#125;&lt;/a&gt;'</span></span><br><span class="line">    html = html.format(title=title,filename=filename)</span><br><span class="line">    <span class="keyword">return</span> HTML(html)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a link to download the dataframe which was saved with .to_csv method</span></span><br><span class="line">create_download_link(filename=<span class="string">'wordavg-model.pt'</span>)</span><br></pre></td></tr></table></figure>

<p>Out[55]:</p>
<p><a href="file:///Users/mmy/Downloads/wordavg-model.pt" target="_blank" rel="noopener">Download model file</a></p>
<p>In [1]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(&quot;wordavg-model.pt&quot;))</span><br><span class="line">#用保存的模型参数预测数据</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">---------------------------------------------------------------------------</span><br><span class="line">NameError                                 Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input-1-f795a3e78d6a&gt; in &lt;module&gt;</span><br><span class="line">----&gt; 1 model.load_state_dict(torch.load(&quot;wordavg-model.pt&quot;))</span><br><span class="line">      2 #用保存的模型参数预测数据</span><br><span class="line"></span><br><span class="line">NameError: name &apos;model&apos; is not defined</span><br></pre></td></tr></table></figure>

<p>In [28]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy  <span class="comment">#分词工具，跟NLTK类似</span></span><br><span class="line">nlp = spacy.load(<span class="string">'en'</span>) </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_sentiment</span><span class="params">(sentence)</span>:</span> <span class="comment"># 传入预测的句子I love This film bad </span></span><br><span class="line">    tokenized = [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> nlp.tokenizer(sentence)] <span class="comment">#分词</span></span><br><span class="line">    <span class="comment"># print(tokenized) = ['I', 'love', 'This', 'film', 'bad']</span></span><br><span class="line">    indexed = [TEXT.vocab.stoi[t] <span class="keyword">for</span> t <span class="keyword">in</span> tokenized] </span><br><span class="line">    <span class="comment">#sentence的在25002中的索引</span></span><br><span class="line">    </span><br><span class="line">    tensor = torch.LongTensor(indexed).to(device) <span class="comment">#seq_len</span></span><br><span class="line">    <span class="comment"># 所有词向量都应该变成LongTensor</span></span><br><span class="line">    </span><br><span class="line">    tensor = tensor.unsqueeze(<span class="number">1</span>) </span><br><span class="line">    <span class="comment">#模型的输入是默认有batch_size的,需要升维，seq_len * batch_size（1）</span></span><br><span class="line">    </span><br><span class="line">    prediction = torch.sigmoid(model(tensor))</span><br><span class="line">    <span class="comment"># 预测准确率，在0，1之间，需要sigmoid下</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> prediction.item()</span><br></pre></td></tr></table></figure>

<p>In [29]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(&quot;I love This film bad&quot;)</span><br></pre></td></tr></table></figure>

<p>Out[29]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9373546242713928</span><br></pre></td></tr></table></figure>

<p>In [30]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(&quot;This film is great&quot;)</span><br></pre></td></tr></table></figure>

<p>Out[30]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.0</span><br></pre></td></tr></table></figure>

<h2 id="RNN模型"><a href="#RNN模型" class="headerlink" title="RNN模型"></a>RNN模型</h2><ul>
<li><p>下面我们尝试把模型换成一个</p>
<p>recurrent neural network</p>
</li>
</ul>
<p>  (RNN)。RNN经常会被用来encode一个sequence</p>
<p>  ℎ𝑡=RNN(𝑥𝑡,ℎ𝑡−1)ht=RNN(xt,ht−1)</p>
<ul>
<li><p>我们使用最后一个hidden state ℎ𝑇hT来表示整个句子。</p>
</li>
<li><p>然后我们把ℎ𝑇hT通过一个线性变换𝑓f，然后用来预测句子的情感。</p>
</li>
</ul>
<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>In [32]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, hidden_dim, output_dim, </span></span></span><br><span class="line"><span class="function"><span class="params">                 n_layers, bidirectional, dropout, pad_idx)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, </span><br><span class="line">                           bidirectional=bidirectional, dropout=dropout)</span><br><span class="line">        <span class="comment">#embedding_dim：每个单词维度</span></span><br><span class="line">        <span class="comment">#hidden_dim：隐藏层维度</span></span><br><span class="line">        <span class="comment">#num_layers：神经网络深度，纵向深度</span></span><br><span class="line">        <span class="comment">#bidirectional：是否双向循环RNN</span></span><br><span class="line">        <span class="comment">#这个自己先得理解LSTM各个维度，不然容易晕，双向RNN网络图示看上面，可以借鉴下</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        self.fc = nn.Linear(hidden_dim*<span class="number">2</span>, output_dim)</span><br><span class="line">        <span class="comment"># 这里hidden_dim乘以2是因为是双向，需要拼接两个方向，跟n_layers的层数无关。</span></span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="comment"># text.shape=[seq_len, batch_size]</span></span><br><span class="line">        embedded = self.dropout(self.embedding(text)) <span class="comment">#[seq_len, batch_size, emb_dim]</span></span><br><span class="line">        output, (hidden, cell) = self.rnn(embedded)</span><br><span class="line">        <span class="comment"># output = [seq_len, batch size, hid_dim * num directions]</span></span><br><span class="line">        <span class="comment"># hidden = [num layers * num directions, batch_size, hid_dim]</span></span><br><span class="line">        <span class="comment"># cell = [num layers * num directions, batch_size, hid_dim]</span></span><br><span class="line">        <span class="comment"># 这里的num layers * num directions可以看上面图，上面图除掉输入输出层只有两层双向网络。</span></span><br><span class="line">        <span class="comment"># num layers = 2表示需要纵向上在加两层双向，总共有4层神经元。</span></span><br><span class="line">        <span class="comment"># 对于LSTM模型的任意一个时间序列t，h层的输出维度</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment">#concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers</span></span><br><span class="line">        <span class="comment">#and apply dropout</span></span><br><span class="line">        hidden = self.dropout(torch.cat((hidden[<span class="number">-2</span>,:,:], hidden[<span class="number">-1</span>,:,:]), dim=<span class="number">1</span>)) </span><br><span class="line">        <span class="comment"># hidden = [batch size, hid dim * num directions]，</span></span><br><span class="line">        <span class="comment"># 看下上面图示，最后前向和后向输出的隐藏层会concat到输出层，4层神经元最后两层作为最终的输出。</span></span><br><span class="line">        <span class="comment"># 这里因为我们只需要得到最后一个时间序列的输出，所以最终输出的hidden跟seq_len无关。</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.fc(hidden.squeeze(<span class="number">0</span>)) <span class="comment"># 在接一个全连接层，最终输出[batch size, output_dim]</span></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>In [36]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = len(TEXT.vocab)</span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">HIDDEN_DIM = <span class="number">256</span></span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line">N_LAYERS = <span class="number">2</span></span><br><span class="line">BIDIRECTIONAL = <span class="literal">True</span></span><br><span class="line">DROPOUT = <span class="number">0.5</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</span><br><span class="line"></span><br><span class="line">model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, </span><br><span class="line">            N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)</span><br><span class="line">model</span><br></pre></td></tr></table></figure>

<p>Out[36]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">RNN(</span><br><span class="line">  (embedding): Embedding(<span class="number">25002</span>, <span class="number">100</span>, padding_idx=<span class="number">1</span>)</span><br><span class="line">  (rnn): LSTM(<span class="number">100</span>, <span class="number">256</span>, num_layers=<span class="number">2</span>, dropout=<span class="number">0.5</span>, bidirectional=<span class="literal">True</span>)</span><br><span class="line">  (fc): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (dropout): Dropout(p=<span class="number">0.5</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>In [34]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br><span class="line"><span class="comment"># 比averge model模型多了一倍的参数</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The model has 4,810,857 trainable parameters</span><br></pre></td></tr></table></figure>

<p>In [37]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同上初始化</span></span><br><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings)</span><br><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]</span><br><span class="line"></span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line"></span><br><span class="line">print(model.embedding.weight.data)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],</span><br><span class="line">        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],</span><br><span class="line">        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],</span><br><span class="line">        ...,</span><br><span class="line">        [-0.1419,  0.0282,  0.2185,  ..., -0.1100, -0.1250,  0.0282],</span><br><span class="line">        [-0.3326, -0.9215,  0.9239,  ...,  0.5057, -1.2898,  0.1782],</span><br><span class="line">        [-0.8304,  0.3732,  0.0726,  ..., -0.0122,  0.2313, -0.2783]])</span><br></pre></td></tr></table></figure>

<h2 id="训练RNN模型"><a href="#训练RNN模型" class="headerlink" title="训练RNN模型"></a>训练RNN模型</h2><p>In [38]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure>

<p>In [39]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同上，这里用的kaggleGPU跑的，花了40分钟。</span></span><br><span class="line">N_EPOCHS = <span class="number">20</span></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line">    </span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'lstm-model.pt'</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 01 | Epoch Time: 2m 1s</span><br><span class="line">	Train Loss: 0.667 | Train Acc: 59.09%</span><br><span class="line">	 Val. Loss: 0.633 |  Val. Acc: 64.67%</span><br><span class="line">Epoch: 02 | Epoch Time: 2m 1s</span><br><span class="line">	Train Loss: 0.663 | Train Acc: 60.33%</span><br><span class="line">	 Val. Loss: 0.669 |  Val. Acc: 69.21%</span><br><span class="line">Epoch: 03 | Epoch Time: 2m 2s</span><br><span class="line">	Train Loss: 0.650 | Train Acc: 61.06%</span><br><span class="line">	 Val. Loss: 0.579 |  Val. Acc: 70.55%</span><br><span class="line">Epoch: 04 | Epoch Time: 2m 2s</span><br><span class="line">	Train Loss: 0.493 | Train Acc: 77.43%</span><br><span class="line">	 Val. Loss: 0.382 |  Val. Acc: 83.43%</span><br><span class="line">Epoch: 05 | Epoch Time: 2m 2s</span><br><span class="line">	Train Loss: 0.394 | Train Acc: 83.71%</span><br><span class="line">	 Val. Loss: 0.338 |  Val. Acc: 85.97%</span><br><span class="line">Epoch: 06 | Epoch Time: 2m 3s</span><br><span class="line">	Train Loss: 0.338 | Train Acc: 86.26%</span><br><span class="line">	 Val. Loss: 0.309 |  Val. Acc: 87.21%</span><br><span class="line">Epoch: 07 | Epoch Time: 2m 2s</span><br><span class="line">	Train Loss: 0.292 | Train Acc: 88.37%</span><br><span class="line">	 Val. Loss: 0.295 |  Val. Acc: 88.73%</span><br><span class="line">Epoch: 08 | Epoch Time: 2m 3s</span><br><span class="line">	Train Loss: 0.252 | Train Acc: 90.26%</span><br><span class="line">	 Val. Loss: 0.300 |  Val. Acc: 89.31%</span><br><span class="line">Epoch: 09 | Epoch Time: 2m 2s</span><br><span class="line">	Train Loss: 0.246 | Train Acc: 90.51%</span><br><span class="line">	 Val. Loss: 0.282 |  Val. Acc: 88.76%</span><br><span class="line">Epoch: 10 | Epoch Time: 2m 3s</span><br><span class="line">	Train Loss: 0.205 | Train Acc: 92.37%</span><br><span class="line">	 Val. Loss: 0.295 |  Val. Acc: 88.31%</span><br><span class="line">Epoch: 11 | Epoch Time: 2m 1s</span><br><span class="line">	Train Loss: 0.203 | Train Acc: 92.46%</span><br><span class="line">	 Val. Loss: 0.289 |  Val. Acc: 89.25%</span><br><span class="line">Epoch: 12 | Epoch Time: 2m 3s</span><br><span class="line">	Train Loss: 0.178 | Train Acc: 93.58%</span><br><span class="line">	 Val. Loss: 0.301 |  Val. Acc: 89.41%</span><br><span class="line">Epoch: 13 | Epoch Time: 2m 3s</span><br><span class="line">	Train Loss: 0.158 | Train Acc: 94.43%</span><br><span class="line">	 Val. Loss: 0.301 |  Val. Acc: 89.51%</span><br><span class="line">Epoch: 14 | Epoch Time: 2m 2s</span><br><span class="line">	Train Loss: 0.158 | Train Acc: 94.63%</span><br><span class="line">	 Val. Loss: 0.289 |  Val. Acc: 89.95%</span><br><span class="line">Epoch: 15 | Epoch Time: 2m 2s</span><br><span class="line">	Train Loss: 0.142 | Train Acc: 95.00%</span><br><span class="line">	 Val. Loss: 0.314 |  Val. Acc: 89.59%</span><br><span class="line">Epoch: 16 | Epoch Time: 2m 2s</span><br><span class="line">	Train Loss: 0.123 | Train Acc: 95.62%</span><br><span class="line">	 Val. Loss: 0.329 |  Val. Acc: 89.99%</span><br><span class="line">Epoch: 17 | Epoch Time: 2m 4s</span><br><span class="line">	Train Loss: 0.107 | Train Acc: 96.16%</span><br><span class="line">	 Val. Loss: 0.325 |  Val. Acc: 89.75%</span><br><span class="line">Epoch: 18 | Epoch Time: 2m 4s</span><br><span class="line">	Train Loss: 0.100 | Train Acc: 96.66%</span><br><span class="line">	 Val. Loss: 0.341 |  Val. Acc: 89.49%</span><br><span class="line">Epoch: 19 | Epoch Time: 2m 3s</span><br><span class="line">	Train Loss: 0.096 | Train Acc: 96.63%</span><br><span class="line">	 Val. Loss: 0.340 |  Val. Acc: 89.79%</span><br><span class="line">Epoch: 20 | Epoch Time: 2m 3s</span><br><span class="line">	Train Loss: 0.080 | Train Acc: 97.31%</span><br><span class="line">	 Val. Loss: 0.380 |  Val. Acc: 89.83%</span><br></pre></td></tr></table></figure>

<p>You may have noticed the loss is not really decreasing and the accuracy is poor. This is due to several issues with the model which we’ll improve in the next notebook.</p>
<p>Finally, the metric we actually care about, the test loss and accuracy, which we get from our parameters that gave us the best validation loss.</p>
<p>In [40]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载文件到本地</span></span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> HTML</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_download_link</span><span class="params">(title = <span class="string">"Download model file"</span>, filename = <span class="string">"wordavg-model.pt"</span>)</span>:</span>  </span><br><span class="line">    html = <span class="string">'&lt;a href=&#123;filename&#125;&gt;&#123;title&#125;&lt;/a&gt;'</span></span><br><span class="line">    html = html.format(title=title,filename=filename)</span><br><span class="line">    <span class="keyword">return</span> HTML(html)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a link to download the dataframe which was saved with .to_csv method</span></span><br><span class="line">create_download_link(filename=<span class="string">'lstm-model.pt'</span>)</span><br></pre></td></tr></table></figure>

<p>Out[40]:</p>
<p><a href="file:///Users/mmy/Downloads/lstm-model.pt" target="_blank" rel="noopener">Download model file</a></p>
<p>In [41]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(<span class="string">'lstm-model.pt'</span>))</span><br><span class="line">test_loss, test_acc = evaluate(model, test_iterator, criterion)</span><br><span class="line">print(<span class="string">f'Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test Acc: <span class="subst">&#123;test_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Test Loss: 0.304 | Test Acc: 88.11%</span><br></pre></td></tr></table></figure>

<p>In [44]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(&quot;I feel This film bad&quot;)</span><br></pre></td></tr></table></figure>

<p>Out[44]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.3637591600418091</span><br></pre></td></tr></table></figure>

<p>In [43]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(&quot;This film is great&quot;)</span><br></pre></td></tr></table></figure>

<p>Out[43]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9947803020477295</span><br></pre></td></tr></table></figure>

<h2 id="CNN模型"><a href="#CNN模型" class="headerlink" title="CNN模型"></a>CNN模型</h2><p>In [45]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, n_filters, </span></span></span><br><span class="line"><span class="function"><span class="params">                 filter_sizes, output_dim, dropout, pad_idx)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        </span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">        self.convs = nn.ModuleList([</span><br><span class="line">                                    nn.Conv2d(in_channels = <span class="number">1</span>, out_channels = n_filters, </span><br><span class="line">                                              kernel_size = (fs, embedding_dim)) </span><br><span class="line">                                    <span class="keyword">for</span> fs <span class="keyword">in</span> filter_sizes</span><br><span class="line">                                    ])</span><br><span class="line">        <span class="comment"># in_channels：输入的channel，文字都是1</span></span><br><span class="line">        <span class="comment"># out_channels：输出的channel维度</span></span><br><span class="line">        <span class="comment"># fs：每次滑动窗口计算用到几个单词</span></span><br><span class="line">        <span class="comment"># for fs in filter_sizes打算用好几个卷积模型最后concate起来看效果。</span></span><br><span class="line">        </span><br><span class="line">        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        text = text.permute(<span class="number">1</span>, <span class="number">0</span>) <span class="comment"># [batch size, sent len]</span></span><br><span class="line">        embedded = self.embedding(text) <span class="comment"># [batch size, sent len, emb dim]</span></span><br><span class="line">        embedded = embedded.unsqueeze(<span class="number">1</span>) <span class="comment"># [batch size, 1, sent len, emb dim]</span></span><br><span class="line">        <span class="comment"># 升维是为了和nn.Conv2d的输入维度吻合，把channel列升维。</span></span><br><span class="line">        conved = [F.relu(conv(embedded)).squeeze(<span class="number">3</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs]</span><br><span class="line">        <span class="comment"># conved = [batch size, n_filters, sent len - filter_sizes+1]</span></span><br><span class="line">        <span class="comment"># 有几个filter_sizes就有几个conved</span></span><br><span class="line">    </span><br><span class="line">        </span><br><span class="line">        pooled = [F.max_pool1d(conv, conv.shape[<span class="number">2</span>]).squeeze(<span class="number">2</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> conved]</span><br><span class="line">        <span class="comment"># 把conv的第三个维度最大池化了</span></span><br><span class="line">        <span class="comment">#pooled_n = [batch size, n_filters]</span></span><br><span class="line">        </span><br><span class="line">        cat = self.dropout(torch.cat(pooled, dim=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># cat = [batch size, n_filters * len(filter_sizes)]</span></span><br><span class="line">        <span class="comment"># 把 len(filter_sizes)个卷积模型concate起来传到全连接层。</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> self.fc(cat)</span><br></pre></td></tr></table></figure>

<p>In [47]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同上</span></span><br><span class="line">INPUT_DIM = len(TEXT.vocab)</span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">N_FILTERS = <span class="number">100</span></span><br><span class="line">FILTER_SIZES = [<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line">DROPOUT = <span class="number">0.5</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)</span><br><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings)</span><br><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]</span><br><span class="line"></span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model = model.to(device)</span><br><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br><span class="line"><span class="comment"># 比averge model模型参数差不多</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The model has 2,620,801 trainable parameters</span><br></pre></td></tr></table></figure>

<p>In [48]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同上，需要花8分钟左右</span></span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">criterion = criterion.to(device)</span><br><span class="line"></span><br><span class="line">N_EPOCHS = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    </span><br><span class="line">    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line">    </span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'CNN-model.pt'</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 01 | Epoch Time: 0m 19s</span><br><span class="line">	Train Loss: 0.652 | Train Acc: 61.81%</span><br><span class="line">	 Val. Loss: 0.527 |  Val. Acc: 76.20%</span><br><span class="line">Epoch: 02 | Epoch Time: 0m 19s</span><br><span class="line">	Train Loss: 0.427 | Train Acc: 80.66%</span><br><span class="line">	 Val. Loss: 0.358 |  Val. Acc: 84.36%</span><br><span class="line">Epoch: 03 | Epoch Time: 0m 19s</span><br><span class="line">	Train Loss: 0.304 | Train Acc: 87.14%</span><br><span class="line">	 Val. Loss: 0.318 |  Val. Acc: 86.45%</span><br><span class="line">Epoch: 04 | Epoch Time: 0m 19s</span><br><span class="line">	Train Loss: 0.215 | Train Acc: 91.42%</span><br><span class="line">	 Val. Loss: 0.313 |  Val. Acc: 86.92%</span><br><span class="line">Epoch: 05 | Epoch Time: 0m 19s</span><br><span class="line">	Train Loss: 0.156 | Train Acc: 94.18%</span><br><span class="line">	 Val. Loss: 0.326 |  Val. Acc: 87.01%</span><br><span class="line">Epoch: 06 | Epoch Time: 0m 19s</span><br><span class="line">	Train Loss: 0.105 | Train Acc: 96.33%</span><br><span class="line">	 Val. Loss: 0.344 |  Val. Acc: 87.16%</span><br><span class="line">Epoch: 07 | Epoch Time: 0m 19s</span><br><span class="line">	Train Loss: 0.075 | Train Acc: 97.61%</span><br><span class="line">	 Val. Loss: 0.372 |  Val. Acc: 87.28%</span><br><span class="line">Epoch: 08 | Epoch Time: 0m 19s</span><br><span class="line">	Train Loss: 0.052 | Train Acc: 98.39%</span><br><span class="line">	 Val. Loss: 0.403 |  Val. Acc: 87.21%</span><br><span class="line">Epoch: 09 | Epoch Time: 0m 19s</span><br><span class="line">	Train Loss: 0.041 | Train Acc: 98.64%</span><br><span class="line">	 Val. Loss: 0.433 |  Val. Acc: 87.09%</span><br><span class="line">Epoch: 10 | Epoch Time: 0m 19s</span><br><span class="line">	Train Loss: 0.031 | Train Acc: 99.10%</span><br><span class="line">	 Val. Loss: 0.462 |  Val. Acc: 87.01%</span><br><span class="line">Epoch: 11 | Epoch Time: 0m 19s</span><br><span class="line">	Train Loss: 0.023 | Train Acc: 99.29%</span><br><span class="line">	 Val. Loss: 0.495 |  Val. Acc: 86.93%</span><br><span class="line">Epoch: 12 | Epoch Time: 0m 19s</span><br><span class="line">	Train Loss: 0.021 | Train Acc: 99.34%</span><br><span class="line">	 Val. Loss: 0.530 |  Val. Acc: 86.84%</span><br><span class="line">Epoch: 13 | Epoch Time: 0m 19s</span><br><span class="line">	Train Loss: 0.015 | Train Acc: 99.60%</span><br><span class="line">	 Val. Loss: 0.559 |  Val. Acc: 86.73%</span><br><span class="line">Epoch: 14 | Epoch Time: 0m 19s</span><br><span class="line">	Train Loss: 0.013 | Train Acc: 99.69%</span><br><span class="line">	 Val. Loss: 0.597 |  Val. Acc: 86.48%</span><br><span class="line">Epoch: 15 | Epoch Time: 0m 19s</span><br><span class="line">	Train Loss: 0.012 | Train Acc: 99.70%</span><br><span class="line">	 Val. Loss: 0.608 |  Val. Acc: 86.63%</span><br><span class="line">Epoch: 16 | Epoch Time: 0m 19s</span><br><span class="line">	Train Loss: 0.009 | Train Acc: 99.76%</span><br><span class="line">	 Val. Loss: 0.640 |  Val. Acc: 86.77%</span><br><span class="line">Epoch: 17 | Epoch Time: 0m 19s</span><br><span class="line">	Train Loss: 0.010 | Train Acc: 99.73%</span><br><span class="line">	 Val. Loss: 0.674 |  Val. Acc: 86.51%</span><br><span class="line">Epoch: 18 | Epoch Time: 0m 19s</span><br><span class="line">	Train Loss: 0.012 | Train Acc: 99.63%</span><br><span class="line">	 Val. Loss: 0.704 |  Val. Acc: 86.71%</span><br><span class="line">Epoch: 19 | Epoch Time: 0m 19s</span><br><span class="line">	Train Loss: 0.010 | Train Acc: 99.65%</span><br><span class="line">	 Val. Loss: 0.757 |  Val. Acc: 86.44%</span><br><span class="line">Epoch: 20 | Epoch Time: 0m 20s</span><br><span class="line">	Train Loss: 0.006 | Train Acc: 99.80%</span><br><span class="line">	 Val. Loss: 0.756 |  Val. Acc: 86.55%</span><br></pre></td></tr></table></figure>

<p>In [49]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 发现上面结果过拟合了，同学们可以自行调参</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">'CNN-model.pt'</span>))</span><br><span class="line">test_loss, test_acc = evaluate(model, test_iterator, criterion)</span><br><span class="line">print(<span class="string">f'Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test Acc: <span class="subst">&#123;test_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Test Loss: 0.339 | Test Acc: 85.68%</span><br></pre></td></tr></table></figure>

<p>In [50]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(&quot;I feel This film bad&quot;)</span><br></pre></td></tr></table></figure>

<p>Out[50]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.6535547375679016</span><br></pre></td></tr></table></figure>

<p>In [52]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(&quot;This film is great well&quot;) </span><br><span class="line"># 我后面加了个well，不加会报错，因为我们的FILTER_SIZES = [3,4,5]有设置为5，所以输出的句子长度不能小于5</span><br></pre></td></tr></table></figure>

<p>Out[52]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9950380921363831</span><br></pre></td></tr></table></figure>

<p>In [54]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kaggle上下载模型文件到本地</span></span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> HTML</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_download_link</span><span class="params">(title = <span class="string">"Download model file"</span>, filename = <span class="string">"CNN-model.pt"</span>)</span>:</span>  </span><br><span class="line">    html = <span class="string">'&lt;a href=&#123;filename&#125;&gt;&#123;title&#125;&lt;/a&gt;'</span></span><br><span class="line">    html = html.format(title=title,filename=filename)</span><br><span class="line">    <span class="keyword">return</span> HTML(html)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a link to download the dataframe which was saved with .to_csv method</span></span><br><span class="line">create_download_link(filename=<span class="string">'CNN-model.pt'</span>)</span><br></pre></td></tr></table></figure>

<p>Out[54]:</p>
<p><a href="file:///Users/mmy/Downloads/CNN-model.pt" target="_blank" rel="noopener">Download model file</a></p>
<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/TorchText/">TorchText</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-聊天机器人二" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/03/11/聊天机器人二/" class="article-date">
      <time datetime="2020-03-11T00:47:43.000Z" itemprop="datePublished">2020-03-11</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/11/聊天机器人二/">聊天机器人二</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>比较著名的聊天系统</p>
<p><img src="https://uploader.shimo.im/f/e5gmAxm4zzEpvbgO.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/MEdcyUq5bw0bd0aG.png!thumbnail" alt="img"></p>
<p>聊天机器人的历史</p>
<ul>
<li><p>1950: Turing Test</p>
</li>
<li><p>1966: ELIZA, MIT chatbot</p>
</li>
<li><p>1995: ALICE, pattern recognition chatbot</p>
</li>
<li><p>2011-2012: Siri, Watson, Google Assistant</p>
</li>
<li><p>2015: Amazon Alexa, Microsoft Cortana</p>
</li>
<li><p>2016: Bot 元年： Microsoft Bot Framework, FB Messenger, Google Assistant …</p>
</li>
<li><p>2017: 百度度秘，腾讯云小微，阿里小蜜 …</p>
</li>
</ul>
<p>ASR: Acoustic speech recognition: Speech –&gt; Text</p>
<p>SLU: Spoken Language Understanding</p>
<p><img src="https://uploader.shimo.im/f/5jqPVZomqwIRiaKY.png!thumbnail" alt="img"></p>
<p>聊天机器人按照其功能主要可以分为两类</p>
<ul>
<li><p>任务导向的聊天机器人(Task-Oriented Chatbot)</p>
</li>
<li><p><strong>有具体的聊天任务，例如酒店、机票、饭店预订，电话目录</strong></p>
</li>
<li><p>也可以做一些更复杂的工作，例如假期日程安排，<strong>讨价还价</strong></p>
</li>
<li><p>goal-oriented chatbot</p>
</li>
<li><p>非任务导向的聊天机器人(Non-Task-Oriented Chatbot)</p>
</li>
<li><p>没有具体的聊天目标，主要目的是闲聊，能够和用户有更多的交互</p>
</li>
<li><p>也有上述任务导向与非任务导向的混合聊天机器人，同时具备两种功能</p>
</li>
</ul>
<p>按照聊天机器人的聊天领域 (Domain)</p>
<ul>
<li><p>对于任务导向的机器人，聊天的领域就是任务的领域</p>
</li>
<li><p>关于某个特定任务的数据库，例如机票信息，酒店信息</p>
</li>
<li><p>也可以同时包含多个领域的信息</p>
</li>
<li><p>对于非任务导向的机器人</p>
</li>
</ul>
<p>按照聊天的发起方</p>
<ul>
<li><p>系统主导（System Initiative）的聊天机器人</p>
</li>
<li><p>适用于一些简单的任务：例如很多大公司的自动电话语音系统</p>
</li>
<li><p>用户主导（User Initiative）的聊天机器人</p>
</li>
<li><p>用户主导聊天的主题</p>
</li>
<li><p>系统需要去尽量迎合用户的喜好，陪他们聊天</p>
</li>
<li><p>系统/用户混合主导（Mixed Initiative）</p>
</li>
</ul>
<p>Alexa Prize Challenge 亚马逊举办的聊天机器人大赛</p>
<p><a href="https://developer.amazon.com/alexaprize/challenges/past-challenges/2018/" target="_blank" rel="noopener">https://developer.amazon.com/alexaprize/challenges/past-challenges/2018/</a></p>
<p>DSTC 2 &amp; 3</p>
<p>关于构建聊天机器人的建议</p>
<ul>
<li><p>迅速创建一个baseline，在这个Baseline的基础上去不断提高</p>
</li>
<li><p><strong>多测试自己的聊天机器人</strong></p>
</li>
</ul>
<p><strong>聊天机器人的评估方法</strong></p>
<ul>
<li><p>对于任务导向性聊天机器人，我们可以使用任务是否完成来评估聊天机器人是否成功</p>
</li>
<li><p><strong>Efficiency</strong></p>
</li>
<li><p><strong>Effectiveness</strong></p>
</li>
<li><p>Usability</p>
</li>
<li><p>针对非任务聊天机器人</p>
</li>
<li><p><strong>自动评估</strong></p>
</li>
<li><p><strong>对话进行的长度/轮数</strong></p>
</li>
<li><p>User sentiment analysis</p>
</li>
<li><p>Positive user responses / total user responses</p>
</li>
<li><p><strong>人类评估</strong> </p>
</li>
<li><p>Coherence</p>
</li>
<li><p>Appropriateness</p>
</li>
<li><p>Rating</p>
</li>
<li><p>基于参考答案的评估指标，<strong>BLEU, ROUGE, METEOR</strong> (有可能不太准确)</p>
</li>
</ul>
<p>也有文章指出，这种基于参考答案的评价指标不能够很好地反映聊天机器人的好坏</p>
<ul>
<li><p>How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation</p>
</li>
<li><p><a href="https://www.aclweb.org/anthology/D16-1230" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D16-1230</a></p>
</li>
</ul>
<p>所以很多时候，我们对聊天机器人，尤其是闲聊机器人的评估依然依赖于人类评估。人们也开始尝试一些<strong>深度学习的模型</strong>来预测人类给模型的打分。</p>
<p>构建聊天机器人的三大模块</p>
<p><img src="https://uploader.shimo.im/f/xAwqqhmELsc4JfFY.png!thumbnail" alt="img"></p>
<ul>
<li><p>Coherence</p>
</li>
<li><p>User experience</p>
</li>
<li><p>Engagement Management</p>
</li>
</ul>
<h3 id="自然语言理解（Natural-Language-Understanding）"><a href="#自然语言理解（Natural-Language-Understanding）" class="headerlink" title="自然语言理解（Natural Language Understanding）"></a>自然语言理解（Natural Language Understanding）</h3><p>NLU的主要任务是从文本中提取信息，包括对话<strong>文本的意图</strong>，文本中<strong>关键的信息</strong>，例如<strong>命名实体</strong>，并且将这些信息转成比较<strong>标准化</strong>的表示方式以供后续聊天机器人模块使用。</p>
<p><strong>面临的挑战</strong></p>
<ul>
<li><p><strong>去除口语化的表达</strong></p>
</li>
<li><p><strong>话语重复</strong></p>
</li>
<li><p><strong>讲话中自我修正</strong></p>
</li>
<li><p><strong>口语化的表述和停顿</strong></p>
</li>
</ul>
<h3 id="Frame-based-SLU-Spoken-Language-Understanding"><a href="#Frame-based-SLU-Spoken-Language-Understanding" class="headerlink" title="Frame-based SLU (Spoken Language Understanding)"></a>Frame-based SLU (Spoken Language Understanding)</h3><p>Meaning Representation Language</p>
<p>把自然语言转变成一种固定的结构化数据表达形式</p>
<ul>
<li><p>有固定的语法结构</p>
</li>
<li><p>计算机可执行的语言</p>
</li>
</ul>
<p>Do you have any flights from <strong>Seattle</strong> to <strong>Boston</strong> on <strong>December 24th</strong>? </p>
<p>O O O O O O BDCity O BACity O BDDate IDDate</p>
<p>BiLSTM –&gt; Classification (CRF)</p>
<p><img src="https://uploader.shimo.im/f/m78yw5M7YBkbMIb8.png!thumbnail" alt="img"></p>
<p>Intent classification –&gt; ShowFlight</p>
<p>POS </p>
<p>这种信息抽取的任务往往可以通过<strong>HMM, CRF</strong>等模型实现</p>
<p>other</p>
<p>B-Departure</p>
<p>I-Departure</p>
<p>B-Arrival</p>
<p>I-Arrival</p>
<p>B-Date</p>
<p>I-Date</p>
<p>conll-2003</p>
<h3 id="意图识别-Intent-Classification"><a href="#意图识别-Intent-Classification" class="headerlink" title="意图识别 (Intent Classification)"></a>意图识别 (Intent Classification)</h3><p>Coarse-grained</p>
<ul>
<li><p>把用户的当前讲话的意图归类到我们提前指定的一些类别中去</p>
</li>
<li><p>给定一句话 (utterance X)，给定一系列 intent classes: C_1, …, C_M。预测当前 utterance 属于哪一个intent类别。</p>
</li>
<li><p>同样的intent可能有很多不同的表达方式</p>
</li>
<li><p>我想要预订后天北京到上海的机票</p>
</li>
<li><p>能帮我订一张后天从首都机场到浦东机场的机票吗？</p>
</li>
<li><p>后天从北京到虹桥的机票有吗？</p>
</li>
</ul>
<p><img src="https://uploader.shimo.im/f/ERAAvjjCqQ80MNQ1.png!thumbnail" alt="img"></p>
<h3 id="命名实体识别"><a href="#命名实体识别" class="headerlink" title="命名实体识别"></a>命名实体识别</h3><p>从一段文本中抽取<strong>命名实体</strong></p>
<p>命名实体的类别</p>
<ul>
<li>机构、任务、地点、时间、日期、金钱、百分比</li>
</ul>
<p>主要的方法</p>
<ul>
<li>基于HMM, CRF, RNN (LSTM) 等模型的 token 标注方法</li>
</ul>
<h3 id="关于-semantic-parsing-和-slot-filling"><a href="#关于-semantic-parsing-和-slot-filling" class="headerlink" title="关于 semantic parsing 和 slot filling"></a>关于 semantic parsing 和 slot filling</h3><p>semantic parsing: 非结构化的语言转换成结构化的指令</p>
<p><a href="https://nlpprogress.com/english/semantic_parsing.html" target="_blank" rel="noopener">https://nlpprogress.com/english/semantic_parsing.html</a></p>
<p><img src="https://uploader.shimo.im/f/qbDxmHsMGfcnMNQa.png!thumbnail" alt="img"></p>
<p>Find_Flight(Boston, New York)</p>
<p>semantic parsing</p>
<ul>
<li><p>intent classification </p>
</li>
<li><p>arguments parsing</p>
</li>
</ul>
<p>Open(“baidumap”, city=”Beijing”);</p>
<p>Open(“Google map”, city=”Beijing”);</p>
<p>Close(“Google map”, city=”Beijing”);</p>
<p>Install(“”)</p>
<p>BERT for Joint Intent Classification and Slot Filling</p>
<p><a href="https://arxiv.org/pdf/1902.10909.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1902.10909.pdf</a></p>
<h3 id="Dialogue-State-Tracking"><a href="#Dialogue-State-Tracking" class="headerlink" title="Dialogue State Tracking"></a>Dialogue State Tracking</h3><p>DSTC 比赛和数据集</p>
<p><a href="http://camdial.org/~mh521/dstc/" target="_blank" rel="noopener">http://camdial.org/~mh521/dstc/</a></p>
<h1 id="对话管理"><a href="#对话管理" class="headerlink" title="对话管理"></a><strong>对话管理</strong></h1><p>什么是对话管理？</p>
<ul>
<li><p>一般是聊天机器人的中心模块，控制整个聊天的过程。上面承接NLU，后面连着NLG，负责<strong>储存聊天的信息和状态(slot filling)</strong>，根据已有的信息 (当前聊天记录和外部信息)，决定下一步做什么。</p>
</li>
<li><p>负责与一些外部的<strong>知识库</strong>做交互，例如Knowledge base，各种数据库</p>
</li>
</ul>
<p>Dialogue <strong>Context</strong> Modeling</p>
<ul>
<li><p>聊天的过程往往是高度基于对话记录的，很多时候我们会用代词指代之前的名词或实体</p>
</li>
<li><p>你想聊一些关于科技的还是民生的话题？</p>
</li>
<li><p>聊第二个吧 –&gt; 民生</p>
</li>
<li><p><strong>共指消解</strong> (Coreference Resolution)</p>
</li>
<li><p>有时候我们会<strong>省略</strong>一些显而易见的信息</p>
</li>
<li><p>你打算什么时候吃饭？</p>
</li>
<li><p>晚上7点（<strong>吃饭</strong>）</p>
</li>
<li><p>有时候我们会省略一些信息（当我们和一些聊天机器人聊天的时候）</p>
</li>
<li><p>放点<strong>好听</strong>的音乐（放什么音乐？根据历史记录放？选用户最喜欢的音乐？）</p>
</li>
<li><p><strong>开灯</strong> （开什么灯？床头灯？顶灯？客厅的灯？卧室的灯？）</p>
</li>
</ul>
<p>聊天<strong>context</strong>的来源</p>
<ul>
<li><p>聊天历史，例如<strong>聊天历史</strong>的纯文本内容，聊天记录中提到过的<strong>命名实体</strong>、<strong>主题</strong>等等</p>
</li>
<li><p>任务记录</p>
</li>
<li><p>对于一个任务型聊天机器人来说，聊天机器人往往会保存一些结构化的聊天记录，这些记录有时候被称为 <strong>form</strong>, frame, template, status graph。例如对于一个订机票聊天机器人来说，它需要收集用户的<strong>姓名，身份证号码，出发机场，到达机场，航班时间要求，价格要求</strong>，等等。然后才可以帮助用户做决策。</p>
</li>
<li><p>我们需要知道哪些信息已经被收集到了，哪些信息还没有被收集到。然后根据需要收集的信息确定下一步机器人需要跟用户说什么。</p>
</li>
</ul>
<p>Knowledge Base</p>
<p>根据<strong>聊天任务的不同</strong>，聊天机器人需要不同的Knowledge Base</p>
<ul>
<li><p>对于航班订票机器人来说，需要航班信息的数据库</p>
</li>
<li><p>对于酒店预订机器人来说，需要酒店数据库</p>
</li>
<li><p>对于闲聊机器人来说，各种闲聊中需要用到的信息，新闻、财经、电影娱乐等等</p>
</li>
</ul>
<p>Dialogue Control (Dialogue Act)   </p>
<p>根据Context, 聊天历史，knowledge base –&gt; action 可能是<strong>rule based 决策</strong>，也可能是基于机器学习模型的决策。</p>
<ul>
<li><p>根据当前（从用户和其他数据来源）获取的信息，决定下一步需要采取怎样的行动</p>
</li>
<li><p>可以做的决策有：</p>
</li>
<li><p>从用户处<strong>收集更多的信息</strong></p>
</li>
<li><p>与用户<strong>确认</strong>之前的信息</p>
</li>
<li><p><strong>向用户输出一些信息</strong></p>
</li>
<li><p>一些设计要素</p>
</li>
<li><p>由用户还是系统来主导对话</p>
</li>
<li><p>是否要向用户解释自己的动作</p>
</li>
</ul>
<p>对话的主导方</p>
<ul>
<li><p>用户主导</p>
</li>
<li><p>用户来控制对话的进程</p>
</li>
<li><p>系统不会自由发挥，而是跟随用户的思路</p>
</li>
<li><p>在一些<strong>QA系统和基于搜索的系统</strong>中较为常见</p>
</li>
<li><p><strong>系统主导</strong></p>
</li>
<li><p>系统控制对话的进程</p>
</li>
<li><p>系统决定了用户能说什么，不能说什么</p>
</li>
<li><p>对于系统听不懂的话，系统可以忽略或者告诉用户自己无法解决</p>
</li>
<li><p>在一些简单任务的机器人中很常见</p>
</li>
<li><p>混合主导</p>
</li>
<li><p>以上两种系统的混合版，可以是简单的组合，也可以设计地更复杂</p>
</li>
</ul>
<p>Dialogue Control 的一些方法</p>
<ul>
<li><p><strong>基于Finite state machine</strong> </p>
</li>
<li><p><strong>基于Frame</strong></p>
</li>
<li><p><strong>基于统计学模型(机器学习）</strong></p>
</li>
<li><p>AI planning</p>
</li>
</ul>
<h3 id="基于-Finite-State-的聊天控制"><a href="#基于-Finite-State-的聊天控制" class="headerlink" title="基于 Finite-State 的聊天控制"></a>基于 Finite-State 的聊天控制</h3><p>Finite State Automata 有限状态机</p>
<ul>
<li><p>聊天机器人跟从一个类似 finite state automata 的系统</p>
</li>
<li><p>聊天机器人在每个节点上可以做的事情是确定的</p>
</li>
<li><p>2018年 Alexa Prize 的 Tartan 模型 </p>
</li>
<li><p><a href="https://m.media-amazon.com/images/G/01/mobile-apps/dex/alexa/alexaprize/assets/pdf/2018/Tartan.pdf" target="_blank" rel="noopener">https://m.media-amazon.com/images/G/01/mobile-apps/dex/alexa/alexaprize/assets/pdf/2018/Tartan.pdf</a></p>
</li>
</ul>
<p><img src="https://uploader.shimo.im/f/J3a3dBGf89MpaIdI.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/QThWtsbtshQkEqNS.png!thumbnail" alt="img"></p>
<ul>
<li><p>系统完全控制了整场对话</p>
</li>
<li><p>系统会一直向用户提问</p>
</li>
<li><p>系统会忽略任何它不需要的信息</p>
</li>
</ul>
<p>这种控制系统的好处是</p>
<ul>
<li><p>容易设计和实现，完全使用<strong>if-else语句</strong></p>
</li>
<li><p>功能非常确定，可控制</p>
</li>
<li><p>其中<strong>State的transition可以基于非常复杂的条件和对话状态</strong></p>
</li>
</ul>
<p>坏处是</p>
<ul>
<li><p>没有什么灵活性，真的是一个机器人</p>
</li>
<li><p>只能支持系统主导的对话</p>
</li>
</ul>
<h3 id="Frame-Based-Dialogue-Control"><a href="#Frame-Based-Dialogue-Control" class="headerlink" title="Frame-Based Dialogue Control"></a>Frame-Based Dialogue Control</h3><ul>
<li>预先指定了一张<strong>表格 (Frame)</strong>，聊天机器人的目标就是把这张表格填满</li>
</ul>
<p><img src="https://uploader.shimo.im/f/uQcxBjpZ5LEGAZfQ.png!thumbnail" alt="img"></p>
<p>我们可以预先指定一些问题，用来从用户处得到我们想要的信息</p>
<table>
<thead>
<tr>
<th><strong>Slot</strong></th>
<th><strong>Question</strong></th>
</tr>
</thead>
<tbody><tr>
<td>出发地</td>
<td>你从哪个城市或机场出发？</td>
</tr>
<tr>
<td>目的地</td>
<td>你要去哪个城市？</td>
</tr>
<tr>
<td>起飞日期</td>
<td>你的起飞日期是？</td>
</tr>
<tr>
<td>起飞时间</td>
<td>你想几点钟起飞？</td>
</tr>
<tr>
<td>航空公司</td>
<td>你有偏好的航空公司吗？</td>
</tr>
<tr>
<td>姓名</td>
<td>你的名字叫什么？</td>
</tr>
<tr>
<td>证件号码</td>
<td>你的身份证号码是多少？ –&gt; 护照</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>提问的先后不需要确定。用户也可以同时回答多个问题。</p>
<p>根据当前未知信息的组合，系统可以提出不同的问题</p>
<ul>
<li><p>未知信息（出发地、目的地）：你的旅行路线是？</p>
</li>
<li><p>未知信息（出发地）：你的出发城市是哪里？</p>
</li>
<li><p>未知信息（目的地）：你的目的地是哪里？</p>
</li>
<li><p>出发地+起飞日期：你打算哪天从哪个机场出发？</p>
</li>
</ul>
<p>只要这张mapping的表格足够全面，我们就可以处理各种情况。</p>
<p><strong>Frame-Based的方法比Finite-State要</strong>灵活一些，但本质上还是一个非常固定的方法，在有限的空间里完成一项特定的任务。</p>
<p>这两种方法的主要缺陷在于：</p>
<ul>
<li><strong>需要花很多时间考虑各种情况，人为设计对话路线，有可能会出现一些情况没有被设计到。</strong></li>
</ul>
<p>intent (entities), state –&gt; <strong>action</strong> , reward –&gt; intent (entities), state –&gt; <strong>action</strong> , reward –&gt; intent (entities), state –&gt; ? </p>
<p>optional below</p>
<h3 id="基于统计模型的Dialogue-Control"><a href="#基于统计模型的Dialogue-Control" class="headerlink" title="基于统计模型的Dialogue Control"></a>基于统计模型的Dialogue Control</h3><p>基于统计学和数据的聊天机器人模型</p>
<ul>
<li><p>一套固定的states S 聊天历史</p>
</li>
<li><p>一套固定的actions A 下一句要讲的话</p>
</li>
<li><p>一套系统performance的评价指标 <strong>reward 自己设计</strong></p>
</li>
<li><p>一个<strong>policy</strong> \pi，决定了在一个state下可以采取怎样的action （可能是一个神经网络）根据当前的聊天历史，决定下一句话讲什么</p>
</li>
</ul>
<p>训练方法</p>
<ul>
<li><p>监督学习，需要很多的训练数据</p>
</li>
<li><p><strong>强化学习 (Reinforcement Learning)，需要优化模型的最终回报 (return)</strong></p>
</li>
<li><p><strong>除了上面的信息之外，还要加入一个回报函数</strong></p>
</li>
</ul>
<p>关于如何做<strong>强化学习</strong>？我们这里不再详细展开，感兴趣的同学可以阅读</p>
<ul>
<li><p><a href="https://hao-cheng.github.io/ee596_spr2019/slides/lecture_5-dialog_management.pdf" target="_blank" rel="noopener">https://hao-cheng.github.io/ee596_spr2019/slides/lecture_5-dialog_management.pdf</a></p>
</li>
<li><p><strong>David Silver</strong>的RL课程 <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="noopener">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html</a></p>
</li>
<li><p>Richard Sutton</p>
</li>
<li><p><strong>强化学习知识大讲堂</strong>  <a href="https://zhuanlan.zhihu.com/p/25498081" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25498081</a></p>
</li>
</ul>
<p>这篇来自Jiwei Li的文章引用量很高</p>
<p>Deep Reinforcement Learning for Dialogue Generation</p>
<p><a href="https://aclweb.org/anthology/D16-1127" target="_blank" rel="noopener">https://aclweb.org/anthology/D16-1127</a></p>
<p>在闲聊机器人中的Dialogue Control</p>
<ul>
<li><p>可能涉及到的话题空间较大，聊天的控制比较复杂</p>
</li>
<li><p>没有明确的任务目标，很难定义reward函数，可能唯一的目标就是让聊天时间变长</p>
</li>
</ul>
<p>常见的做法</p>
<ul>
<li><p><strong>把聊天机器人分成几个不同的模块</strong>，每个<strong>模块可以负责一些聊天的子话题</strong>，或者由一些不同的模型实现</p>
</li>
<li><p>有一个master模块负责分配聊天任务给不同的模块</p>
</li>
</ul>
<p>阶梯化的模块</p>
<ul>
<li><p>聊天历史记录模块 (Dialogue State/Context Tracking)</p>
</li>
<li><p><strong>Master Dialogue Manage</strong>r</p>
</li>
<li><p><strong>Miniskill Dialogue Manager</strong></p>
</li>
</ul>
<p>很多Miniskill Dialogue Manager是由finite-state-machine来实现的，可以通过引入一些non-deterministic finite automata来增加聊天的丰富度和多样性</p>
<p><strong>action, slot values (key value pairs)</strong></p>
<p>dialogue history</p>
<h1 id="自然语言生成-Natural-Language-Generation-NLG"><a href="#自然语言生成-Natural-Language-Generation-NLG" class="headerlink" title="自然语言生成 Natural Language Generation (NLG)"></a>自然语言生成 Natural Language Generation (NLG)</h1><h2 id="Template-based"><a href="#Template-based" class="headerlink" title="Template based"></a>Template based</h2><p>display_flight_info </p>
<p>action: ask_departure_city</p>
<ul>
<li><p>你要从哪个机场离开？</p>
</li>
<li><p>你从哪里起飞？</p>
</li>
</ul>
<p>ask_time</p>
<p>使用模板来生成句子</p>
<ul>
<li><p>[DEPARTURE-CITY]:  你打算几点钟离开 [DEPARTURE-CITY]?</p>
</li>
<li><p>[TOPIC]: 不如聊聊 [TOPIC]?</p>
</li>
</ul>
<h2 id="Retrieval-Based"><a href="#Retrieval-Based" class="headerlink" title="Retrieval Based"></a>Retrieval Based</h2><p>Response <strong>R**</strong>e<strong><strong>tr</strong></strong>i<strong>**eval</strong></p>
<ul>
<li><p>根据当前的对话场景/历史决定提取怎样的回复</p>
</li>
<li><p>基于retrieval而不是generation的方法</p>
</li>
<li><p>可以使用机器学习的模型来训练抽取模型</p>
</li>
<li><p>可以根据<strong>similarity</strong> <strong>mat**</strong>c<strong><strong>hi</strong></strong>n<strong>**g</strong>的方法：ELMo, average, cosine similarity. Google universal sentence encoder. 自己训练一个模型？</p>
</li>
<li><p>可以利用一些别的基于搜索的方法</p>
</li>
</ul>
<p><img src="https://uploader.shimo.im/f/2QSr9OIvUKEFJ9B0.png!thumbnail" alt="img"></p>
<p>思考一下你会怎么构建这个模型？</p>
<p>可以用它来制作一个问答机器人，回答常见的问题。</p>
<h2 id="基于深度学习的聊天机器人（偏向实验性质）"><a href="#基于深度学习的聊天机器人（偏向实验性质）" class="headerlink" title="基于深度学习的聊天机器人（偏向实验性质）"></a>基于深度学习的聊天机器人（偏向实验性质）</h2><p>用Seq2Seq模型来做生成</p>
<p><img src="https://uploader.shimo.im/f/WV5HzLQb0DsDD4Cj.png!thumbnail" alt="img"></p>
<ul>
<li><p>多样性很差</p>
</li>
<li><p>很难控制</p>
</li>
</ul>
<p>Hierarchical LSTM, Hierachical BERT? </p>
<p>在聊天机器人中使用生成模型有一个很大的问题，就是你无法完全掌控生成句子的各种属性，我们无法知道模型会生成什么样的句子。这也导致了基于神经网络的模型，例如Seq2Seq，在有任务的聊天机器人中并没有得到非常多的使用，而是更多地出现在一些娱乐性的项目之中。例如如果我们想要训练一只“<strong>小黄鸡</strong>”，那么你可以大胆地使用Seq2Seq等神经网络模型。可是如果你想要</p>
<p>Jiwei Li的一系列基于深度学习的Dialogue Generation</p>
<p>Jiwei 在 斯坦福的slides <a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">h</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">t</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">tps://w</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">e</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">b.</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">sta</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">nford.edu/class/cs224s/lec</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">t</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">ur</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">e</a><a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec12.pdf" target="_blank" rel="noopener">s/224s.17.lec12.pdf</a></p>
<h3 id="Deep-Reinforcement-Learning-for-Dialogue-Generation"><a href="#Deep-Reinforcement-Learning-for-Dialogue-Generation" class="headerlink" title="Deep Reinforcement Learning for Dialogue Generation"></a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">Deep Reinfo</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">r</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">ce</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">me</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">nt</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener"> </a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">Le</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">a</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">r</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">ning </a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">for D</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">ia</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">logu</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">e </a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">Ge</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">ne</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">r</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">at</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">i</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">o</a><a href="https://arxiv.org/pdf/1606.01541.pdf" target="_blank" rel="noopener">n</a></h3><p>使用深度增强学习来训练聊天机器人。</p>
<p>增强学习 Framework</p>
<p>action: 下一句要生成的话</p>
<p>state: 当前的聊天历史，在本文中使用最近的两句对话</p>
<p>policy: 一个基于LSTM的 Seq2Seq 模型</p>
<p>reward: 对当前生成对话的评价指标，本文中采用了三项指标。</p>
<ul>
<li><p>Ease of answering: 这句对话是不是很容易回答，不要“把天聊死”。</p>
</li>
<li><p>Information Flow: 当前生成的对话应该和之前的聊天记录有所变化。</p>
</li>
<li><p>Semantic Coherence：上下文是否连贯。</p>
</li>
</ul>
<h3 id="Adversarial-Learning-for-Neural-Dialogue-Generation"><a href="#Adversarial-Learning-for-Neural-Dialogue-Generation" class="headerlink" title="Adversarial Learning for Neural Dialogue Generation"></a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">Adve</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">r</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">s</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">a</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">rial L</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">ea</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">rning for N</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">e</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">ural</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener"> Dialogue </a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">Ge</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">n</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">era</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">t</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">i</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">o</a><a href="https://arxiv.org/pdf/1701.06547.pdf" target="_blank" rel="noopener">n</a></h3><p>把GAN的想法用于聊天机器人的评估中。与上文相同，生成器是一个聊天机器人，可以生成对话。判别器是一个打分系统，可以对聊天机器人生成的对话进行打分。然后使用增强学习(REINFORCE, Policy Gradient算法)来训练生成器。</p>
<p>关于如何实现policy gradient </p>
<p><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">http</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">s</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">://</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">di</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">scuss.py</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">t</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">orch.org/t/whats-th</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">e-</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">righ</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">t</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">-w</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">a</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">y-of-implemen</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">t</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">ing-policy-gradi</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">e</a><a href="https://discuss.pytorch.org/t/whats-the-right-way-of-implementing-policy-gradient/4003/2" target="_blank" rel="noopener">nt/4003/2</a></p>
<p>参考该项目 <a href="https://github.com/suragnair/seqGAN" target="_blank" rel="noopener">https://github.com/suragnair/seqGAN</a></p>
<h1 id="Alexa-Prize-Challenge"><a href="#Alexa-Prize-Challenge" class="headerlink" title="Alexa Prize Challenge"></a>Alexa Prize Challenge</h1><p>2018年冠军 <strong>Gunrock: Building A Human-Like Social Bot By Leveraging Large Scale Real User Data</strong></p>
<p><a href="https://m.media-amazon.com/images/G/01/mobile-apps/dex/alexa/alexaprize/assets/pdf/2018/Gunrock.pdf" target="_blank" rel="noopener">https://m.media-amazon.com/images/G/01/mobile-apps/dex/alexa/alexaprize/assets/pdf/2018/Gunrock.pdf</a></p>
<p><img src="https://uploader.shimo.im/f/APzsIXJhTrsQRYOw.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/JZM2qSNuXLorcVr3.png!thumbnail" alt="img"></p>
<p>2017年冠军 Sounding Board – University of Washington’s Alexa Prize Submission</p>
<p><a href="https://m.media-amazon.com/images/G/01/mobile-apps/dex/alexa/alexaprize/assets/pdf/2017/Soundingboard.pdf" target="_blank" rel="noopener">https://m.media-amazon.com/images/G/01/mobile-apps/dex/alexa/alexaprize/assets/pdf/2017/Soundingboard.pdf</a></p>
<p><img src="https://uploader.shimo.im/f/xAwqqhmELsc4JfFY.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/E7m8GJsDFTcXJ9xY.png!thumbnail" alt="img"></p>
<p>参考课程资料</p>
<ul>
<li><p><a href="https://hao-cheng.github.io/ee596_spr2019/" target="_blank" rel="noopener">https://hao-cheng.github.io/ee596_spr2019/</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/30533380" target="_blank" rel="noopener">Neural Response Generation – 关于回复生成工作的一些总结</a></p>
</li>
<li><p>斯坦福的课程资料 <a href="https://web.stanford.edu/class/cs224s/lectures/224s.17.lec10.pdf" target="_blank" rel="noopener">https://web.stanford.edu/class/cs224s/lectures/224s.17.lec10.pdf</a></p>
</li>
<li><p>EMNLP 2018 tutorial <a href="http://www.ruiyan.me/pubs/tutorial-emnlp18.pdf" target="_blank" rel="noopener">http://www.ruiyan.me/pubs/tutorial-emnlp18.pdf</a></p>
</li>
</ul>
<p>中文聊天机器人的资料</p>
<ul>
<li><p><a href="https://blog.csdn.net/hfutdog/article/details/78155467" target="_blank" rel="noopener">https://blog.csdn.net/hfutdog/article/details/78155467</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/23356655" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/23356655</a></p>
</li>
</ul>
<p>一些公司</p>
<ul>
<li><p>Gowild <a href="https://www.gowild.cn/home/ours/index.html" target="_blank" rel="noopener">https://www.gowild.cn/home/ours/index.html</a></p>
</li>
<li><p>Huggingface <a href="https://huggingface.co/" target="_blank" rel="noopener">https://huggingface.co/</a></p>
</li>
</ul>
<p>数据集</p>
<ul>
<li><p>movie dialogue dataset</p>
</li>
<li><p>ubuntu dialogue dataset</p>
</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/聊天机器人/">聊天机器人</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-聊天机器人一" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/03/10/聊天机器人一/" class="article-date">
      <time datetime="2020-03-09T23:59:07.000Z" itemprop="datePublished">2020-03-10</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/10/聊天机器人一/">聊天机器人一</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="聊天机器人"><a href="#聊天机器人" class="headerlink" title="聊天机器人"></a>聊天机器人</h2><h3 id="流行平台框架与实战"><a href="#流行平台框架与实战" class="headerlink" title="流行平台框架与实战"></a>流行平台框架与实战</h3><p>这堂课主要是带大家一起看看国内外一些主流的聊天机器人搭建平台。</p>
<h3 id="图灵机器人"><a href="#图灵机器人" class="headerlink" title="图灵机器人"></a>图灵机器人</h3><p>link: <a href="http://www.tuling123.com/" target="_blank" rel="noopener">http://www.tuling123.com/</a></p>
<p><img src="https://s1.ax1x.com/2020/06/09/t4iSHI.png" alt="title"></p>
<p>传说中在中文语境下智能度最高的机器人大脑。</p>
<h4 id="用法Demo"><a href="#用法Demo" class="headerlink" title="用法Demo"></a>用法Demo</h4><ol>
<li>注册</li>
<li>创建自己的机器人</li>
<li>连接微信公众号或QQ</li>
<li>社区</li>
</ol>
<h3 id="wxBot"><a href="#wxBot" class="headerlink" title="wxBot"></a>wxBot</h3><p>图灵机器人很棒，基本帮我们做到了傻瓜式接入主流中文社交软件。</p>
<p>但是，我们可以发现，这里依旧有限制，</p>
<p>官方提供的傻瓜式服务只能接入微信公众号或者QQ，</p>
<p>如果有更多的需求，那我们就得自己动手了。</p>
<p>比如，<strong>将个人微信变成聊天机器人</strong></p>
<p>这里我们介绍一个很棒的GitHub项目：WxBot（credit: @liuwons）</p>
<p>link: <a href="https://github.com/liuwons/wxBot" target="_blank" rel="noopener">https://github.com/liuwons/wxBot</a></p>
<p>这是一个是用Python包装Web微信协议实现的微信机器人框架。</p>
<p>换句话说，就是模拟了我们网页登录微信的状态，并通过网页微信协议传输对话。</p>
<p>好，接下来，我们来用wxbot实现一些有趣的功能：</p>
<h3 id="群聊天机器人"><a href="#群聊天机器人" class="headerlink" title="群聊天机器人"></a>群聊天机器人</h3><p>（转载自：<a href="http://blog.csdn.net/tobacco5648/article/details/50802922" target="_blank" rel="noopener">http://blog.csdn.net/tobacco5648/article/details/50802922</a> ）</p>
<p>实现效果</p>
<p><img src="https://s1.ax1x.com/2020/06/09/t4PzDA.png" alt="title"></p>
<p>有点像谷歌旗下的聊天工具 Allo</p>
<p><img src="https://s1.ax1x.com/2020/06/09/t4Pxud.jpg" alt="title"></p>
<p>简单说就是，在你聊天的时候，可以随时把小机器人给at出来，并且拉出来跟你聊天。</p>
<p>同时，如果调用图灵机器人的服务的话，你可以让它做很多复杂的工作，</p>
<p>比如，找附近的商店啊，查火车时刻表啊，等等。</p>
<h4 id="运行方法"><a href="#运行方法" class="headerlink" title="运行方法"></a>运行方法</h4><ol>
<li><p>下载wxBot， 安装python的依赖包。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install requests</span><br><span class="line">pip install pyqrcode</span><br><span class="line">pip install pypng</span><br><span class="line">pip install Pillow</span><br></pre></td></tr></table></figure>
</li>
<li><p>在图灵机器人官网注册账号，申请图灵API key</p>
</li>
<li><p>在bot.py文件所在目录下新建conf.ini文件，内容为(key字段内容为申请到的图灵key):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[main]</span><br><span class="line">key=1d2678900f734aa0a23734ace8aec5b1</span><br></pre></td></tr></table></figure>
</li>
<li><p>最后我们运行bot.py即可</p>
</li>
</ol>
<p>运行之后，你的terminal会跳出一个二维码。</p>
<p>你按照登录网页版微信的方式，扫一下 登录一下。</p>
<p>你的微信就被”托管“了 &gt;.&lt;</p>
<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># coding: utf-8</span><br><span class="line"></span><br><span class="line">from wxbot import *</span><br><span class="line">import ConfigParser</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">class TulingWXBot(WXBot):</span><br><span class="line">    # 拿到API Key</span><br><span class="line">    def __init__(self):</span><br><span class="line">        WXBot.__init__(self)</span><br><span class="line"></span><br><span class="line">        self.tuling_key = &quot;&quot;</span><br><span class="line">        self.robot_switch = True</span><br><span class="line"></span><br><span class="line">        try:</span><br><span class="line">            cf = ConfigParser.ConfigParser()</span><br><span class="line">            cf.read(&apos;conf.ini&apos;)</span><br><span class="line">            self.tuling_key = cf.get(&apos;main&apos;, &apos;key&apos;)</span><br><span class="line">        except Exception:</span><br><span class="line">            pass</span><br><span class="line">        print &apos;tuling_key:&apos;, self.tuling_key</span><br><span class="line">    </span><br><span class="line">    # 从图灵API返回自动回复</span><br><span class="line">    def tuling_auto_reply(self, uid, msg):</span><br><span class="line">        if self.tuling_key:</span><br><span class="line">            # API url</span><br><span class="line">            url = &quot;http://www.tuling123.com/openapi/api&quot;</span><br><span class="line">            user_id = uid.replace(&apos;@&apos;, &apos;&apos;)[:30]</span><br><span class="line">            # API request</span><br><span class="line">            body = &#123;&apos;key&apos;: self.tuling_key, &apos;info&apos;: msg.encode(&apos;utf8&apos;), &apos;userid&apos;: user_id&#125;</span><br><span class="line">            r = requests.post(url, data=body)</span><br><span class="line">            respond = json.loads(r.text)</span><br><span class="line">            result = &apos;&apos;</span><br><span class="line">            # 拿到回复，进行处理</span><br><span class="line">                # 按照API返回的code进行分类</span><br><span class="line">                # 是否是一个link</span><br><span class="line">                # 还是一句话</span><br><span class="line">                # 还是一组(list)回复</span><br><span class="line">            if respond[&apos;code&apos;] == 100000:</span><br><span class="line">                result = respond[&apos;text&apos;].replace(&apos;&lt;br&gt;&apos;, &apos;  &apos;)</span><br><span class="line">                result = result.replace(u&apos;\xa0&apos;, u&apos; &apos;)</span><br><span class="line">            elif respond[&apos;code&apos;] == 200000:</span><br><span class="line">                result = respond[&apos;url&apos;]</span><br><span class="line">            elif respond[&apos;code&apos;] == 302000:</span><br><span class="line">                for k in respond[&apos;list&apos;]:</span><br><span class="line">                    result = result + u&quot;【&quot; + k[&apos;source&apos;] + u&quot;】 &quot; +\</span><br><span class="line">                        k[&apos;article&apos;] + &quot;\t&quot; + k[&apos;detailurl&apos;] + &quot;\n&quot;</span><br><span class="line">            else:</span><br><span class="line">                result = respond[&apos;text&apos;].replace(&apos;&lt;br&gt;&apos;, &apos;  &apos;)</span><br><span class="line">                result = result.replace(u&apos;\xa0&apos;, u&apos; &apos;)</span><br><span class="line"></span><br><span class="line">            print &apos;    ROBOT:&apos;, result</span><br><span class="line">            return result</span><br><span class="line">        # 加了个exception，如果没有图灵API Key的话，</span><br><span class="line">        # 那就无脑回知道了。</span><br><span class="line">        else:</span><br><span class="line">            return u&quot;知道啦&quot;</span><br><span class="line">    </span><br><span class="line">    # 如果，用户不想机器人继续BB了，</span><br><span class="line">    # 或者，用户想言语调出机器人：</span><br><span class="line">    def auto_switch(self, msg):</span><br><span class="line">        msg_data = msg[&apos;content&apos;][&apos;data&apos;]</span><br><span class="line">        stop_cmd = [u&apos;退下&apos;, u&apos;走开&apos;, u&apos;关闭&apos;, u&apos;关掉&apos;, u&apos;休息&apos;, u&apos;滚开&apos;]</span><br><span class="line">        start_cmd = [u&apos;出来&apos;, u&apos;启动&apos;, u&apos;工作&apos;]</span><br><span class="line">        if self.robot_switch:</span><br><span class="line">            for i in stop_cmd:</span><br><span class="line">                if i == msg_data:</span><br><span class="line">                    self.robot_switch = False</span><br><span class="line">                    self.send_msg_by_uid(u&apos;[Robot]&apos; + u&apos;机器人已关闭！&apos;, msg[&apos;to_user_id&apos;])</span><br><span class="line">        else:</span><br><span class="line">            for i in start_cmd:</span><br><span class="line">                if i == msg_data:</span><br><span class="line">                    self.robot_switch = True</span><br><span class="line">                    self.send_msg_by_uid(u&apos;[Robot]&apos; + u&apos;机器人已开启！&apos;, msg[&apos;to_user_id&apos;])</span><br><span class="line">    </span><br><span class="line">    # 从微信回复</span><br><span class="line">    def handle_msg_all(self, msg):</span><br><span class="line">        if not self.robot_switch and msg[&apos;msg_type_id&apos;] != 1:</span><br><span class="line">            return</span><br><span class="line">        if msg[&apos;msg_type_id&apos;] == 1 and msg[&apos;content&apos;][&apos;type&apos;] == 0:  # reply to self</span><br><span class="line">            self.auto_switch(msg)</span><br><span class="line">        elif msg[&apos;msg_type_id&apos;] == 4 and msg[&apos;content&apos;][&apos;type&apos;] == 0:  # text message from contact</span><br><span class="line">            self.send_msg_by_uid(self.tuling_auto_reply(msg[&apos;user&apos;][&apos;id&apos;], msg[&apos;content&apos;][&apos;data&apos;]), msg[&apos;user&apos;][&apos;id&apos;])</span><br><span class="line">        elif msg[&apos;msg_type_id&apos;] == 3 and msg[&apos;content&apos;][&apos;type&apos;] == 0:  # group text message</span><br><span class="line">            if &apos;detail&apos; in msg[&apos;content&apos;]:</span><br><span class="line">                my_names = self.get_group_member_name(msg[&apos;user&apos;][&apos;id&apos;], self.my_account[&apos;UserName&apos;])</span><br><span class="line">                if my_names is None:</span><br><span class="line">                    my_names = &#123;&#125;</span><br><span class="line">                if &apos;NickName&apos; in self.my_account and self.my_account[&apos;NickName&apos;]:</span><br><span class="line">                    my_names[&apos;nickname2&apos;] = self.my_account[&apos;NickName&apos;]</span><br><span class="line">                if &apos;RemarkName&apos; in self.my_account and self.my_account[&apos;RemarkName&apos;]:</span><br><span class="line">                    my_names[&apos;remark_name2&apos;] = self.my_account[&apos;RemarkName&apos;]</span><br><span class="line"></span><br><span class="line">                is_at_me = False</span><br><span class="line">                for detail in msg[&apos;content&apos;][&apos;detail&apos;]:</span><br><span class="line">                    if detail[&apos;type&apos;] == &apos;at&apos;:</span><br><span class="line">                        for k in my_names:</span><br><span class="line">                            if my_names[k] and my_names[k] == detail[&apos;value&apos;]:</span><br><span class="line">                                is_at_me = True</span><br><span class="line">                                break</span><br><span class="line">                if is_at_me:</span><br><span class="line">                    src_name = msg[&apos;content&apos;][&apos;user&apos;][&apos;name&apos;]</span><br><span class="line">                    reply = &apos;to &apos; + src_name + &apos;: &apos;</span><br><span class="line">                    if msg[&apos;content&apos;][&apos;type&apos;] == 0:  # text message</span><br><span class="line">                        reply += self.tuling_auto_reply(msg[&apos;content&apos;][&apos;user&apos;][&apos;id&apos;], msg[&apos;content&apos;][&apos;desc&apos;])</span><br><span class="line">                    else:</span><br><span class="line">                        reply += u&quot;对不起，只认字，其他杂七杂八的我都不认识，,,Ծ‸Ծ,,&quot;</span><br><span class="line">                    self.send_msg_by_uid(reply, msg[&apos;user&apos;][&apos;id&apos;])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    bot = TulingWXBot()</span><br><span class="line">    bot.DEBUG = True</span><br><span class="line">    bot.conf[&apos;qr&apos;] = &apos;png&apos;</span><br><span class="line"></span><br><span class="line">    bot.run()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<p>GitHub上还有不少相似的项目，大家都可以关注一下：</p>
<p><a href="https://github.com/feit/Weixinbot" target="_blank" rel="noopener">feit/Weixinbot</a> Nodejs 封装网页版微信的接口，可编程控制微信消息</p>
<p><a href="https://github.com/littlecodersh/ItChat" target="_blank" rel="noopener">littlecodersh/ItChat</a> 微信个人号接口、微信机器人及命令行微信，Command line talks through Wechat</p>
<p><a href="https://github.com/Urinx/WeixinBot" target="_blank" rel="noopener">Urinx/WeixinBot</a> 网页版微信API，包含终端版微信及微信机器人</p>
<p><a href="https://github.com/zixia/wechaty" target="_blank" rel="noopener">zixia/wechaty</a> Wechaty is wechat for bot in Javascript(ES6). It’s a Personal Account Robot Framework/Library.</p>
<p><a href="https://coding.net/u/vivre/p/WxbotManage/git" target="_blank" rel="noopener">WxbotManage</a> 基于Wxbot的微信多开管理和Webapi系统</p>
<h3 id="自定义API接口"><a href="#自定义API接口" class="headerlink" title="自定义API接口"></a>自定义API接口</h3><p>刚刚我们讲的部分，还都是调用图灵机器人的API。</p>
<p>我们来看看，如何使用自己的ChatBot模型。</p>
<p>我们这里用之前讲过的Chatterbot库做个例子。</p>
<p>思路还是一样。我们用WxBot来处理微信端的工作，</p>
<p>然后，我们架设一个API，来把chatterbot的回复给传到微信去。</p>
<p>这里我们用一个简单粗暴的API框架：Hug。</p>
<p>大家也可以用其他各种框架，比如Flask。</p>
<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># coding: utf-8</span><br><span class="line"></span><br><span class="line"># 导入chatterbot自带的语料库</span><br><span class="line">from chatterbot import ChatBot</span><br><span class="line">from chatterbot.trainers import ChatterBotCorpusTrainer</span><br><span class="line">import hug</span><br><span class="line"></span><br><span class="line">deepThought = ChatBot(&quot;deepThought&quot;)</span><br><span class="line">deepThought.set_trainer(ChatterBotCorpusTrainer)</span><br><span class="line"># 使用中文语料库训练它</span><br><span class="line">deepThought.train(&quot;chatterbot.corpus.chinese&quot;)  # 语料库</span><br><span class="line"></span><br><span class="line"># API框架</span><br><span class="line">@hug.get()</span><br><span class="line">def get_response(user_input):</span><br><span class="line">    response = deepThought.get_response(user_input).text</span><br><span class="line">    return &#123;&quot;response&quot;:response&#125;</span><br></pre></td></tr></table></figure>

<p>hug -f bot_api.py</p>
<p>跑起来以后，你的terminal大概长这样：</p>
<p><img src="https://s1.ax1x.com/2020/06/09/t4PjjH.png" alt="title"></p>
<p>于是你可以在浏览器中尝试：</p>
<p><img src="https://s1.ax1x.com/2020/06/09/t4PXge.png" alt="title"></p>
<p>好，</p>
<p>接下来的部分，依旧是感谢@liuwons的wxbot：</p>
<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># coding: utf-8</span><br><span class="line"></span><br><span class="line">from wxbot import WXBot</span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line"># base url</span><br><span class="line">bot_api=&quot;http://127.0.0.1:8000/get_response&quot;</span><br><span class="line"></span><br><span class="line"># 处理回复</span><br><span class="line">class MyWXBot(WXBot):</span><br><span class="line">    def handle_msg_all(self, msg):</span><br><span class="line">        if msg[&apos;msg_type_id&apos;] == 4 and msg[&apos;content&apos;][&apos;type&apos;] == 0:</span><br><span class="line">            user_input = msg[&quot;content&quot;][&quot;data&quot;]</span><br><span class="line">            payload=&#123;&quot;user_input&quot;:user_input&#125;</span><br><span class="line">            response = requests.get(bot_api,params=payload).json()[&quot;response&quot;]</span><br><span class="line">            self.send_msg_by_uid(response, msg[&apos;user&apos;][&apos;id&apos;])</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    bot = MyWXBot()</span><br><span class="line">    bot.DEBUG = True</span><br><span class="line">    bot.conf[&apos;qr&apos;] = &apos;png&apos;</span><br><span class="line">    bot.run()</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<p>聊起来大概是这个节奏。</p>
<p><img src="https://s1.ax1x.com/2020/06/09/t4PL9O.jpg" alt="title"></p>
<p>好，如此，我们能做的显然不仅仅是用了个chatterbot</p>
<p>因为我们这样的一个结构，我们其实可以把任何聊天模型应用进来。</p>
<p>简单那来说，</p>
<p>Model &lt;–API–&gt; WxBot &lt;–web–&gt; WeChat</p>
<p>我们要做的就是封装一下我们的模型，让它看到一句话，给一个回复。</p>
<p>或者甚至是把VQA加到微信机器人中，发图片再发问题，求回复~</p>
<h3 id="一些有趣的微信机器人的idea"><a href="#一些有趣的微信机器人的idea" class="headerlink" title="一些有趣的微信机器人的idea"></a>一些有趣的微信机器人的idea</h3><ul>
<li>跨群转发。这是个非常实用的功能。对群来说，因为微信一个群最多500人， 跨群转发可以有效地把两个群拼到一起，实现更广泛的讨论。对个人来说，也可以用有选择的转发来把信息归档。比如看老板或者妹子在你加的几个群里每天都说了啥等等。 </li>
<li>聊天消息的主题归并，分析和搜索。微信聊天的基本单位是消息，但消息本身是非常碎片化的，很不适合搜索和分析。机器人可以把相关主题的消息归并起来，一方面可以大幅减小信息过载，一方面也可以从中得到更有价值的信息（类似视频分析里面把帧变成镜头）。这样分析以后可以做知识归档，用OneNote/印象笔记甚至公众号把讨论的成果沉淀下来。 </li>
<li>聊天脉络的梳理。群里的人一多，经常会出现几个话题并行出现的情况。这种情况对于理解和搜索都是非常不利的。机器人也需要把聊天的脉络进行梳理，在同一时间，把不同主题分别开。 </li>
<li>基本的统计数据。比如发言时间的分布，群的活跃度，成员的活跃度等等。做成漂亮的可视化，用户应该也会喜欢，给产品加分。 </li>
</ul>
<h3 id="国际上比较主流的几大聊天机器人框架"><a href="#国际上比较主流的几大聊天机器人框架" class="headerlink" title="国际上比较主流的几大聊天机器人框架"></a>国际上比较主流的几大聊天机器人框架</h3><ul>
<li>wit.ai</li>
<li>api.ai</li>
<li>microsoft bot framework</li>
</ul>
<p>虽然…</p>
<p>但是我们还是要稍微了解一下行业内的一些insights，</p>
<p>看看现在业内大火的各种炒聊天机器人概念的startups都是怎么玩的。</p>
<p>我们选点简单的开始玩起。</p>
<p>机器人端：api.ai，背后是谷歌</p>
<p>聊天端，我们选个Telegram，主打安全和快速的画风清奇的聊天软件~</p>
<h4 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h4><ol>
<li>注册api.ai</li>
<li>创建api.ai机器人</li>
<li>设置intents, entities等等</li>
<li>及时测试</li>
<li>走起telegram</li>
<li>跟机器人爸爸聊天, 设置</li>
<li><img src="https://s1.ax1x.com/2020/06/09/t4Pb4K.png" alt="avatar"></li>
<li>integrate</li>
<li>托管GitHub, Heroku</li>
<li>手机端试试玩儿</li>
<li><img src="https://s1.ax1x.com/2020/06/09/t4PIBR.png" alt="avatar"></li>
</ol>
<h3 id="直接调用api-ai进你自己的app-平台"><a href="#直接调用api-ai进你自己的app-平台" class="headerlink" title="直接调用api.ai进你自己的app/平台"></a>直接调用api.ai进你自己的app/平台</h3><p>其实这几个大平台都有自己的各个语言的官方支持库，让生活变得灰常简单：</p>
<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">!/usr/bin/env python</span><br><span class="line"> -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">import os.path</span><br><span class="line">import sys</span><br><span class="line">import json</span><br><span class="line">try:</span><br><span class="line">    import apiai</span><br><span class="line">except ImportError:</span><br><span class="line">    sys.path.append(</span><br><span class="line">        os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir)</span><br><span class="line">    )</span><br><span class="line">    import apiai</span><br><span class="line">    </span><br><span class="line"># api token</span><br><span class="line">CLIENT_ACCESS_TOKEN = &apos;your client access token&apos;</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    while(1):</span><br><span class="line">        ai = apiai.ApiAI(CLIENT_ACCESS_TOKEN)</span><br><span class="line">        request = ai.text_request()</span><br><span class="line">        request.lang = &apos;en&apos;  # 中文英文法语西语等等各种</span><br><span class="line"></span><br><span class="line">        # request.session_id = &quot;&lt;SESSION ID, UBIQUE FOR EACH USER&gt;&quot;</span><br><span class="line">        print(&quot;\n\nYour Input : &quot;,end=&quot; &quot;)</span><br><span class="line">        request.query = input()</span><br><span class="line"></span><br><span class="line">        print(&quot;\n\nBot\&apos;s response :&quot;,end=&quot; &quot;)</span><br><span class="line">        response = request.getresponse()</span><br><span class="line">        responsestr = response.read().decode(&apos;utf-8&apos;)</span><br><span class="line">        response_obj = json.loads(responsestr)</span><br><span class="line"></span><br><span class="line">        print(response_obj[&quot;result&quot;][&quot;fulfillment&quot;][&quot;speech&quot;])</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<p>跟之前的思路其实都差不多，</p>
<p>就是用框架的API调用来传送『对话』，解决问题</p>
<p>:P</p>
<p>就是这样！</p>
<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/wxBot/">wxBot</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-结构化预测" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/03/09/结构化预测/" class="article-date">
      <time datetime="2020-03-09T00:41:14.000Z" itemprop="datePublished">2020-03-09</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/09/结构化预测/">结构化预测</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Beam search: greedy</p>
<p>P(y|x): x中文句子, y英文句子</p>
<p>|V|^n 50000^20</p>
<p>sampling: </p>
<p>dynamic programming: HMM</p>
<h3 id="什么是结构化预测？"><a href="#什么是结构化预测？" class="headerlink" title="什么是结构化预测？"></a>什么是结构化预测？</h3><p>分类器</p>
<ul>
<li><p>将输入x匹配到输出y</p>
</li>
<li><p>一种简单的分类器</p>
</li>
<li><p>对任意一个输入x，计算每个可能的y的分数 score (x, y, \theta)，其中\theta是模型参数</p>
</li>
<li><p>选择分数最高的label y作为预测的类别</p>
</li>
<li><p>一般来说，如果输出空间是指数(exponential)级别或者无限的，我们把这类问题成为结构化预测(structured prediction)</p>
</li>
</ul>
<p>y = argmax_{candidate}(score(x, candidate, \theta))</p>
<p>结构化预测案例</p>
<ul>
<li><p>POS Tagging</p>
</li>
<li><p>Unlabeled Segmentation</p>
</li>
<li><p>莎拉波娃现在居住在美国东南部的佛罗里达。 </p>
</li>
<li><p>莎拉波娃 现在 居住 在 美国 东南部 的 佛罗里达 。</p>
</li>
<li><p>Labeled Segmentation (Named Entity Recognition 命名实体识别)</p>
</li>
<li><p>Some questioned if <strong>Tim Cook</strong>’s first product would be a breakaway hit for <strong>Apple</strong>.</p>
</li>
</ul>
<p><img src="https://uploader.shimo.im/f/0b2FQCgcQDoaFgYO.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/2rIXUkEW7EABqhny.png!thumbnail" alt="img"></p>
<ul>
<li>Constituency Parsing</li>
</ul>
<p><img src="https://uploader.shimo.im/f/Y67R5D7B5xU9NIfx.png!thumbnail" alt="img"></p>
<ul>
<li>Coreference Resolution</li>
</ul>
<p><img src="https://uploader.shimo.im/f/JBZC4rYqXikNmCAF.png!thumbnail" alt="img"></p>
<ul>
<li><p>语言生成：有很多NLP任务涉及到生成一段话，一个短语，一篇文章等等</p>
</li>
<li><p>问答系统</p>
</li>
<li><p>文本翻译</p>
</li>
<li><p>文本摘要</p>
</li>
</ul>
<p>什么是结构化预测？</p>
<ul>
<li><p>比较官方的定义是，当parts functions无法被拆分成minimal parts的时候，这就是一个结构化预测的问题。但是我们这里不详细展开。</p>
</li>
<li><p>part是一个问题的一部分子问题</p>
</li>
<li><p>parts function：用来把输入/输出拆分成parts</p>
</li>
<li><p>parts可以重叠</p>
</li>
<li><p>minimal parts：该任务最小的parts</p>
</li>
<li><p>minimal parts是不重叠的</p>
</li>
<li><p>结构化score/loss函数是没有办法拆分成minimal parts的。当我们使用一个结构化的score或者损失函数的时候，我们就在做structured predcition。</p>
</li>
</ul>
<h1 id="结构化预测解决的问题"><a href="#结构化预测解决的问题" class="headerlink" title="结构化预测解决的问题"></a>结构化预测解决的问题</h1><h2 id="序列标签问题"><a href="#序列标签问题" class="headerlink" title="序列标签问题"></a>序列标签问题</h2><ul>
<li><p>输入长度为T</p>
</li>
<li><p>输出长度也是T</p>
</li>
<li><p>每个位置可能的候选是N个label中的一个</p>
</li>
<li><p>输出空间为 N ^ T</p>
</li>
</ul>
<p>序列标签的案例</p>
<ul>
<li><p>前向神经网络做POS tagging</p>
</li>
<li><p>输入是一个单词和它相邻的单词</p>
</li>
<li><p>输出是中心词的POS tag</p>
</li>
<li><p>训练loss: 每个位置中心词的log loss，每个位置的loss相加</p>
</li>
</ul>
<p><img src="https://uploader.shimo.im/f/MD4bUVFlbRIubgvg.png!thumbnail" alt="img"></p>
<ul>
<li>这不是一个”结构化预测“问题</li>
</ul>
<p>如果我们采用一个RNN模型来做词性标注，这就成为了一个结构化预测的问题。</p>
<p><img src="https://uploader.shimo.im/f/Zw4WT0bR9vckZebr.png!thumbnail" alt="img"></p>
<p>如果我们使用HMM模型，这也是一个结构化预测问题。</p>
<p><img src="https://uploader.shimo.im/f/LXDKAKfdUAk7t4HK.png!thumbnail" alt="img"></p>
<ul>
<li>不要把语言当做“bag of words”，单词之间有“结构”。</li>
</ul>
<p>训练完模型之后，如何很好地快速地做搜索？</p>
<p>Viterbi 算法 </p>
<p><a href="https://shimo.im/docs/TRvGRjwJP8TDrCjc" target="_blank" rel="noopener">https://shimo.im/docs/TRvGRjwJP8TDrCjc</a></p>
<p>图解Viterbi维特比算法</p>
<p><a href="https://zhuanlan.zhihu.com/p/63087935" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/63087935</a></p>
<p>Viterbi算法的时间复杂度比较高。（回忆一下Viterbi算法的时间复杂度是多少？）</p>
<p>所以人们发明了一些近似的算法，例如greedy decoding，以及Beam Seaech。</p>
<h1 id="CRF模型"><a href="#CRF模型" class="headerlink" title="CRF模型"></a>CRF模型</h1><p><img src="https://uploader.shimo.im/f/0u599eTQh9YH50Mk.png!thumbnail" alt="img"></p>
<p>CRF的条件概率公式</p>
<p><img src="https://uploader.shimo.im/f/HsY8kd4QK9sSyi9W.png!thumbnail" alt="img"></p>
<p>分母是很大的</p>
<p>y 有五种不同的可能性</p>
<p>20   5^20</p>
<p><img src="https://uploader.shimo.im/f/7JqSkdgEheIIZ6c9.png!thumbnail" alt="img"></p>
<p>参考资料</p>
<p><a href="http://www.robots.ox.ac.uk/~davidc/pubs/crfs_jan2015.pdf" target="_blank" rel="noopener">http://www.robots.ox.ac.uk/~davidc/pubs/crfs_jan2015.pdf</a></p>
<p><a href="http://www.cs.cmu.edu/~10715-f18/lectures/lecture2-crf.pdf" target="_blank" rel="noopener">http://www.cs.cmu.edu/~10715-f18/lectures/lecture2-crf.pdf</a></p>
<p><a href="http://www.davidsbatista.net/blog/2017/11/13/Conditional_Random_Fields/" target="_blank" rel="noopener">http://www.davidsbatista.net/blog/2017/11/13/Conditional_Random_Fields/</a></p>
<h2 id="Michael-Collins’-notes"><a href="#Michael-Collins’-notes" class="headerlink" title="Michael Collins’ notes"></a>Michael Collins’ notes</h2><p>Log Linear tagggers <a href="http://www.cs.columbia.edu/~mcollins/fall2014-loglineartaggers.pdf" target="_blank" rel="noopener">http://www.cs.columbia.edu/~mcollins/fall2014-loglineartaggers.pdf</a></p>
<p>CRF Model <a href="http://www.cs.columbia.edu/~mcollins/crf.pdf" target="_blank" rel="noopener">http://www.cs.columbia.edu/~mcollins/crf.pdf</a></p>
<p>Forward-Backward算法 <a href="http://www.cs.columbia.edu/~mcollins/fb.pdf" target="_blank" rel="noopener">http://www.cs.columbia.edu/~mcollins/fb.pdf</a></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><p><a href="https://taehwanptl.github.io/" target="_blank" rel="noopener">Advanced Topics in Machine Learning: Structured Prediction</a></p>
</li>
<li><p><a href="http://www.cs.cmu.edu/~nasmith/slides/sp4nlp.icml09.pdf" target="_blank" rel="noopener">Noah Smith的课件</a></p>
</li>
<li><p><a href="https://www.zhihu.com/question/35866596/answer/236886066?hb_wx_block=0" target="_blank" rel="noopener">条件随机场</a></p>
</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Beam-search/">Beam search</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CRF/">CRF</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-SVM" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/02/20/SVM/" class="article-date">
      <time datetime="2020-02-20T06:17:41.000Z" itemprop="datePublished">2020-02-20</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/20/SVM/">SVM</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <ul>
<li><a href="https://zhuanlan.zhihu.com/p/36332083" target="_blank" rel="noopener">攀登传统机器学习的珠峰-SVM (上)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/36379394" target="_blank" rel="noopener">攀登传统机器学习的珠峰-SVM (中)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/36535299" target="_blank" rel="noopener">攀登传统机器学习的珠峰-SVM (下)</a></li>
</ul>
<h2 id="机器学习中的SVM"><a href="#机器学习中的SVM" class="headerlink" title="机器学习中的SVM"></a>机器学习中的SVM</h2><p>支持向量机是一种经典的二分类模型，基本模型定义为特征空间中最大间隔的线性分类器，其学习的优化目标便是间隔最大化，因此支持向量机本身可以转化为一个凸二次规划求解的问题。</p>
<h3 id="函数间隔与几何间隔"><a href="#函数间隔与几何间隔" class="headerlink" title="函数间隔与几何间隔"></a>函数间隔与几何间隔</h3><p>对于二分类学习，假设现在的数据是线性可分的，这时分类学习最基本的想法就是找到一个合适的超平面，该超平面能够将不同类别的样本分开，类似二维平面使用ax+by+c=0来表示，超平面实际上表示的就是高维的平面，如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f6a2ec8a.png" alt="1.png"></p>
<p>对数据点进行划分时，易知：当超平面距离与它最近的数据点的间隔越大，分类的鲁棒性越好，即当新的数据点加入时，超平面对这些点的适应性最强，出错的可能性最小。因此需要让所选择的超平面能够最大化这个间隔Gap（如下图所示）， 常用的间隔定义有两种，一种称之为函数间隔，一种为几何间隔，下面将分别介绍这两种间隔，并对SVM为什么会选用几何间隔做了一些阐述。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f6a06d5a.png" alt="2.png"></p>
<h4 id="函数间隔"><a href="#函数间隔" class="headerlink" title="函数间隔"></a>函数间隔</h4><p>在超平面w’x+b=0确定的情况下，|w’x<em>+b|能够代表点x*距离超平面的远近，易知：当w’x</em>+b&gt;0时，表示x<em>在超平面的一侧（正类，类标为1），而当w’x</em>+b&lt;0时，则表示x<em>在超平面的另外一侧（负类，类别为-1），因此（w’x</em>+b）y* 的正负性恰能表示数据点x<em>是否被分类正确。于是便引出了*</em>函数间隔**的定义（functional margin）:</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f690a14b.png" alt="3.png"></p>
<p>而超平面（w,b）关于所有样本点（Xi，Yi）的函数间隔最小值则为超平面在训练数据集T上的函数间隔：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f690ac26.png" alt="4.png"></p>
<p>可以看出：这样定义的函数间隔在处理SVM上会有问题，当超平面的两个参数w和b同比例改变时，函数间隔也会跟着改变，但是实际上超平面还是原来的超平面，并没有变化。例如：w1x1+w2x2+w3x3+b=0其实等价于2w1x1+2w2x2+2w3x3+2b=0，但计算的函数间隔却翻了一倍。从而引出了能真正度量点到超平面距离的概念–几何间隔（geometrical margin）。</p>
<h4 id="几何间隔"><a href="#几何间隔" class="headerlink" title="几何间隔"></a>几何间隔</h4><p><strong>几何间隔</strong>代表的则是数据点到超平面的真实距离，对于超平面w’x+b=0，w代表的是该超平面的法向量，设x<em>为超平面外一点x在法向量w方向上的投影点，x与超平面的距离为r，则有x</em>=x-r(w/||w||)，又x<em>在超平面上，即w’x</em>+b=0，代入即可得：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f697d499.png" alt="5.png"></p>
<p>为了得到r的绝对值，令r呈上其对应的类别y，即可得到几何间隔的定义：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f696fd10.png" alt="6.png"></p>
<p>从上述函数间隔与几何间隔的定义可以看出：实质上函数间隔就是|w’x+b|，而几何间隔就是点到超平面的距离。</p>
<h3 id="最大间隔与支持向量"><a href="#最大间隔与支持向量" class="headerlink" title="最大间隔与支持向量"></a>最大间隔与支持向量</h3><p>通过前面的分析可知：函数间隔不适合用来最大化间隔，因此这里我们要找的最大间隔指的是几何间隔，于是最大间隔分类器的目标函数定义为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f69af163.png" alt="7.png"></p>
<p>一般地，我们令r^为1（这样做的目的是为了方便推导和目标函数的优化），从而上述目标函数转化为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f697bb1d.png" alt="8.png"></p>
<p>对于y(w’x+b)=1的数据点，即下图中位于w’x+b=1或w’x+b=-1上的数据点，我们称之为<strong>支持向量</strong>（support vector），易知：对于所有的支持向量，它们恰好满足y<em>(w’x</em>+b)=1，而所有不是支持向量的点，有y<em>(w’x</em>+b)&gt;1。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f6a838c4.png" alt="9.png"></p>
<h3 id="从原始优化问题到对偶问题"><a href="#从原始优化问题到对偶问题" class="headerlink" title="从原始优化问题到对偶问题"></a>从原始优化问题到对偶问题</h3><p>对于上述得到的目标函数，求1/||w||的最大值相当于求||w||^2的最小值，因此很容易将原来的目标函数转化为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f6978cbb.png" alt="10.png"></p>
<p>即变为了一个带约束的凸二次规划问题，按书上所说可以使用现成的优化计算包（QP优化包）求解，但由于SVM的特殊性，一般我们将原问题变换为它的<strong>对偶问题</strong>，接着再对其对偶问题进行求解。为什么通过对偶问题进行求解，有下面两个原因：</p>
<pre><code>* 一是因为使用对偶问题更容易求解；
* 二是因为通过对偶问题求解出现了向量内积的形式，从而能更加自然地引出核函数。</code></pre><p>对偶问题，顾名思义，可以理解成优化等价的问题，更一般地，是将一个原始目标函数的最小化转化为它的对偶函数最大化的问题。对于当前的优化问题，首先我们写出它的朗格朗日函数：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f9332be7.png" alt="11.png"></p>
<p>上式很容易验证：当其中有一个约束条件不满足时，L的最大值为 ∞（只需令其对应的α为 ∞即可）；当所有约束条件都满足时，L的最大值为1/2||w||^2（此时令所有的α为0），因此实际上原问题等价于：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f93321c5.png" alt="12.png"></p>
<p>由于这个的求解问题不好做，因此一般我们将最小和最大的位置交换一下（需满足KKT条件） ，变成原问题的对偶问题：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f9330967.png" alt="13.png"></p>
<p>这样就将原问题的求最小变成了对偶问题求最大（用对偶这个词还是很形象），接下来便可以先求L对w和b的极小，再求L对α的极大。</p>
<p>（1）首先求L对w和b的极小，分别求L关于w和b的偏导，可以得出：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f9333e66.png" alt="14.png"></p>
<p>将上述结果代入L得到：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f935ae21.png" alt="15.png"></p>
<p>（2）接着L关于α极大求解α（通过SMO算法求解，此处不做深入）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f9338a9d.png" alt="16.png"></p>
<p>（3）最后便可以根据求解出的α，计算出w和b，从而得到分类超平面函数。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f93419ca.png" alt="17.png"></p>
<p>在对新的点进行预测时，实际上就是将数据点x*代入分类函数f(x)=w’x+b中，若f(x)&gt;0，则为正类，f(x)&lt;0，则为负类，根据前面推导得出的w与b，分类函数如下所示，此时便出现了上面所提到的内积形式。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f9353166.png" alt="18.png"></p>
<p>这里实际上只需计算新样本与支持向量的内积，因为对于非支持向量的数据点，其对应的拉格朗日乘子一定为0，根据最优化理论（K-T条件），对于不等式约束y(w’x+b)-1≥0，满足：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f933c947.png" alt="19.png">        </p>
<h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>由于上述的超平面只能解决线性可分的问题，对于线性不可分的问题，例如：异或问题，我们需要使用核函数将其进行推广。一般地，解决线性不可分问题时，常常采用<strong>映射</strong>的方式，将低维原始空间映射到高维特征空间，使得数据集在高维空间中变得线性可分，从而再使用线性学习器分类。如果原始空间为有限维，即属性数有限，那么总是存在一个高维特征空间使得样本线性可分。若∅代表一个映射，则在特征空间中的划分函数变为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72f934303e.png" alt="20.png"></p>
<p>按照同样的方法，先写出新目标函数的拉格朗日函数，接着写出其对偶问题，求L关于w和b的极大，最后运用SOM求解α。可以得出：</p>
<p>（1）原对偶问题变为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730cc68b3b.png" alt="21.png"></p>
<p>（2）原分类函数变为：<br><img src="https://i.loli.net/2018/10/17/5bc730cc1b673.png" alt="22.png"></p>
<p>求解的过程中，只涉及到了高维特征空间中的内积运算，由于特征空间的维数可能会非常大，例如：若原始空间为二维，映射后的特征空间为5维，若原始空间为三维，映射后的特征空间将是19维，之后甚至可能出现无穷维，根本无法进行内积运算了，此时便引出了<strong>核函数</strong>（Kernel）的概念。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730cc49adc.png" alt="23.png"></p>
<p>因此，核函数可以直接计算隐式映射到高维特征空间后的向量内积，而不需要显式地写出映射后的结果，它虽然完成了将特征从低维到高维的转换，但最终却是在低维空间中完成向量内积计算，与高维特征空间中的计算等效<strong>（低维计算，高维表现）</strong>，从而避免了直接在高维空间无法计算的问题。引入核函数后，原来的对偶问题与分类函数则变为：</p>
<p>（1）对偶问题：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730cc173b2.png" alt="24.png"></p>
<p>（2）分类函数：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730cc05959.png" alt="25.png"></p>
<p>因此，在线性不可分问题中，核函数的选择成了支持向量机的最大变数，若选择了不合适的核函数，则意味着将样本映射到了一个不合适的特征空间，则极可能导致性能不佳。同时，核函数需要满足以下这个必要条件：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730ccc468c.png" alt="26.png"></p>
<p>由于核函数的构造十分困难，通常我们都是从一些常用的核函数中选择，下面列出了几种常用的核函数：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730ccc541a.png" alt="27.png"></p>
<h3 id="软间隔支持向量机"><a href="#软间隔支持向量机" class="headerlink" title="软间隔支持向量机"></a>软间隔支持向量机</h3><p>前面的讨论中，我们主要解决了两个问题：当数据线性可分时，直接使用最大间隔的超平面划分；当数据线性不可分时，则通过核函数将数据映射到高维特征空间，使之线性可分。然而在现实问题中，对于某些情形还是很难处理，例如数据中有<strong>噪声</strong>的情形，噪声数据（<strong>outlier</strong>）本身就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束，如果不要这些噪声数据还好，当加入这些outlier后导致划分超平面被挤歪了，如下图所示，对支持向量机的泛化性能造成很大的影响。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730ccce68e.png" alt="28.png"></p>
<p>为了解决这一问题，我们需要允许某一些数据点不满足约束，即可以在一定程度上偏移超平面，同时使得不满足约束的数据点尽可能少，这便引出了<strong>“软间隔”支持向量机</strong>的概念</p>
<pre><code>* 允许某些数据点不满足约束y(w&apos;x+b)≥1；
* 同时又使得不满足约束的样本尽可能少。</code></pre><p>这样优化目标变为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730cc6c9fe.png" alt="29.png"></p>
<p>如同阶跃函数，0/1损失函数虽然表示效果最好，但是数学性质不佳。因此常用其它函数作为“替代损失函数”。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc730cc5e5a9.png" alt="30.png"></p>
<p>支持向量机中的损失函数为<strong>hinge损失</strong>，引入<strong>“松弛变量”</strong>，目标函数与约束条件可以写为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7317aa3411.png" alt="31.png"></p>
<p>其中C为一个参数，控制着目标函数与新引入正则项之间的权重，这样显然每个样本数据都有一个对应的松弛变量，用以表示该样本不满足约束的程度，将新的目标函数转化为拉格朗日函数得到：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7317a4c96e.png" alt="32.png"></p>
<p>按照与之前相同的方法，先让L求关于w，b以及松弛变量的极小，再使用SMO求出α，有：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7317a6dff2.png" alt="33.png"></p>
<p>将w代入L化简，便得到其对偶问题：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7317ab6646.png" alt="34.png"></p>
<p>将“软间隔”下产生的对偶问题与原对偶问题对比可以发现：新的对偶问题只是约束条件中的α多出了一个上限C，其它的完全相同，因此在引入核函数处理线性不可分问题时，便能使用与“硬间隔”支持向量机完全相同的方法。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SVM/">SVM</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-word-embedding" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/02/11/word-embedding/" class="article-date">
      <time datetime="2020-02-11T11:20:12.000Z" itemprop="datePublished">2020-02-11</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/11/word-embedding/">word-embedding</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h3><p>学习目标</p>
<ul>
<li>学习词向量的概念</li>
<li>用Skip-thought模型训练词向量</li>
<li>学习使用PyTorch dataset和dataloader</li>
<li>学习定义PyTorch模型</li>
<li>学习torch.nn中常见的Module<ul>
<li>Embedding</li>
</ul>
</li>
<li>学习常见的PyTorch operations<ul>
<li>bmm</li>
<li>logsigmoid</li>
</ul>
</li>
<li>保存和读取PyTorch模型</li>
</ul>
<p>使用的训练数据可以从以下链接下载到。</p>
<p>链接:<a href="https://pan.baidu.com/s/1tFeK3mXuVXEy3EMarfeWvg" target="_blank" rel="noopener">https://pan.baidu.com/s/1tFeK3mXuVXEy3EMarfeWvg</a> 密码:v2z5</p>
<p>在这一份notebook中，我们会（尽可能）尝试复现论文<a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">Distributed Representations of Words and Phrases and their Compositionality</a>中训练词向量的方法. 我们会实现Skip-gram模型，并且使用论文中noice contrastive sampling的目标函数。</p>
<p>这篇论文有很多模型实现的细节，这些细节对于词向量的好坏至关重要。我们虽然无法完全复现论文中的实验结果，主要是由于计算资源等各种细节原因，但是我们还是可以大致展示如何训练词向量。</p>
<p>以下是一些我们没有实现的细节</p>
<ul>
<li>subsampling：参考论文section 2.3</li>
</ul>
<p>In [1]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from torch import nn</span><br></pre></td></tr></table></figure>

<p>In [2]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn  <span class="comment">#神经网络工具箱torch.nn </span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F  <span class="comment">#神经网络函数torch.nn.functional</span></span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> tud  <span class="comment">#Pytorch读取训练集需要用到torch.utils.data类</span></span><br></pre></td></tr></table></figure>

<p><strong>两个模块的区别：</strong><a href="https://blog.csdn.net/hawkcici160/article/details/80140059" target="_blank" rel="noopener">torch.nn 和 torch.functional 的区别</a></p>
<p>In [3]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.parameter <span class="keyword">import</span> Parameter  <span class="comment">#参数更新和优化函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter <span class="comment">#Counter 计数器</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> scipy <span class="comment">#SciPy是基于NumPy开发的高级模块，它提供了许多数学算法和函数的实现</span></span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity <span class="comment">#余弦相似度函数</span></span><br></pre></td></tr></table></figure>

<p>开始看代码前，请确保对word2vec有了解。</p>
<p><a href="https://blog.csdn.net/lilong117194/article/details/81979522" target="_blank" rel="noopener">CBOW模型理解</a></p>
<p><a href="https://www.jianshu.com/p/da235893e4a5" target="_blank" rel="noopener">Skip-Gram模型理解</a></p>
<p>负例采样就是Skip-Gram模型的输出不是周围词的概率了，是正例和负例的概率</p>
<p>In [4]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">USE_CUDA = torch.cuda.is_available() <span class="comment">#有GPU可以用</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了保证实验结果可以复现，我们经常会把各种random seed固定在某一个值</span></span><br><span class="line">random.seed(<span class="number">53113</span>)</span><br><span class="line">np.random.seed(<span class="number">53113</span>)</span><br><span class="line">torch.manual_seed(<span class="number">53113</span>)</span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    torch.cuda.manual_seed(<span class="number">53113</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 设定一些超参数   </span></span><br><span class="line">K = <span class="number">10</span> <span class="comment"># number of negative samples 负样本随机采样数量</span></span><br><span class="line">C = <span class="number">3</span> <span class="comment"># nearby words threshold 指定周围三个单词进行预测</span></span><br><span class="line">NUM_EPOCHS = <span class="number">2</span> <span class="comment"># The number of epochs of training 迭代轮数</span></span><br><span class="line">MAX_VOCAB_SIZE = <span class="number">30000</span> <span class="comment"># the vocabulary size 词汇表多大</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span> <span class="comment"># the batch size 每轮迭代1个batch的数量</span></span><br><span class="line">LEARNING_RATE = <span class="number">0.2</span> <span class="comment"># the initial learning rate #学习率</span></span><br><span class="line">EMBEDDING_SIZE = <span class="number">100</span> <span class="comment">#词向量维度</span></span><br><span class="line">       </span><br><span class="line">    </span><br><span class="line">LOG_FILE = <span class="string">"word-embedding.log"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tokenize函数，把一篇文本转化成一个个单词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_tokenize</span><span class="params">(text)</span>:</span> </span><br><span class="line">    <span class="keyword">return</span> text.split()</span><br></pre></td></tr></table></figure>

<ul>
<li>从文本文件中读取所有的文字，通过这些文本创建一个vocabulary</li>
<li>由于单词数量可能太大，我们只选取最常见的MAX_VOCAB_SIZE个单词</li>
<li>我们添加一个UNK单词表示所有不常见的单词</li>
<li>我们需要记录单词到index的mapping，以及index到单词的mapping，单词的count，单词的(normalized) frequency，以及单词总数。</li>
</ul>
<p>In [5]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">"./text8/text8.train.txt"</span>, <span class="string">"r"</span>) <span class="keyword">as</span> fin: <span class="comment">#读入文件</span></span><br><span class="line">    text = fin.read() <span class="comment"># 一次性读入文件所有内容</span></span><br><span class="line">    </span><br><span class="line">text = [w <span class="keyword">for</span> w <span class="keyword">in</span> word_tokenize(text.lower())] </span><br><span class="line"><span class="comment">#分词，在这里类似于text.split()</span></span><br><span class="line"><span class="comment">#print(len(text)) # 15313011，有辣么多单词</span></span><br><span class="line"></span><br><span class="line">vocab = dict(Counter(text).most_common(MAX_VOCAB_SIZE<span class="number">-1</span>))</span><br><span class="line"><span class="comment">#字典格式，把（MAX_VOCAB_SIZE-1）个最频繁出现的单词取出来，-1是留给不常见的单词</span></span><br><span class="line"><span class="comment">#print(len(vocab)) # 29999</span></span><br><span class="line"></span><br><span class="line">vocab[<span class="string">"&lt;unk&gt;"</span>] = len(text) - np.sum(list(vocab.values()))</span><br><span class="line"><span class="comment">#unk表示不常见单词数=总单词数-常见单词数</span></span><br><span class="line"><span class="comment"># print(vocab["&lt;unk&gt;"]) # 617111</span></span><br><span class="line">print(vocab[<span class="string">"&lt;unk&gt;"</span>])</span><br><span class="line">idx_to_word = [word <span class="keyword">for</span> word <span class="keyword">in</span> vocab.keys()] </span><br><span class="line"><span class="comment">#取出字典的所有最常见30000单词</span></span><br><span class="line"></span><br><span class="line">word_to_idx = &#123;word:i <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(idx_to_word)&#125;</span><br><span class="line"><span class="comment">#取出所有单词的单词和对应的索引，索引值与单词出现次数相反，最常见单词索引为0。</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">617111</span><br></pre></td></tr></table></figure>

<p>In [1]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#print(vocab)</span></span><br></pre></td></tr></table></figure>

<p>In [2]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#print(list(word_to_idx.items())[29900:]) </span></span><br><span class="line"><span class="comment"># 敲黑板：字典是怎么像列表那样切片的</span></span><br></pre></td></tr></table></figure>

<p>In [9]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">word_counts = np.array([count <span class="keyword">for</span> count <span class="keyword">in</span> vocab.values()], dtype=np.float32)</span><br><span class="line"><span class="comment">#vocab所有单词的频数values</span></span><br><span class="line"></span><br><span class="line">word_freqs = word_counts / np.sum(word_counts)</span><br><span class="line"><span class="comment">#所有单词的词频概率值</span></span><br><span class="line"><span class="comment"># print(np.sum(word_freqs))=1</span></span><br><span class="line"></span><br><span class="line">word_freqs = word_freqs ** (<span class="number">3.</span>/<span class="number">4.</span>)</span><br><span class="line"><span class="comment">#论文里乘以3/4次方</span></span><br><span class="line"><span class="comment"># print(np.sum(word_freqs)) = 7.7</span></span><br><span class="line"></span><br><span class="line">word_freqs = word_freqs / np.sum(word_freqs) <span class="comment"># 用来做 negative sampling</span></span><br><span class="line"><span class="comment"># 重新计算所有单词的频率，老师这里代码好像写错了</span></span><br><span class="line"><span class="comment"># print(np.sum(word_freqs)) = 1</span></span><br><span class="line"></span><br><span class="line">VOCAB_SIZE = len(idx_to_word) <span class="comment">#词汇表单词数30000=MAX_VOCAB_SIZE</span></span><br><span class="line">VOCAB_SIZE</span><br></pre></td></tr></table></figure>

<p>Out[9]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">30000</span><br></pre></td></tr></table></figure>

<h3 id="实现Dataloader"><a href="#实现Dataloader" class="headerlink" title="实现Dataloader"></a>实现Dataloader</h3><p>一个dataloader需要以下内容：</p>
<ul>
<li>把所有text编码成数字，然后用subsampling预处理这些文字。</li>
<li>保存vocabulary，单词count，normalized word frequency</li>
<li>每个iteration sample一个中心词</li>
<li>根据当前的中心词返回context单词</li>
<li>根据中心词sample一些negative单词</li>
<li>返回单词的counts</li>
</ul>
<p>这里有一个好的tutorial介绍如何使用<a href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html" target="_blank" rel="noopener">PyTorch dataloader</a>. 为了使用dataloader，我们需要定义以下两个function:</p>
<ul>
<li><code>__len__</code> function需要返回整个数据集中有多少个item</li>
<li><code>__get__</code> 根据给定的index返回一个item</li>
</ul>
<p>有了dataloader之后，我们可以轻松随机打乱整个数据集，拿到一个batch的数据等等。</p>
<p>torch.utils.data.DataLoader理解：<a href="https://blog.csdn.net/qq_36653505/article/details/83351808" target="_blank" rel="noopener">https://blog.csdn.net/qq_36653505/article/details/83351808</a></p>
<p>In [10]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordEmbeddingDataset</span><span class="params">(tud.Dataset)</span>:</span> <span class="comment">#tud.Dataset父类</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, text, word_to_idx, idx_to_word, word_freqs, word_counts)</span>:</span></span><br><span class="line">        <span class="string">''' text: a list of words, all text from the training dataset</span></span><br><span class="line"><span class="string">            word_to_idx: the dictionary from word to idx</span></span><br><span class="line"><span class="string">            idx_to_word: idx to word mapping</span></span><br><span class="line"><span class="string">            word_freq: the frequency of each word</span></span><br><span class="line"><span class="string">            word_counts: the word counts</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super(WordEmbeddingDataset, self).__init__() <span class="comment">#初始化模型</span></span><br><span class="line">        self.text_encoded = [word_to_idx.get(t, VOCAB_SIZE<span class="number">-1</span>) <span class="keyword">for</span> t <span class="keyword">in</span> text]</span><br><span class="line">        <span class="comment">#字典 get() 函数返回指定键的值（第一个参数），如果值不在字典中返回默认值（第二个参数）。</span></span><br><span class="line">        <span class="comment">#取出text里每个单词word_to_idx字典里对应的索引,不在字典里返回"&lt;unk&gt;"的索引=29999</span></span><br><span class="line">        <span class="comment"># 这样text里的所有词都编码好了，从单词转化为了向量，</span></span><br><span class="line">        <span class="comment"># 共有15313011个单词，词向量取值范围是0～29999，0是最常见单词向量。</span></span><br><span class="line">        </span><br><span class="line">        self.text_encoded = torch.Tensor(self.text_encoded).long()</span><br><span class="line">        <span class="comment">#变成tensor类型，这里变成longtensor，也可以torch.LongTensor(self.text_encoded)</span></span><br><span class="line">        </span><br><span class="line">        self.word_to_idx = word_to_idx <span class="comment">#保存数据</span></span><br><span class="line">        self.idx_to_word = idx_to_word  <span class="comment">#保存数据</span></span><br><span class="line">        self.word_freqs = torch.Tensor(word_freqs) <span class="comment">#保存数据</span></span><br><span class="line">        self.word_counts = torch.Tensor(word_counts) <span class="comment">#保存数据</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span> <span class="comment">#数据集有多少个item </span></span><br><span class="line">        <span class="comment">#魔法函数__len__</span></span><br><span class="line">        <span class="string">''' 返回整个数据集（所有单词）的长度</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> len(self.text_encoded) <span class="comment">#所有单词的总数</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="comment">#魔法函数__getitem__，这个函数跟普通函数不一样</span></span><br><span class="line">        <span class="string">''' 这个function返回以下数据用于训练</span></span><br><span class="line"><span class="string">            - 中心词</span></span><br><span class="line"><span class="string">            - 这个单词附近的(positive)单词</span></span><br><span class="line"><span class="string">            - 随机采样的K个单词作为negative sample</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        center_word = self.text_encoded[idx] </span><br><span class="line">        <span class="comment">#中心词</span></span><br><span class="line">        <span class="comment">#这里__getitem__函数是个迭代器，idx代表了所有的单词索引。</span></span><br><span class="line">        </span><br><span class="line">        pos_indices = list(range(idx-C, idx)) + list(range(idx+<span class="number">1</span>, idx+C+<span class="number">1</span>))</span><br><span class="line">        <span class="comment">#周围词的索引，比如idx=0时。pos_indices = [-3, -2, -1, 1, 2, 3] </span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        pos_indices = [i%len(self.text_encoded) <span class="keyword">for</span> i <span class="keyword">in</span> pos_indices]</span><br><span class="line">        <span class="comment">#超出词汇总数时，需要特别处理，取余数，比如pos_indices = [15313009,15313010,15313011,1,2,3]</span></span><br><span class="line">        </span><br><span class="line">        pos_words = self.text_encoded[pos_indices]</span><br><span class="line">        <span class="comment">#周围词，就是希望出现的正例单词</span></span><br><span class="line">        </span><br><span class="line">        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[<span class="number">0</span>], <span class="literal">True</span>)</span><br><span class="line">        <span class="comment">#负例采样单词，torch.multinomial作用是按照self.word_freqs的概率做K * pos_words.shape[0]次取值，</span></span><br><span class="line">        <span class="comment">#输出的是self.word_freqs对应的下标。取样方式采用有放回的采样，并且self.word_freqs数值越大，取样概率越大。</span></span><br><span class="line">        <span class="comment">#每个正确的单词采样K个，pos_words.shape[0]是正确单词数量=6</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> center_word, pos_words, neg_words</span><br></pre></td></tr></table></figure>

<p>创建dataset和dataloader</p>
<p>In [11]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataset = WordEmbeddingDataset(text, word_to_idx, idx_to_word, word_freqs, word_counts)</span><br><span class="line"># list(dataset) 可以把尝试打印下center_word, pos_words, neg_words看看</span><br></pre></td></tr></table></figure>

<p>torch.utils.data.DataLoader理解：<a href="https://blog.csdn.net/qq_36653505/article/details/83351808" target="_blank" rel="noopener">https://blog.csdn.net/qq_36653505/article/details/83351808</a></p>
<p>In [12]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataloader = tud.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<p>In [13]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(next(iter(dataloader))[<span class="number">0</span>].shape) <span class="comment"># 一个batch中间词维度</span></span><br><span class="line">print(next(iter(dataloader))[<span class="number">1</span>].shape) <span class="comment"># 一个batch周围词维度</span></span><br><span class="line">print(next(iter(dataloader))[<span class="number">2</span>].shape) <span class="comment"># 一个batch负样本维度</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">128</span>])</span><br><span class="line">torch.Size([<span class="number">128</span>, <span class="number">6</span>])</span><br><span class="line">torch.Size([<span class="number">128</span>, <span class="number">60</span>])</span><br></pre></td></tr></table></figure>

<h3 id="定义PyTorch模型"><a href="#定义PyTorch模型" class="headerlink" title="定义PyTorch模型"></a>定义PyTorch模型</h3><p>In [14]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmbeddingModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size)</span>:</span></span><br><span class="line">        <span class="string">''' 初始化输出和输出embedding</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super(EmbeddingModel, self).__init__()</span><br><span class="line">        self.vocab_size = vocab_size  <span class="comment">#30000</span></span><br><span class="line">        self.embed_size = embed_size  <span class="comment">#100</span></span><br><span class="line">        </span><br><span class="line">        initrange = <span class="number">0.5</span> / self.embed_size</span><br><span class="line">        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment">#模型输出nn.Embedding(30000, 100)</span></span><br><span class="line">        self.out_embed.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        <span class="comment">#权重初始化的一种方法</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=<span class="literal">False</span>)</span><br><span class="line">         <span class="comment">#模型输入nn.Embedding(30000, 100)</span></span><br><span class="line">        self.in_embed.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        <span class="comment">#权重初始化的一种方法</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_labels, pos_labels, neg_labels)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        input_labels: 中心词, [batch_size]</span></span><br><span class="line"><span class="string">        pos_labels: 中心词周围 context window 出现过的单词 [batch_size * (window_size * 2)]</span></span><br><span class="line"><span class="string">        neg_labelss: 中心词周围没有出现过的单词，从 negative sampling 得到 [batch_size, (window_size * 2 * K)]</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        return: loss, [batch_size]</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        </span><br><span class="line">        batch_size = input_labels.size(<span class="number">0</span>)  <span class="comment">#input_labels是输入的标签，tud.DataLoader()返回的。已经被分成batch了。</span></span><br><span class="line">        </span><br><span class="line">        input_embedding = self.in_embed(input_labels) </span><br><span class="line">        <span class="comment"># B * embed_size</span></span><br><span class="line">        <span class="comment">#这里估计进行了运算：（128,30000）*（30000,100）= 128(Batch) * 100 (embed_size)</span></span><br><span class="line">        </span><br><span class="line">        pos_embedding = self.out_embed(pos_labels) <span class="comment"># B * (2*C=6) * embed_size </span></span><br><span class="line">        <span class="comment"># 这里估计进行了运算：（128,6,30000）*（128,30000,100）= 128(Batch) * 6 * 100 (embed_size)</span></span><br><span class="line">        <span class="comment">#同上，增加了维度(2*C)，表示一个batch有128组周围词单词，一组周围词有(2*C)个单词，每个单词有embed_size个维度。</span></span><br><span class="line">        </span><br><span class="line">        neg_embedding = self.out_embed(neg_labels) <span class="comment"># B * (2*C * K) * embed_size</span></span><br><span class="line">        <span class="comment">#同上，增加了维度(2*C*K)</span></span><br><span class="line">      </span><br><span class="line">    </span><br><span class="line">        <span class="comment">#torch.bmm()为batch间的矩阵相乘（b,n.m)*(b,m,p)=(b,n,p)</span></span><br><span class="line">        log_pos = torch.bmm(pos_embedding, input_embedding.unsqueeze(<span class="number">2</span>)).squeeze() <span class="comment"># B * (2*C)</span></span><br><span class="line">        <span class="comment"># log_pos = (128,6,100)*(128,100,1) = (128,6,1) = (128,6)</span></span><br><span class="line">        <span class="comment"># 这里如果没有负采样，只有周围单词来训练的话，每个周围单词30000个one-hot向量的维度</span></span><br><span class="line">        <span class="comment"># 而负采样大大降低了维度，每个周围单词仅仅只有一个维度。每个样本输出共有2*C个维度</span></span><br><span class="line">        log_neg = torch.bmm(neg_embedding, -input_embedding.unsqueeze(<span class="number">2</span>)).squeeze() <span class="comment"># B * (2*C*K)</span></span><br><span class="line">        <span class="comment"># log_neg = (128,6*K,100)*(128,100,1) = (128,6*K,1) = (128,6*K)，注意这里有个负号，区别与正样本</span></span><br><span class="line">        <span class="comment"># unsqueeze(2)指定位置升维，.squeeze()压缩维度。</span></span><br><span class="line">        <span class="comment"># 而负采样降低了维度，每个负例单词仅仅只有一个维度，每个样本输出共有2*C*K个维度</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#下面loss计算就是论文里的公式</span></span><br><span class="line">        log_pos = F.logsigmoid(log_pos).sum(<span class="number">1</span>)</span><br><span class="line">        log_neg = F.logsigmoid(log_neg).sum(<span class="number">1</span>) <span class="comment"># batch_size     </span></span><br><span class="line">        loss = log_pos + log_neg <span class="comment"># 正样本损失和负样本损失和尽量最大</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> -loss <span class="comment"># 最大转化成最小</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#取出self.in_embed数据参数，维度：（30000,100），就是我们要训练的词向量</span></span><br><span class="line">    <span class="comment"># 这里本来模型训练有两个矩阵的，self.in_embed和self.out_embed两个</span></span><br><span class="line">    <span class="comment"># 只是作者认为输入矩阵比较好，就舍弃了输出矩阵。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">input_embeddings</span><span class="params">(self)</span>:</span>   </span><br><span class="line">        <span class="keyword">return</span> self.in_embed.weight.data.cpu().numpy()</span><br></pre></td></tr></table></figure>

<p>定义一个模型以及把模型移动到GPU</p>
<p>In [15]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_SIZE)</span><br><span class="line"><span class="comment">#得到model，有参数，有loss，可以优化了</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    model = model.cuda()</span><br></pre></td></tr></table></figure>

<p>In [28]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.iloc[:, <span class="number">0</span>:<span class="number">2</span>].index</span><br></pre></td></tr></table></figure>

<p>Out[28]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RangeIndex(start=<span class="number">0</span>, stop=<span class="number">353</span>, step=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">"wordsim353.csv"</span>, sep=<span class="string">","</span>)</span><br><span class="line"><span class="comment"># else:</span></span><br><span class="line"><span class="comment">#     data = pd.read_csv("simlex-999.txt", sep="\t")</span></span><br><span class="line">print(data.head())</span><br><span class="line">human_similarity = []</span><br><span class="line">model_similarity = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data.iloc[:, <span class="number">0</span>:<span class="number">2</span>].index:</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure>

<p>下面是评估模型的代码，以及训练模型的代码</p>
<p>In [16]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(filename, embedding_weights)</span>:</span> </span><br><span class="line">    <span class="comment"># 传入的有三个文件课选择,两个txt，一个csv，可以先自己打开看看</span></span><br><span class="line">    <span class="comment"># embedding_weights是训练之后的embedding向量。</span></span><br><span class="line">    <span class="keyword">if</span> filename.endswith(<span class="string">".csv"</span>):</span><br><span class="line">        data = pd.read_csv(filename, sep=<span class="string">","</span>) <span class="comment"># csv文件打开</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data = pd.read_csv(filename, sep=<span class="string">"\t"</span>) <span class="comment"># txt文件打开，以\t制表符分割</span></span><br><span class="line">    human_similarity = []</span><br><span class="line">    model_similarity = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> data.iloc[:, <span class="number">0</span>:<span class="number">2</span>].index: <span class="comment"># 这里只是取出行索引，用data.index也可以</span></span><br><span class="line">        word1, word2 = data.iloc[i, <span class="number">0</span>], data.iloc[i, <span class="number">1</span>] <span class="comment"># 依次取出每行的2个单词</span></span><br><span class="line">        <span class="keyword">if</span> word1 <span class="keyword">not</span> <span class="keyword">in</span> word_to_idx <span class="keyword">or</span> word2 <span class="keyword">not</span> <span class="keyword">in</span> word_to_idx:</span><br><span class="line">            <span class="comment"># 如果取出的单词不在我们建的30000万个词汇表，就舍弃，评估不了</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            word1_idx, word2_idx = word_to_idx[word1], word_to_idx[word2]</span><br><span class="line">            <span class="comment"># 否则，分别取出这两个单词对应的向量，</span></span><br><span class="line">            word1_embed, word2_embed = embedding_weights[[word1_idx]], embedding_weights[[word2_idx]]</span><br><span class="line">            <span class="comment"># 在分别取出这两个单词对应的embedding向量，具体为啥是这种取出方式[[word1_idx]]，可以自行研究</span></span><br><span class="line">            model_similarity.append(float(sklearn.metrics.pairwise.cosine_similarity(word1_embed, word2_embed)))</span><br><span class="line">            <span class="comment"># 用余弦相似度计算这两个100维向量的相似度。这个是模型算出来的相似度</span></span><br><span class="line">            human_similarity.append(float(data.iloc[i, <span class="number">2</span>]))</span><br><span class="line">            <span class="comment"># 这个是人类统计得到的相似度</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> scipy.stats.spearmanr(human_similarity, model_similarity)<span class="comment"># , model_similarity</span></span><br><span class="line">    <span class="comment"># 因为相似度是浮点数，不是0 1 这些固定标签值，所以不能用准确度评估指标</span></span><br><span class="line">    <span class="comment"># scipy.stats.spearmanr网址：https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html</span></span><br><span class="line">    <span class="comment"># scipy.stats.spearmanr评估两个分布的相似度，有两个返回值correlation, pvalue</span></span><br><span class="line">    <span class="comment"># correlation是评估相关性的指标（-1，1），越接近1越相关，pvalue值大家可以自己搜索理解</span></span><br></pre></td></tr></table></figure>

<p>训练模型：</p>
<ul>
<li>模型一般需要训练若干个epoch</li>
<li>每个epoch我们都把所有的数据分成若干个batch</li>
<li>把每个batch的输入和输出都包装成cuda tensor</li>
<li>forward pass，通过输入的句子预测每个单词的下一个单词</li>
<li>用模型的预测和正确的下一个单词计算cross entropy loss</li>
<li>清空模型当前gradient</li>
<li>backward pass</li>
<li>更新模型参数</li>
<li>每隔一定的iteration输出模型在当前iteration的loss，以及在验证数据集上做模型的评估</li>
</ul>
<p>In [17]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, (input_labels, pos_labels, neg_labels) <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">        print(input_labels.shape, pos_labels.shape, neg_labels.shape)</span><br><span class="line">        <span class="keyword">if</span> i&gt;<span class="number">5</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">128</span>]) torch.Size([<span class="number">128</span>, <span class="number">6</span>]) torch.Size([<span class="number">128</span>, <span class="number">60</span>])</span><br><span class="line">torch.Size([<span class="number">128</span>]) torch.Size([<span class="number">128</span>, <span class="number">6</span>]) torch.Size([<span class="number">128</span>, <span class="number">60</span>])</span><br><span class="line">torch.Size([<span class="number">128</span>]) torch.Size([<span class="number">128</span>, <span class="number">6</span>]) torch.Size([<span class="number">128</span>, <span class="number">60</span>])</span><br><span class="line">torch.Size([<span class="number">128</span>]) torch.Size([<span class="number">128</span>, <span class="number">6</span>]) torch.Size([<span class="number">128</span>, <span class="number">60</span>])</span><br><span class="line">torch.Size([<span class="number">128</span>]) torch.Size([<span class="number">128</span>, <span class="number">6</span>]) torch.Size([<span class="number">128</span>, <span class="number">60</span>])</span><br><span class="line">torch.Size([<span class="number">128</span>]) torch.Size([<span class="number">128</span>, <span class="number">6</span>]) torch.Size([<span class="number">128</span>, <span class="number">60</span>])</span><br><span class="line">torch.Size([<span class="number">128</span>]) torch.Size([<span class="number">128</span>, <span class="number">6</span>]) torch.Size([<span class="number">128</span>, <span class="number">60</span>])</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line"><span class="comment">#随机梯度下降</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(NUM_EPOCHS): <span class="comment">#开始迭代</span></span><br><span class="line">    <span class="keyword">for</span> i, (input_labels, pos_labels, neg_labels) <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">        <span class="comment">#print(input_labels, pos_labels, neg_labels)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># TODO</span></span><br><span class="line">        input_labels = input_labels.long() <span class="comment">#longtensor</span></span><br><span class="line">        pos_labels = pos_labels.long()</span><br><span class="line">        neg_labels = neg_labels.long()</span><br><span class="line">        <span class="keyword">if</span> USE_CUDA: <span class="comment"># 变成cuda类型</span></span><br><span class="line">            input_labels = input_labels.cuda()</span><br><span class="line">            pos_labels = pos_labels.cuda()</span><br><span class="line">            neg_labels = neg_labels.cuda()</span><br><span class="line">       </span><br><span class="line">        <span class="comment">#下面第一节课都讲过的   </span></span><br><span class="line">        optimizer.zero_grad() <span class="comment">#梯度归零</span></span><br><span class="line">        loss = model(input_labels, pos_labels, neg_labels).mean() </span><br><span class="line">        <span class="comment"># model返回的是一个batch所有样本的损失，需要求个平均</span></span><br><span class="line">        </span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">       </span><br><span class="line">        <span class="comment">#打印结果。</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">with</span> open(LOG_FILE, <span class="string">"a"</span>) <span class="keyword">as</span> fout: <span class="comment"># 写进日志文件，LOG_FILE前面定义了</span></span><br><span class="line">                fout.write(<span class="string">"epoch: &#123;&#125;, iter: &#123;&#125;, loss: &#123;&#125;\n"</span>.format(e, i, loss.item()))</span><br><span class="line">                print(<span class="string">"epoch: &#123;&#125;, iter: &#123;&#125;, loss: &#123;&#125;"</span>.format(e, i, loss.item()))</span><br><span class="line">                <span class="comment"># 训练过程，我没跑，本地肯定跑不动的</span></span><br><span class="line">            </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">0</span>: <span class="comment"># 每过2000个batch就评估一次效果</span></span><br><span class="line">            embedding_weights = model.input_embeddings() </span><br><span class="line">            <span class="comment"># 取出（30000，100）训练的词向量</span></span><br><span class="line">            sim_simlex = evaluate(<span class="string">"simlex-999.txt"</span>, embedding_weights)</span><br><span class="line">            sim_men = evaluate(<span class="string">"men.txt"</span>, embedding_weights)</span><br><span class="line">            sim_353 = evaluate(<span class="string">"wordsim353.csv"</span>, embedding_weights)</span><br><span class="line">            <span class="keyword">with</span> open(LOG_FILE, <span class="string">"a"</span>) <span class="keyword">as</span> fout:</span><br><span class="line">                print(<span class="string">"epoch: &#123;&#125;, iteration: &#123;&#125;, simlex-999: &#123;&#125;, men: &#123;&#125;, sim353: &#123;&#125;, nearest to monster: &#123;&#125;\n"</span>.format(</span><br><span class="line">                    e, i, sim_simlex, sim_men, sim_353, find_nearest(<span class="string">"monster"</span>)))</span><br><span class="line">                fout.write(<span class="string">"epoch: &#123;&#125;, iteration: &#123;&#125;, simlex-999: &#123;&#125;, men: &#123;&#125;, sim353: &#123;&#125;, nearest to monster: &#123;&#125;\n"</span>.format(</span><br><span class="line">                    e, i, sim_simlex, sim_men, sim_353, find_nearest(<span class="string">"monster"</span>)))</span><br><span class="line">                </span><br><span class="line">    embedding_weights = model.input_embeddings() <span class="comment"># 调用最终训练好的embeding词向量</span></span><br><span class="line">    np.save(<span class="string">"embedding-&#123;&#125;"</span>.format(EMBEDDING_SIZE), embedding_weights) <span class="comment"># 保存参数</span></span><br><span class="line">    torch.save(model.state_dict(), <span class="string">"embedding-&#123;&#125;.th"</span>.format(EMBEDDING_SIZE)) <span class="comment"># 保存参数</span></span><br></pre></td></tr></table></figure>

<p>In [11]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(<span class="string">"embedding-&#123;&#125;.th"</span>.format(EMBEDDING_SIZE))) <span class="comment"># 加载模型</span></span><br></pre></td></tr></table></figure>

<h2 id="在-MEN-和-Simplex-999-数据集上做评估"><a href="#在-MEN-和-Simplex-999-数据集上做评估" class="headerlink" title="在 MEN 和 Simplex-999 数据集上做评估"></a>在 MEN 和 Simplex-999 数据集上做评估</h2><p>In [12]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 代码同上</span></span><br><span class="line">embedding_weights = model.input_embeddings()</span><br><span class="line">print(<span class="string">"simlex-999"</span>, evaluate(<span class="string">"simlex-999.txt"</span>, embedding_weights))</span><br><span class="line">print(<span class="string">"men"</span>, evaluate(<span class="string">"men.txt"</span>, embedding_weights))</span><br><span class="line">print(<span class="string">"wordsim353"</span>, evaluate(<span class="string">"wordsim353.csv"</span>, embedding_weights))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">simlex<span class="number">-999</span> SpearmanrResult(correlation=<span class="number">0.17251697429101504</span>, pvalue=<span class="number">7.863946056740345e-08</span>)</span><br><span class="line">men SpearmanrResult(correlation=<span class="number">0.1778096817088841</span>, pvalue=<span class="number">7.565661657312768e-20</span>)</span><br><span class="line">wordsim353 SpearmanrResult(correlation=<span class="number">0.27153702278146635</span>, pvalue=<span class="number">8.842165885381714e-07</span>)</span><br></pre></td></tr></table></figure>

<h2 id="寻找nearest-neighbors"><a href="#寻找nearest-neighbors" class="headerlink" title="寻找nearest neighbors"></a>寻找nearest neighbors</h2><p>In [13]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_nearest</span><span class="params">(word)</span>:</span></span><br><span class="line">    index = word_to_idx[word] </span><br><span class="line">    embedding = embedding_weights[index] <span class="comment"># 取出这个单词的embedding向量</span></span><br><span class="line">    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) <span class="keyword">for</span> e <span class="keyword">in</span> embedding_weights])</span><br><span class="line">    <span class="comment"># 计算所有30000个embedding向量与传入单词embedding向量的相似度距离</span></span><br><span class="line">    <span class="keyword">return</span> [idx_to_word[i] <span class="keyword">for</span> i <span class="keyword">in</span> cos_dis.argsort()[:<span class="number">10</span>]] <span class="comment"># 返回前10个最相似的</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> [<span class="string">"good"</span>, <span class="string">"fresh"</span>, <span class="string">"monster"</span>, <span class="string">"green"</span>, <span class="string">"like"</span>, <span class="string">"america"</span>, <span class="string">"chicago"</span>, <span class="string">"work"</span>, <span class="string">"computer"</span>, <span class="string">"language"</span>]:</span><br><span class="line">    print(word, find_nearest(word))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">good [&apos;good&apos;, &apos;bad&apos;, &apos;perfect&apos;, &apos;hard&apos;, &apos;questions&apos;, &apos;alone&apos;, &apos;money&apos;, &apos;false&apos;, &apos;truth&apos;, &apos;experience&apos;]</span><br><span class="line">fresh [&apos;fresh&apos;, &apos;grain&apos;, &apos;waste&apos;, &apos;cooling&apos;, &apos;lighter&apos;, &apos;dense&apos;, &apos;mild&apos;, &apos;sized&apos;, &apos;warm&apos;, &apos;steel&apos;]</span><br><span class="line">monster [&apos;monster&apos;, &apos;giant&apos;, &apos;robot&apos;, &apos;hammer&apos;, &apos;clown&apos;, &apos;bull&apos;, &apos;demon&apos;, &apos;triangle&apos;, &apos;storyline&apos;, &apos;slogan&apos;]</span><br><span class="line">green [&apos;green&apos;, &apos;blue&apos;, &apos;yellow&apos;, &apos;white&apos;, &apos;cross&apos;, &apos;orange&apos;, &apos;black&apos;, &apos;red&apos;, &apos;mountain&apos;, &apos;gold&apos;]</span><br><span class="line">like [&apos;like&apos;, &apos;unlike&apos;, &apos;etc&apos;, &apos;whereas&apos;, &apos;animals&apos;, &apos;soft&apos;, &apos;amongst&apos;, &apos;similarly&apos;, &apos;bear&apos;, &apos;drink&apos;]</span><br><span class="line">america [&apos;america&apos;, &apos;africa&apos;, &apos;korea&apos;, &apos;india&apos;, &apos;australia&apos;, &apos;turkey&apos;, &apos;pakistan&apos;, &apos;mexico&apos;, &apos;argentina&apos;, &apos;carolina&apos;]</span><br><span class="line">chicago [&apos;chicago&apos;, &apos;boston&apos;, &apos;illinois&apos;, &apos;texas&apos;, &apos;london&apos;, &apos;indiana&apos;, &apos;massachusetts&apos;, &apos;florida&apos;, &apos;berkeley&apos;, &apos;michigan&apos;]</span><br><span class="line">work [&apos;work&apos;, &apos;writing&apos;, &apos;job&apos;, &apos;marx&apos;, &apos;solo&apos;, &apos;label&apos;, &apos;recording&apos;, &apos;nietzsche&apos;, &apos;appearance&apos;, &apos;stage&apos;]</span><br><span class="line">computer [&apos;computer&apos;, &apos;digital&apos;, &apos;electronic&apos;, &apos;audio&apos;, &apos;video&apos;, &apos;graphics&apos;, &apos;hardware&apos;, &apos;software&apos;, &apos;computers&apos;, &apos;program&apos;]</span><br><span class="line">language [&apos;language&apos;, &apos;languages&apos;, &apos;alphabet&apos;, &apos;arabic&apos;, &apos;grammar&apos;, &apos;pronunciation&apos;, &apos;dialect&apos;, &apos;programming&apos;, &apos;chinese&apos;, &apos;spelling&apos;]</span><br></pre></td></tr></table></figure>

<h2 id="单词之间的关系"><a href="#单词之间的关系" class="headerlink" title="单词之间的关系"></a>单词之间的关系</h2><p>In [14]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">man_idx = word_to_idx[<span class="string">"man"</span>] </span><br><span class="line">king_idx = word_to_idx[<span class="string">"king"</span>] </span><br><span class="line">woman_idx = word_to_idx[<span class="string">"woman"</span>]</span><br><span class="line">embedding = embedding_weights[woman_idx] - embedding_weights[man_idx] + embedding_weights[king_idx]</span><br><span class="line">cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) <span class="keyword">for</span> e <span class="keyword">in</span> embedding_weights])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> cos_dis.argsort()[:<span class="number">20</span>]:</span><br><span class="line">    print(idx_to_word[i])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">king</span><br><span class="line">henry</span><br><span class="line">charles</span><br><span class="line">pope</span><br><span class="line">queen</span><br><span class="line">iii</span><br><span class="line">prince</span><br><span class="line">elizabeth</span><br><span class="line">alexander</span><br><span class="line">constantine</span><br><span class="line">edward</span><br><span class="line">son</span><br><span class="line">iv</span><br><span class="line">louis</span><br><span class="line">emperor</span><br><span class="line">mary</span><br><span class="line">james</span><br><span class="line">joseph</span><br><span class="line">frederick</span><br><span class="line">francis</span><br></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/word-embedding/">word-embedding</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-酒店评价情感分类与CNN模型" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/02/08/酒店评价情感分类与CNN模型/" class="article-date">
      <time datetime="2020-02-08T10:58:23.000Z" itemprop="datePublished">2020-02-08</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/08/酒店评价情感分类与CNN模型/">酒店评价情感分类与CNN模型</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h4 id="酒店评价情感分类与CNN模型"><a href="#酒店评价情感分类与CNN模型" class="headerlink" title="酒店评价情感分类与CNN模型"></a>酒店评价情感分类与CNN模型</h4><p>参考了<a href="https://github.com/bentrevett/pytorch-sentiment-analysis" target="_blank" rel="noopener">https://github.com/bentrevett/pytorch-sentiment-analysis</a></p>
<p>我们会用PyTorch模型来做情感分析(检测一段文字的情感是正面的还是负面的)。我们会使用<a href="https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/ChnSentiCorp_htl_all/intro.ipynb" target="_blank" rel="noopener">ChnSentiCorp_htl</a>数据集，即酒店评论数据集。</p>
<p>数据下载链接：<a href="https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv" target="_blank" rel="noopener">https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv</a></p>
<p>模型从简单到复杂，我们会依次构建：</p>
<ul>
<li>Word Averaging模型</li>
<li>RNN/LSTM模型</li>
<li>CNN模型</li>
</ul>
<h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><ul>
<li>首先让我们加载数据，来看看这一批酒店评价数据长得怎样</li>
</ul>
<p>In [1]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">path = &quot;ChnSentiCorp_htl_all.csv&quot;</span><br><span class="line">pd_all = pd.read_csv(path)</span><br><span class="line"></span><br><span class="line">print(&apos;评论数目（总体）：%d&apos; % pd_all.shape[0])</span><br><span class="line">print(&apos;评论数目（正向）：%d&apos; % pd_all[pd_all.label==1].shape[0])</span><br><span class="line">print(&apos;评论数目（负向）：%d&apos; % pd_all[pd_all.label==0].shape[0])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">评论数目（总体）：7766</span><br><span class="line">评论数目（正向）：5322</span><br><span class="line">评论数目（负向）：2444</span><br></pre></td></tr></table></figure>

<p>In [2]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd_all.sample(5)</span><br></pre></td></tr></table></figure>

<p>Out[2]:</p>
<table>
<thead>
<tr>
<th align="right"></th>
<th align="right">label</th>
<th align="right">review</th>
</tr>
</thead>
<tbody><tr>
<td align="right">914</td>
<td align="right">1</td>
<td align="right">地点看上去不错，在北京西客站对面，但出行十分不便，周边没有地铁，门口出租车倒是挺多，但就是不…</td>
</tr>
<tr>
<td align="right">7655</td>
<td align="right">0</td>
<td align="right">酒店位置较偏僻，环境清净，交通也方便，但酒店及周边就餐选择不多;浴场海水中有水草,水亦太浅,…</td>
</tr>
<tr>
<td align="right">3424</td>
<td align="right">1</td>
<td align="right">酒店给人感觉很温欣,服务员也挺有礼貌,房间内的舒适度也非常不错,离开李公递也很近,下次来苏州…</td>
</tr>
<tr>
<td align="right">4854</td>
<td align="right">1</td>
<td align="right">离故宫不太远，走路大概10分钟不到点，环境还好，有一点非常不好的是窗帘就只有一层，早上很早就…</td>
</tr>
<tr>
<td align="right">5852</td>
<td align="right">0</td>
<td align="right">宾馆背面就是省道,交通是方便的,停车场很大也很方便,但晚上尤其半夜路过的汽车声音很响,拖拉机…</td>
</tr>
</tbody></table>
<p>In [3]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import pkuseg</span><br><span class="line"></span><br><span class="line">seg = pkuseg.pkuseg()           # 以默认配置加载模型</span><br><span class="line">text = seg.cut(&apos;我爱北京天安门&apos;)  # 进行分词</span><br><span class="line">print(text)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&apos;我&apos;, &apos;爱&apos;, &apos;北京&apos;, &apos;天安门&apos;]</span><br></pre></td></tr></table></figure>

<p>下面我们先手工把数据分成train, dev, test三个部分</p>
<p>In [4]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">pd_all_shuf = pd_all.sample(frac=1)</span><br><span class="line"></span><br><span class="line"># 总共有多少ins</span><br><span class="line">total_num_ins = pd_all_shuf.shape[0]</span><br><span class="line">pd_train = pd_all_shuf.iloc[:int(total_num_ins*0.8)]</span><br><span class="line">pd_dev = pd_all_shuf.iloc[int(total_num_ins*0.8):int(total_num_ins*0.9)]</span><br><span class="line">pd_test = pd_all_shuf.iloc[int(total_num_ins*0.9):]</span><br><span class="line"></span><br><span class="line"># text, label</span><br><span class="line">train_text = [seg.cut(str(text)) for text in pd_train.review.tolist()]</span><br><span class="line">dev_text = [seg.cut(str(text)) for text in pd_dev.review.tolist()]</span><br><span class="line">test_text = [seg.cut(str(text)) for text in pd_test.review.tolist()]</span><br><span class="line">train_label = pd_train.label.tolist()</span><br><span class="line">dev_label = pd_dev.label.tolist()</span><br><span class="line">test_label = pd_test.label.tolist()</span><br></pre></td></tr></table></figure>

<p>In [6]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_label[0]</span><br></pre></td></tr></table></figure>

<p>Out[6]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0</span><br></pre></td></tr></table></figure>

<p>我们从训练数据构造出一个由单词到index的单词表</p>
<p>In [7]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from collections import Counter</span><br><span class="line">def build_vocab(sents, max_words=50000):</span><br><span class="line">    word_counts = Counter()</span><br><span class="line">    for sent in sents:</span><br><span class="line">        for word in sent:</span><br><span class="line">            word_counts[word] += 1</span><br><span class="line">    itos = [w for w, c in word_counts.most_common(max_words)]</span><br><span class="line">    itos = [&quot;UNK&quot;, &quot;PAD&quot;] + itos</span><br><span class="line">    stoi = &#123;w:i for i, w in enumerate(itos)&#125;</span><br><span class="line">    return itos, stoi</span><br><span class="line"></span><br><span class="line">itos, stoi = build_vocab(train_text)</span><br></pre></td></tr></table></figure>

<p>查看一下比较高频的单词</p>
<p>In [8]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">itos[:10]</span><br></pre></td></tr></table></figure>

<p>Out[8]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&apos;UNK&apos;, &apos;PAD&apos;, &apos;，&apos;, &apos;的&apos;, &apos;。&apos;, &apos;了&apos;, &apos;,&apos;, &apos;酒店&apos;, &apos;是&apos;, &apos;很&apos;]</span><br></pre></td></tr></table></figure>

<p>In [10]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stoi[&quot;酒店&quot;]</span><br></pre></td></tr></table></figure>

<p>Out[10]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">7</span><br></pre></td></tr></table></figure>

<p>我们把文本中的单词都转换成index</p>
<p>In [12]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_idx = [[stoi.get(word, stoi.get(&quot;UNK&quot;)) for word in text] for text in train_text ]</span><br><span class="line">dev_idx = [[stoi.get(word, stoi.get(&quot;UNK&quot;)) for word in text] for text in dev_text ]</span><br><span class="line">test_idx = [[stoi.get(word, stoi.get(&quot;UNK&quot;)) for word in text] for text in test_text ]</span><br></pre></td></tr></table></figure>

<p>把数据和label都转成batch</p>
<p>In [15]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def get_minibatches(text_idx, labels, batch_size=64, sort=True):</span><br><span class="line">    if sort:</span><br><span class="line">        text_idx_and_labels = sorted(list(zip(text_idx, labels)), key=lambda x: len(x[0]))</span><br><span class="line">        </span><br><span class="line">    text_idx_batches = []</span><br><span class="line">    label_batches = []</span><br><span class="line">    for i in range(0, len(text_idx), batch_size):</span><br><span class="line">        text_batch = [t for t, l in text_idx_and_labels[i:i+batch_size]]</span><br><span class="line">        label_batch = [l for t, l in text_idx_and_labels[i:i+batch_size]]</span><br><span class="line">        max_len = max([len(t) for t in text_batch])</span><br><span class="line">        text_batch_np = np.ones((len(text_batch), max_len), dtype=np.int) # batch_size * max_seq_ength</span><br><span class="line">        for i, t in enumerate(text_batch):</span><br><span class="line">            text_batch_np[i, :len(t)] = t</span><br><span class="line">        text_idx_batches.append(text_batch_np)</span><br><span class="line">        label_batches.append(np.array(label_batch))</span><br><span class="line">        </span><br><span class="line">    return text_idx_batches, label_batches</span><br><span class="line"></span><br><span class="line">train_batches, train_label_batches = get_minibatches(train_idx, train_label)</span><br><span class="line">dev_batches, dev_label_batches = get_minibatches(dev_idx, dev_label)</span><br><span class="line">test_batches, test_label_batches = get_minibatches(test_idx, test_label)</span><br></pre></td></tr></table></figure>

<p>In [17]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_batches[20]</span><br></pre></td></tr></table></figure>

<p>Out[17]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">array([[  80,  177,  149, ...,  191,    3,    1],</span><br><span class="line">       [  49,   18,   20, ...,   53,    4,    1],</span><br><span class="line">       [   7,   18,   17, ...,  702,    4,    1],</span><br><span class="line">       ...,</span><br><span class="line">       [1107, 2067,   10, ...,  748,  172,  442],</span><br><span class="line">       [ 241,    9,   19, ...,   17,   44,   30],</span><br><span class="line">       [3058,   20,    6, ...,    9,   19,   98]])</span><br></pre></td></tr></table></figure>

<p>In [18]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_label_batches[20]</span><br></pre></td></tr></table></figure>

<p>Out[18]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,</span><br><span class="line">       1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,</span><br><span class="line">       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1])</span><br></pre></td></tr></table></figure>

<ul>
<li>和之前一样，我们会设定random seeds使实验可以复现。</li>
</ul>
<p>In [19]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torchtext import data</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line">SEED = 1234</span><br><span class="line"></span><br><span class="line">torch.manual_seed(SEED)</span><br><span class="line">torch.cuda.manual_seed(SEED)</span><br><span class="line">torch.backends.cudnn.deterministic = True</span><br><span class="line"></span><br><span class="line">device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br></pre></td></tr></table></figure>

<h2 id="Word-Averaging模型"><a href="#Word-Averaging模型" class="headerlink" title="Word Averaging模型"></a>Word Averaging模型</h2><ul>
<li>我们首先介绍一个简单的Word Averaging模型。这个模型非常简单，我们把每个单词都通过<code>Embedding</code>层投射成word embedding vector，然后把一句话中的所有word vector做个平均，就是整个句子的vector表示了。接下来把这个sentence vector传入一个<code>Linear</code>层，做分类即可。</li>
</ul>
<p>In [32]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">class WordAVGModel(nn.Module):</span><br><span class="line">    def __init__(self, vocab_size, embedding_size, output_size, pad_idx, dropout_p=0.2):</span><br><span class="line">        super(WordAVGModel, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx=pad_idx)</span><br><span class="line">        self.linear = nn.Linear(embedding_size, output_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout_p) # 这个参数经常拿来调节</span><br><span class="line">    </span><br><span class="line">    def forward(self, text, mask):</span><br><span class="line">        # text: batch_size * max_seq_len</span><br><span class="line">        # mask: batch_size * max_seq_len</span><br><span class="line">        embedded = self.embed(text) # [batch_size, max_seq_len, embedding_size]</span><br><span class="line">        embedded = self.dropout(embedded)</span><br><span class="line">        # dropout</span><br><span class="line">        mask = (1. - mask.float()).unsqueeze(2) # [batch_size, seq_len, 1], 1 represents word, 0 represents padding</span><br><span class="line">        embedded = embedded * mask # [batch_size, seq_len, embedding_size]</span><br><span class="line">        # 求平均</span><br><span class="line">        sent_embed = embedded.sum(1) / (mask.sum(1) + 1e-9) # 防止mask.sum为0，那么不能除以零。</span><br><span class="line">        # dropout</span><br><span class="line">        return self.linear(sent_embed)</span><br></pre></td></tr></table></figure>

<p>In [75]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">class WordMaxModel(nn.Module):</span><br><span class="line">    def __init__(self, vocab_size, embedding_size, output_size, pad_idx, dropout_p=0.2):</span><br><span class="line">        super(WordMaxModel, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx=pad_idx)</span><br><span class="line">        self.linear = nn.Linear(embedding_size, output_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout_p) # 这个参数经常拿来调节</span><br><span class="line">    </span><br><span class="line">    def forward(self, text, mask):</span><br><span class="line">        # text: batch_size * max_seq_len</span><br><span class="line">        # mask: batch_size * max_seq_len</span><br><span class="line">        embedded = self.embed(text) # [batch_size, max_seq_len, embedding_size]</span><br><span class="line">        embedded = self.dropout(embedded)</span><br><span class="line">        embedded.masked_fill(mask.unsqueeze(2), -999999)</span><br><span class="line">        # dropout</span><br><span class="line">        sent_embed = torch.max(embedded, 1)[0]</span><br><span class="line">        # dropout</span><br><span class="line">        return self.linear(sent_embed)</span><br></pre></td></tr></table></figure>

<p>In [76]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">VOCAB_SIZE = len(itos)</span><br><span class="line">EMBEDDING_SIZE = 100</span><br><span class="line">OUTPUT_SIZE = 1</span><br><span class="line">PAD_IDX = stoi[&quot;PAD&quot;]</span><br><span class="line"></span><br><span class="line">model = WordMaxModel(vocab_size=VOCAB_SIZE, </span><br><span class="line">                     embedding_size=EMBEDDING_SIZE, </span><br><span class="line">                     output_size=OUTPUT_SIZE, </span><br><span class="line">                     pad_idx=PAD_IDX)</span><br></pre></td></tr></table></figure>

<p>In [77]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">VOCAB_SIZE</span><br></pre></td></tr></table></figure>

<p>Out[77]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">24001</span><br></pre></td></tr></table></figure>

<p>In [78]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># model</span><br><span class="line">def count_parameters(model):</span><br><span class="line">    return sum(p.numel() for p in model.parameters() if p.requires_grad)</span><br><span class="line"></span><br><span class="line">count_parameters(model)</span><br></pre></td></tr></table></figure>

<p>Out[78]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2400201</span><br></pre></td></tr></table></figure>

<p>In [79]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UNK_IDX = stoi[&quot;UNK&quot;]</span><br></pre></td></tr></table></figure>

<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>In [80]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(model.parameters())</span><br><span class="line">crit = nn.BCEWithLogitsLoss()</span><br><span class="line"></span><br><span class="line">model = model.to(device)</span><br><span class="line"># crit = crit.to(device)</span><br></pre></td></tr></table></figure>

<p>计算预测的准确率</p>
<p>In [81]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def binary_accuracy(preds, y):</span><br><span class="line">    rounded_preds = torch.round(torch.sigmoid(preds))</span><br><span class="line">    correct = (rounded_preds == y).float()</span><br><span class="line">    acc = correct.sum() / len(correct)</span><br><span class="line">    return acc</span><br></pre></td></tr></table></figure>

<p>In [82]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def train(model, text_idxs, labels, optimizer, crit):</span><br><span class="line">    epoch_loss, epoch_acc = 0., 0.</span><br><span class="line">    model.train()</span><br><span class="line">    total_len = 0.</span><br><span class="line">    for text, label in zip(text_idxs, labels):</span><br><span class="line">        text = torch.from_numpy(text).to(device)</span><br><span class="line">        label = torch.from_numpy(label).to(device)</span><br><span class="line">        mask = text == PAD_IDX</span><br><span class="line">        preds = model(text, mask).squeeze() # [batch_size, sent_length]</span><br><span class="line">        loss = crit(preds, label.float()) </span><br><span class="line">        acc = binary_accuracy(preds, label)</span><br><span class="line">        </span><br><span class="line">        # sgd</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">#         print(&quot;batch loss: &#123;&#125;&quot;.format(loss.item()))</span><br><span class="line">        </span><br><span class="line">        epoch_loss += loss.item() * len(label)</span><br><span class="line">        epoch_acc += acc.item() * len(label)</span><br><span class="line">        total_len += len(label)</span><br><span class="line">        </span><br><span class="line">    return epoch_loss / total_len, epoch_acc / total_len</span><br></pre></td></tr></table></figure>

<p>In [83]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def evaluate(model, text_idxs, labels, crit):</span><br><span class="line">    epoch_loss, epoch_acc = 0., 0.</span><br><span class="line">    model.eval()</span><br><span class="line">    total_len = 0.</span><br><span class="line">    for text, label in zip(text_idxs, labels):</span><br><span class="line">        text = torch.from_numpy(text).to(device)</span><br><span class="line">        label = torch.from_numpy(label).to(device)</span><br><span class="line">        mask = text == PAD_IDX</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            preds = model(text, mask).squeeze()</span><br><span class="line">        loss = crit(preds, label.float())</span><br><span class="line">        acc = binary_accuracy(preds, label)</span><br><span class="line">        </span><br><span class="line">        epoch_loss += loss.item() * len(label)</span><br><span class="line">        epoch_acc += acc.item() * len(label)</span><br><span class="line">        total_len += len(label)</span><br><span class="line">    model.train()</span><br><span class="line">        </span><br><span class="line">    return epoch_loss / total_len, epoch_acc / total_len</span><br></pre></td></tr></table></figure>

<p>In [84]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">N_EPOCHS = 10</span><br><span class="line">best_valid_acc = 0.</span><br><span class="line">for epoch in range(N_EPOCHS):</span><br><span class="line">    train_loss, train_acc = train(model, train_batches, train_label_batches, optimizer, crit)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, dev_batches, dev_label_batches, crit)</span><br><span class="line">    </span><br><span class="line">    if valid_acc &gt; best_valid_acc:</span><br><span class="line">        best_valid_acc = valid_acc</span><br><span class="line">        torch.save(model.state_dict(), &quot;wordavg-model.pth&quot;)</span><br><span class="line">        </span><br><span class="line">    print(&quot;Epoch&quot;, epoch, &quot;Train Loss&quot;, train_loss, &quot;Train Acc&quot;, train_acc)</span><br><span class="line">    print(&quot;Epoch&quot;, epoch, &quot;Valid Loss&quot;, valid_loss, &quot;Valid Acc&quot;, valid_acc)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0 Train Loss 0.6241851981347865 Train Acc 0.6785254346426272</span><br><span class="line">Epoch 0 Valid Loss 0.7439712684126895 Valid Acc 0.396396396434752</span><br><span class="line">Epoch 1 Train Loss 0.6111872896254639 Train Acc 0.6775595621377978</span><br><span class="line">Epoch 1 Valid Loss 0.7044879783381213 Valid Acc 0.4761904762288318</span><br><span class="line">Epoch 2 Train Loss 0.5826128212314072 Train Acc 0.7041210560206053</span><br><span class="line">Epoch 2 Valid Loss 0.667004818775172 Valid Acc 0.5791505791889348</span><br><span class="line">Epoch 3 Train Loss 0.5516750626769744 Train Acc 0.7293947198969736</span><br><span class="line">Epoch 3 Valid Loss 0.6347547087583456 Valid Acc 0.6473616476684924</span><br><span class="line">Epoch 4 Train Loss 0.5236469646921791 Train Acc 0.7512878300064392</span><br><span class="line">Epoch 4 Valid Loss 0.5953507212444928 Valid Acc 0.7348777351845799</span><br><span class="line">Epoch 5 Train Loss 0.4969042095152394 Train Acc 0.7707662588538313</span><br><span class="line">Epoch 5 Valid Loss 0.5561252224092471 Valid Acc 0.7786357789426237</span><br><span class="line">Epoch 6 Train Loss 0.46501466702892946 Train Acc 0.797005795235029</span><br><span class="line">Epoch 6 Valid Loss 0.5206214915217887 Valid Acc 0.8018018021086468</span><br><span class="line">Epoch 7 Train Loss 0.43595607032794303 Train Acc 0.8163232453316163</span><br><span class="line">Epoch 7 Valid Loss 0.4846100037776058 Valid Acc 0.8159588161889497</span><br><span class="line">Epoch 8 Train Loss 0.40671270164611334 Train Acc 0.8386992916934964</span><br><span class="line">Epoch 8 Valid Loss 0.45964578196809097 Valid Acc 0.8211068213369549</span><br><span class="line">Epoch 9 Train Loss 0.38044816804408105 Train Acc 0.8539922730199614</span><br><span class="line">Epoch 9 Valid Loss 0.4279780917953187 Valid Acc 0.8416988419289755</span><br></pre></td></tr></table></figure>

<p>In [85]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(&quot;wordavg-model.pth&quot;))</span><br></pre></td></tr></table></figure>

<p>Out[85]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;All keys matched successfully&gt;</span><br></pre></td></tr></table></figure>

<p>In [86]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def predict_sentiment(model, sentence):</span><br><span class="line">    model.eval()</span><br><span class="line">    indexed = [stoi.get(t, PAD_IDX) for t in seg.cut(sentence)]</span><br><span class="line">    tensor = torch.LongTensor(indexed).to(device) # seq_len</span><br><span class="line">    tensor = tensor.unsqueeze(0) # batch_size* seq_len </span><br><span class="line">    mask = tensor == PAD_IDX</span><br><span class="line">#     print(tensor, &quot;\n&quot;, mask)</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        pred = torch.sigmoid(model(tensor, mask))</span><br><span class="line">    return pred.item()</span><br></pre></td></tr></table></figure>

<p>In [88]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;这个酒店非常脏乱差，不推荐&quot;)</span><br></pre></td></tr></table></figure>

<p>Out[88]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.6831367611885071</span><br></pre></td></tr></table></figure>

<p>In [90]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;这个酒店非常好，强烈推荐！&quot;)</span><br></pre></td></tr></table></figure>

<p>Out[90]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.8252924680709839</span><br></pre></td></tr></table></figure>

<p>In [91]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;房间设备太破,连喷头都是不好用,空调几乎感觉不到,虽然我开了最大另外就是设备维修不及时,洗澡用品感觉都是廉价货,味道很奇怪的洗头液等等...总体感觉服务还可以,设备招待所水平...&quot;)</span><br></pre></td></tr></table></figure>

<p>Out[91]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.5120517611503601</span><br></pre></td></tr></table></figure>

<p>In [92]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;房间稍小，但清洁，非常实惠。不足之处是：双人房的洗澡用品只有一套.宾馆反馈2008年8月5日：尊敬的宾客：您好！感谢您选择入住金陵溧阳宾馆！对于酒店双人房内的洗漱用品只有一套的问题，我们已经召集酒店相关部门对此问题进行了研究和整改。努力将我们的管理与服务工作做到位，进一步关注宾客，关注细节！再次向您表示我们最衷心的感谢！期待您能再次来溧阳并入住金陵溧阳宾馆！让我们有给您提供更加优质服务的机会！顺祝您工作顺利！身体健康！金陵溧阳宾馆客务关系主任&quot;)</span><br></pre></td></tr></table></figure>

<p>Out[92]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.7319579124450684</span><br></pre></td></tr></table></figure>

<p>In [93]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;该酒店对去溧阳公务或旅游的人都很适合，自助早餐很丰富，酒店内部环境和服务很好。唯一的不足是酒店大门口在晚上时太乱，各种车辆和人在门口挤成一团。补充点评2008年5月9日：房间淋浴水压不稳，一会热、一会冷，很不好调整。宾馆反馈2008年5月13日：非常感谢您选择入住金陵溧阳宾馆。您给予我们的肯定与赞赏让我们倍受鼓舞，也使我们更加自信地去做好每一天的服务工作。正是有许多像您一样的宾客给予我们不断的鼓励和赞赏，酒店的服务品质才能得以不断提升。对于酒店大门口的秩序和房间淋浴水的问题我们已做出了相应的措施。再次向您表示我们最衷心的感谢！我们期待您的再次光临！&quot;)</span><br></pre></td></tr></table></figure>

<p>Out[93]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.793725311756134</span><br></pre></td></tr></table></figure>

<p>In [94]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;环境不错，室内色调很温馨，MM很满意！就是窗户收拾得太马虎了，拉开窗帘就觉得很凌乱的感觉。最不足的地方就是淋浴了，一是地方太小了，二是洗澡时水时大时小的，中间还停了几秒！！&quot;)</span><br></pre></td></tr></table></figure>

<p>Out[94]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.7605408430099487</span><br></pre></td></tr></table></figure>

<p>In [95]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(&quot;wordavg-model.pth&quot;))</span><br><span class="line">test_loss, test_acc = evaluate(model, test_batches, test_label_batches, crit)</span><br><span class="line">print(&quot;CNN model test loss: &quot;, test_loss, &quot;accuracy:&quot;, test_acc)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CNN model test loss:  0.44893962796897346 accuracy: 0.8133848134615247</span><br></pre></td></tr></table></figure>

<h2 id="RNN模型"><a href="#RNN模型" class="headerlink" title="RNN模型"></a>RNN模型</h2><ul>
<li><p>下面我们尝试把模型换成一个</p>
<p>recurrent neural network</p>
</li>
</ul>
<p>  (RNN)。RNN经常会被用来encode一个sequence</p>
<p>  ℎ𝑡=RNN(𝑥𝑡,ℎ𝑡−1)ht=RNN(xt,ht−1)</p>
<ul>
<li><p>我们使用最后一个hidden state ℎ𝑇hT来表示整个句子。</p>
</li>
<li><p>然后我们把ℎ𝑇hT通过一个线性变换𝑓f，然后用来预测句子的情感。</p>
</li>
</ul>
<p><img src="file:///Users/mmy/Downloads/assets/sentiment1.png" alt="img"></p>
<p><img src="file:///Users/mmy/Downloads/assets/sentiment7.png" alt="img"></p>
<p>In [57]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">class RNNModel(nn.Module):</span><br><span class="line">    def __init__(self, vocab_size, embedding_size, output_size, pad_idx, hidden_size, dropout, avg_hidden=True):</span><br><span class="line">        super(RNNModel, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx=pad_idx)</span><br><span class="line">        self.lstm = nn.LSTM(embedding_size, hidden_size, bidirectional=True, num_layers=2, batch_first=True)</span><br><span class="line">        self.linear = nn.Linear(hidden_size*2, output_size)</span><br><span class="line">            </span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.avg_hidden = avg_hidden</span><br><span class="line">    </span><br><span class="line">    def forward(self, text, mask):</span><br><span class="line">        embedded = self.embed(text) # [batch_size, seq_len, embedding_size] 其中包含一些pad</span><br><span class="line">        embedded  = self.dropout(embedded)</span><br><span class="line">        </span><br><span class="line">        # mask: batch_size * seq_length</span><br><span class="line">        seq_length = (1. - mask.float()).sum(1)</span><br><span class="line">        embedded = torch.nn.utils.rnn.pack_padded_sequence(</span><br><span class="line">            input=embedded,</span><br><span class="line">            lengths=seq_length,</span><br><span class="line">            batch_first=True,</span><br><span class="line">            enforce_sorted=False</span><br><span class="line">        ) # batch_size * seq_len * ...,    seq_len * batch_size * ...</span><br><span class="line">        output, (hidden, cell) = self.lstm(embedded)</span><br><span class="line">        output, seq_length = torch.nn.utils.rnn.pad_packed_sequence(</span><br><span class="line">            sequence=output,</span><br><span class="line">            batch_first=True,</span><br><span class="line">            padding_value=0,</span><br><span class="line">            total_length=mask.shape[1]</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        # output: [batch_size, seq_length, hidden_dim * num_directions]</span><br><span class="line">        # hidden: [num_layers * num_directions, batch_size, hidden_dim]</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        if self.avg_hidden:</span><br><span class="line">            hidden = torch.sum(output * (1. - mask.float()).unsqueeze(2), 1) / torch.sum((1. - mask.float()), 1).unsqueeze(1)</span><br><span class="line">        else:</span><br><span class="line">            # 拿最后一个hidden state作为句子的表示</span><br><span class="line">            # hidden: 2 * batch_size * hidden_size</span><br><span class="line">            hidden = torch.cat([hidden[-1], hidden[-2]], dim=1)</span><br><span class="line">            hidden = self.dropout(hidden.squeeze())</span><br><span class="line">        return self.linear(hidden)</span><br></pre></td></tr></table></figure>

<p>In [58]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = RNNModel(vocab_size=VOCAB_SIZE, </span><br><span class="line">                 embedding_size=EMBEDDING_SIZE, </span><br><span class="line">                 output_size=OUTPUT_SIZE, </span><br><span class="line">                 pad_idx=PAD_IDX, </span><br><span class="line">                 hidden_size=100, </span><br><span class="line">                 dropout=0.5)</span><br></pre></td></tr></table></figure>

<h2 id="训练RNN模型"><a href="#训练RNN模型" class="headerlink" title="训练RNN模型"></a>训练RNN模型</h2><p>In [59]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(model.parameters()) # L2</span><br><span class="line">crit = nn.BCEWithLogitsLoss()</span><br><span class="line"></span><br><span class="line">model = model.to(device)</span><br><span class="line">crit = crit.to(device)</span><br></pre></td></tr></table></figure>

<p>In [60]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">N_EPOCHS = 10</span><br><span class="line">best_valid_acc = 0.</span><br><span class="line">for epoch in range(N_EPOCHS):</span><br><span class="line">    train_loss, train_acc = train(model, train_batches, train_label_batches, optimizer, crit)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, dev_batches, dev_label_batches, crit)</span><br><span class="line">    </span><br><span class="line">    if valid_acc &gt; best_valid_acc:</span><br><span class="line">        best_valid_acc = valid_acc</span><br><span class="line">        torch.save(model.state_dict(), &quot;lstm-model.pth&quot;)</span><br><span class="line">        </span><br><span class="line">    print(&quot;Epoch&quot;, epoch, &quot;Train Loss&quot;, train_loss, &quot;Train Acc&quot;, train_acc)</span><br><span class="line">    print(&quot;Epoch&quot;, epoch, &quot;Valid Loss&quot;, valid_loss, &quot;Valid Acc&quot;, valid_acc)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0 Train Loss 0.5140281977456996 Train Acc 0.7472633612363168</span><br><span class="line">Epoch 0 Valid Loss 0.7321655497894631 Valid Acc 0.8133848134615247</span><br><span class="line">Epoch 1 Train Loss 0.4205178504441526 Train Acc 0.8209916291049582</span><br><span class="line">Epoch 1 Valid Loss 0.5658483397086155 Valid Acc 0.8391248392782616</span><br><span class="line">Epoch 2 Train Loss 0.3576773465620036 Train Acc 0.8473921442369607</span><br><span class="line">Epoch 2 Valid Loss 0.6089477152437777 Valid Acc 0.8545688548756996</span><br><span class="line">Epoch 3 Train Loss 0.3190276817504391 Train Acc 0.8647778493238892</span><br><span class="line">Epoch 3 Valid Loss 0.5731698980355968 Valid Acc 0.8622908625977073</span><br><span class="line">Epoch 4 Train Loss 0.2850390273336434 Train Acc 0.8881197681905988</span><br><span class="line">Epoch 4 Valid Loss 0.6073675444073966 Valid Acc 0.8622908625209961</span><br><span class="line">Epoch 5 Train Loss 0.26827128295812463 Train Acc 0.8884417256922086</span><br><span class="line">Epoch 5 Valid Loss 0.4971172449057934 Valid Acc 0.8700128701662925</span><br><span class="line">Epoch 6 Train Loss 0.23699480644442233 Train Acc 0.9059884095299421</span><br><span class="line">Epoch 6 Valid Loss 0.5370476412343549 Valid Acc 0.8635778636545748</span><br><span class="line">Epoch 7 Train Loss 0.22414902945487483 Train Acc 0.9072762395363811</span><br><span class="line">Epoch 7 Valid Loss 0.48257371315317876 Valid Acc 0.8725868726635838</span><br><span class="line">Epoch 8 Train Loss 0.2119196125996435 Train Acc 0.9162910495814552</span><br><span class="line">Epoch 8 Valid Loss 0.59562370292315 Valid Acc 0.8468468471536919</span><br><span class="line">Epoch 9 Train Loss 0.20756761220698194 Train Acc 0.9207984546039922</span><br><span class="line">Epoch 9 Valid Loss 0.6451035161122699 Valid Acc 0.8700128701662925</span><br></pre></td></tr></table></figure>

<p>In [62]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;沈阳市政府的酒店，比较大气，交通便利，出门往左就是北陵公园，环境好。&quot;)</span><br></pre></td></tr></table></figure>

<p>Out[62]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9994519352912903</span><br></pre></td></tr></table></figure>

<p>In [63]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;这个酒店非常脏乱差，不推荐！&quot;)</span><br></pre></td></tr></table></figure>

<p>Out[63]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.01588270254433155</span><br></pre></td></tr></table></figure>

<p>In [68]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;这个酒店不乱，非常推荐！&quot;)</span><br></pre></td></tr></table></figure>

<p>Out[68]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.04462616145610809</span><br></pre></td></tr></table></figure>

<p>在test上做模型预测</p>
<p>In [69]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(&quot;lstm-model.pth&quot;))</span><br><span class="line">test_loss, test_acc = evaluate(model, test_batches, test_label_batches, crit)</span><br><span class="line">print(&quot;CNN model test loss: &quot;, test_loss, &quot;accuracy:&quot;, test_acc)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CNN model test loss:  0.5639284941220376 accuracy: 0.8481338484406932</span><br></pre></td></tr></table></figure>

<h2 id="CNN模型"><a href="#CNN模型" class="headerlink" title="CNN模型"></a>CNN模型</h2><p>In [70]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">class CNN(nn.Module):</span><br><span class="line">    def __init__(self, vocab_size, embedding_size, output_size, pad_idx, num_filters, filter_sizes, dropout):</span><br><span class="line">        super(CNN, self).__init__()</span><br><span class="line">        self.filter_sizes = filter_sizes</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx=pad_idx)</span><br><span class="line">        self.convs = nn.ModuleList([</span><br><span class="line">            nn.Conv2d(in_channels=1, out_channels=num_filters, </span><br><span class="line">                          kernel_size=(fs, embedding_size)) </span><br><span class="line">            for fs in filter_sizes</span><br><span class="line">        ]) # 3个CNN </span><br><span class="line">        # fs实际上就是n-gram的n</span><br><span class="line">#         self.conv = nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(filter_size, embedding_size))</span><br><span class="line">        self.linear = nn.Linear(num_filters * len(filter_sizes), output_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">    </span><br><span class="line">    def forward(self, text, mask):</span><br><span class="line">        embedded = self.embed(text) # [batch_size, seq_len, embedding_size]</span><br><span class="line">        embedded = embedded.unsqueeze(1) # # [batch_size, 1, seq_len, embedding_size]</span><br><span class="line">#         conved = F.relu(self.conv(embedded)) # [batch_size, num_filters, seq_len-filter_size+1, 1]</span><br><span class="line">#         conved = conved.squeeze(3) # [batch_size, num_filters, seq_len-filter_size+1]</span><br><span class="line">        conved = [</span><br><span class="line">            F.relu(conv(embedded)).squeeze(3) for conv in self.convs</span><br><span class="line">        ] # [batch_size, num_filters, seq_len-filter_size+1]</span><br><span class="line">    </span><br><span class="line">        # [2, 5, 1, 1]</span><br><span class="line">    </span><br><span class="line">        # mask [[0, 0, 1, 1]]</span><br><span class="line">        # fs: 2</span><br><span class="line">        # [0, 0, 1]</span><br><span class="line">        conved = [</span><br><span class="line">            conv.masked_fill(mask[:, :-filter_size+1].unsqueeze(1) , -999999) for (conv, filter_size) in zip(conved, self.filter_sizes)</span><br><span class="line">        ]</span><br><span class="line">        # max over time pooling</span><br><span class="line">#         pooled = F.max_pool1d(conved, conved.shape[2]) # [batch_size, num_filters, 1]</span><br><span class="line">#         pooled = pooled.squeeze(2)</span><br><span class="line">        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]</span><br><span class="line">        pooled = torch.cat(pooled, dim=1) # batch_size, 3*num_filters</span><br><span class="line">        pooled = self.dropout(pooled)</span><br><span class="line">        </span><br><span class="line">        return self.linear(pooled)</span><br><span class="line">    </span><br><span class="line">#     Conv1d? 1x1 Conv2d?</span><br></pre></td></tr></table></figure>

<p>In [71]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">model = CNN(vocab_size=VOCAB_SIZE, </span><br><span class="line">           embedding_size=EMBEDDING_SIZE, </span><br><span class="line">           output_size=OUTPUT_SIZE, </span><br><span class="line">           pad_idx=PAD_IDX,</span><br><span class="line">           num_filters=100, </span><br><span class="line">           filter_sizes=[3,4,5],  # 3-gram, 4-gram, 5-gram</span><br><span class="line">           dropout=0.5)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters())</span><br><span class="line">crit = nn.BCEWithLogitsLoss()</span><br><span class="line"></span><br><span class="line">model = model.to(device)</span><br><span class="line">crit = crit.to(device)</span><br><span class="line"></span><br><span class="line">N_EPOCHS = 10</span><br><span class="line">best_valid_acc = 0.</span><br><span class="line">for epoch in range(N_EPOCHS):</span><br><span class="line">    train_loss, train_acc = train(model, train_batches, train_label_batches, optimizer, crit)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, dev_batches, dev_label_batches, crit)</span><br><span class="line">    </span><br><span class="line">    if valid_acc &gt; best_valid_acc:</span><br><span class="line">        best_valid_acc = valid_acc</span><br><span class="line">        torch.save(model.state_dict(), &quot;cnn-model.pth&quot;)</span><br><span class="line">        </span><br><span class="line">    print(&quot;Epoch&quot;, epoch, &quot;Train Loss&quot;, train_loss, &quot;Train Acc&quot;, train_acc)</span><br><span class="line">    print(&quot;Epoch&quot;, epoch, &quot;Valid Loss&quot;, valid_loss, &quot;Valid Acc&quot;, valid_acc)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0 Train Loss 0.5229088294452341 Train Acc 0.7443657437218287</span><br><span class="line">Epoch 0 Valid Loss 0.39319338566087847 Valid Acc 0.8108108110409445</span><br><span class="line">Epoch 1 Train Loss 0.3683148011498043 Train Acc 0.837894397939472</span><br><span class="line">Epoch 1 Valid Loss 0.3534783678778964 Valid Acc 0.840411840565263</span><br><span class="line">Epoch 2 Train Loss 0.3185185533801318 Train Acc 0.8644558918222794</span><br><span class="line">Epoch 2 Valid Loss 0.34023444222207233 Valid Acc 0.8545688547222771</span><br><span class="line">Epoch 3 Train Loss 0.27130810793366883 Train Acc 0.8889246619446233</span><br><span class="line">Epoch 3 Valid Loss 0.30879392936116173 Valid Acc 0.8648648650182874</span><br><span class="line">Epoch 4 Train Loss 0.24334710945314694 Train Acc 0.9034127495170637</span><br><span class="line">Epoch 4 Valid Loss 0.3020249246553718 Valid Acc 0.8790218791753015</span><br><span class="line">Epoch 5 Train Loss 0.2156534520195556 Train Acc 0.912105602060528</span><br><span class="line">Epoch 5 Valid Loss 0.326562241774575 Valid Acc 0.8571428572962797</span><br><span class="line">Epoch 6 Train Loss 0.189559489642123 Train Acc 0.9245009658725049</span><br><span class="line">Epoch 6 Valid Loss 0.28917587651095644 Valid Acc 0.885456885610308</span><br><span class="line">Epoch 7 Train Loss 0.16508568145445063 Train Acc 0.9356084996780425</span><br><span class="line">Epoch 7 Valid Loss 0.2982815937876241 Valid Acc 0.8790218791753015</span><br><span class="line">Epoch 8 Train Loss 0.14198238390007764 Train Acc 0.9452672247263362</span><br><span class="line">Epoch 8 Valid Loss 0.2929042390184513 Valid Acc 0.8880308881843105</span><br><span class="line">Epoch 9 Train Loss 0.11862559608529824 Train Acc 0.9552479072762395</span><br><span class="line">Epoch 9 Valid Loss 0.29382622203618247 Valid Acc 0.886743886820598</span><br></pre></td></tr></table></figure>

<p>In [72]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(&quot;cnn-model.pth&quot;))</span><br><span class="line">test_loss, test_acc = evaluate(model, test_batches, test_label_batches, crit)</span><br><span class="line">print(&quot;CNN model test loss: &quot;, test_loss, &quot;accuracy:&quot;, test_acc)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CNN model test loss:  0.32514461861537386 accuracy: 0.8674388674388674</span><br></pre></td></tr></table></figure>

<p>In [74]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(model, &quot;酒店位于昆明中心区,地理位置不错,可惜酒店服务有些差,第一天晚上可能入住的客人不多,空调根本没开,打了电话问,说是中央空调要晚上统一开,结果晚上也没开,就热了一晚上,第二天有开会的入住,晚上就有了空调,不得不说酒店经济帐作的好.房间的床太硬,睡的不好.酒店的早餐就如其他人评价一样,想法的难吃.不过携程的预订价钱还不错.&quot;)</span><br></pre></td></tr></table></figure>

<p>Out[74]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.893503725528717</span><br></pre></td></tr></table></figure>

<p>learning representation</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>


      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CNN/">CNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RNN-LSTM/">RNN/LSTM</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-NLP技术基础整理" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/01/28/NLP技术基础整理/" class="article-date">
      <time datetime="2020-01-28T09:50:41.000Z" itemprop="datePublished">2020-01-28</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/28/NLP技术基础整理/">NLP技术基础整理</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="什么是自然语言处理？"><a href="#什么是自然语言处理？" class="headerlink" title="什么是自然语言处理？"></a>什么是自然语言处理？</h2><p>自然语言处理（NLP）是一门融语言学、计算机科学、人工智能于一体的（实验性）科学，解决的是“让机器可以理解自然语言”。</p>
<p>NLP = NLU + NLG</p>
<h2 id="NLP问题的难点"><a href="#NLP问题的难点" class="headerlink" title="NLP问题的难点"></a>NLP问题的难点</h2><ul>
<li>自然语言有歧义（ambiguity），同样的含义又有不同的表达方式（variability）<ul>
<li>ambiguity：同样的一段表述能表示不同的意思</li>
<li>variability：不同的表达方式是同一个意思</li>
</ul>
</li>
</ul>
<p>coreference resolution</p>
<p>爸爸已经抱不动<strong>小明</strong>了，因为<strong>他</strong>太胖了。</p>
<p><strong>爸爸</strong>已经抱不动小明了，因为<strong>他</strong>太虚弱了。</p>
<p>WSC: GPT-2</p>
<h2 id="机器学习与NLP"><a href="#机器学习与NLP" class="headerlink" title="机器学习与NLP"></a>机器学习与NLP</h2><p>使用机器学习的方法让模型能够学到输入和输出之间的映射关系。在NLP中，输入一般都是语言文字，而输出则是各种不同的label。</p>
<h2 id="单词"><a href="#单词" class="headerlink" title="单词"></a>单词</h2><p>自然语言的基本构成单元。</p>
<h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><p>英文中的单词一般用空格隔开（标点符号等特殊情况除外），所以天然地完成了分词。中文的分词则不那么自然，需要人为分词。比较好用的分词工具：<a href="https://github.com/lancopku/pkuseg-python" target="_blank" rel="noopener">https://github.com/lancopku/pkuseg-python</a> </p>
<p>jieba</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pip install pkuseg</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> pkuseg</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>seg = pkuseg.pkuseg()           <span class="comment"># 以默认配置加载模型</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text = seg.cut(<span class="string">'我爱北京天安门'</span>)  <span class="comment"># 进行分词</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(text)</span><br><span class="line">[<span class="string">'我'</span>, <span class="string">'爱'</span>, <span class="string">'北京'</span>, <span class="string">'天安门'</span>]</span><br></pre></td></tr></table></figure>

<p>英文分词可以使用NLTK</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> nltk</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sentence = “hello, world<span class="string">"</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; tokens = nltk.word_tokenize(sentence)</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; tokens</span></span><br><span class="line"><span class="string">['hello', ‘,', 'world']</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; sents = nltk.sent_tokenize(documents)</span></span><br></pre></td></tr></table></figure>

<p>NLTK还有一些好用的功能，例如POS Tagging</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> nltk</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text = nltk.word_tokenize(<span class="string">'what does the fox say'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text</span><br><span class="line">[<span class="string">'what'</span>, <span class="string">'does'</span>, <span class="string">'the'</span>, <span class="string">'fox'</span>, <span class="string">'say'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>nltk.pos_tag(text)</span><br><span class="line">[(<span class="string">'what'</span>, <span class="string">'WDT'</span>), (<span class="string">'does'</span>, <span class="string">'VBZ'</span>), (<span class="string">'the'</span>, <span class="string">'DT'</span>), (<span class="string">'fox'</span>, <span class="string">'NNS'</span>), (<span class="string">'say'</span>, <span class="string">'VBP'</span>)]</span><br></pre></td></tr></table></figure>

<p>Named Entity Recognition</p>
<p><img src="https://uploader.shimo.im/f/56iORCYdcewm9en3.png!thumbnail" alt="img"></p>
<p>去除停用词</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="comment"># 先token一把，得到一个word_list</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="comment"># 然后filter一把</span></span><br><span class="line">filtered_words = </span><br><span class="line">[word <span class="keyword">for</span> word <span class="keyword">in</span> word_list <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stopwords.words(<span class="string">'english'</span>)]</span><br></pre></td></tr></table></figure>

<p>one hot vector [0, 0, 0, 1, 0, 0…]</p>
<h2 id="Bag-of-Words和TF-IDF"><a href="#Bag-of-Words和TF-IDF" class="headerlink" title="Bag of Words和TF-IDF"></a>Bag of Words和TF-IDF</h2><p>词包模型</p>
<p>vocab: 50000个单词</p>
<p>文本–&gt; 50000维向量</p>
<p>{a: 0, an: 1, the:2, ….}</p>
<p>[100, 50, 30, …]</p>
<p><strong>TF: Term Frequency</strong>, 衡量一个term在文档中出现得有多频繁。</p>
<p>TF(t) = (t出现在文档中的次数) / (文档中的term总数)</p>
<p>文档一个10000个单词，100个the</p>
<p>TF(the) = 0.01</p>
<p><strong>IDF: Inverse Document Frequency</strong>, 衡量一个term有多重要。</p>
<p>有些词出现的很多，但是信息量可能不大，比如’is’，’the‘，’and‘之类。</p>
<p>为了平衡，我们把罕见的词的重要性（weight）调高，把常见词的重要性调低。</p>
<p>IDF(t) = lg(文档总数 / 含有t的文档总数 + 1)</p>
<p>语料一共在3篇文章中出现，但是我们一共有100,000篇文章。IDF(julyedu) = log(100,000/3)</p>
<p><strong>TF-IDF = TF * IDF</strong></p>
<p>TFIDF词包</p>
<p>a, 100*0.000001</p>
<p>[0.0001, ]</p>
<h2 id="Distributional-Word-Vectors-词向量"><a href="#Distributional-Word-Vectors-词向量" class="headerlink" title="Distributional Word Vectors 词向量"></a>Distributional Word Vectors 词向量</h2><p>distributional semantics</p>
<p>“The distributional hypothesis in linguistics is derived from the semantic theory of language usage, i.e. words that are used and occur in the same contexts tend to purport similar meanings.”</p>
<p>如果两个单词总是在同样的语境下出现，那么表示他们之间存在某种相关性/相似性。</p>
<p>Counting Context Words</p>
<p><img src="https://uploader.shimo.im/f/OClAoRm660cjqZR3.png!thumbnail" alt="img"></p>
<p>50000 * 50000</p>
<p>50000*300  </p>
<p>300*300</p>
<p>300*50000</p>
<p>在我们定义的固定大小的context window下出现的单词组，就是co-occuring word pairs。</p>
<p>对于句子的开头和结尾，我们可以定义两个特殊的符号  &lt;s&gt;   和  &lt;/s&gt;。</p>
<h2 id="单词相似度"><a href="#单词相似度" class="headerlink" title="单词相似度"></a>单词相似度</h2><p>使用词向量间的cosine相似度（cosine 夹角）, u, v是两个词向量</p>
<p><img src="https://uploader.shimo.im/f/tQdVXIYvqV4KYDft.png!thumbnail" alt="img">= cosine(u, v)</p>
<p>单词”cooked”周围context windows最常见的单词</p>
<p><img src="https://uploader.shimo.im/f/5yQkBDvdSLMwl5D7.png!thumbnail" alt="img"></p>
<p>Pointwise Mutual Information (PMI)</p>
<p><img src="https://uploader.shimo.im/f/5y0BeYWd6FcaIvsY.png!thumbnail" alt="img"></p>
<p>独立 P(x)*P(y) = P(x, y)</p>
<p>PMI表示了事件 x 和事件 y 之间是否存在相关性。</p>
<p>与 “cooked” PMI值最高的单词</p>
<p><img src="https://uploader.shimo.im/f/ywl7Mm0fkBYm9phX.png!thumbnail" alt="img"></p>
<h2 id="如何评估词向量的好坏？"><a href="#如何评估词向量的好坏？" class="headerlink" title="如何评估词向量的好坏？"></a>如何评估词向量的好坏？</h2><p>标准化的单词相似度数据集</p>
<ul>
<li>Assign a numerical similarity score between 0 and 10 (0 = words are totally    unrelated, 10 = words are VERY closely related).</li>
</ul>
<p><img src="https://uploader.shimo.im/f/ToYFke24QkcZ1Nnc.png!thumbnail" alt="img"><img src="https://uploader.shimo.im/f/QTXGcqteVe4Gw2ly.png!thumbnail" alt="img"></p>
<p>cosine(journey, voyage) = </p>
<p>cosine(king, queen) = </p>
<p>spearman’s R 分数</p>
<p><strong>Sparse vs. dense vectors</strong></p>
<p>根据context window定义的词向量非常长，<strong>很多位置上都是0.</strong> 表示我们的信息密度是很低的</p>
<ul>
<li><p>低维度词向量更容易训练模型，占用的内存/硬盘也会比较小。</p>
</li>
<li><p>低维度词向量能够学到一些单词间的关系，例如有些单词之间是近义词。</p>
</li>
</ul>
<p>降维算法</p>
<ul>
<li><p>PCA</p>
</li>
<li><p>SVD</p>
</li>
<li><p>Brown cluster</p>
</li>
<li><p><strong>Word2Vec</strong></p>
</li>
</ul>
<h2 id="Contextualized-Word-Vectors"><a href="#Contextualized-Word-Vectors" class="headerlink" title="Contextualized Word Vectors"></a>Contextualized Word Vectors</h2><p>近两年非常流行的做法，不仅仅是针对单个单词训练词向量，而是根据单词出现的语境给出词向量，是的该词向量既包含当前单词的信息，又包含单词周围context的信息。</p>
<ul>
<li><p>BERT, RoBERTa, ALBERT, T5</p>
</li>
<li><p>GPT2</p>
</li>
</ul>
<h2 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h2><p>NLP数据集</p>
<ul>
<li>NLP数据集一般包含输入（inputs，一般是文字）和输出（outputs，一般是某种标注）。</li>
</ul>
<p>标注</p>
<ul>
<li><p>监督学习需要标注过的数据集，这些标注一般被称为ground truth。</p>
</li>
<li><p>在自然语言处理数据集中，标注往往是由人手动标注的</p>
</li>
<li><p>人们往往会对数据的标注有不同的意见，因为很多时候不同的人对同样的语言会有不同的理解。所以我们也会把这些标注称为gold standard，而不是ground truth。</p>
</li>
</ul>
<p>NLP数据集如何构建</p>
<ul>
<li><p>付钱请人标注</p>
<ul>
<li>比较传统的做法</li>
<li>研究员写下标注的guideline，然后花钱请人标注（以前一般请一些专业的语言学家）</li>
<li>标注的质量会比较高，但是成本也高</li>
<li>例如，Penn Treebank(1993)</li>
</ul>
</li>
<li><p>Croudsourcing</p>
<ul>
<li>现在比较流行</li>
<li>一般不专门训练标注者（annotator），但是可以对同一条数据取得多条样本</li>
<li>例如，Stanford Sentiment Treebank</li>
</ul>
</li>
<li><p>自然拥有标注的数据集</p>
<ul>
<li>法律文件的中英文版本，可以用于训练翻译模型</li>
<li>报纸的内容分类</li>
<li>聊天记录</li>
<li>文本摘要（新闻的全文和摘要）</li>
</ul>
</li>
</ul>
<p>标注者同意度 Annotator Agreement</p>
<ul>
<li>给定两个标注者给出的所有标注，如何计算他们之间的标注是否比较统一？<ul>
<li>相同标注的百分比？</li>
<li>Cohen’s Kappa</li>
</ul>
</li>
</ul>
<p><img src="https://uploader.shimo.im/f/tPj4cDeD0qwzoRl8.png!thumbnail" alt="img"></p>
<p>来自维基百科</p>
<ul>
<li>也有更多别的测量方法</li>
</ul>
<h2 id="常见的文本分类数据集"><a href="#常见的文本分类数据集" class="headerlink" title="常见的文本分类数据集"></a>常见的文本分类数据集</h2><p>英文：</p>
<ul>
<li>AGNEWS, DBPedia, TREC: <a href="http://nlpprogress.com/english/text_classification.html" target="_blank" rel="noopener">http://nlpprogress.com/english/text_classification.html</a></li>
</ul>
<p>中文：</p>
<ul>
<li><a href="https://github.com/SophonPlus/ChineseNlpCorpus" target="_blank" rel="noopener">https://github.com/SophonPlus/ChineseNlpCorpus</a></li>
</ul>
<h2 id="文本分类模型"><a href="#文本分类模型" class="headerlink" title="文本分类模型"></a>文本分类模型</h2><p>什么是一个分类器？</p>
<ul>
<li><p>一个从输入（inputs）特征x投射到标注y的函数</p>
</li>
<li><p>一个简单的分类器：</p>
<ul>
<li>对于输入<strong>x</strong>，给每一个label y打一个分数，score(<strong>x</strong>, y, <strong>w</strong>)，其中<strong>w</strong>是模型的参数</li>
<li>分类问题也就是选出分数最高的y：classify(<strong>x, w</strong>) = argmax_y score(<strong>x</strong>, y, <strong>w</strong>)</li>
</ul>
</li>
</ul>
<p>Modeling, Inference, Learning</p>
<p><img src="https://uploader.shimo.im/f/ZMjjEt9ljYYZOe0M.png!thumbnail" alt="img"></p>
<h2 id="Modeling"><a href="#Modeling" class="headerlink" title="Modeling"></a>Modeling</h2><p>二元情感分类</p>
<p>classify(<strong>x, w</strong>)  = argmax_y score(<strong>x,</strong> y, <strong>w</strong>)</p>
<p>如果我们采用线性模型，那么模型可以被写为</p>
<p><img src="https://uploader.shimo.im/f/OS6m4GnrJW8nxTQx.png!thumbnail" alt="img"></p>
<p>现在的问题是，我们如何定义f？对于比较常见的机器学习问题，我们的输入往往是格式固定的，但是NLP的输入一般是长度不固定的文本。这里就涉及到如何把文本转换成特征（feature）。</p>
<ul>
<li><p>过去25年：特征工程（feature engineering），人为定制，比较复杂，只适用于某一类问题</p>
</li>
<li><p>过去5年：表示学习（representation learning）, ICLR， international conference for learning representations</p>
</li>
</ul>
<p>常见的features：</p>
<p>f1: 文本是正面情感，文本包含“好”</p>
<p>f2: 文本是负面情感，文本包含“好”</p>
<p>。。。</p>
<h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><p>比较直观，给定一段话，在每个Label上打分，然后取分数最大的label作为预测。</p>
<h2 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h2><ul>
<li><p>根据训练数据得到模型权重<strong>w</strong></p>
</li>
<li><p>把数据分为训练集（train），验证集（dev, val）测试集（test）</p>
</li>
<li><p>在NLP中，我们常常使用一种learning framework: Empirical Risk Minimization</p>
<ul>
<li>损失函数（cost function）：对比模型的预测和gold standard，计算一个分数<img src="https://uploader.shimo.im/f/d4DeJ9HiUWo9s7Lm.png!thumbnail" alt="img"></li>
<li>损失函数与我们真正优化的目标要尽量保持一致</li>
<li>一般来说如果cost为0，表示我们的模型预测完全正确</li>
<li>对于文本分类来说，我们应该使用怎样的损失函数呢？</li>
</ul>
</li>
</ul>
<p>错误率：<img src="https://uploader.shimo.im/f/E9r4QuAPgOgrMABu.png!thumbnail" alt="img"></p>
<p>Risk Minimization:</p>
<p>给定训练数据 <img src="https://uploader.shimo.im/f/osePhB4L13wW9hVW.png!thumbnail" alt="img">x表示输入，y表示label</p>
<p>我们的目标是<img src="https://uploader.shimo.im/f/HsQ3wAnDSF05627e.png!thumbnail" alt="img"></p>
<p>Empirical Risk Minimization <img src="https://uploader.shimo.im/f/j8m6S8mQx20BXvuG.png!thumbnail" alt="img"></p>
<p>我们之前定义的0-1损失函数是很难优化的，因为0-1loss不连续，所以无法使用基于梯度的优化方法。</p>
<p>loss.backward() # \d loss / \d w = gradient</p>
<p>optimizer.step() # w - learning_rate*gradient</p>
<p>cost = -score(x, y_label, w) 问题：没有考虑到label之间的关系！</p>
<p>一些其他的损失函数</p>
<p><strong>perceptron loss</strong></p>
<p><img src="https://uploader.shimo.im/f/Z0tFJCnDmosbrqSl.png!thumbnail" alt="img"></p>
<p><strong>hinge loss</strong></p>
<p><img src="https://uploader.shimo.im/f/4NtGiIfAhsUB1CD0.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/IYZsqDJtKqE1xTNS.png!thumbnail" alt="img"></p>
<h3 id="Log-Loss-Cross-Entropy-Loss"><a href="#Log-Loss-Cross-Entropy-Loss" class="headerlink" title="Log Loss/Cross Entropy Loss"></a>Log Loss/Cross Entropy Loss</h3><p><img src="https://uploader.shimo.im/f/t5msLvPFMsQMSZ7n.png!thumbnail" alt="img"></p>
<p>我们之前只有score(x, y, w)，怎么样定义p_w(y | z)</p>
<ul>
<li><p>让gold standard label的条件概率尽可能大</p>
</li>
<li><p>使用softmax把score转化成概率</p>
</li>
<li><p>其中的score function可以是各种函数，例如一个神经网络</p>
</li>
</ul>
<p>损失函数往往会结合regularization 正则项</p>
<ul>
<li><p>L2 regularization <img src="https://uploader.shimo.im/f/pZ2L5AVPvWE5Hksz.png!thumbnail" alt="img"></p>
</li>
<li><p>L1 regularization <img src="https://uploader.shimo.im/f/fG2yM4re1x8ZURLk.png!thumbnail" alt="img"></p>
</li>
</ul>
<p>模型训练</p>
<ul>
<li>(stochastic, batch) gradient descent</li>
</ul>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p>语言模型：给句子计算一个概率</p>
<p>为什么会有这样一个奇怪的任务？</p>
<ul>
<li><p>机器翻译：P（我喜欢吃水果）&gt; P（我喜欢喝水果）</p>
</li>
<li><p>拼写检查：P（我想吃饭）&gt; P（我像吃饭）</p>
</li>
<li><p>语音识别：P （我看见了一架飞机）&gt; P（我看见了一架斐济）</p>
</li>
<li><p>summarizaton, question answering, etc. </p>
</li>
</ul>
<p>文本自动补全。。。</p>
<h2 id="概率语言模型（probablistic-language-modeling）"><a href="#概率语言模型（probablistic-language-modeling）" class="headerlink" title="概率语言模型（probablistic language modeling）"></a>概率语言模型（probablistic language modeling）</h2><ul>
<li><p>目标：计算一串单词连成一个句子的概率 P(<strong>w</strong>) = P(w_1, …, w_n)</p>
</li>
<li><p>相关的任务 P(w_4|w_1, …, w_3) </p>
</li>
<li><p>这两个任务的模型都称之为语言模型</p>
</li>
</ul>
<p>条件概率</p>
<p><img src="https://uploader.shimo.im/f/jnAscu6ZX6Ua0iKb.png!thumbnail" alt="img"></p>
<p>马尔科夫假设</p>
<ul>
<li><p>上述条件概率公式只取决于最近的n-1个单词 P(w_i|w_1, …, w_{i-1}) = P(w_i | w_{i-n+1}, …, w_{i-1})</p>
</li>
<li><p>我们创建出了n-gram模型</p>
</li>
<li><p>简单的案例，bigram模型</p>
</li>
</ul>
<p><img src="https://uploader.shimo.im/f/I2HL7hp9YtQpnrCY.png!thumbnail" alt="img"></p>
<p>一些Smoothing方法</p>
<ul>
<li>“Add-1” estimation</li>
</ul>
<p><img src="https://uploader.shimo.im/f/K8GCxNZ0tvAHbT6k.png!thumbnail" alt="img"></p>
<ul>
<li><p>Backoff，如果一些trigram存在，就是用trigram，如果不存在，就是用bigram，如果bigram也不存在，就退而求其次使用unigram。</p>
</li>
<li><p>interpolation：混合使用unigram, bigram, trigram</p>
</li>
</ul>
<p>Perplexity: 用于评估语言模型的好坏。评估的语言，我现在给你一套比较好的语言，我希望自己的语言模型能够给这段话尽可能高的分数。</p>
<p><img src="https://uploader.shimo.im/f/tq4vTXGrt80lNPky.png!thumbnail" alt="img"> l 越大越好，-l 越小越好</p>
<p><img src="https://uploader.shimo.im/f/QGDnJuSZy4U48KFA.png!thumbnail" alt="img">PP 越小越好 困惑度</p>
<p>perplexity越低 = 模型越好</p>
<h2 id="简单的Trigram神经网络语言模型"><a href="#简单的Trigram神经网络语言模型" class="headerlink" title="简单的Trigram神经网络语言模型"></a>简单的Trigram神经网络语言模型</h2><p><img src="https://uploader.shimo.im/f/CJrPG8AsEUktoMmU.png!thumbnail" alt="img"></p>
<p>这个模型可以使用log loss来训练。</p>
<p>我们还可以在这个模型的基础上增加hidden layer </p>
<p><img src="https://uploader.shimo.im/f/bV1ttfAFazcOcXpj.png!thumbnail" alt="img"></p>
<h2 id="循环神经网络（Recurrent-Neural-Networks）"><a href="#循环神经网络（Recurrent-Neural-Networks）" class="headerlink" title="循环神经网络（Recurrent Neural Networks）"></a>循环神经网络（Recurrent Neural Networks）</h2><p><img src="https://uploader.shimo.im/f/WUuDdC1NvcoEvYtG.png!thumbnail" alt="img"></p>
<p>基于循环神经网络的语言模型</p>
<p><img src="https://uploader.shimo.im/f/xveC6xbloAUcZNsk.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/4OYGZmwTiZIi45JQ.png!thumbnail" alt="img"></p>
<p>Long Short-term Memory</p>
<p><img src="https://uploader.shimo.im/f/LAcBJG4tRCEhosvb.png!thumbnail" alt="img"></p>
<p>Gates</p>
<p><img src="https://uploader.shimo.im/f/ALMg6pQ29vIRto5c.png!thumbnail" alt="img"></p>
<p>Gated Recurrent Unit (GRU)</p>
<p><img src="https://uploader.shimo.im/f/F7xsa6NCDqQwhXLD.png!thumbnail" alt="img"></p>
<p><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><p><img src="https://uploader.shimo.im/f/w3BqZxsJscgnTdOo.png!thumbnail" alt="img"></p>
<p>我们的目标是利用没有标注过的纯文本训练有用的词向量（word vectors）</p>
<p>skip-gram (window size = 5)</p>
<blockquote>
<p>agriculture is the tradional mainstay of the cambodian economy . but benares has been destroyed by an earthquake.</p>
</blockquote>
<p><img src="https://uploader.shimo.im/f/HCdEvmDFLjQaKEjy.png!thumbnail" alt="img"></p>
<p>skip-gram中使用的score function</p>
<p><img src="https://uploader.shimo.im/f/2zIZmqQGMHspT70Y.png!thumbnail" alt="img"></p>
<p>模型参数，所有的单词的词向量，包括输入向量和输出向量</p>
<h2 id="learning"><a href="#learning" class="headerlink" title="learning"></a>learning</h2><p><img src="https://uploader.shimo.im/f/N7MJfS38rIYSTgPJ.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/gONaJMt6zNYKXwOw.png!thumbnail" alt="img"></p>
<p>注意这个概率模型需要汇总单词表中的所有单词，计算量非常之大</p>
<h2 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h2><p><img src="https://uploader.shimo.im/f/3z40P9PADe4YxhYr.png!thumbnail" alt="img"></p>
<p>随机生成一些负例，然后优化以上损失函数</p>
<p><img src="https://uploader.shimo.im/f/sa2fnlo4tvA5zP4I.png!thumbnail" alt="img"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RNN/">RNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/语言模型/">语言模型</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-CNN-Image-Classification" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/01/18/CNN-Image-Classification/" class="article-date">
      <time datetime="2020-01-18T11:15:32.000Z" itemprop="datePublished">2020-01-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/18/CNN-Image-Classification/">CNN-Image-Classification</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line">print(<span class="string">"PyTorch Version: "</span>,torch.__version__)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PyTorch Version:  1.0.0</span><br></pre></td></tr></table></figure>

<p>首先我们定义一个基于ConvNet的简单神经网络</p>
<p>In [4]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">50</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">4</span>*<span class="number">4</span>*<span class="number">50</span>, <span class="number">500</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">500</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        x = F.relu(self.conv2(x))</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">4</span>*<span class="number">4</span>*<span class="number">50</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>NLL loss的定义</p>
<p>ℓ(𝑥,𝑦)=𝐿={𝑙1,…,𝑙𝑁}⊤,𝑙𝑛=−𝑤𝑦𝑛𝑥𝑛,𝑦𝑛,𝑤𝑐=weight[𝑐]⋅𝟙{𝑐≠ignore_index}ℓ(x,y)=L={l1,…,lN}⊤,ln=−wynxn,yn,wc=weight[c]⋅1{c≠ignore_index}</p>
<p>In [7]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, device, train_loader, optimizer, epoch, log_interval=<span class="number">100</span>)</span>:</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        data, target = data.to(device), target.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data)</span><br><span class="line">        loss = F.nll_loss(output, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % log_interval == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:0f&#125;%)]\tLoss: &#123;:.6f&#125;"</span>.format(</span><br><span class="line">                epoch, batch_idx * len(data), len(train_loader.dataset), </span><br><span class="line">                <span class="number">100.</span> * batch_idx / len(train_loader), loss.item()</span><br><span class="line">            ))</span><br></pre></td></tr></table></figure>

<p>In [8]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(model, device, test_loader)</span>:</span></span><br><span class="line">    model.eval()</span><br><span class="line">    test_loss = <span class="number">0</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line">            data, target = data.to(device), target.to(device)</span><br><span class="line">            output = model(data)</span><br><span class="line">            test_loss += F.nll_loss(output, target, reduction=<span class="string">'sum'</span>).item() <span class="comment"># sum up batch loss</span></span><br><span class="line">            pred = output.argmax(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) <span class="comment"># get the index of the max log-probability</span></span><br><span class="line">            correct += pred.eq(target.view_as(pred)).sum().item()</span><br><span class="line"></span><br><span class="line">    test_loss /= len(test_loader.dataset)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\n'</span>.format(</span><br><span class="line">        test_loss, correct, len(test_loader.dataset),</span><br><span class="line">        <span class="number">100.</span> * correct / len(test_loader.dataset)))</span><br></pre></td></tr></table></figure>

<p>In [13]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">53113</span>)</span><br><span class="line"></span><br><span class="line">use_cuda = torch.cuda.is_available()</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> use_cuda <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">batch_size = test_batch_size = <span class="number">32</span></span><br><span class="line">kwargs = &#123;<span class="string">'num_workers'</span>: <span class="number">1</span>, <span class="string">'pin_memory'</span>: <span class="literal">True</span>&#125; <span class="keyword">if</span> use_cuda <span class="keyword">else</span> &#123;&#125;</span><br><span class="line">train_loader = torch.utils.data.DataLoader(</span><br><span class="line">    datasets.MNIST(<span class="string">'./mnist_data'</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>,</span><br><span class="line">                   transform=transforms.Compose([</span><br><span class="line">                       transforms.ToTensor(),</span><br><span class="line">                       transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">                   ])),</span><br><span class="line">    batch_size=batch_size, shuffle=<span class="literal">True</span>, **kwargs)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">    datasets.MNIST(<span class="string">'./mnist_data'</span>, train=<span class="literal">False</span>, transform=transforms.Compose([</span><br><span class="line">                       transforms.ToTensor(),</span><br><span class="line">                       transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">                   ])),</span><br><span class="line">    batch_size=test_batch_size, shuffle=<span class="literal">True</span>, **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line">momentum = <span class="number">0.5</span></span><br><span class="line">model = Net().to(device)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">2</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">    train(model, device, train_loader, optimizer, epoch)</span><br><span class="line">    test(model, device, test_loader)</span><br><span class="line"></span><br><span class="line">save_model = <span class="literal">True</span></span><br><span class="line"><span class="keyword">if</span> (save_model):</span><br><span class="line">    torch.save(model.state_dict(),<span class="string">"mnist_cnn.pt"</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">Train Epoch: 1 [0/60000 (0.000000%)]	Loss: 2.297938</span><br><span class="line">Train Epoch: 1 [3200/60000 (5.333333%)]	Loss: 0.567845</span><br><span class="line">Train Epoch: 1 [6400/60000 (10.666667%)]	Loss: 0.206370</span><br><span class="line">Train Epoch: 1 [9600/60000 (16.000000%)]	Loss: 0.094653</span><br><span class="line">Train Epoch: 1 [12800/60000 (21.333333%)]	Loss: 0.180530</span><br><span class="line">Train Epoch: 1 [16000/60000 (26.666667%)]	Loss: 0.041645</span><br><span class="line">Train Epoch: 1 [19200/60000 (32.000000%)]	Loss: 0.135092</span><br><span class="line">Train Epoch: 1 [22400/60000 (37.333333%)]	Loss: 0.054001</span><br><span class="line">Train Epoch: 1 [25600/60000 (42.666667%)]	Loss: 0.111863</span><br><span class="line">Train Epoch: 1 [28800/60000 (48.000000%)]	Loss: 0.059039</span><br><span class="line">Train Epoch: 1 [32000/60000 (53.333333%)]	Loss: 0.089227</span><br><span class="line">Train Epoch: 1 [35200/60000 (58.666667%)]	Loss: 0.186015</span><br><span class="line">Train Epoch: 1 [38400/60000 (64.000000%)]	Loss: 0.093208</span><br><span class="line">Train Epoch: 1 [41600/60000 (69.333333%)]	Loss: 0.077090</span><br><span class="line">Train Epoch: 1 [44800/60000 (74.666667%)]	Loss: 0.038075</span><br><span class="line">Train Epoch: 1 [48000/60000 (80.000000%)]	Loss: 0.036247</span><br><span class="line">Train Epoch: 1 [51200/60000 (85.333333%)]	Loss: 0.052358</span><br><span class="line">Train Epoch: 1 [54400/60000 (90.666667%)]	Loss: 0.013201</span><br><span class="line">Train Epoch: 1 [57600/60000 (96.000000%)]	Loss: 0.036660</span><br><span class="line"></span><br><span class="line">Test set: Average loss: 0.0644, Accuracy: 9802/10000 (98%)</span><br><span class="line"></span><br><span class="line">Train Epoch: 2 [0/60000 (0.000000%)]	Loss: 0.054402</span><br><span class="line">Train Epoch: 2 [3200/60000 (5.333333%)]	Loss: 0.032239</span><br><span class="line">Train Epoch: 2 [6400/60000 (10.666667%)]	Loss: 0.092350</span><br><span class="line">Train Epoch: 2 [9600/60000 (16.000000%)]	Loss: 0.058544</span><br><span class="line">Train Epoch: 2 [12800/60000 (21.333333%)]	Loss: 0.029762</span><br><span class="line">Train Epoch: 2 [16000/60000 (26.666667%)]	Loss: 0.012521</span><br><span class="line">Train Epoch: 2 [19200/60000 (32.000000%)]	Loss: 0.101891</span><br><span class="line">Train Epoch: 2 [22400/60000 (37.333333%)]	Loss: 0.127773</span><br><span class="line">Train Epoch: 2 [25600/60000 (42.666667%)]	Loss: 0.009259</span><br><span class="line">Train Epoch: 2 [28800/60000 (48.000000%)]	Loss: 0.013482</span><br><span class="line">Train Epoch: 2 [32000/60000 (53.333333%)]	Loss: 0.039676</span><br><span class="line">Train Epoch: 2 [35200/60000 (58.666667%)]	Loss: 0.016707</span><br><span class="line">Train Epoch: 2 [38400/60000 (64.000000%)]	Loss: 0.168691</span><br><span class="line">Train Epoch: 2 [41600/60000 (69.333333%)]	Loss: 0.056318</span><br><span class="line">Train Epoch: 2 [44800/60000 (74.666667%)]	Loss: 0.008174</span><br><span class="line">Train Epoch: 2 [48000/60000 (80.000000%)]	Loss: 0.075149</span><br><span class="line">Train Epoch: 2 [51200/60000 (85.333333%)]	Loss: 0.205798</span><br><span class="line">Train Epoch: 2 [54400/60000 (90.666667%)]	Loss: 0.019762</span><br><span class="line">Train Epoch: 2 [57600/60000 (96.000000%)]	Loss: 0.012056</span><br><span class="line"></span><br><span class="line">Test set: Average loss: 0.0464, Accuracy: 9850/10000 (98%)</span><br></pre></td></tr></table></figure>

<p>In [15]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">53113</span>)</span><br><span class="line"></span><br><span class="line">use_cuda = torch.cuda.is_available()</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> use_cuda <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">batch_size = test_batch_size = <span class="number">32</span></span><br><span class="line">kwargs = &#123;<span class="string">'num_workers'</span>: <span class="number">1</span>, <span class="string">'pin_memory'</span>: <span class="literal">True</span>&#125; <span class="keyword">if</span> use_cuda <span class="keyword">else</span> &#123;&#125;</span><br><span class="line">train_loader = torch.utils.data.DataLoader(</span><br><span class="line">    datasets.FashionMNIST(<span class="string">'./fashion_mnist_data'</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>,</span><br><span class="line">                   transform=transforms.Compose([</span><br><span class="line">                       transforms.ToTensor(),</span><br><span class="line">                       transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">                   ])),</span><br><span class="line">    batch_size=batch_size, shuffle=<span class="literal">True</span>, **kwargs)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">    datasets.FashionMNIST(<span class="string">'./fashion_mnist_data'</span>, train=<span class="literal">False</span>, transform=transforms.Compose([</span><br><span class="line">                       transforms.ToTensor(),</span><br><span class="line">                       transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">                   ])),</span><br><span class="line">    batch_size=test_batch_size, shuffle=<span class="literal">True</span>, **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line">momentum = <span class="number">0.5</span></span><br><span class="line">model = Net().to(device)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">2</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">    train(model, device, train_loader, optimizer, epoch)</span><br><span class="line">    test(model, device, test_loader)</span><br><span class="line"></span><br><span class="line">save_model = <span class="literal">True</span></span><br><span class="line"><span class="keyword">if</span> (save_model):</span><br><span class="line">    torch.save(model.state_dict(),<span class="string">"fashion_mnist_cnn.pt"</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz</span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz</span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz</span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz</span><br><span class="line">Processing...</span><br><span class="line">Done!</span><br><span class="line">Train Epoch: 1 [0/60000 (0.000000%)]	Loss: 2.279603</span><br><span class="line">Train Epoch: 1 [3200/60000 (5.333333%)]	Loss: 0.962251</span><br><span class="line">Train Epoch: 1 [6400/60000 (10.666667%)]	Loss: 1.019635</span><br><span class="line">Train Epoch: 1 [9600/60000 (16.000000%)]	Loss: 0.544330</span><br><span class="line">Train Epoch: 1 [12800/60000 (21.333333%)]	Loss: 0.629807</span><br><span class="line">Train Epoch: 1 [16000/60000 (26.666667%)]	Loss: 0.514437</span><br><span class="line">Train Epoch: 1 [19200/60000 (32.000000%)]	Loss: 0.555741</span><br><span class="line">Train Epoch: 1 [22400/60000 (37.333333%)]	Loss: 0.528186</span><br><span class="line">Train Epoch: 1 [25600/60000 (42.666667%)]	Loss: 0.656440</span><br><span class="line">Train Epoch: 1 [28800/60000 (48.000000%)]	Loss: 0.294654</span><br><span class="line">Train Epoch: 1 [32000/60000 (53.333333%)]	Loss: 0.293626</span><br><span class="line">Train Epoch: 1 [35200/60000 (58.666667%)]	Loss: 0.227645</span><br><span class="line">Train Epoch: 1 [38400/60000 (64.000000%)]	Loss: 0.473842</span><br><span class="line">Train Epoch: 1 [41600/60000 (69.333333%)]	Loss: 0.724678</span><br><span class="line">Train Epoch: 1 [44800/60000 (74.666667%)]	Loss: 0.519580</span><br><span class="line">Train Epoch: 1 [48000/60000 (80.000000%)]	Loss: 0.465854</span><br><span class="line">Train Epoch: 1 [51200/60000 (85.333333%)]	Loss: 0.378200</span><br><span class="line">Train Epoch: 1 [54400/60000 (90.666667%)]	Loss: 0.503832</span><br><span class="line">Train Epoch: 1 [57600/60000 (96.000000%)]	Loss: 0.616502</span><br><span class="line"></span><br><span class="line">Test set: Average loss: 0.4365, Accuracy: 8425/10000 (84%)</span><br><span class="line"></span><br><span class="line">Train Epoch: 2 [0/60000 (0.000000%)]	Loss: 0.385171</span><br><span class="line">Train Epoch: 2 [3200/60000 (5.333333%)]	Loss: 0.329045</span><br><span class="line">Train Epoch: 2 [6400/60000 (10.666667%)]	Loss: 0.308792</span><br><span class="line">Train Epoch: 2 [9600/60000 (16.000000%)]	Loss: 0.360471</span><br><span class="line">Train Epoch: 2 [12800/60000 (21.333333%)]	Loss: 0.445865</span><br><span class="line">Train Epoch: 2 [16000/60000 (26.666667%)]	Loss: 0.357145</span><br><span class="line">Train Epoch: 2 [19200/60000 (32.000000%)]	Loss: 0.376523</span><br><span class="line">Train Epoch: 2 [22400/60000 (37.333333%)]	Loss: 0.389735</span><br><span class="line">Train Epoch: 2 [25600/60000 (42.666667%)]	Loss: 0.308655</span><br><span class="line">Train Epoch: 2 [28800/60000 (48.000000%)]	Loss: 0.352300</span><br><span class="line">Train Epoch: 2 [32000/60000 (53.333333%)]	Loss: 0.499613</span><br><span class="line">Train Epoch: 2 [35200/60000 (58.666667%)]	Loss: 0.282398</span><br><span class="line">Train Epoch: 2 [38400/60000 (64.000000%)]	Loss: 0.330232</span><br><span class="line">Train Epoch: 2 [41600/60000 (69.333333%)]	Loss: 0.430427</span><br><span class="line">Train Epoch: 2 [44800/60000 (74.666667%)]	Loss: 0.406084</span><br><span class="line">Train Epoch: 2 [48000/60000 (80.000000%)]	Loss: 0.443538</span><br><span class="line">Train Epoch: 2 [51200/60000 (85.333333%)]	Loss: 0.348947</span><br><span class="line">Train Epoch: 2 [54400/60000 (90.666667%)]	Loss: 0.424920</span><br><span class="line">Train Epoch: 2 [57600/60000 (96.000000%)]	Loss: 0.231494</span><br><span class="line"></span><br><span class="line">Test set: Average loss: 0.3742, Accuracy: 8652/10000 (87%)</span><br></pre></td></tr></table></figure>

<h3 id="CNN模型的迁移学习"><a href="#CNN模型的迁移学习" class="headerlink" title="CNN模型的迁移学习"></a>CNN模型的迁移学习</h3><ul>
<li>很多时候当我们需要训练一个新的图像分类任务，我们不会完全从一个随机的模型开始训练，而是利用_预训练_的模型来加速训练的过程。我们经常使用在<code>ImageNet</code>上的预训练模型。</li>
<li>这是一种transfer learning的方法。我们常用以下两种方法做迁移学习。<ul>
<li>fine tuning: 从一个预训练模型开始，我们改变一些模型的架构，然后继续训练整个模型的参数。</li>
<li>feature extraction: 我们不再改变与训练模型的参数，而是只更新我们改变过的部分模型参数。我们之所以叫它feature extraction是因为我们把预训练的CNN模型当做一个特征提取模型，利用提取出来的特征做来完成我们的训练任务。</li>
</ul>
</li>
</ul>
<p>以下是构建和训练迁移学习模型的基本步骤：</p>
<ul>
<li>初始化预训练模型</li>
<li>把最后一层的输出层改变成我们想要分的类别总数</li>
<li>定义一个optimizer来更新参数</li>
<li>模型训练</li>
</ul>
<p>In [87]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms, models</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">print(<span class="string">"Torchvision Version: "</span>,torchvision.__version__)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Torchvision Version:  0.2.0</span><br></pre></td></tr></table></figure>

<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>我们会使用<em>hymenoptera_data</em>数据集，<a href="https://download.pytorch.org/tutorial/hymenoptera_data.zip" target="_blank" rel="noopener">下载</a>.</p>
<p>这个数据集包括两类图片, <strong>bees</strong> 和 <strong>ants</strong>, 这些数据都被处理成了可以使用<code>ImageFolder &lt;https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder&gt;</code>来读取的格式。我们只需要把<code>data_dir</code>设置成数据的根目录，然后把<code>model_name</code>设置成我们想要使用的与训练模型： :: [resnet, alexnet, vgg, squeezenet, densenet, inception]</p>
<p>其他的参数有：</p>
<ul>
<li><code>num_classes</code>表示数据集分类的类别数</li>
<li><code>batch_size</code></li>
<li><code>num_epochs</code></li>
<li><code>feature_extract</code>表示我们训练的时候使用fine tuning还是feature extraction方法。如果<code>feature_extract = False</code>，整个模型都会被同时更新。如果<code>feature_extract = True</code>，只有模型的最后一层被更新。</li>
</ul>
<p>In [36]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Top level data directory. Here we assume the format of the directory conforms </span><br><span class="line">#   to the ImageFolder structure</span><br><span class="line">data_dir = &quot;./hymenoptera_data&quot;</span><br><span class="line"># Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]</span><br><span class="line">model_name = &quot;resnet&quot;</span><br><span class="line"># Number of classes in the dataset</span><br><span class="line">num_classes = 2</span><br><span class="line"># Batch size for training (change depending on how much memory you have)</span><br><span class="line">batch_size = 32</span><br><span class="line"># Number of epochs to train for </span><br><span class="line">num_epochs = 15</span><br><span class="line"># Flag for feature extracting. When False, we finetune the whole model, </span><br><span class="line">#   when True we only update the reshaped layer params</span><br><span class="line">feature_extract = True</span><br></pre></td></tr></table></figure>

<p>In [120]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model, dataloaders, criterion, optimizer, num_epochs=<span class="number">5</span>)</span>:</span></span><br><span class="line">    since = time.time()</span><br><span class="line">    val_acc_history = []</span><br><span class="line">    best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line">    best_acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        print(<span class="string">"Epoch &#123;&#125;/&#123;&#125;"</span>.format(epoch, num_epochs<span class="number">-1</span>))</span><br><span class="line">        print(<span class="string">"-"</span>*<span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">"train"</span>, <span class="string">"val"</span>]:</span><br><span class="line">            running_loss = <span class="number">0.</span></span><br><span class="line">            running_corrects = <span class="number">0.</span></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">"train"</span>:</span><br><span class="line">                model.train()</span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                model.eval()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[phase]:</span><br><span class="line">                inputs = inputs.to(device)</span><br><span class="line">                labels = labels.to(device)</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">with</span> torch.autograd.set_grad_enabled(phase==<span class="string">"train"</span>):</span><br><span class="line">                    outputs = model(inputs)</span><br><span class="line">                    loss = criterion(outputs, labels)</span><br><span class="line">                    </span><br><span class="line">                _, preds = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">if</span> phase == <span class="string">"train"</span>:</span><br><span class="line">                    optimizer.zero_grad()</span><br><span class="line">                    loss.backward()</span><br><span class="line">                    optimizer.step()</span><br><span class="line">                    </span><br><span class="line">                running_loss += loss.item() * inputs.size(<span class="number">0</span>)</span><br><span class="line">                running_corrects += torch.sum(preds.view(<span class="number">-1</span>) == labels.view(<span class="number">-1</span>)).item()</span><br><span class="line">            </span><br><span class="line">            epoch_loss = running_loss / len(dataloaders[phase].dataset)</span><br><span class="line">            epoch_acc = running_corrects / len(dataloaders[phase].dataset)</span><br><span class="line">       </span><br><span class="line">            print(<span class="string">"&#123;&#125; Loss: &#123;&#125; Acc: &#123;&#125;"</span>.format(phase, epoch_loss, epoch_acc))</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">"val"</span> <span class="keyword">and</span> epoch_acc &gt; best_acc:</span><br><span class="line">                best_acc = epoch_acc</span><br><span class="line">                best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">"val"</span>:</span><br><span class="line">                val_acc_history.append(epoch_acc)</span><br><span class="line">            </span><br><span class="line">        print()</span><br><span class="line">    </span><br><span class="line">    time_elapsed = time.time() - since</span><br><span class="line">    print(<span class="string">"Training compete in &#123;&#125;m &#123;&#125;s"</span>.format(time_elapsed // <span class="number">60</span>, time_elapsed % <span class="number">60</span>))</span><br><span class="line">    print(<span class="string">"Best val Acc: &#123;&#125;"</span>.format(best_acc))</span><br><span class="line">    </span><br><span class="line">    model.load_state_dict(best_model_wts)</span><br><span class="line">    <span class="keyword">return</span> model, val_acc_history</span><br></pre></td></tr></table></figure>

<p>In [121]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># it = iter(dataloaders_dict[&quot;train&quot;])</span><br><span class="line"># inputs, labels = next(it)</span><br><span class="line"># for inputs, labels in dataloaders_dict[&quot;train&quot;]:</span><br><span class="line">#     print(labels.size())</span><br></pre></td></tr></table></figure>

<p>In [122]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(dataloaders_dict[&quot;train&quot;].dataset.imgs)</span><br></pre></td></tr></table></figure>

<p>Out[122]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">244</span><br></pre></td></tr></table></figure>

<p>In [123]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(dataloaders_dict[&quot;train&quot;].dataset)</span><br></pre></td></tr></table></figure>

<p>Out[123]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">244</span><br></pre></td></tr></table></figure>

<p>In [124]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def set_parameter_requires_grad(model, feature_extracting):</span><br><span class="line">    if feature_extracting:</span><br><span class="line">        for param in model.parameters():</span><br><span class="line">            param.requires_grad = False</span><br></pre></td></tr></table></figure>

<p>In [125]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_model</span><span class="params">(model_name, num_classes, feature_extract, use_pretrained=True)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> model_name == <span class="string">"resnet"</span>:</span><br><span class="line">        model_ft = models.resnet18(pretrained=use_pretrained)</span><br><span class="line">        set_parameter_requires_grad(model_ft, feature_extract)</span><br><span class="line">        num_ftrs = model_ft.fc.in_features</span><br><span class="line">        model_ft.fc = nn.Linear(num_ftrs, num_classes)</span><br><span class="line">        input_size = <span class="number">224</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> model_ft, input_size</span><br><span class="line">model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=<span class="literal">True</span>)</span><br><span class="line">print(model_ft)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">ResNet(</span><br><span class="line">  (conv1): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">7</span>, <span class="number">7</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">3</span>, <span class="number">3</span>), bias=<span class="literal">False</span>)</span><br><span class="line">  (bn1): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">  (relu): ReLU(inplace)</span><br><span class="line">  (maxpool): MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">  (layer1): Sequential(</span><br><span class="line">    (<span class="number">0</span>): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (relu): ReLU(inplace)</span><br><span class="line">      (conv2): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (relu): ReLU(inplace)</span><br><span class="line">      (conv2): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer2): Sequential(</span><br><span class="line">    (<span class="number">0</span>): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (relu): ReLU(inplace)</span><br><span class="line">      (conv2): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (relu): ReLU(inplace)</span><br><span class="line">      (conv2): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer3): Sequential(</span><br><span class="line">    (<span class="number">0</span>): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (relu): ReLU(inplace)</span><br><span class="line">      (conv2): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (relu): ReLU(inplace)</span><br><span class="line">      (conv2): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer4): Sequential(</span><br><span class="line">    (<span class="number">0</span>): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (relu): ReLU(inplace)</span><br><span class="line">      (conv2): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      (relu): ReLU(inplace)</span><br><span class="line">      (conv2): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AvgPool2d(kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">  (fc): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">2</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h2 id="读入数据"><a href="#读入数据" class="headerlink" title="读入数据"></a>读入数据</h2><p>现在我们知道了模型输入的size，我们就可以把数据预处理成相应的格式。</p>
<p>In [126]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">data_transforms = &#123;</span><br><span class="line">    <span class="string">"train"</span>: transforms.Compose([</span><br><span class="line">        transforms.RandomResizedCrop(input_size),</span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">    <span class="string">"val"</span>: transforms.Compose([</span><br><span class="line">        transforms.Resize(input_size),</span><br><span class="line">        transforms.CenterCrop(input_size),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Initializing Datasets and Dataloaders..."</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create training and validation datasets</span></span><br><span class="line">image_datasets = &#123;x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br><span class="line"><span class="comment"># Create training and validation dataloaders</span></span><br><span class="line">dataloaders_dict = &#123;x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Detect if we have a GPU available</span></span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Initializing Datasets and Dataloaders...</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>In [127]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Send the model to GPU</span></span><br><span class="line">model_ft = model_ft.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Gather the parameters to be optimized/updated in this run. If we are</span></span><br><span class="line"><span class="comment">#  finetuning we will be updating all parameters. However, if we are </span></span><br><span class="line"><span class="comment">#  doing feature extract method, we will only update the parameters</span></span><br><span class="line"><span class="comment">#  that we have just initialized, i.e. the parameters with requires_grad</span></span><br><span class="line"><span class="comment">#  is True.</span></span><br><span class="line">params_to_update = model_ft.parameters()</span><br><span class="line">print(<span class="string">"Params to learn:"</span>)</span><br><span class="line"><span class="keyword">if</span> feature_extract:</span><br><span class="line">    params_to_update = []</span><br><span class="line">    <span class="keyword">for</span> name,param <span class="keyword">in</span> model_ft.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> param.requires_grad == <span class="literal">True</span>:</span><br><span class="line">            params_to_update.append(param)</span><br><span class="line">            print(<span class="string">"\t"</span>,name)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">for</span> name,param <span class="keyword">in</span> model_ft.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> param.requires_grad == <span class="literal">True</span>:</span><br><span class="line">            print(<span class="string">"\t"</span>,name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Observe that all parameters are being optimized</span></span><br><span class="line">optimizer_ft = optim.SGD(params_to_update, lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Params to learn:</span><br><span class="line">	 fc.weight</span><br><span class="line">	 fc.bias</span><br></pre></td></tr></table></figure>

<p>In [133]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Setup the loss fxn</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train and evaluate</span></span><br><span class="line">model_ft, ohist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.2623850886450439 Acc: 0.8975409836065574</span><br><span class="line">val Loss: 0.22199168762350394 Acc: 0.9215686274509803</span><br><span class="line"></span><br><span class="line">Epoch 1/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.20775875546893136 Acc: 0.9262295081967213</span><br><span class="line">val Loss: 0.21329789413930544 Acc: 0.9215686274509803</span><br><span class="line"></span><br><span class="line">Epoch 2/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.24463887243974405 Acc: 0.9098360655737705</span><br><span class="line">val Loss: 0.2308054333613589 Acc: 0.9215686274509803</span><br><span class="line"></span><br><span class="line">Epoch 3/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.2108444703406975 Acc: 0.930327868852459</span><br><span class="line">val Loss: 0.20637644174831365 Acc: 0.954248366013072</span><br><span class="line"></span><br><span class="line">Epoch 4/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.22102872954040279 Acc: 0.9221311475409836</span><br><span class="line">val Loss: 0.19902625017695957 Acc: 0.9281045751633987</span><br><span class="line"></span><br><span class="line">Epoch 5/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.22044393127081824 Acc: 0.9221311475409836</span><br><span class="line">val Loss: 0.2212505256818011 Acc: 0.9281045751633987</span><br><span class="line"></span><br><span class="line">Epoch 6/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.1636357441788814 Acc: 0.9467213114754098</span><br><span class="line">val Loss: 0.1969745449380937 Acc: 0.934640522875817</span><br><span class="line"></span><br><span class="line">Epoch 7/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.1707800094221459 Acc: 0.9385245901639344</span><br><span class="line">val Loss: 0.20569930824578977 Acc: 0.934640522875817</span><br><span class="line"></span><br><span class="line">Epoch 8/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.18224841185280535 Acc: 0.9344262295081968</span><br><span class="line">val Loss: 0.192565394480244 Acc: 0.9411764705882353</span><br><span class="line"></span><br><span class="line">Epoch 9/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.17762072372143387 Acc: 0.9385245901639344</span><br><span class="line">val Loss: 0.19549715163466197 Acc: 0.9411764705882353</span><br><span class="line"></span><br><span class="line">Epoch 10/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.19314993575948183 Acc: 0.9180327868852459</span><br><span class="line">val Loss: 0.2000840900380627 Acc: 0.934640522875817</span><br><span class="line"></span><br><span class="line">Epoch 11/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.21551114418467537 Acc: 0.9057377049180327</span><br><span class="line">val Loss: 0.18960770005299374 Acc: 0.934640522875817</span><br><span class="line"></span><br><span class="line">Epoch 12/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.1847396502729322 Acc: 0.9426229508196722</span><br><span class="line">val Loss: 0.1871058808432685 Acc: 0.9411764705882353</span><br><span class="line"></span><br><span class="line">Epoch 13/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.17342406132670699 Acc: 0.9508196721311475</span><br><span class="line">val Loss: 0.20636656588199093 Acc: 0.9215686274509803</span><br><span class="line"></span><br><span class="line">Epoch 14/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.16013679030488748 Acc: 0.9508196721311475</span><br><span class="line">val Loss: 0.18491691759988374 Acc: 0.9411764705882353</span><br><span class="line"></span><br><span class="line">Training compete in 0.0m 14.700076580047607s</span><br><span class="line">Best val Acc: 0.954248366013072</span><br></pre></td></tr></table></figure>

<p>In [130]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the non-pretrained version of the model used for this run</span></span><br><span class="line">scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=<span class="literal">False</span>, use_pretrained=<span class="literal">False</span>)</span><br><span class="line">scratch_model = scratch_model.to(device)</span><br><span class="line">scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">scratch_criterion = nn.CrossEntropyLoss()</span><br><span class="line">_,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.7185551504619786 Acc: 0.4426229508196721</span><br><span class="line">val Loss: 0.6956208067781785 Acc: 0.45751633986928103</span><br><span class="line"></span><br><span class="line">Epoch 1/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.6852761008700387 Acc: 0.5778688524590164</span><br><span class="line">val Loss: 0.6626271987273022 Acc: 0.6601307189542484</span><br><span class="line"></span><br><span class="line">Epoch 2/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.6603062289660094 Acc: 0.5942622950819673</span><br><span class="line">val Loss: 0.6489538297154545 Acc: 0.5816993464052288</span><br><span class="line"></span><br><span class="line">Epoch 3/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.6203305486772881 Acc: 0.639344262295082</span><br><span class="line">val Loss: 0.6013184107986151 Acc: 0.673202614379085</span><br><span class="line"></span><br><span class="line">Epoch 4/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.5989709232674271 Acc: 0.6680327868852459</span><br><span class="line">val Loss: 0.5929347966231552 Acc: 0.6993464052287581</span><br><span class="line"></span><br><span class="line">Epoch 5/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.5821619336722327 Acc: 0.6557377049180327</span><br><span class="line">val Loss: 0.5804777059679717 Acc: 0.6928104575163399</span><br><span class="line"></span><br><span class="line">Epoch 6/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.6114685896967278 Acc: 0.6270491803278688</span><br><span class="line">val Loss: 0.5674225290616354 Acc: 0.7189542483660131</span><br><span class="line"></span><br><span class="line">Epoch 7/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.5681056575696977 Acc: 0.6680327868852459</span><br><span class="line">val Loss: 0.5602688086188696 Acc: 0.7189542483660131</span><br><span class="line"></span><br><span class="line">Epoch 8/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.5701596453541615 Acc: 0.7090163934426229</span><br><span class="line">val Loss: 0.5554519264526616 Acc: 0.7450980392156863</span><br><span class="line"></span><br><span class="line">Epoch 9/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.5476810380083615 Acc: 0.7254098360655737</span><br><span class="line">val Loss: 0.5805927063125411 Acc: 0.7189542483660131</span><br><span class="line"></span><br><span class="line">Epoch 10/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.5508710468401674 Acc: 0.6926229508196722</span><br><span class="line">val Loss: 0.5859468777974447 Acc: 0.7058823529411765</span><br><span class="line"></span><br><span class="line">Epoch 11/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.5344281519045595 Acc: 0.7172131147540983</span><br><span class="line">val Loss: 0.5640550851821899 Acc: 0.7058823529411765</span><br><span class="line"></span><br><span class="line">Epoch 12/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.5125471890949812 Acc: 0.7295081967213115</span><br><span class="line">val Loss: 0.5665123891207128 Acc: 0.7058823529411765</span><br><span class="line"></span><br><span class="line">Epoch 13/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.496260079204059 Acc: 0.7254098360655737</span><br><span class="line">val Loss: 0.5820710787586137 Acc: 0.7058823529411765</span><br><span class="line"></span><br><span class="line">Epoch 14/14</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.49067981907578767 Acc: 0.7704918032786885</span><br><span class="line">val Loss: 0.5722863315756804 Acc: 0.7058823529411765</span><br><span class="line"></span><br><span class="line">Training compete in 0.0m 18.418847799301147s</span><br><span class="line">Best val Acc: 0.7450980392156863</span><br></pre></td></tr></table></figure>

<p>In [134]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the training curves of validation accuracy vs. number </span></span><br><span class="line"><span class="comment">#  of training epochs for the transfer learning method and</span></span><br><span class="line"><span class="comment">#  the model trained from scratch</span></span><br><span class="line"><span class="comment"># ohist = []</span></span><br><span class="line"><span class="comment"># shist = []</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ohist = [h.cpu().numpy() for h in ohist]</span></span><br><span class="line"><span class="comment"># shist = [h.cpu().numpy() for h in scratch_hist]</span></span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">"Validation Accuracy vs. Number of Training Epochs"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Training Epochs"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Validation Accuracy"</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,num_epochs+<span class="number">1</span>),ohist,label=<span class="string">"Pretrained"</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,num_epochs+<span class="number">1</span>),scratch_hist,label=<span class="string">"Scratch"</span>)</span><br><span class="line">plt.ylim((<span class="number">0</span>,<span class="number">1.</span>))</span><br><span class="line">plt.xticks(np.arange(<span class="number">1</span>, num_epochs+<span class="number">1</span>, <span class="number">1.0</span>))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CNN/">CNN</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-丘吉尔的人物传记char级别的文本生成代码注释" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/11/08/丘吉尔的人物传记char级别的文本生成代码注释/" class="article-date">
      <time datetime="2019-11-08T10:50:42.000Z" itemprop="datePublished">2019-11-08</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/11/08/丘吉尔的人物传记char级别的文本生成代码注释/">丘吉尔的人物传记char级别的文本生成代码注释</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h1><p>举个小小的例子，来看看LSTM是怎么玩的</p>
<p>我们这里用温斯顿丘吉尔的人物传记作为我们的学习语料。</p>
<p>第一步，一样，先导入各种库</p>
<p>关于LSTM的详细原理先看博客：<a href="https://www.julyedu.com/question/big/kp_id/26/ques_id/1851" target="_blank" rel="noopener">如何从RNN起步，一步一步通俗理解LSTM</a></p>
<p>In [68]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dropout</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> LSTM</span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ModelCheckpoint</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br></pre></td></tr></table></figure>

<p>In [69]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">raw_text = open(<span class="string">'./input/Winston_Churchil.txt'</span>).read()</span><br><span class="line"><span class="comment"># .read() 读入整个文件为一个字符串</span></span><br><span class="line">raw_text = raw_text.lower() <span class="comment"># 小写</span></span><br><span class="line">print(raw_text[:<span class="number">100</span>]) <span class="comment">#打印前100个字符</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">﻿project gutenberg’s real soldiers of fortune, by richard harding davis</span><br><span class="line"></span><br><span class="line">this ebook <span class="keyword">is</span> <span class="keyword">for</span> the use o</span><br></pre></td></tr></table></figure>

<p>既然我们是以每个字母为层级，字母总共才26个，所以我们可以很方便的用One-Hot来编码出所有的字母（当然，可能还有些标点符号和其他noise）</p>
<p>In [70]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">chars = sorted(list(set(raw_text))) </span><br><span class="line"><span class="comment"># 这里去重不是单词去重，是字母去重，去重后只有英文字母和标点符号等字符</span></span><br><span class="line">print(len(raw_text))</span><br><span class="line">print(len(chars))</span><br><span class="line">char_to_int = dict((c, i) <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(chars)) </span><br><span class="line"><span class="comment"># 去重后的字符排序好后，xi</span></span><br><span class="line">int_to_char = dict((i, c) <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(chars))</span><br><span class="line">print(char_to_int)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">276830</span><br><span class="line">61</span><br><span class="line">&#123;&apos;\n&apos;: 0, &apos; &apos;: 1, &apos;!&apos;: 2, &apos;#&apos;: 3, &apos;$&apos;: 4, &apos;%&apos;: 5, &apos;(&apos;: 6, &apos;)&apos;: 7, &apos;*&apos;: 8, &apos;,&apos;: 9, &apos;-&apos;: 10, &apos;.&apos;: 11, &apos;/&apos;: 12, &apos;0&apos;: 13, &apos;1&apos;: 14, &apos;2&apos;: 15, &apos;3&apos;: 16, &apos;4&apos;: 17, &apos;5&apos;: 18, &apos;6&apos;: 19, &apos;7&apos;: 20, &apos;8&apos;: 21, &apos;9&apos;: 22, &apos;:&apos;: 23, &apos;;&apos;: 24, &apos;?&apos;: 25, &apos;@&apos;: 26, &apos;[&apos;: 27, &apos;]&apos;: 28, &apos;_&apos;: 29, &apos;a&apos;: 30, &apos;b&apos;: 31, &apos;c&apos;: 32, &apos;d&apos;: 33, &apos;e&apos;: 34, &apos;f&apos;: 35, &apos;g&apos;: 36, &apos;h&apos;: 37, &apos;i&apos;: 38, &apos;j&apos;: 39, &apos;k&apos;: 40, &apos;l&apos;: 41, &apos;m&apos;: 42, &apos;n&apos;: 43, &apos;o&apos;: 44, &apos;p&apos;: 45, &apos;q&apos;: 46, &apos;r&apos;: 47, &apos;s&apos;: 48, &apos;t&apos;: 49, &apos;u&apos;: 50, &apos;v&apos;: 51, &apos;w&apos;: 52, &apos;x&apos;: 53, &apos;y&apos;: 54, &apos;z&apos;: 55, &apos;‘&apos;: 56, &apos;’&apos;: 57, &apos;“&apos;: 58, &apos;”&apos;: 59, &apos;\ufeff&apos;: 60&#125;</span><br></pre></td></tr></table></figure>

<h1 id="构造训练集"><a href="#构造训练集" class="headerlink" title="构造训练集"></a>构造训练集</h1><p>我们这里简单的文本预测就是，给了前置的字母以后，下一个字母是谁？<br>我们需要把我们的raw text变成可以用来训练的x,y:</p>
<p>x 是前置字母们 y 是后一个字母</p>
<p>In [71]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">seq_length = <span class="number">100</span> </span><br><span class="line"><span class="comment"># 输入的字符的长度，一个字符对应一个神经元，总共100个神经元</span></span><br><span class="line"><span class="comment"># 输入是100个字符，输出是预测的一个字符</span></span><br><span class="line"></span><br><span class="line">x = []</span><br><span class="line">y = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(raw_text) - seq_length): <span class="comment"># 每次循环都滑动一个字符距离</span></span><br><span class="line">    given = raw_text[i:i + seq_length] <span class="comment"># 从零先取前100个字符作为输入</span></span><br><span class="line">    predict = raw_text[i + seq_length] <span class="comment"># y是后一个字符</span></span><br><span class="line">    x.append([char_to_int[char] <span class="keyword">for</span> char <span class="keyword">in</span> given]) <span class="comment"># 把字符转化为向量</span></span><br><span class="line">    y.append(char_to_int[predict]) <span class="comment"># # 把字符转化为向量</span></span><br></pre></td></tr></table></figure>

<p>In [72]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(x[:<span class="number">3</span>]) </span><br><span class="line">print(y[:<span class="number">3</span>])</span><br><span class="line">print(set(y)) <span class="comment"># 这里注意下，'\ufeff': 60这个字符是文本的首字符，只有这么一个，y是取不到的，所以y值的可能只有60种</span></span><br><span class="line">print(len(set(y)))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[60, 45, 47, 44, 39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 57, 48, 1, 47, 34, 30, 41, 1, 48, 44, 41, 33, 38, 34, 47, 48, 1, 44, 35, 1, 35, 44, 47, 49, 50, 43, 34, 9, 1, 31, 54, 1, 47, 38, 32, 37, 30, 47, 33, 1, 37, 30, 47, 33, 38, 43, 36, 1, 33, 30, 51, 38, 48, 0, 0, 49, 37, 38, 48, 1, 34, 31, 44, 44, 40, 1, 38, 48, 1, 35, 44, 47, 1, 49, 37, 34, 1, 50, 48, 34, 1, 44], [45, 47, 44, 39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 57, 48, 1, 47, 34, 30, 41, 1, 48, 44, 41, 33, 38, 34, 47, 48, 1, 44, 35, 1, 35, 44, 47, 49, 50, 43, 34, 9, 1, 31, 54, 1, 47, 38, 32, 37, 30, 47, 33, 1, 37, 30, 47, 33, 38, 43, 36, 1, 33, 30, 51, 38, 48, 0, 0, 49, 37, 38, 48, 1, 34, 31, 44, 44, 40, 1, 38, 48, 1, 35, 44, 47, 1, 49, 37, 34, 1, 50, 48, 34, 1, 44, 35], [47, 44, 39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 57, 48, 1, 47, 34, 30, 41, 1, 48, 44, 41, 33, 38, 34, 47, 48, 1, 44, 35, 1, 35, 44, 47, 49, 50, 43, 34, 9, 1, 31, 54, 1, 47, 38, 32, 37, 30, 47, 33, 1, 37, 30, 47, 33, 38, 43, 36, 1, 33, 30, 51, 38, 48, 0, 0, 49, 37, 38, 48, 1, 34, 31, 44, 44, 40, 1, 38, 48, 1, 35, 44, 47, 1, 49, 37, 34, 1, 50, 48, 34, 1, 44, 35, 1]]</span><br><span class="line">[35, 1, 30]</span><br><span class="line">&#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59&#125;</span><br><span class="line">60</span><br></pre></td></tr></table></figure>

<p>此刻，楼上这些表达方式，类似就是一个词袋，或者说 index。</p>
<p>接下来我们做两件事：</p>
<p>1.我们已经有了一个input的数字表达（index），我们要把它变成LSTM需要的数组格式： [样本数，时间步伐，特征]</p>
<p>2.对于output，我们在Word2Vec里学过，用one-hot做output的预测可以给我们更好的效果，相对于直接预测一个准确的y数值的话。</p>
<p>In [73]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">n_patterns = len(x) <span class="comment"># 样本数量</span></span><br><span class="line">n_vocab = len(chars) <span class="comment"># </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把x变成LSTM需要的样子</span></span><br><span class="line">x = numpy.reshape(x, (n_patterns, seq_length, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># n_patterns 样本的数量</span></span><br><span class="line"><span class="comment"># seq_length 每个样本每次训练100个字符</span></span><br><span class="line"><span class="comment"># 1 代表我们的一个字符的维度是1维，这里1维是特殊情况，如果我们的输入不是一个字符，</span></span><br><span class="line"><span class="comment">#是一个单词的话，单词是可以embedding成100维向量，那这里就是100.</span></span><br><span class="line"></span><br><span class="line">x = x / float(n_vocab) <span class="comment"># 简单normal到0-1之间，归一化，防止梯度爆炸</span></span><br><span class="line">y = np_utils.to_categorical(y) </span><br><span class="line"><span class="comment"># y值总共取值61种，直接做个onehot编码</span></span><br><span class="line"><span class="comment"># 对类别进行onehot编码一个很重要的原因在于计算loss时的问题。loss一般用距离来表示，</span></span><br><span class="line"><span class="comment"># 如果用1~5来表示，那么1和2的距离是1，而1和5的距离是4，但是按道理1和2、1和5的距离应该一样。</span></span><br><span class="line"><span class="comment"># 如果用one hot编码表示，那么1和2的距离跟1和5的距离时一样的。</span></span><br><span class="line"></span><br><span class="line">print(x[<span class="number">10</span>][:<span class="number">10</span>]) <span class="comment"># 第10个样本的前10个字符向量</span></span><br><span class="line">print(y[<span class="number">10</span>])</span><br><span class="line">print(y.shape) <span class="comment">#</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[[0.81967213]</span><br><span class="line"> [0.80327869]</span><br><span class="line"> [0.55737705]</span><br><span class="line"> [0.70491803]</span><br><span class="line"> [0.50819672]</span><br><span class="line"> [0.55737705]</span><br><span class="line"> [0.7704918 ]</span><br><span class="line"> [0.59016393]</span><br><span class="line"> [0.93442623]</span><br><span class="line"> [0.78688525]]</span><br><span class="line">[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.</span><br><span class="line"> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.</span><br><span class="line"> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</span><br><span class="line">(276730, 60)</span><br></pre></td></tr></table></figure>

<h1 id="构建和训练模型"><a href="#构建和训练模型" class="headerlink" title="构建和训练模型"></a>构建和训练模型</h1><p>In [74]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># Sequential()：序贯模型是多个网络层的线性堆叠，可以通过.add()一个一个添加层</span></span><br><span class="line">model.add(LSTM(<span class="number">256</span>, input_shape=(x.shape[<span class="number">1</span>], x.shape[<span class="number">2</span>])))</span><br><span class="line"><span class="comment"># 256是LSTM的隐藏层的维度。</span></span><br><span class="line"><span class="comment"># input_shape，不需要考虑样本数量，对模型本身结构没有意义</span></span><br><span class="line">model.add(Dropout(<span class="number">0.2</span>)) <span class="comment"># 去掉20%的神经元</span></span><br><span class="line">model.add(Dense(y.shape[<span class="number">1</span>], activation=<span class="string">'softmax'</span>)) </span><br><span class="line"><span class="comment"># y.shape[1] = 60，'softmax'转化为概率，概率和为1</span></span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>)</span><br><span class="line"><span class="comment"># 多分类交叉熵损失，adam优化器</span></span><br></pre></td></tr></table></figure>

<p>In [75]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.fit(x, y, nb_epoch=<span class="number">1</span>, batch_size=<span class="number">1024</span>)</span><br><span class="line"><span class="comment"># 这里本地跑的太慢了，nb_epoch设置为1了</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/Users/yyg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.</span><br><span class="line">  &quot;&quot;&quot;Entry point for launching an IPython kernel.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/1</span><br><span class="line">276730/276730 [==============================] - 1152s 4ms/step - loss: 3.0591</span><br></pre></td></tr></table></figure>

<p>Out[75]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;keras.callbacks.History at 0xb27cb07f0&gt;</span><br></pre></td></tr></table></figure>

<h2 id="预测模型"><a href="#预测模型" class="headerlink" title="预测模型"></a>预测模型</h2><p>In [95]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_next</span><span class="params">(input_array)</span>:</span></span><br><span class="line">    x = numpy.reshape(input_array, (<span class="number">1</span>, seq_length, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 同上，只有一个样本，reshape成lstm需要的维度。</span></span><br><span class="line">    x = x / float(n_vocab) <span class="comment"># 归一化</span></span><br><span class="line">    y = model.predict(x) <span class="comment"># y是60维的向量</span></span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">string_to_index</span><span class="params">(raw_input)</span>:</span></span><br><span class="line">    res = [] </span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> raw_input[(len(raw_input)-seq_length):]:</span><br><span class="line"><span class="comment"># 这步一个问题是len(raw_input)一定要大于seq_length：100,不然会报错</span></span><br><span class="line">        res.append(char_to_int[c]) <span class="comment"># 得到输入句子中后100个字符的向量表示</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">y_to_char</span><span class="params">(y)</span>:</span> </span><br><span class="line">    largest_index = y.argmax() <span class="comment"># 取出概率最大的值对应的索引</span></span><br><span class="line">    c = int_to_char[largest_index] <span class="comment"># 根据索引得到对应的字符</span></span><br><span class="line">    <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure>

<p>In [96]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_article</span><span class="params">(init, rounds=<span class="number">200</span>)</span>:</span> </span><br><span class="line">    <span class="comment"># rounds=200 代表预测生成200个新的字符</span></span><br><span class="line">    <span class="comment"># init是输入的字符串</span></span><br><span class="line">    in_string = init.lower() <span class="comment"># 跟上面一样的预处理步骤</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(rounds): <span class="comment"># 每次模型预测一个字符</span></span><br><span class="line">        n = y_to_char(predict_next(string_to_index(in_string)))</span><br><span class="line">        <span class="comment"># n是预测的新的字符</span></span><br><span class="line">        in_string += n <span class="comment"># 把新的字符拼接，重新作为新的输入</span></span><br><span class="line">    <span class="keyword">return</span> in_string</span><br></pre></td></tr></table></figure>

<p>In [97]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">init = <span class="string">'His object in coming to New York was to engage officers for that service. He came at an opportune moment'</span></span><br><span class="line">article = generate_article(init)</span><br><span class="line">print(article) <span class="comment"># 这里上面只是迭代了一次，导致后面所有的预测都是空格</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">his object in coming to new york was to engage officers for that service. he came at an opportune moment</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LSTM/">LSTM</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/2/">&laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/4/">Next &raquo;</a>
    </nav>
  
</div>
      <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2019-2020 MingmingYe
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>

    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>