<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no">
<meta name="author" content="MingmingYe">


    
    


<meta property="og:type" content="website">
<meta property="og:title" content="Stay hungry, Stay foolish.">
<meta property="og:url" content="http://mmyblog.cn/page/2/index.html">
<meta property="og:site_name" content="Stay hungry, Stay foolish.">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Stay hungry, Stay foolish.">

<link rel="apple-touch-icon" href="/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="Stay hungry, Stay foolish." type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">


    <style> .article { opacity: 0;} </style>


<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>Stay hungry, Stay foolish.</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: true
    }
</script>


    <script>
        yiliaConfig.jquery_ui = [true, "//cdn.bootcss.com/jqueryui/1.10.4/jquery-ui.min.js", "//cdn.bootcss.com/jqueryui/1.10.4/css/jquery-ui.min.css"];
    </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






</head></html>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/deep.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">MingmingYe</a></h1>
        </hgroup>

        
        <p class="header-subtitle">当你的才华撑不起你的野心时，只有静下心来好好学习！纵使命运注定是个打酱油的，也要打一瓶与别人不一样的酱油！</p>
        

        
            <form id="search-form">
            <input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false" />
            <i class="fa fa-times" onclick="resetSearch()"></i>
            </form>
            <div id="local-search-result"></div>
            <p class='no-result'>No results found <i class='fa fa-spinner fa-pulse'></i></p>
        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:878759487@qq.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AUC/">AUC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention/">Attention</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/">BERT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BLUE/">BLUE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Beam-search/">Beam search</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CRF/">CRF</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ConvNet/">ConvNet</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ELMo/">ELMo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GPT/">GPT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GRU/">GRU</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gradient-Clipping/">Gradient Clipping</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LR/">LR</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LSTM/">LSTM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linear/">Linear</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Parsing/">Parsing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PyTorch/">PyTorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/QA/">QA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN-LSTM/">RNN/LSTM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ROC/">ROC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recursive-Neural-Networks/">Recursive Neural Networks</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQuAD-BiDAF/">SQuAD-BiDAF</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Seq2Seq/">Seq2Seq</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TorchText/">TorchText</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/">Transformer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer-XL/">Transformer-XL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/XLNet/">XLNet</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/boosting/">boosting</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cbow/">cbow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hierarchical-softmax/">hierarchical softmax</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/inference/">inference</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jiaba/">jiaba</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jieba/">jieba</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mumpy/">mumpy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/negative-sampling/">negative sampling</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pandas/">pandas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pyspark/">pyspark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/seaborn/">seaborn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/skip-gram/">skip-gram</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/softmax/">softmax</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/word-embedding/">word-embedding</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/wxBot/">wxBot</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/中文分词/">中文分词</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/优化方法/">优化方法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/决策树/">决策树</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/图神经网络/">图神经网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/微积分/">微积分</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/朴素贝叶斯/">朴素贝叶斯</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/概率/">概率</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/模型调优/">模型调优</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/正则表达式/">正则表达式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习速查表/">深度学习速查表</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/特征工程/">特征工程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/线性代数/">线性代数</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/统计/">统计</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/聊天机器人/">聊天机器人</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/聚类/">聚类</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/语言模型/">语言模型</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/贝叶斯分类器/">贝叶斯分类器</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/降维/">降维</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/集成学习/">集成学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/面试/">面试</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://mmyblog.cn/">mmy</a>
                    
                    </div>
                </section>
                

                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">MingmingYe</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/deep.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">MingmingYe</a></h1>
            </hgroup>
            
            <p class="header-subtitle">当你的才华撑不起你的野心时，只有静下心来好好学习！纵使命运注定是个打酱油的，也要打一瓶与别人不一样的酱油！</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:878759487@qq.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap">
  
    <article id="post-文本生成任务" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/05/10/文本生成任务/" class="article-date">
      <time datetime="2020-05-10T00:39:03.000Z" itemprop="datePublished">2020-05-10</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/10/文本生成任务/">文本生成任务</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>主要讨论</p>
<ul>
<li><p>文本生成的方法：<strong>inference</strong></p>
</li>
<li><p>增加文本生成的多样性：<strong>variational auto encoder</strong></p>
</li>
<li><p>可以<strong>控制的文本生成、文本风格迁移</strong></p>
</li>
<li><p>Generative Adversarial Networks</p>
</li>
<li><p>Data to text</p>
</li>
</ul>
<p>log loss:</p>
<ul>
<li><p>[s1, s2, …, s_n] –&gt; softmax(s) = exp(s_i) / sum_i exp(s_i)  </p>
</li>
<li><p>p_i log q_i</p>
</li>
</ul>
<h1 id="关于文本生成"><a href="#关于文本生成" class="headerlink" title="关于文本生成"></a>关于文本生成</h1><p>之前的课程中，我们主要讨论了Natural Language Understanding，也就是给你一段文字，如何从各个方面去理解它。常见的NLU任务有：文本分类，情感分类，<strong>命名实体识别（Named Entity Recognition, NER），Relation Extraction</strong>等等。也就是说，从文字中提取出我们想要了解的关键信息。</p>
<p>这节课我们来讨论文本生成的一些方法。</p>
<p>对于文本生成，我们关心哪些问题？</p>
<ul>
<li><p>与文本理解相反，我们有一些想要表达的信息，这些信息可能来自于对话的历史，可能来自于结构化的数据 (structured data, data-to-text generation)。现在我们要考虑的是如何把这些我们想要表达的信息转换成自然语言的方式。这一任务在构建聊天机器人中显得尤为重要。目前看来，基于<strong>模板 (template)</strong> 的方法仍然是最保险的，但是在研究领域中，人们越来越关注<strong>基于神经网络的文本生成方法</strong>。</p>
</li>
<li><p>基于上文的文本补全任务，故事生成，生成式聊天机器人</p>
</li>
<li><p>人们一直希望计算机可以完成一些人类才可以完成的创造性任务，例如作画。AI作画实际上已经不是什么新闻了，Portrait of Edmond de Belamy，一幅AI创作的画像，拍卖出了43.2万美金的高价。</p>
</li>
<li><p>那么AI能不能写文章讲故事呢？关于文本生成的研究相对来说没有特别客观的评价指标，所以很多时候人们会按照自己的主观评价来判断模型的好坏。例如给定故事的上文，AI系统能不能很好地补全这个故事呢？</p>
</li>
<li><p>文本补全这个任务本质上就是训练一个语言模型，当然也有人尝试使用Seq2Seq的方法做文本生成。目前看来最强的模型是基于GPT-2预训练的语言模型。很多研究者使用GPT-2来进行文本生成相关的实验。由于训练GPT-2这样规模的语言模型需要大量的算力和数据资源，所以大部分的研究都关注在如何使用模型，也就是inference的步骤，而不在于模型的训练环节。</p>
</li>
</ul>
<h2 id="Greedy-Decoding"><a href="#Greedy-Decoding" class="headerlink" title="Greedy Decoding"></a>Greedy Decoding</h2><p><strong>autoregressive</strong>: 基于之前生成的文字来生成后续的文字? </p>
<p>P(y_i | y_1, … y_{i-1})</p>
<p>parallel generation</p>
<p>大部分基于神经网络的文本生成模型采用的是一种条件语言模型的方法，也就是说，我们有一些先决条件，例如 auto encoder 中的隐向量，然后我们基于这个隐向量来生成句子。</p>
<p>大部分语言模型的基本假设是从左往右的条件概率模型，也就是说，给定了单词1至n-1，我们希望生成第n个单词。假设我们现在采用一个基于LSTM的语言模型，在当前第i个位置上，我们预测下一个生成单词的概率分布为 p = (p_1, <strong>p_2</strong>, … p_|V|)，那么在当前位置上我们应该生成什么单词呢？</p>
<p>argmax_i p_i = 2</p>
<p>一个最简单的方法是使用Greedy Decoding，也就是说，我们直接采用 argmax_i (p_i) 即可。当然，同学们很容易联想到，这种decoding的方法是有问题的，因为每次都选择最大概率的单词并不能保证我们生成出来的句子的总体概率分布是最大的。事实上，大部分时候这样生成的句子其实是不好的。然而我们没有办法遍历所有可能的句子：首先句子的长度是不确定的；即使我们假定自己知道句子的长度 l，如果在每个位置上考虑每个可能的单词，我们需要考虑 |V|^l 种可能的情况，在计算资源上也是不现实的。</p>
<p>一种妥协的方法是采用 <strong>Beam Search</strong> （<a href="https://shimo.im/docs/rHwdq8wd8txyXjP6）。也就是说，在decoding的每个步骤，我们都保留着" target="_blank" rel="noopener">https://shimo.im/docs/rHwdq8wd8txyXjP6）。也就是说，在decoding的每个步骤，我们都保留着</a> <strong>top K</strong> 个可能的候选单词，然后到了下一个步骤的时候，我们对这 K 个单词都做下一步 decoding，分别选出 top K，然后对这 K^2 个候选句子再挑选出 <strong>top K 个句子</strong>。以此类推一直到 decoding 结束为止。当然 Beam Search 本质上也是一个 greedy decoding 的方法，所以我们无法保证自己一定可以得到最好的 decoding 结果。</p>
<p>p(x_1, x_2, …, x_n) = log (p(x_1) * p(x_2 | x_1) … p(x_n | x_1, …, x_{n-1})) / n</p>
<p><strong>Greedy Decoding</strong>的问题</p>
<ul>
<li><p>容易出现很无聊的回答：I don’t know. </p>
</li>
<li><p>容易重复自己：I don’t know. I don’t know. I don’t know. I don’t know. I don’t know. I don’t know. </p>
</li>
<li><p>Beam search K = 200</p>
</li>
</ul>
<h2 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h2><p>argmax 不一定是最好的</p>
<p>vocab(y_i) = [<strong>0.9</strong>, 0.05, 0.01, 0.01, 0.01, …., 0.01]  softmax(logits/temperature)</p>
<p>sample(vocab(y_i))</p>
<p>sample很多个句子，然后用另一个模型来打分，找出最佳generated text</p>
<p>sampling over the full vocabulary：我们可以在生成文本的时候引入一些随机性。例如现在语言模型告诉我们下一个单词在整个单词表上的概率分布是 p = (p_1, p_2, … p_|V|)，那么我们就可以按照这个概率分布进行随机采样，然后决定下一个单词生成什么。采样相对于greedy方法的好处是，我们生成的文字开始有了一些随机性，不会总是生成很机械的回复了。</p>
<p>1 - 0.98^n</p>
<p>Sampling的问题</p>
<ul>
<li><p>生成的话容易不连贯，上下文比较矛盾。</p>
</li>
<li><p>容易生成奇怪的话，出现<strong>罕见词</strong>。</p>
</li>
</ul>
<p>top-k sampling 可以缓解生成罕见单词的问题。比如说，我们可以每次只在概率最高的50个单词中按照概率分布做采样。</p>
<p>我只保留top-k个probability的单词，然后在这些单词中根据概率做sampling</p>
<h2 id="Neucleus-Sampling"><a href="#Neucleus-Sampling" class="headerlink" title="Neucleus Sampling"></a>Neucleus Sampling</h2><h3 id="The-Curious-Case-of-Neural-Text-Degeneration"><a href="#The-Curious-Case-of-Neural-Text-Degeneration" class="headerlink" title="The Curious Case of Neural Text Degeneration"></a>The Curious Case of Neural Text Degeneration</h3><p><a href="https://arxiv.org/pdf/1904.09751.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.09751.pdf</a></p>
<p><img src="https://uploader.shimo.im/f/8IjYXmjBFmAwnJy3.png!thumbnail" alt="img"></p>
<p>这篇文章在前些日子引起了不小的关注。文章提出了一种做sampling的方法，叫做 Neucleus Sampling。</p>
<p>Neucleus Sampling的基本思想是，我们不做beam search，而是做top p sampling。</p>
<p>设置一个threshold，p=0.95</p>
<p>top-k sampling 和 neucleus sampling 的代码：<a href="https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317" target="_blank" rel="noopener">https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317</a></p>
<h1 id="Variational-Auto-Encoder-VAE"><a href="#Variational-Auto-Encoder-VAE" class="headerlink" title="Variational Auto Encoder (VAE)"></a>Variational Auto Encoder (VAE)</h1><h2 id="Auto-Encoder-自编码器"><a href="#Auto-Encoder-自编码器" class="headerlink" title="Auto Encoder 自编码器"></a>Auto Encoder 自编码器</h2><p>NLP中的一个重要问题是获得一种语言的表示，无论是单词的表示还是句子的表示。为了获得句子的表示，一种直观的思路是训练一个auto encoder，也就是说一个encoder用来编码一个句子，把一个句子转换成一个vector；另一个decoder用来解码一个句子，也就是说把一个vector解码成一个句子。auto encoder 事实上是一种数据压缩的方法。</p>
<p>Encoder(text) –&gt; vector</p>
<p>Decoder(vector) –&gt; text</p>
<p>Encoder：得到很好的文本表示，这个文本表示你可用用于任何其他的任务。</p>
<p>Decoder: conditional language model</p>
<p>generalize能力不一定好。过拟合。</p>
<p>预期：希望类似的句子，能够变成比较相近的vector。不类似的句子，能够距离比较远。</p>
<p>Decoder(0,200,-23, 122) –&gt; text?</p>
<p>我爱[MASK]然语[MASK]处理 –&gt; vector –&gt; 我爱自然语言处理</p>
<p>在 auto encoder 的基础上又衍生出了各种类型的 auto encoder，例如 <strong>denoising</strong> auto encoder （<a href="https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf）。denoising" target="_blank" rel="noopener">https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf）。denoising</a> auto encoder 的基本思想是要加强 auto encoder 的 robustness。也就是说，我们希望把输入句子的一部分给“污染” (corrupt) 了，但是我们希望在经过编码和解码的过程之后，我们能够得到原来的正确的句子。事实上 BERT 的 masking 就是一种“污染”的手段。</p>
<p>Encoder(corrupt(text)) –&gt; vector</p>
<p>Decoder(vector) –&gt; text</p>
<p>随机产生一个vector –&gt; decoder –&gt; 生成一个句子</p>
<p>mapping </p>
<p>N(0, 1) –&gt; 各种各样的文字</p>
<p>从一个分布去生成一些东西</p>
<p>为了训练出可以用来sample文字的模型，人们发明了variational auto encoder (VAE)。VAE与普通auto encoder的不同之处在于，我们添加了一个constraint，希望encoder编码的每个句子都能够局限在某些特定的位置。例如，我们可以要求每个句子的encoding在空间上满足一个多维标准高斯分布。</p>
<p><strong>vector ~ N(0, 1)</strong></p>
<h2 id="什么是VAE？"><a href="#什么是VAE？" class="headerlink" title="什么是VAE？"></a>什么是VAE？</h2><p>网上有很多VAE的论文，博客，建议感兴趣的同学可以选择性阅读。我们这节课不会讨论太多的数学公式，而是从比较high level的层面介绍一下VAE模型以及它所解决的一些问题。</p>
<p>简单来说，VAE本质上是一种生成模型，我们希望能够通过隐向量z生成数据样本x。在文本生成的问题中，这个x往往表示的是一些文本/句子等内容。</p>
<p><img src="https://uploader.shimo.im/f/OoUmk9RItWAALQ69.png!thumbnail" alt="img"></p>
<p>下面是 Kingma 在 <strong>VAE</strong> 论文中定义的优化目标。</p>
<p>文本–&gt; 向量表示 –&gt; 文本</p>
<p>auto encoder: sentence –&gt; vector –&gt; sentence</p>
<p>Loss = -log P_{sentence}(dec(enc(sentence)))</p>
<p><img src="https://uploader.shimo.im/f/1HQEntdhGB8sSbbd.png!thumbnail" alt="img"></p>
<p>z -&gt; z’ -&gt; decoder(z) –&gt; 一个句子</p>
<p><strong>crossentropyloss(decoder(encoder(x)), x)</strong></p>
<p><strong>我们对z没有任何的约束条件</strong></p>
<p>q: encoder</p>
<p>p: decoder</p>
<p>KL divergence: 计算两个概率分布的差值</p>
<p>z: 把句子变成一个概率分布</p>
<p>z: (\mu, \sigma) –&gt; 正态分布的参数</p>
<p>用z做采样</p>
<p>KL Divergence的定义</p>
<p><img src="https://uploader.shimo.im/f/YQtU3e2EeBc0e4VH.png!thumbnail" alt="img"></p>
<p>sampling</p>
<p>N(0,1): sampling: 0.1, 0.05, 0.2, -0.1, -100</p>
<p>我们可以发现，VAE模型本质上就是要最大化样本的生成概率，并且最小化样本encode之后的参数表示与某种分布(正态分布)的KL散度。之所以我们会限制数据被编码后的向量服从某个局部的正态分布，是因为我们不希望这些数据被编码之后杂乱地散布在一个空间上，而是希望信息能够得到一定程度上的压缩。之所以让他们服从一个分布而不是一些固定的值，是因为我们希望模型中能够有一些随机性，好让模型的解码器能够生成各种各样的句子。</p>
<p>有了这个VAE模型的架构之后，人们就可以在各种任务上玩出各种不同的花样了。</p>
<p>例如对于图像来说，这里的<img src="https://uploader.shimo.im/f/FSNiIgsmVLc0HyUH.png!thumbnail" alt="img">和<img src="https://uploader.shimo.im/f/XpeVKp5c144d4RWl.png!thumbnail" alt="img">可能是CNN模型，对于自然语言来说，它们可能是一些RNN/LSTM之类的模型。</p>
<p>下面我们来看一些VAE在NLP领域的具体模型。</p>
<h3 id="Generating-Sentences-from-a-Continuous-Space"><a href="#Generating-Sentences-from-a-Continuous-Space" class="headerlink" title="Generating Sentences from a Continuous Space"></a>Generating Sentences from a Continuous Space</h3><p><a href="https://arxiv.org/pdf/1511.06349.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1511.06349.pdf</a></p>
<p><img src="https://uploader.shimo.im/f/LeCZ7dpW9JcvYC6T.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/qAE9oBPi2Lg7LJro.png!thumbnail" alt="img"></p>
<p>从上图可以看到，这篇论文的思路非常简单，就是把一个句子用RNN编码起来，编码之后得到的隐向量输出两个信息\mu和\simga，分别表示一个正太分布的平均值和标准差。然后这个分布应该尽可能地接近标准正态分布，在KL散度的表示下。并且如果我们用这个分布去采样得到新的向量表示，那么decoder应该要尽可能好地复原我们原来的这个句子。</p>
<p>具体的实验细节我们就不展开了，但是我们看一些论文中展示的生成的句子。</p>
<p><img src="https://uploader.shimo.im/f/WTJqo8usWYYxMR8k.png!thumbnail" alt="img"></p>
<p>下面看看VAE当中编码的空间是否具有某种连续性。</p>
<p><img src="https://uploader.shimo.im/f/uLmDTf81yPUZJbae.png!thumbnail" alt="img"></p>
<p>代码阅读：</p>
<ul>
<li><p><a href="https://github.com/timbmg/Sentence-VAE/blob/master/model.py" target="_blank" rel="noopener">https://github.com/timbmg/Sentence-VAE/blob/master/model.py</a></p>
</li>
<li><p><strong>练习</strong>：这份代码已经一年多没有更新了，感兴趣的同学可以把它更新到最新版本的PyTorch上，作为写代码练习，并且在自己的数据集上做一些实验，看看能否得到与论文中类似的效果（sentence interpolation）。</p>
</li>
</ul>
<p>GAN: generative adversarial networks</p>
<ul>
<li><p>generator: G(z) –&gt; x 一张逼真的汽车照片</p>
</li>
<li><p>discriminator: D(x) –&gt; 这个到底是不是一张汽车的照片 二分类</p>
</li>
</ul>
<p>Discriminator的目标</p>
<p>D(G(z)) –&gt; False</p>
<p>D(true photo) –&gt; True</p>
<p>Generator 的目标 D(G(z)) –&gt; True</p>
<h2 id="可控制的文本生成"><a href="#可控制的文本生成" class="headerlink" title="可控制的文本生成"></a>可控制的文本生成</h2><h3 id="Toward-Controlled-Generation-of-Text"><a href="#Toward-Controlled-Generation-of-Text" class="headerlink" title="Toward Controlled Generation of Text"></a>Toward Controlled Generation of Text</h3><p><a href="https://arxiv.org/pdf/1703.00955.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1703.00955.pdf</a></p>
<ul>
<li><p>Controlled Text Generation: 控制生成文本的一些特征</p>
</li>
<li><p>Learning disentangled latent representations: 对于文本不同的特征有不同的向量表示</p>
</li>
</ul>
<p>模型</p>
<p><img src="https://uploader.shimo.im/f/NejrcnoWDRgz7k58.png!thumbnail" alt="img"></p>
<p>To model and control the attributes of interest in an interpretable way, we augment the unstructured variables z with a set of structured variables c each of which targets a salient and independent semantic feature of sentences.</p>
<p>这篇文章试图解决这样一个问题，能不能把一句话编码成几个向量(z和c)。z和c分别包含了一些不同的关于句子的信息。</p>
<p><img src="https://uploader.shimo.im/f/LWIaBAzxmwkLY0pj.png!thumbnail" alt="img"></p>
<p>模型包含几个部分，一个generator可以基于若干个向量(z和c)生成句子，几个encoder可以从句子生成z和c的分布，几个discriminator用来判断模型编码出的向量(c)是否符合example的正确分类。这个模型的好处是，我们在某种程度上分离了句子的信息。例如如果向量c用来表示的是句子的情感正负，那么模型就具备了生成正面情感的句子和负面情感句子的能力。</p>
<p><img src="https://uploader.shimo.im/f/IGw674vLSf4tiqHe.png!thumbnail" alt="img"></p>
<p>参考代码</p>
<p><a href="https://github.com/wiseodd/controlled-text-generation" target="_blank" rel="noopener">https://github.com/wiseodd/controlled-text-generation</a></p>
<p>更多阅读</p>
<p>VAE论文：Auto-Encoding Variational Bayes <a href="https://arxiv.org/pdf/1312.6114.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1312.6114.pdf</a></p>
<p>An Introduction to Variational Autoencoders <a href="https://arxiv.org/pdf/1906.02691.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1906.02691.pdf</a></p>
<p>Stype Transfer</p>
<p>文本 –&gt; 内容z，风格c</p>
<p>z, 换一个风格c’ –&gt; 同样内容，不同风格的文本</p>
<h1 id="文本生成的应用：文本风格迁移"><a href="#文本生成的应用：文本风格迁移" class="headerlink" title="文本生成的应用：文本风格迁移"></a>文本生成的应用：文本风格迁移</h1><h3 id="Style-Transfer-from-Non-Parallel-Text-by-Cross-Alignment"><a href="#Style-Transfer-from-Non-Parallel-Text-by-Cross-Alignment" class="headerlink" title="Style Transfer from Non-Parallel Text by Cross-Alignment"></a>Style Transfer from Non-Parallel Text by Cross-Alignment</h3><p>论文：<a href="https://papers.nips.cc/paper/7259-style-transfer-from-non-parallel-text-by-cross-alignment.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/7259-style-transfer-from-non-parallel-text-by-cross-alignment.pdf</a></p>
<p>代码：<a href="https://github.com/shentianxiao/language-style-transfer/blob/master/code/style_transfer.py" target="_blank" rel="noopener">https://github.com/shentianxiao/language-style-transfer/blob/master/code/style_transfer.py</a></p>
<p>style transfer 其实也是controlled text generation的一种，只是它control的是文本的风格。文本风格有很多种，例如情感的正负面，文章是随意的还是严肃的。</p>
<p><img src="https://uploader.shimo.im/f/2BUGTCxcajoerOmj.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/fE6u9Ap33JIRO8So.png!thumbnail" alt="img"></p>
<p>一个很好的repo，总结了文本风格迁移领域的paper</p>
<p><a href="https://github.com/fuzhenxin/Style-Transfer-in-Text" target="_blank" rel="noopener">https://github.com/fuzhenxin/Style-Transfer-in-Text</a></p>
<h1 id="Generative-Adversarial-Networks-GAN-在NLP上的应用"><a href="#Generative-Adversarial-Networks-GAN-在NLP上的应用" class="headerlink" title="Generative Adversarial Networks (GAN) 在NLP上的应用"></a>Generative Adversarial Networks (GAN) 在NLP上的应用</h1><p>最早Ian Goodfellow的关于GAN的文章，其基本做法就是一个<strong>generator</strong>和一个<strong>discriminator</strong>(辅助角色)，然后让两个模型互相竞争对抗，在对抗的过程中逐渐提升各自的模型能力。而其中的generator就是我们希望能够最终optimize并且被拿来使用的模型。</p>
<p>早期GAN主要成功应用都在于图像领域。其关键原因在于，图像的每个像素都是三个连续的RGB数值。discriminator如果给图像计算一个概率分数，当我们在优化generator希望提高这个分数的时候，我们可以使用Back Propagation算法计算梯度，然后做梯度上升/下降来完成我们想要优化的目标。</p>
<p>discriminator: 二分类问题 图片–&gt;分类</p>
<p>D(G(z)) –&gt; cross entropyloss –&gt; backprop 到generator</p>
<p>文本–&gt; </p>
<p>LSTM –&gt; P_vocab() –&gt; <strong>argmax 文字</strong> –&gt; discriminator</p>
<p>LSTM –&gt; P_vocab() –&gt; discriminator</p>
<p>而文本生成是一个不同的问题，其特殊之处在于我们在做文本生成的时候有一步<strong>argmax</strong>的操作，也就是说当我们做inference生成文字的时候，在输出层使用了argmax或者sampling的操作。当我们把argmax或者sampling得到的文字传给discriminator打分的时候，我们无法用这个分数做<strong>back propagation</strong>对生成器做优化操作。</p>
<p>真正的sample –&gt; one hot vector ([1, 0, 0, 0, 0, 0])</p>
<p>预测一个输出单词的时候：([0.8, 0.1, 0, 0.05, 0, 0.05]) –&gt; gumbel_softmax –&gt; discriminator判断一下</p>
<p>为了解决这个问题，人们大致走了两条路线，一条是将普通的argmax转变成可导的Gumbel-softmax，然后我们就可以同时优化generator和discriminator了。</p>
<p>预测一个输出单词的时候：([0.8, 0.1, 0, 0.05, 0, 0.05]) –&gt; gumbel_softmax –&gt; discriminator判断一下</p>
<p><a href="https://arxiv.org/pdf/1611.04051.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1611.04051.pdf</a></p>
<p><a href="https://www.zhihu.com/question/62631725" target="_blank" rel="noopener">https://www.zhihu.com/question/62631725</a></p>
<p>另外一种方法是使用<strong>Reinforcement Learning</strong>中的<strong>Policy Gradient</strong>来估算模型的gradient，并做优化。</p>
<p>根据当前的policy来<strong>sample</strong> steps。</p>
<p>NLP： policy就是我们的<strong>语言模型</strong>，也就是说根据当前的hidden state, 决定我下一步要生成什么单词。</p>
<p>P_vocab –&gt; argmax</p>
<p>P_vocab –&gt; sampling</p>
<p>backpropagation –&gt; 没有办法更新模型</p>
<p>文本翻译 – 优化<strong>BLEU?</strong></p>
<p>训练？ cross entropy loss</p>
<p>policy gradient直接优化BLEU</p>
<p>可以不可以找个方法估算gradient。</p>
<p>Policy: 当前执行的策略,在文本生成模型中，这个Policy一般就是指我们的decoder(LSTM)</p>
<p>Policy Gradient: 根据当前的policy执行任务，然后得到reward，并估算每个参数的gradient, SGD</p>
<p>这里就涉及到一些Reinforcement Learning当中的基本知识。我们可以认为一个语言模型，例如LSTM，是在做一连串连续的决策。每一个decoding的步骤，每个hidden state对应一个<strong>状态state</strong>，每个输出对应一个<strong>observation</strong>。如果我们每次输出一个文字的时候使用sampling的方法，Reinforcement Learning有一套成熟的算法可以帮助我们估算模型的梯度，这种算法叫做policy gradient。如果采用这种方法，我们也可以对模型进行优化。</p>
<p><a href="https://arxiv.org/pdf/1609.05473.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1609.05473.pdf</a></p>
<p>这一套policy gradient的做法在很多文本生成（例如翻译，image captioning）的优化问题上也经常见到。</p>
<p>翻译：优化BLEU</p>
<p>Improved Image Captioning via Policy Gradient optimization of SPIDEr</p>
<p><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Improved_Image_Captioning_ICCV_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Improved_Image_Captioning_ICCV_2017_paper.pdf</a></p>
<p>还有一些方法是，我们不做最终的文本采样，我们直接使用模型输出的在单词表上的输出分布，或者是使用LSTM中的一些hidden vector来传给discriminator，并直接优化语言模型。</p>
<p>我个人的看法是GAN在文本生成上的作用大小还不明确，一部分原因在于我们没有一种很好的机制去评估文本生成的好坏。我们看到很多论文其实对模型的好坏没有明确的评价，很多时候是随机产生几个句子，然后由作者来评价一下生成句子的好坏。</p>
<h1 id="Data-to-text"><a href="#Data-to-text" class="headerlink" title="Data-to-text"></a>Data-to-text</h1><p><img src="https://uploader.shimo.im/f/Fo4ylW4dnbgm68U9.png!thumbnail" alt="img"></p>
<ul>
<li><p>Content selection: 选择什么数据需要进入到我们的文本之中</p>
</li>
<li><p>Sentence planning: 决定句子的结构</p>
</li>
<li><p>Surface realization: 把句子结构转化成具体的字符串</p>
</li>
</ul>
<p><img src="https://uploader.shimo.im/f/9QQQLucVUMsIVS6P.png!thumbnail" alt="img"></p>
<p>问题定义</p>
<ul>
<li><p>输入: A table of records。每个record包含四个features: type, entity, value, home or away</p>
</li>
<li><p>输出: 一段文字描述</p>
</li>
</ul>
<h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="https://github.com/Morde-kaiser/LearningNotes/blob/master/GAN-Overview-Chinese.pdf" target="_blank" rel="noopener">https://github.com/Morde-kaiser/LearningNotes/blob/master/GAN-Overview-Chinese.pdf</a></p>
<p>William Wang关于GAN in NLP的slides: <a href="http://sameersingh.org/files/ppts/naacl19-advnlp-part1-william-slides.pdf" target="_blank" rel="noopener">http://sameersingh.org/files/ppts/naacl19-advnlp-part1-william-slides.pdf</a></p>
<p>这篇博文也讲的很好</p>
<p><a href="https://zhuanlan.zhihu.com/p/29168803" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29168803</a></p>
<p>参考该知乎专栏文章 <a href="https://zhuanlan.zhihu.com/p/36880287" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/36880287</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/inference/">inference</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-常见预训练模型" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/05/09/常见预训练模型/" class="article-date">
      <time datetime="2020-05-09T00:25:57.000Z" itemprop="datePublished">2020-05-09</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/09/常见预训练模型/">BERT&amp;ELMo&amp;co</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="【译】The-Illustrated-BERT-ELMo-and-co"><a href="#【译】The-Illustrated-BERT-ELMo-and-co" class="headerlink" title="【译】The Illustrated BERT, ELMo, and co."></a>【译】The Illustrated BERT, ELMo, and co.</h2><p><a href="https://www.mihaileric.com/posts/deep-contextualized-word-representations-elmo/" target="_blank" rel="noopener">ELMo: Contextualized Word Vectors</a></p>
<p>本文由Adam Liu授权转载，源链接 <a href="https://blog.csdn.net/qq_41664845/article/details/84787969#comments" target="_blank" rel="noopener">https://blog.csdn.net/qq_41664845/article/details/84787969</a></p>
<p>原文链接：The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</p>
<p>作者：Jay Alammar</p>
<p>修改：褚则伟 <a href="mailto:zeweichu@gmail.com" target="_blank" rel="noopener">zeweichu@gmail.com</a></p>
<p>BERT论文地址：BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding <a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">https://arxiv.org/abs/1810.04805</a></p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>2018年可谓是自然语言处理（NLP）的元年，在我们如何以最能捕捉潜在语义关系的方式  来辅助计算机对的句子概念性的理解 这方面取得了极大的发展进步。此外， NLP领域的一些开源社区已经发布了很多强大的组件，我们可以在自己的模型训练过程中免费的下载使用。（可以说今年是NLP的ImageNet时刻，因为这和几年前计算机视觉的发展很相似）</p>
<p><img src="https://uploader.shimo.im/f/Z0tgsQt24GAoKjkj.png!thumbnail" alt="img"></p>
<p>上图中，最新发布的BERT是一个NLP任务的里程碑式模型，它的发布势必会带来一个NLP的新时代。BERT是一个算法模型，它的出现打破了大量的自然语言处理任务的记录。在BERT的论文发布不久后，Google的研发团队还开放了该模型的代码，并提供了一些在大量数据集上预训练好的算法模型下载方式。Goole开源这个模型，并提供预训练好的模型，这使得所有人都可以通过它来构建一个涉及NLP的算法模型，节约了大量训练语言模型所需的时间，精力，知识和资源。</p>
<p><img src="https://uploader.shimo.im/f/6xxJC31NvvYDCGFQ.png!thumbnail" alt="img"></p>
<p>BERT集成了最近一段时间内NLP领域中的一些顶尖的思想，包括但不限于 Semi-supervised Sequence Learning (by Andrew Dai and Quoc Le), ELMo (by Matthew Peters and researchers from AI2 and UW CSE), ULMFiT (by fast.ai founder Jeremy Howard and Sebastian Ruder), and the OpenAI transformer (by OpenAI researchers Radford, Narasimhan, Salimans, and Sutskever), and the Transformer (Vaswani et al).。</p>
<p>你需要注意一些事情才能恰当的理解BERT的内容，不过，在介绍模型涉及的概念之前可以使用BERT的方法。 </p>
<h2 id="示例：句子分类"><a href="#示例：句子分类" class="headerlink" title="示例：句子分类"></a>示例：句子分类</h2><p>使用BERT最简单的方法就是做一个文本分类模型，这样的模型结构如下图所示：</p>
<p><img src="https://uploader.shimo.im/f/8T7zkJ6MWgwE98oi.png!thumbnail" alt="img"></p>
<p>为了训练一个这样的模型，（主要是训练一个分类器），在训练阶段BERT模型发生的变化很小。该训练过程称为微调，并且源于 Semi-supervised Sequence Learning 和 ULMFiT.。</p>
<p>为了更方便理解，我们下面举一个分类器的例子。分类器是属于监督学习领域的，这意味着你需要一些标记的数据来训练这些模型。对于垃圾邮件分类器的示例，标记的数据集由邮件的内容和邮件的类别2部分组成（类别分为“垃圾邮件”或“非垃圾邮件”）。</p>
<h1 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h1><p>现在您已经了解了如何使用BERT的示例，让我们仔细了解一下他的工作原理。</p>
<p><img src="https://uploader.shimo.im/f/1jNYmPwPIDEzhsLv.png!thumbnail" alt="img"></p>
<p>BERT的论文中介绍了2种版本：</p>
<ul>
<li><p>BERT BASE - 与OpenAI Transformer的尺寸相当，以便比较性能</p>
</li>
<li><p>BERT LARGE - 一个非常庞大的模型，它完成了本文介绍的最先进的结果。</p>
</li>
</ul>
<p>BERT的基础集成单元是Transformer的Encoder。关于Transformer的介绍可以阅读作者之前的文章：The Illustrated Transformer，该文章解释了Transformer模型 - BERT的基本概念以及我们接下来要讨论的概念。</p>
<p>2个BERT的模型都有一个很大的编码器层数，（论文里面将此称为Transformer Blocks） - 基础版本就有12层，进阶版本有24层。同时它也有很大的前馈神经网络（ 768和1024个隐藏层神经元），还有很多attention heads（12-16个）。这超过了Transformer论文中的参考配置参数（6个编码器层，512个隐藏层单元，和8个注意头）</p>
<h2 id="模型输入"><a href="#模型输入" class="headerlink" title="模型输入"></a>模型输入</h2><p>输入的第一个字符为[CLS]，在这里字符[CLS]表达的意思很简单 - Classification （分类）。</p>
<p>BERT与Transformer 的编码方式一样。将固定长度的字符串作为输入，数据由下而上传递计算，每一层都用到了self attention，并通过前馈神经网络传递其结果，将其交给下一个编码器。</p>
<p><img src="https://uploader.shimo.im/f/4VwjFpmDltoJInZh.png!thumbnail" alt="img"></p>
<p>这样的架构，似乎是沿用了Transformer 的架构（除了层数，不过这是我们可以设置的参数）。那么BERT与Transformer 不同之处在哪里呢？可能在模型的输出上，我们可以发现一些端倪。</p>
<h2 id="模型输出"><a href="#模型输出" class="headerlink" title="模型输出"></a>模型输出</h2><p>每个位置返回的输出都是一个隐藏层大小的向量（基本版本BERT为768）。以文本分类为例，我们重点关注第一个位置上的输出（第一个位置是分类标识[CLS]） 。如下图</p>
<p>该向量现在可以用作我们选择的分类器的输入，在论文中指出使用单层神经网络作为分类器就可以取得很好的效果。原理如下：</p>
<p><img src="https://uploader.shimo.im/f/X6bsq7gTFDERfO9s.png!thumbnail" alt="img"></p>
<p>例子中只有垃圾邮件和非垃圾邮件，如果你有更多的label，你只需要增加输出神经元的个数即可，另外把最后的激活函数换成softmax即可。</p>
<h2 id="Parallels-with-Convolutional-Nets（BERT-VS卷积神经网络）"><a href="#Parallels-with-Convolutional-Nets（BERT-VS卷积神经网络）" class="headerlink" title="Parallels with Convolutional Nets（BERT VS卷积神经网络）"></a>Parallels with Convolutional Nets（BERT VS卷积神经网络）</h2><p>对于那些具有计算机视觉背景的人来说，这个矢量切换应该让人联想到VGGNet等网络的卷积部分与网络末端的完全连接的分类部分之间发生的事情。你可以这样理解，实质上这样理解也很方便。</p>
<p><img src="https://uploader.shimo.im/f/SIlXQTqB9vM4DEDi.png!thumbnail" alt="img"></p>
<h1 id="词嵌入的新时代〜"><a href="#词嵌入的新时代〜" class="headerlink" title="词嵌入的新时代〜"></a>词嵌入的新时代〜</h1><p>BERT的开源随之而来的是一种词嵌入的更新。到目前为止，词嵌入已经成为NLP模型处理自然语言的主要组成部分。诸如Word2vec和Glove 等方法已经广泛的用于处理这些问题，在我们使用新的词嵌入之前，我们有必要回顾一下其发展。</p>
<h2 id="Word-Embedding-Recap"><a href="#Word-Embedding-Recap" class="headerlink" title="Word Embedding Recap"></a>Word Embedding Recap</h2><p>为了让机器可以学习到文本的特征属性，我们需要一些将文本数值化的表示的方式。Word2vec算法通过使用一组固定维度的向量来表示单词，计算其方式可以捕获到单词的语义及单词与单词之间的关系。使用Word2vec的向量化表示方式可以用于判断单词是否相似，对立，或者说判断“男人‘与’女人”的关系就如同“国王”与“王后”。（这些话是不是听腻了〜 emmm水文必备）。另外还能捕获到一些语法的关系，这个在英语中很实用。例如“had”与“has”的关系如同“was”与“is”的关系。</p>
<p>这样的做法，我们可以使用大量的文本数据来预训练一个词嵌入模型，而这个词嵌入模型可以广泛用于其他NLP的任务，这是个好主意，这使得一些初创公司或者计算资源不足的公司，也能通过下载已经开源的词嵌入模型来完成NLP的任务。</p>
<h2 id="ELMo：语境问题"><a href="#ELMo：语境问题" class="headerlink" title="ELMo：语境问题"></a>ELMo：语境问题</h2><p>上面介绍的词嵌入方式有一个很明显的问题，因为使用预训练好的词向量模型，那么无论上下文的语境关系如何，每个单词都只有一个唯一的且已经固定保存的向量化形式“。Wait a minute “ - 出自(Peters et. al., 2017, McCann et. al., 2017, and yet again Peters et. al., 2018 in the ELMo paper )</p>
<blockquote>
<p>“ Wait a minute ”这是一个欧美日常梗，示例：</p>
</blockquote>
<blockquote>
<p>​                         我：兄弟，你认真学习深度，没准能拿80W年薪啊。</p>
</blockquote>
<blockquote>
<p>​                         你：Wait a minute，这么好，你为啥不做。 </p>
</blockquote>
<p>这和中文的同音字其实也类似，用这个举一个例子吧， ‘长’ 这个字，在 ‘长度’ 这个词中表示度量，在 ‘长高’ 这个词中表示增加。那么为什么我们不通过”长’周围是度或者是高来判断它的读音或者它的语义呢？嗖嘎，这个问题就派生出语境化的词嵌入模型。</p>
<p><img src="https://uploader.shimo.im/f/AahBpyq3tDodAsMn.png!thumbnail" alt="img"></p>
<p>EMLo改变Word2vec类的将单词固定为指定长度的向量的处理方式，它是在为每个单词分配词向量之前先查看整个句子，然后使用bi-LSTM来训练它对应的词向量。</p>
<p><img src="https://uploader.shimo.im/f/IPV3LOYXmr8m8GN7.png!thumbnail" alt="img"></p>
<p>ELMo为解决NLP的语境问题作出了重要的贡献，它的LSTM可以使用与我们任务相关的大量文本数据来进行训练，然后将训练好的模型用作其他NLP任务的词向量的基准。</p>
<p>ELMo的秘密是什么？</p>
<p>ELMo会训练一个模型，这个模型接受一个句子或者单词的输入,输出最有可能出现在后面的一个单词。想想输入法，对啦，就是这样的道理。这个在NLP中我们也称作Language Modeling。这样的模型很容易实现，因为我们拥有大量的文本数据且我们可以在不需要标签的情况下去学习。</p>
<p><img src="https://uploader.shimo.im/f/7z7sv9ALI24kQSst.png!thumbnail" alt="img"></p>
<p>上图介绍了ELMo预训练的过程的步骤的一部分：</p>
<p>我们需要完成一个这样的任务：输入“Lets stick to”，预测下一个最可能出现的单词，如果在训练阶段使用大量的数据集进行训练，那么在预测阶段我们可能准确的预测出我们期待的下一个单词。比如输入“机器”，在‘’学习‘和‘买菜’中它最有可能的输出会是‘学习’而不是‘买菜’。</p>
<p>从上图可以发现，每个展开的LSTM都在最后一步完成预测。</p>
<p>对了真正的ELMo会更进一步，它不仅能判断下一个词，还能预测前一个词。（Bi-Lstm）</p>
<p><img src="https://uploader.shimo.im/f/HWw1FQCwDbUJkIi5.png!thumbnail" alt="img"></p>
<p>ELMo通过下图的方式将hidden states（的初始的嵌入）组合咋子一起来提炼出具有语境意义的词嵌入方式（全连接后加权求和）</p>
<p><img src="https://uploader.shimo.im/f/ZldUQJvmyjsiR5fx.png!thumbnail" alt="img"></p>
<p>ELMo pretrained embedding可以在AllenNLP的repo下找到</p>
<p><a href="https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md" target="_blank" rel="noopener">https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md</a></p>
<p>顺便说一下AllenNLP有个非常不错的关于NLP的教程</p>
<p><a href="https://github.com/allenai/writing-code-for-nlp-research-emnlp2018" target="_blank" rel="noopener">https://github.com/allenai/writing-code-for-nlp-research-emnlp2018</a></p>
<p>ELMo的几位作者都是NLP圈内的知名人士</p>
<ul>
<li><p><a href="https://people.cs.umass.edu/~miyyer/" target="_blank" rel="noopener">Mohit Iyyer: UMass</a></p>
</li>
<li><p><a href="https://www.cs.washington.edu/people/faculty/lsz/" target="_blank" rel="noopener">Luke Zettlemoyer: UWashington</a></p>
</li>
<li><p><a href="https://matt-gardner.github.io/" target="_blank" rel="noopener">Matt Gardner: Allan AI</a></p>
</li>
</ul>
<h3 id="更多ELMo的模型图片"><a href="#更多ELMo的模型图片" class="headerlink" title="更多ELMo的模型图片"></a>更多ELMo的模型图片</h3><p><img src="https://uploader.shimo.im/f/khgCdxx0pNIaAVe3.png!thumbnail" alt="img"></p>
<p>图片来源（<a href="https://tsenghungchen.github.io/posts/elmo/）" target="_blank" rel="noopener">https://tsenghungchen.github.io/posts/elmo/）</a></p>
<p><img src="https://uploader.shimo.im/f/TId9a8gwTE0DTjua.png!thumbnail" alt="img"></p>
<p>图片来源（<a href="https://www.mihaileric.com/posts/deep-contextualized-word-representations-elmo/）" target="_blank" rel="noopener">https://www.mihaileric.com/posts/deep-contextualized-word-representations-elmo/）</a></p>
<h2 id="ULM-FiT：NLP领域应用迁移学习"><a href="#ULM-FiT：NLP领域应用迁移学习" class="headerlink" title="ULM-FiT：NLP领域应用迁移学习"></a>ULM-FiT：NLP领域应用迁移学习</h2><p>ULM-FiT机制让模型的预训练参数得到更好的利用。所利用的参数不仅限于embeddings，也不仅限于语境embedding，ULM-FiT引入了Language Model和一个有效微调该Language Model来执行各种NLP任务的流程。这使得NLP任务也能像计算机视觉一样方便的使用迁移学习。</p>
<h2 id="The-Transformer：超越LSTM的结构"><a href="#The-Transformer：超越LSTM的结构" class="headerlink" title="The Transformer：超越LSTM的结构"></a>The Transformer：超越LSTM的结构</h2><p>Transformer论文和代码的发布，以及其在机器翻译等任务上取得的优异成果，让一些研究人员认为它是LSTM的替代品，事实上却是Transformer比LSTM更好的处理long-term dependancies（长程依赖）问题。Transformer Encoding和Decoding的结构非常适合机器翻译，但是怎么利用他来做文本分类的任务呢？实际上你只用使用它来预训练可以针对其他任务微调的语言模型即可。</p>
<h2 id="OpenAI-Transformer：用于语言模型的Transformer解码器预训练"><a href="#OpenAI-Transformer：用于语言模型的Transformer解码器预训练" class="headerlink" title="OpenAI Transformer：用于语言模型的Transformer解码器预训练"></a>OpenAI Transformer：用于语言模型的Transformer解码器预训练</h2><p>事实证明，我们并不需要一个完整的transformer结构来使用迁移学习和一个很好的语言模型来处理NLP任务。我们只需要Transformer的解码器就行了。The decoder is a good choice because it’s a natural choice for language modeling (predicting the next word) since it’s built to mask future tokens – a valuable feature when it’s generating a translation word by word.</p>
<p><img src="https://uploader.shimo.im/f/cBZwrEweFIQLuP0O.png!thumbnail" alt="img"></p>
<p>该模型堆叠了十二个Decoder层。 由于在该设置中没有Encoder，因此这些Decoder将不具有Transformer Decoder层具有的Encoder - Decoder attention层。 然而，取而代之的是一个self attention层（masked so it doesn’t peak at future tokens）。</p>
<p>通过这种结构调整，我们可以继续在相似的语言模型任务上训练模型：使用大量的未标记数据集训练，来预测下一个单词。举个列子：你那7000本书喂给你的模型，（书籍是极好的训练样本~比博客和推文好很多。）训练框架如下：</p>
<p><img src="https://uploader.shimo.im/f/KdcfSdkeNBIb5iRT.png!thumbnail" alt="img"></p>
<h2 id="Transfer-Learning-to-Downstream-Tasks"><a href="#Transfer-Learning-to-Downstream-Tasks" class="headerlink" title="Transfer Learning to Downstream Tasks"></a>Transfer Learning to Downstream Tasks</h2><p>通过OpenAI的transformer的预训练和一些微调后，我们就可以将训练好的模型，用于其他下游NLP任务啦。（比如训练一个语言模型，然后拿他的hidden state来做分类。），下面就介绍一下这个骚操作。（还是如上面例子：分为垃圾邮件和非垃圾邮件）</p>
<p><img src="https://uploader.shimo.im/f/7x6X4ngskaEUd6sY.png!thumbnail" alt="img"></p>
<p>OpenAI论文概述了许多Transformer使用迁移学习来处理不同类型NLP任务的例子。如下图例子所示：</p>
<p><img src="https://uploader.shimo.im/f/P4V9NbGQz9Q2k213.png!thumbnail" alt="img"></p>
<h2 id="BERT-From-Decoders-to-Encoders"><a href="#BERT-From-Decoders-to-Encoders" class="headerlink" title="BERT: From Decoders to Encoders"></a>BERT: From Decoders to Encoders</h2><p>OpenAI transformer为我们提供了基于Transformer的精密的预训练模型。但是从LSTM到Transformer的过渡中，我们发现少了些东西。ELMo的语言模型是双向的，但是OpenAI的transformer是前向训练的语言模型。我们能否让我们的Transformer模型也具有Bi-Lstm的特性呢？</p>
<p>R-BERT：“Hold my beer”</p>
<h2 id="Masked-Language-Model"><a href="#Masked-Language-Model" class="headerlink" title="Masked Language Model"></a>Masked Language Model</h2><p>BERT说：“我要用 transformer 的 encoders”</p>
<p>Ernie不屑道：“呵呵，你不能像Bi-Lstm一样考虑文章”</p>
<p>BERT自信回答道：“我们会用masks”</p>
<blockquote>
<p>解释一下Mask：</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>语言模型会根据前面单词来预测下一个单词，但是self-attention的注意力只会放在自己身上，那么这样100%预测到自己，毫无意义，所以用Mask，把需要预测的词给挡住。</p>
</blockquote>
<p>如下图：</p>
<p><img src="https://uploader.shimo.im/f/jvcJ8SPeBEwszR8M.png!thumbnail" alt="img"></p>
<h2 id="Two-sentence-Tasks"><a href="#Two-sentence-Tasks" class="headerlink" title="Two-sentence Tasks"></a>Two-sentence Tasks</h2><p>我们回顾一下OpenAI transformer处理不同任务的输入转换，你会发现在某些任务上我们需要2个句子作为输入，并做一些更为智能的判断，比如是否相似，比如 给出一个维基百科的内容作为输入，同时在放入一条针对该条目的问题，那么我们的算法模型能够处理这个问题吗？</p>
<p>为了使BERT更好的处理2个句子之间的关系，预训练的过程还有一个额外的任务：给定2个句子（A和B）,A与B是否相似？（0或者1）</p>
<h2 id="特殊NLP任务"><a href="#特殊NLP任务" class="headerlink" title="特殊NLP任务"></a>特殊NLP任务</h2><p>BERT的论文为我们介绍了几种BERT可以处理的NLP任务：</p>
<ol>
<li><p>短文本相似 </p>
</li>
<li><p>文本分类</p>
</li>
<li><p>QA机器人</p>
</li>
<li><p>语义标注</p>
</li>
</ol>
<p><img src="https://uploader.shimo.im/f/yKFxOevBvMQXvjnv.png!thumbnail" alt="img"></p>
<h2 id="BERT用做特征提取"><a href="#BERT用做特征提取" class="headerlink" title="BERT用做特征提取"></a>BERT用做特征提取</h2><p>微调方法并不是使用BERT的唯一方法，就像ELMo一样，你可以使用预选训练好的BERT来创建语境化词嵌入。然后你可以将这些嵌入提供给现有的模型。</p>
<p><img src="https://uploader.shimo.im/f/uKUkG73gELQGry4L.png!thumbnail" alt="img"></p>
<p>哪个向量最适合作为上下文嵌入？ 我认为这取决于任务。 本文考察了六种选择（与微调模型相比，得分为96.4）：</p>
<p><img src="https://uploader.shimo.im/f/bfpUyWE9YCEP9IU2.png!thumbnail" alt="img"></p>
<h1 id="如何使用BERT"><a href="#如何使用BERT" class="headerlink" title="如何使用BERT"></a>如何使用BERT</h1><p>使用BERT的最佳方式是通过 BERT FineTuning with Cloud TPUs (<a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb" target="_blank" rel="noopener">https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb</a>) 谷歌云上托管的笔记。如果你未使用过谷歌云TPU可以试试看，这是个不错的尝试。另外BERT也适用于TPU，CPU和GPU</p>
<p>下一步是查看BERT仓库中的代码：</p>
<ol>
<li><p>该模型在modeling.py（BertModel类）中构建，与vanilla Transformer编码器完全相同。</p>
</li>
<li><p>run_classifier.py是微调过程的一个示例。它还构建了监督模型的分类层。如果要构建自己的分类器，请查看该文件中的create_model()方法。</p>
</li>
<li><p>可以下载几种预先训练的模型。涵盖102种语言的多语言模型，这些语言都是在维基百科的数据基础上训练而成的。</p>
</li>
<li><p>BERT不会将单词视为tokens。相反，它注重WordPieces。 tokenization.py是将你的单词转换为适合BERT的wordPieces的tokensizer。</p>
</li>
</ol>
<p>我自己给BERT的代码增加了一些注解</p>
<p><a href="https://github.com/ZeweiChu/bert/blob/master/modeling.py" target="_blank" rel="noopener">https://github.com/ZeweiChu/bert/blob/master/modeling.py</a></p>
<p>重点关注其中的：</p>
<ul>
<li><p>attention_layer: <a href="https://github.com/ZeweiChu/bert/blob/master/modeling.py#L638" target="_blank" rel="noopener">https://github.com/ZeweiChu/bert/blob/master/modeling.py#L638</a></p>
</li>
<li><p>transformer_model: <a href="https://github.com/ZeweiChu/bert/blob/master/modeling.py#L868" target="_blank" rel="noopener">https://github.com/ZeweiChu/bert/blob/master/modeling.py#L868</a></p>
</li>
</ul>
<p>BERT的很多任务基于GLUE benchmark</p>
<p><a href="https://gluebenchmark.com/tasks/" target="_blank" rel="noopener">https://gluebenchmark.com/tasks/</a></p>
<p><a href="https://openreview.net/pdf?id=rJ4km2R5t7" target="_blank" rel="noopener">https://openreview.net/pdf?id=rJ4km2R5t7</a></p>
<p>最近还有一个SuperGLUE</p>
<p><a href="https://w4ngatang.github.io/static/papers/superglue.pdf" target="_blank" rel="noopener">https://w4ngatang.github.io/static/papers/superglue.pdf</a></p>
<p>您还可以查看BERT的PyTorch实现 (<a href="https://github.com/huggingface/pytorch-transformers)。" target="_blank" rel="noopener">https://github.com/huggingface/pytorch-transformers)。</a> AllenNLP库使用此实现允许将BERT嵌入与任何模型一起使用。</p>
<p>最近NVIDIA开源了他们53分钟训练BERT的代码</p>
<p><a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT" target="_blank" rel="noopener">https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT</a></p>
<hr>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>BERT全文翻译成中文</p>
<p><a href="https://zhuanlan.zhihu.com/p/59775981" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/59775981</a></p>
<p>图解 BERT 模型：从零开始构建 BERT</p>
<p><a href="https://flashgene.com/archives/20062.html" target="_blank" rel="noopener">https://flashgene.com/archives/20062.html</a></p>
<p>NLP必读：十分钟读懂谷歌BERT模型</p>
<p><a href="https://zhuanlan.zhihu.com/p/51413773" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/51413773</a></p>
<p>BERT Explained: State of the art language model for NLP</p>
<p><a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270" target="_blank" rel="noopener">https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BERT/">BERT</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ELMo/">ELMo</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-大规模无监督预训练语言模型与应用上" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/05/01/大规模无监督预训练语言模型与应用上/" class="article-date">
      <time datetime="2020-05-01T00:23:44.000Z" itemprop="datePublished">2020-05-01</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/01/大规模无监督预训练语言模型与应用上/">大规模无监督预训练语言模型与应用上</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="Subword-Modeling"><a href="#Subword-Modeling" class="headerlink" title="Subword Modeling"></a>Subword Modeling</h3><p>以单词作为模型的基本单位有一些问题：</p>
<ul>
<li><p>单词量有限，我们一般会把单词量固定在50k-300k，然后没有见过的单词只能用<strong>UNK</strong>表示</p>
</li>
<li><p>zipf distribution: given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.: the rank-frequency distribution is an inverse relation. </p>
</li>
<li><p>模型参数量太大，100K * 300 = 30M个参数，仅仅是embedding层</p>
</li>
<li><p>对于很多语言，例如英语来说，很多时候单词是由几个subword拼接而成的</p>
</li>
<li><p>对于中文来说，很多常用的模型会采用分词后得到的词语作为模型的基本单元，同样存在上述问题</p>
</li>
</ul>
<p>可能的解决方案：</p>
<ul>
<li><p>使用subword information，例如字母作为语言的基本单元 Char-CNN</p>
</li>
<li><p>用wordpiece</p>
</li>
</ul>
<h2 id="解决方案：character-level-modeling"><a href="#解决方案：character-level-modeling" class="headerlink" title="解决方案：character level modeling"></a>解决方案：character level modeling</h2><ul>
<li>使用字母作为模型的基本输入单元</li>
</ul>
<h3 id="Ling-et-al-Finding-Function-in-Form-Compositional-Character-Models-for-Open-Vocabulary-Word-Representation"><a href="#Ling-et-al-Finding-Function-in-Form-Compositional-Character-Models-for-Open-Vocabulary-Word-Representation" class="headerlink" title="Ling et. al, Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation"></a>Ling et. al, <a href="https://aclweb.org/anthology/D15-1176" target="_blank" rel="noopener">Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation</a></h3><p>用BiLSTM把单词中的每个字母encode到一起</p>
<p><img src="https://uploader.shimo.im/f/V49ti0noVOsqeLRH.png!thumbnail" alt="img"></p>
<h3 id="Yoon-Kim-et-al-Character-Aware-Neural-Language-Models"><a href="#Yoon-Kim-et-al-Character-Aware-Neural-Language-Models" class="headerlink" title="Yoon Kim et. al, Character-Aware Neural Language Models"></a>Yoon Kim et. al, <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/viewFile/12489/12017" target="_blank" rel="noopener">Character-Aware Neural Language Models</a></h3><p><img src="https://uploader.shimo.im/f/bsR8NzGROvs0scpq.png!thumbnail" alt="img"></p>
<p>根据以上模型示意图思考以下问题：</p>
<ul>
<li><p>character emebdding的的维度是多少？4</p>
</li>
<li><p>有几个character 4-gram的filter？filter-size=4? 红色的 5个filter</p>
</li>
<li><p>max-over-time pooling: 3-gram 4维， 2-gram 3维 4-gram 55维</p>
</li>
<li><p>为什么不同的filter (kernel size)长度会导致不同长度的feature map?  seq_length - kernel_size + 1</p>
</li>
</ul>
<p>fastText</p>
<ul>
<li>与word2vec类似，但是每个单词是它的character n-gram embeddings + word emebdding</li>
</ul>
<h2 id="解决方案：使用subword作为模型的基本单元"><a href="#解决方案：使用subword作为模型的基本单元" class="headerlink" title="解决方案：使用subword作为模型的基本单元"></a>解决方案：使用subword作为模型的基本单元</h2><h3 id="Botha-amp-Blunsom-2014-Composional-Morphology-for-Word-Representations-and-Language-Modelling"><a href="#Botha-amp-Blunsom-2014-Composional-Morphology-for-Word-Representations-and-Language-Modelling" class="headerlink" title="Botha &amp; Blunsom (2014): Composional Morphology for Word Representations    and    Language Modelling"></a>Botha &amp; Blunsom (2014): <a href="http://proceedings.mlr.press/v32/botha14.pdf" target="_blank" rel="noopener">Composional Morphology for Word Representations    and    Language Modelling</a></h3><p><img src="https://uploader.shimo.im/f/FplKX422O5owOuVV.png!thumbnail" alt="img"></p>
<p>subword embedding</p>
<p><img src="https://uploader.shimo.im/f/3nQUw9cZGvwNSCyx.png!thumbnail" alt="img"></p>
<h3 id="Byte-Pair-Encoding-需要知道什么是BPE"><a href="#Byte-Pair-Encoding-需要知道什么是BPE" class="headerlink" title="Byte Pair Encoding (需要知道什么是BPE)"></a>Byte Pair Encoding (需要知道什么是BPE)</h3><p><a href="https://www.aclweb.org/anthology/P16-1162" target="_blank" rel="noopener">Neural Machine Translation of Rare Words with Subword Units</a></p>
<p>关于什么是BPE可以参考下面的文章</p>
<p><a href="https://www.cnblogs.com/huangyc/p/10223075.html" target="_blank" rel="noopener">https://www.cnblogs.com/huangyc/p/10223075.html</a></p>
<p><a href="https://leimao.github.io/blog/Byte-Pair-Encoding/" target="_blank" rel="noopener">https://leimao.github.io/blog/Byte-Pair-Encoding/</a></p>
<ul>
<li><p>首先定义所有可能的基本字符（abcde…）</p>
</li>
<li><p>然后开始循环数出最经常出现的pairs，加入到我们的候选字符（基本组成单元）中去</p>
</li>
</ul>
<p>a, b, c, d, …, z, A, B, …., Z.. !, @, ?, st, est, lo, low, </p>
<p>控制单词表的大小</p>
<ul>
<li>我只要确定iteration的次数 30000个iteartion，30000+原始字母表当中的字母数 个单词</li>
</ul>
<p>happiest</p>
<p>h a p p i est</p>
<p>LSTM</p>
<p>emb(h), emb(a), emb(p), emb(p), emb(i), emb(est)</p>
<p>happ, iest</p>
<p>emb(happ), emb(iest)</p>
<p><img src="https://uploader.shimo.im/f/0zx2ooI2uzoWLfOg.png!thumbnail" alt="img"></p>
<p><a href="https://www.aclweb.org/anthology/P16-1162.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/P16-1162.pdf</a></p>
<h2 id="中文词向量"><a href="#中文词向量" class="headerlink" title="中文词向量"></a>中文词向量</h2><h3 id="Meng-et-al-Is-Word-Segmentation-Necessary-for-Deep-Learning-of-Chinese-Representations"><a href="#Meng-et-al-Is-Word-Segmentation-Necessary-for-Deep-Learning-of-Chinese-Representations" class="headerlink" title="Meng et. al, Is Word Segmentation Necessary for Deep Learning of Chinese Representations?"></a>Meng et. al, <a href="https://arxiv.org/pdf/1905.05526.pdf" target="_blank" rel="noopener">Is Word Segmentation Necessary for Deep Learning of Chinese Representations?</a></h3><p>简单来说，这篇文章的作者生成通过他们的实验发现Chinese Word Segmentation对于语言模型、文本分类，翻译和文本关系分类并没有什么帮助，直接使用单个字作为模型的输入可以达到更好的效果。</p>
<blockquote>
<p>We benchmark neural word-based models which rely on word segmentation against neural char-based models which do not involve word segmentation in four end-to-end NLP benchmark tasks: language modeling, machine translation, sentence matching/paraphrase and text classification. Through direct comparisons between these two types of models, we find that charbased models consistently outperform wordbased models.</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>word-based models are more vulnerable to data sparsity and the presence of out-of-vocabulary (OOV) words, and thus more prone to overfitting</p>
</blockquote>
<p>Jiwei Li</p>
<p><a href="https://nlp.stanford.edu/~bdlijiwei/" target="_blank" rel="noopener">https://nlp.stanford.edu/~bdlijiwei/</a></p>
<h3 id="中文分词工具"><a href="#中文分词工具" class="headerlink" title="中文分词工具"></a>中文分词工具</h3><p>建议同学们可以在自己的项目中尝试以下工具</p>
<ul>
<li><p>北大中文分词工具 </p>
</li>
<li><p><a href="https://github.com/lancopku/pkuseg-python" target="_blank" rel="noopener">https://github.com/lancopku/pkuseg-python</a> </p>
</li>
<li><p>机器之心报道 <a href="https://www.jiqizhixin.com/articles/2019-01-09-12" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2019-01-09-12</a></p>
</li>
<li><p>清华分词工具 <a href="https://github.com/thunlp/THULAC-Python" target="_blank" rel="noopener">https://github.com/thunlp/THULAC-Python</a></p>
</li>
<li><p>结巴 <a href="https://github.com/fxsjy/jieba" target="_blank" rel="noopener">https://github.com/fxsjy/jieba</a></p>
</li>
</ul>
<h1 id="预训练句子-文档向量"><a href="#预训练句子-文档向量" class="headerlink" title="预训练句子/文档向量"></a>预训练句子/文档向量</h1><p>既然有词向量，那么我们是否可以更进一步，把句子甚至一整个文档也编码成一个向量呢？</p>
<p>在之前的课程中我们已经涉及到了一些句子级别的任务，例如文本分类，常常就是把一句或者若干句文本分类成一定的类别。此类模型的一般实现方式是首先把文本编码成某种文本表示方式，例如averaged word embeddings，或者双向LSTM头尾拼接，或者CNN模型等等。</p>
<p>文本分类</p>
<ul>
<li><p>文本通过某种方式变成一个向量</p>
</li>
<li><p>WORDAVG</p>
</li>
<li><p>LSTM</p>
</li>
<li><p>CNN</p>
</li>
<li><p>最后是一个linear layer 300维句子向量 –》 2 情感分类</p>
</li>
</ul>
<p>猫图片/狗图片</p>
<p>图片 –&gt; <strong>ResNet</strong> –&gt; 2048维向量 –&gt; (2, 2048) –&gt; 2维向量 binary cross entropy loss</p>
<p><strong>ResNet</strong> 预训练模型</p>
<p>文本 –&gt; TextResNet –&gt; 2048维向量</p>
<p>apply to any downstream tasks</p>
<p>TextResNet：LSTM模型</p>
<p>不同的任务（例如不同的文本分类：情感分类，话题分类）虽然最终的输出不同，但是往往拥有着相似甚至完全一样的编码层。如果我们能够预训练一个非常好的编码层，那么后续模型的负担就可以在一定程度上得到降低。这样的思想很多是来自图像处理的相关工作。例如人们在各类图像任务中发现，如果使用在ImageNet上预训练过的深层CNN网络（例如ResNet），只把最终的输出层替换成自己需要的样子，往往可以取得非常好的效果，且可以在少量数据的情况下训练出优质的模型。</p>
<p>在句子/文本向量预训练的领域涌现出了一系列的工作，下面我们选取一些有代表性的工作供大家学习参考。</p>
<h2 id="Skip-Thought"><a href="#Skip-Thought" class="headerlink" title="Skip-Thought"></a>Skip-Thought</h2><p>Kiros et. al, <a href="https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf" target="_blank" rel="noopener">Skip-Thought Vectors</a></p>
<p>skip-gram: distributional semantics of words 用中心词–》周围词</p>
<p>skip-thought: distributional semantics of sentences 用中心句–》周围句</p>
<p>两个句子如果总是在同一个环境下出现，那么这两个句子可能有某种含义上的联系</p>
<p>如何把句子map成一个向量：compositional model，RNN, LSTM, CNN, WordAvg, <strong>GRU</strong></p>
<p>Skip-thought 模型的思想非常简单，我们训练一个基于GRU的模型作为句子的编码器。事实上Skip-thought这个名字与Skip-gram有着千丝万缕的联系，它们基于一个共同的思想，就是一句话（一个单词）的含义与它所处的环境（context，周围句子/单词）高度相关。</p>
<p>如下图所示，Skipthought采用一个GRU encoder，使用编码器最后一个hidden state来表示整个句子。然后使用这个hidden state作为初始状态来解码它之前和之后的句子。</p>
<p>decoder: 两个conditional语言模型。</p>
<p>基于中心句的句子向量，优化conditional log likelihood</p>
<p><img src="https://uploader.shimo.im/f/keW15vUeJX4E7gam.png!thumbnail" alt="img"></p>
<p>一个encoder GRU</p>
<p><img src="https://uploader.shimo.im/f/XbXbCNlxSpk5PLuh.png!thumbnail" alt="img"></p>
<p>两个decoder GRU</p>
<p><img src="https://uploader.shimo.im/f/atuHcc6hYNIE2QOd.png!thumbnail" alt="img"></p>
<p>训练目标</p>
<p><img src="https://uploader.shimo.im/f/XCVPs561UVADzFkO.png!thumbnail" alt="img"></p>
<p>然后我们就可以把encoder当做feature extractor了。</p>
<p>类似的工作还有<a href="https://arxiv.org/pdf/1602.03483.pdf" target="_blank" rel="noopener">FastSent</a>。FastSent直接使用词向量之和来表示整个句子，然后用该句子向量来解码周围句子中的单个单词们。</p>
<h2 id="InferSent"><a href="#InferSent" class="headerlink" title="InferSent"></a>InferSent</h2><p><a href="https://www.aclweb.org/anthology/D17-1070" target="_blank" rel="noopener">Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</a></p>
<p>Natural Language Inference (NLI)</p>
<ul>
<li><p>给定两个句子，判断这两个句子之间的关系</p>
</li>
<li><p>entailment 承接关系</p>
</li>
<li><p>neutral 没有关系</p>
</li>
<li><p>contradiction 矛盾</p>
</li>
<li><p>(non_entailment)</p>
</li>
</ul>
<h3 id="SNLI任务"><a href="#SNLI任务" class="headerlink" title="SNLI任务"></a>SNLI任务</h3><p>给定两个句子，预测这两个句子的关系是entailment, contradiction，还是neutral  </p>
<p>一个简单有效的模型</p>
<p><img src="https://uploader.shimo.im/f/bdClv9vmULUCXgcc.png!thumbnail" alt="img"></p>
<p>Encoder是BiLSTM + max pooling</p>
<p><img src="https://uploader.shimo.im/f/Uiw1y8KX5pEw5Lri.png!thumbnail" alt="img"></p>
<p>模型效果</p>
<p><img src="https://uploader.shimo.im/f/kzvejsQ7DMI3369N.png!thumbnail" alt="img"></p>
<h2 id="SentEval"><a href="#SentEval" class="headerlink" title="SentEval"></a>SentEval</h2><p><a href="https://www.aclweb.org/anthology/L18-1269" target="_blank" rel="noopener">SentEval: An Evaluation Toolkit for Universal Sentence Representations</a></p>
<p>一个非常通用的benchmark，用来评估句子embedding是否能够很好地应用于downstream tasks。</p>
<p>Github: <a href="https://github.com/facebookresearch/SentEval" target="_blank" rel="noopener">https://github.com/facebookresearch/SentEval</a></p>
<h2 id="Document-Vector"><a href="#Document-Vector" class="headerlink" title="Document Vector"></a>Document Vector</h2><p>事实上研究者在句子向量上的各种尝试是不太成功的。主要体现在这些预训练向量并不能非常好地提升模型在各种下游任务上的表现，人们大多数时候还是从头开始训练模型。</p>
<p>在document vector上的尝试就更不尽如人意了，因为一个文本往往包含非常丰富的信息，而一个向量能够编码的信息量实在太小。</p>
<p>Learning Deep Structured Semantic Models for Web Search using Clickthrough Data</p>
<p><a href="https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/" target="_blank" rel="noopener">https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/</a></p>
<p>Hierarchical Attention Networks for Document Classification</p>
<p><a href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf" target="_blank" rel="noopener">https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf</a></p>
<h1 id="ELMo-BERT"><a href="#ELMo-BERT" class="headerlink" title="ELMo, BERT"></a><a href="https://shimo.im/docs/Y6q3gX8yGGjpWqXx" target="_blank" rel="noopener">ELMo, BERT</a></h1><p>ELMO paper: <a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1802.05365.pdf</a></p>
<h1 id="Transformer中的Encoder"><a href="#Transformer中的Encoder" class="headerlink" title="Transformer中的Encoder"></a><a href="https://shimo.im/docs/gPwkqCXrkJyRW89V" target="_blank" rel="noopener">Transformer中的Encoder</a></h1>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BERT/">BERT</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ELMo/">ELMo</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/">Transformer</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-word2vec" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/04/24/word2vec/" class="article-date">
      <time datetime="2020-04-24T05:29:22.000Z" itemprop="datePublished">2020-04-24</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/24/word2vec/">word2vec</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h4 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h4><ul>
<li>有一个很大的词表库</li>
<li>在词表中的每个词都可以通过向量表征</li>
<li>有一个中心词c，有一个输出词o</li>
<li>用词c和o的相似度来计算他们之间同时出现的概率</li>
<li>调整这个词向量来获得最大输出概率</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/word2vec/">word2vec</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cbow/">cbow</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hierarchical-softmax/">hierarchical softmax</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/negative-sampling/">negative sampling</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/skip-gram/">skip-gram</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-特征工程与模型调优" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/04/20/特征工程与模型调优/" class="article-date">
      <time datetime="2020-04-20T06:17:41.000Z" itemprop="datePublished">2020-04-20</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/20/特征工程与模型调优/">特征工程与模型调优</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="机器学习特征工程"><a href="#机器学习特征工程" class="headerlink" title="机器学习特征工程"></a>机器学习特征工程</h2><h3 id="机器学习流程与概念"><a href="#机器学习流程与概念" class="headerlink" title="机器学习流程与概念"></a>机器学习流程与概念</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBncGV.jpg" alt></p>
<h3 id="机器学习建模流程"><a href="#机器学习建模流程" class="headerlink" title="机器学习建模流程"></a>机器学习建模流程</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBn2xU.png" alt></p>
<h3 id="机器学习特征工程一览"><a href="#机器学习特征工程一览" class="headerlink" title="机器学习特征工程一览"></a>机器学习特征工程一览</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBnOMD.jpg" alt></p>
<h3 id="机器学习特征工程介绍"><a href="#机器学习特征工程介绍" class="headerlink" title="机器学习特征工程介绍"></a>机器学习特征工程介绍</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBnjqH.jpg" alt></p>
<h3 id="特征清洗"><a href="#特征清洗" class="headerlink" title="特征清洗"></a>特征清洗</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBumon.jpg" alt><br><img src="https://s1.ax1x.com/2020/04/24/JBuKJ0.jpg" alt><br><img src="https://s1.ax1x.com/2020/04/24/JBu3yF.jpg" alt></p>
<h3 id="数值型数据上的特征工程"><a href="#数值型数据上的特征工程" class="headerlink" title="数值型数据上的特征工程"></a>数值型数据上的特征工程</h3><p>数值型数据通常以标量的形式表示数据，描述观测值、记录或者测量值。本文的数值型数据是指连续型数据而不是离散型数据，表示不同类目的数据就是后者。数值型数据也可以用向量来表示，向量的每个值或分量代表一个特征。整数和浮点数是连续型数值数据中最常见也是最常使用的数值型数据类型。即使数值型数据可以直接输入到机器学习模型中，你仍需要在建模前设计与场景、问题和领域相关的特征。因此仍需要特征工程。让我们利用 python 来看看在数值型数据上做特征工程的一些策略。我们首先加载下面一些必要的依赖（通常在 <a href="http://jupyter.org/" target="_blank" rel="noopener"><strong>Jupyter</strong> </a> botebook 上）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">&gt;</span><br><span class="line">&gt; <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">&gt;</span><br><span class="line">&gt; <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">&gt;</span><br><span class="line">&gt; <span class="keyword">import</span> scipy.stats <span class="keyword">as</span> spstats</span><br><span class="line">&gt;</span><br><span class="line">&gt; %matplotlib inline</span><br></pre></td></tr></table></figure>

<p>原始度量</p>
<p>正如我们先前提到的，根据上下文和数据的格式，原始数值型数据通常可直接输入到机器学习模型中。原始的度量方法通常用数值型变量来直接表示为特征，而不需要任何形式的变换或特征工程。通常这些特征可以表示一些值或总数。让我们加载四个数据集之一的 <a href="https://www.kaggle.com/abcsds/pokemon/data" target="_blank" rel="noopener">Pokemon </a>数据集，该数据集也在 <a href="https://www.kaggle.com/abcsds/pokemon/data" target="_blank" rel="noopener">Kaggle </a>上公布了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">poke_df = pd.read_csv(<span class="string">'datasets/Pokemon.csv'</span>, encoding=<span class="string">'utf-8'</span>) </span><br><span class="line"></span><br><span class="line">poke_df.head()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f55514768e.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="我们的Pokemon数据集截图"><a href="#我们的Pokemon数据集截图" class="headerlink" title="我们的Pokemon数据集截图"></a>我们的Pokemon数据集截图</h5><p>Pokemon 是一个大型多媒体游戏，包含了各种口袋妖怪（Pokemon）角色。简而言之，你可以认为他们是带有超能力的动物！这些数据集由这些口袋妖怪角色构成，每个角色带有各种统计信息。</p>
<h4 id="数值"><a href="#数值" class="headerlink" title="数值"></a>数值</h4><p>如果你仔细地观察上图中这些数据，你会看到几个代表数值型原始值的属性，它可以被直接使用。下面的这行代码挑出了其中一些重点特征。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poke_df[[&apos;HP&apos;, &apos;Attack&apos;, &apos;Defense&apos;]].head()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f557552811.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="带（连续型）数值数据的特征"><a href="#带（连续型）数值数据的特征" class="headerlink" title="带（连续型）数值数据的特征"></a>带（连续型）数值数据的特征</h5><p>这样，你可以直接将这些属性作为特征，如上图所示。这些特征包括 Pokemon 的 HP（血量），Attack（攻击）和 Defense（防御）状态。事实上，我们也可以基于这些字段计算出一些基本的统计量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poke_df[[&apos;HP&apos;, &apos;Attack&apos;, &apos;Defense&apos;]].describe()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f559f61c14.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p><strong>数值特征形式的基本描述性统计量</strong></p>
<p>这样你就对特征中的统计量如总数、平均值、标准差和四分位数有了一个很好的印象。</p>
<h4 id="记数"><a href="#记数" class="headerlink" title="记数"></a>记数</h4><p>原始度量的另一种形式包括代表频率、总数或特征属性发生次数的特征。让我们看看 <a href="https://labrosa.ee.columbia.edu/millionsong/" target="_blank" rel="noopener"><strong>millionsong</strong></a> <strong>数据集</strong>中的一个例子，其描述了某一歌曲被各种用户收听的总数或频数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">popsong_df = pd.read_csv(&apos;datasets/song_views.csv&apos;,encoding=&apos;utf-8&apos;)</span><br><span class="line"></span><br><span class="line">popsong_df.head(10)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f55bf6176f.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="数值特征形式的歌曲收听总数"><a href="#数值特征形式的歌曲收听总数" class="headerlink" title="数值特征形式的歌曲收听总数"></a>数值特征形式的歌曲收听总数</h5><p>根据这张截图，显而易见 listen_count 字段可以直接作为基于数值型特征的频数或总数。</p>
<h4 id="二值化"><a href="#二值化" class="headerlink" title="二值化"></a>二值化</h4><p>基于要解决的问题构建模型时，通常原始频数或总数可能与此不相关。比如如果我要建立一个推荐系统用来推荐歌曲，我只希望知道一个人是否感兴趣或是否听过某歌曲。我不需要知道一首歌被听过的次数，因为我更关心的是一个人所听过的各种各样的歌曲。在这个例子中，二值化的特征比基于计数的特征更合适。我们二值化 listen_count 字段如下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; watched = np.array(popsong_df[&apos;listen_count&apos;])</span><br><span class="line">&gt;</span><br><span class="line">&gt; watched[watched &gt;= 1] = 1</span><br><span class="line">&gt;</span><br><span class="line">&gt; popsong_df[&apos;watched&apos;] = watched</span><br></pre></td></tr></table></figure>

<p>你也可以使用 scikit-learn 中 preprocessing 模块的 Binarizer 类来执行同样的任务，而不一定使用 numpy 数组。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import Binarizer</span><br><span class="line"></span><br><span class="line">bn = Binarizer(threshold=0.9)</span><br><span class="line"></span><br><span class="line">pd_watched =bn.transform([popsong_df[&apos;listen_count&apos;]])[0]</span><br><span class="line"></span><br><span class="line">popsong_df[&apos;pd_watched&apos;] = pd_watched</span><br><span class="line"></span><br><span class="line">popsong_df.head(11)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56505e8ff.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="歌曲收听总数的二值化结构"><a href="#歌曲收听总数的二值化结构" class="headerlink" title="歌曲收听总数的二值化结构"></a>歌曲收听总数的二值化结构</h5><p>你可以从上面的截图中清楚地看到，两个方法得到了相同的结果。因此我们得到了一个二值化的特征来表示一首歌是否被每个用户听过，并且可以在相关的模型中使用它。</p>
<h4 id="数据舍入"><a href="#数据舍入" class="headerlink" title="数据舍入"></a>数据舍入</h4><p>处理连续型数值属性如比例或百分比时，我们通常不需要高精度的原始数值。因此通常有必要将这些高精度的百分比舍入为整数型数值。这些整数可以直接作为原始数值甚至分类型特征（基于离散类的）使用。让我们试着将这个观念应用到一个虚拟数据集上，该数据集描述了库存项和他们的流行度百分比。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">items_popularity =pd.read_csv(<span class="string">'datasets/item_popularity.csv'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">items_popularity[<span class="string">'popularity_scale_10'</span>] = np.array(np.round((items_popularity[<span class="string">'pop_percent'</span>] * <span class="number">10</span>)),dtype=<span class="string">'int'</span>)</span><br><span class="line"></span><br><span class="line">items_popularity[<span class="string">'popularity_scale_100'</span>] = np.array(np.round((items_popularity[<span class="string">'pop_percent'</span>] * <span class="number">100</span>)),dtype=<span class="string">'int'</span>)</span><br><span class="line"></span><br><span class="line">items_popularity</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f566e30ad2.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="不同尺度下流行度舍入结果"><a href="#不同尺度下流行度舍入结果" class="headerlink" title="不同尺度下流行度舍入结果"></a>不同尺度下流行度舍入结果</h5><p>基于上面的输出，你可能猜到我们试了两种不同的舍入方式。这些特征表明项目流行度的特征现在既有 1-10 的尺度也有 1-100 的尺度。基于这个场景或问题你可以使用这些值同时作为数值型或分类型特征。</p>
<h4 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h4><p>高级机器学习模型通常会对作为输入特征变量函数的输出响应建模（离散类别或连续数值）。例如，一个简单的线性回归方程可以表示为</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56ab22fb7.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>其中输入特征用变量表示为</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56c69ac66.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>权重或系数可以分别表示为</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f56de74ee7.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>目标是预测响应 <strong>*y*</strong>.</p>
<p>在这个例子中，仅仅根据单个的、分离的输入特征，这个简单的线性模型描述了输出与输入之间的关系。</p>
<p>然而，在一些真实场景中，有必要试着捕获这些输入特征集一部分的特征变量之间的相关性。上述带有相关特征的线性回归方程的展开式可以简单表示为</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f5701419ee.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>此处特征可表示为</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f57162d4f7.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>表示了相关特征。现在让我们试着在 Pokemon 数据集上设计一些相关特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">atk_def = poke_df[[<span class="string">'Attack'</span>, <span class="string">'Defense'</span>]]</span><br><span class="line"></span><br><span class="line">atk_def.head()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f572bad2cc.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>从输出数据框中，我们可以看到我们有两个数值型（连续的）特征，Attack 和 Defence。现在我们可以利用 scikit-learn 建立二度特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">pf = PolynomialFeatures(degree=<span class="number">2</span>,</span><br><span class="line"></span><br><span class="line">interaction_only=<span class="literal">False</span>,include_bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">res = pf.fit_transform(atk_def)</span><br><span class="line"></span><br><span class="line">res</span><br><span class="line"></span><br><span class="line">**Output**</span><br><span class="line"></span><br><span class="line">**------**</span><br><span class="line"></span><br><span class="line">array([[ <span class="number">49.</span>, <span class="number">49.</span>, <span class="number">2401.</span>, <span class="number">2401.</span>, <span class="number">2401.</span>],</span><br><span class="line"></span><br><span class="line">  [ <span class="number">62.</span>, <span class="number">63.</span>, <span class="number">3844.</span>, <span class="number">3906.</span>, <span class="number">3969.</span>],</span><br><span class="line"></span><br><span class="line">  [ <span class="number">82.</span>, <span class="number">83.</span>, <span class="number">6724.</span>, <span class="number">6806.</span>, <span class="number">6889.</span>],</span><br><span class="line"></span><br><span class="line">  ...,</span><br><span class="line"></span><br><span class="line">  [ <span class="number">110.</span>, <span class="number">60.</span>, <span class="number">12100.</span>, <span class="number">6600.</span>, <span class="number">3600.</span>],</span><br><span class="line"></span><br><span class="line">  [ <span class="number">160.</span>, <span class="number">60.</span>, <span class="number">25600.</span>, <span class="number">9600.</span>, <span class="number">3600.</span>],</span><br><span class="line"></span><br><span class="line">[ <span class="number">110.</span>, <span class="number">120.</span>, <span class="number">12100.</span>, <span class="number">13200.</span>, <span class="number">14400.</span>]])</span><br></pre></td></tr></table></figure>

<p>上面的特征矩阵一共描述了 5 个特征，其中包括新的相关特征。我们可以看到上述矩阵中每个特征的度，如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.DataFrame(pf.powers_, columns=[<span class="string">'Attack_degree'</span>,<span class="string">'Defense_degree'</span>])</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f575a65683.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>基于这个输出，现在我们可以通过每个特征的度知道它实际上代表什么。在此基础上，现在我们可以对每个特征进行命名如下。这仅仅是为了便于理解，你可以给这些特征取更好的、容易使用和简单的名字。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">intr_features = pd.DataFrame(res, columns=[<span class="string">'Attack'</span>,<span class="string">'Defense'</span>,<span class="string">'Attack^2'</span>,<span class="string">'Attack x Defense'</span>,<span class="string">'Defense^2'</span>])</span><br><span class="line"></span><br><span class="line">intr_features.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f576e91376.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="数值型特征及其相关特征"><a href="#数值型特征及其相关特征" class="headerlink" title="数值型特征及其相关特征"></a>数值型特征及其相关特征</h5><p>因此上述数据代表了我们原始的特征以及它们的相关特征。</p>
<h4 id="分区间处理数据"><a href="#分区间处理数据" class="headerlink" title="分区间处理数据"></a>分区间处理数据</h4><p>处理原始、连续的数值型特征问题通常会导致这些特征值的分布被破坏。这表明有些值经常出现而另一些值出现非常少。除此之外，另一个问题是这些特征的值的变化范围。比如某个音乐视频的观看总数会非常大（<a href="https://www.youtube.com/watch?v=kJQP7kiw5Fk" target="_blank" rel="noopener">Despacito</a>，说你呢）而一些值会非常小。直接使用这些特征会产生很多问题，反而会影响模型表现。因此出现了处理这些问题的技巧，包括分区间法和变换。</p>
<p>分区间（Bining），也叫做量化，用于将连续型数值特征转换为离散型特征（类别）。可以认为这些离散值或数字是类别或原始的连续型数值被分区间或分组之后的数目。每个不同的区间大小代表某种密度，因此一个特定范围的连续型数值会落在里面。对数据做分区间的具体技巧包括等宽分区间以及自适应分区间。我们使用从 <a href="https://github.com/freeCodeCamp/2016-new-coder-survey" target="_blank" rel="noopener">2016 年 FreeCodeCamp 开发者和编码员调查报告</a>中抽取出来的一个子集中的数据，来讨论各种针对编码员和软件开发者的属性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df =pd.read_csv(<span class="string">'datasets/fcc_2016_coder_survey_subset.csv'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'EmploymentField'</span>, <span class="string">'Age'</span>,<span class="string">'Income'</span>]].head()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f578e01139.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="来自FCC编码员调查数据集的样本属性"><a href="#来自FCC编码员调查数据集的样本属性" class="headerlink" title="来自FCC编码员调查数据集的样本属性"></a>来自FCC编码员调查数据集的样本属性</h5><p>对于每个参加调查的编码员或开发者，ID.x 变量基本上是一个唯一的标识符而其他字段是可自我解释的。</p>
<h4 id="等宽分区间"><a href="#等宽分区间" class="headerlink" title="等宽分区间"></a>等宽分区间</h4><p>就像名字表明的那样，在等宽分区间方法中，每个区间都是固定宽度的，通常可以预先分析数据进行定义。基于一些领域知识、规则或约束，每个区间有个预先固定的值的范围，只有处于范围内的数值才被分配到该区间。基于数据舍入操作的分区间是一种方式，你可以使用数据舍入操作来对原始值进行分区间，我们前面已经讲过。</p>
<p>现在我们分析编码员调查报告数据集的 Age 特征并看看它的分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Age'</span>].hist(color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Age Histogram'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Age'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f57b05846b.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="描述开发者年龄分布的直方图"><a href="#描述开发者年龄分布的直方图" class="headerlink" title="描述开发者年龄分布的直方图"></a>描述开发者年龄分布的直方图</h5><p>上面的直方图表明，如预期那样，开发者年龄分布仿佛往左侧倾斜（上年纪的开发者偏少）。现在我们根据下面的模式，将这些原始年龄值分配到特定的区间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Age Range: Bin</span><br><span class="line"></span><br><span class="line">\---------------</span><br><span class="line"></span><br><span class="line"><span class="number">0</span> - <span class="number">9</span> : <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="number">10</span> - <span class="number">19</span> : <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="number">20</span> - <span class="number">29</span> : <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="number">30</span> - <span class="number">39</span> : <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="number">40</span> - <span class="number">49</span> : <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="number">50</span> - <span class="number">59</span> : <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="number">60</span> - <span class="number">69</span> : <span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="meta">... </span><span class="keyword">and</span> so on</span><br></pre></td></tr></table></figure>

<p>我们可以简单地使用我们先前学习到的数据舍入部分知识，先将这些原始年龄值除以 10，然后通过 floor 函数对原始年龄数值进行截断。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df[<span class="string">'Age_bin_round'</span>] = np.array(np.floor(np.array(fcc_survey_df[<span class="string">'Age'</span>]) / <span class="number">10.</span>))</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>,<span class="string">'Age_bin_round'</span>]].iloc[<span class="number">1071</span>:<span class="number">1076</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f57d916a6f.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="通过舍入法分区间"><a href="#通过舍入法分区间" class="headerlink" title="通过舍入法分区间"></a>通过舍入法分区间</h5><p>你可以看到基于数据舍入操作的每个年龄对应的区间。但是如果我们需要更灵活的操作怎么办？如果我们想基于我们的规则或逻辑，确定或修改区间的宽度怎么办？基于常用范围的分区间方法将帮助我们完成这个。让我们来定义一些通用年龄段位，使用下面的方式来对开发者年龄分区间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Age Range : Bin</span><br><span class="line"></span><br><span class="line">\---------------</span><br><span class="line"></span><br><span class="line"><span class="number">0</span> - <span class="number">15</span> : <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="number">16</span> - <span class="number">30</span> : <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="number">31</span> - <span class="number">45</span> : <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="number">46</span> - <span class="number">60</span> : <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="number">61</span> - <span class="number">75</span> : <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="number">75</span> - <span class="number">100</span> : <span class="number">6</span></span><br></pre></td></tr></table></figure>

<p>基于这些常用的分区间方式，我们现在可以对每个开发者年龄值的区间打标签，我们将存储区间的范围和相应的标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">bin_ranges = [<span class="number">0</span>, <span class="number">15</span>, <span class="number">30</span>, <span class="number">45</span>, <span class="number">60</span>, <span class="number">75</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">bin_names = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Age_bin_custom_range'</span>] = pd.cut(np.array(fcc_survey_df[<span class="string">'Age'</span>]),bins=bin_ranges)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Age_bin_custom_label'</span>] = pd.cut(np.array(fcc_survey_df[<span class="string">'Age'</span>]),bins=bin_ranges, labels=bin_names)</span><br><span class="line"></span><br><span class="line">\<span class="comment"># view the binned features</span></span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Age_bin_round'</span>,<span class="string">'Age_bin_custom_range'</span>,<span class="string">'Age_bin_custom_label'</span>]].iloc[<span class="number">10</span>a71:<span class="number">1076</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58143c35f.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="开发者年龄的常用分区间方式"><a href="#开发者年龄的常用分区间方式" class="headerlink" title="开发者年龄的常用分区间方式"></a>开发者年龄的常用分区间方式</h5><h4 id="自适应分区间"><a href="#自适应分区间" class="headerlink" title="自适应分区间"></a>自适应分区间</h4><p>使用等宽分区间的不足之处在于，我们手动决定了区间的值范围，而由于落在某个区间中的数据点或值的数目是不均匀的，因此可能会得到不规则的区间。一些区间中的数据可能会非常的密集，一些区间会非常稀疏甚至是空的！自适应分区间方法是一个更安全的策略，在这些场景中，我们让数据自己说话！这样，我们使用数据分布来决定区间的范围。</p>
<p>基于分位数的分区间方法是自适应分箱方法中一个很好的技巧。量化对于特定值或切点有助于将特定数值域的连续值分布划分为离散的互相挨着的区间。因此 q 分位数有助于将数值属性划分为 q 个相等的部分。关于量化比较流行的例子包括 2 分位数，也叫中值，将数据分布划分为2个相等的区间；4 分位数，也简称分位数，它将数据划分为 4 个相等的区间；以及 10 分位数，也叫十分位数，创建 10 个相等宽度的区间，现在让我们看看开发者数据集的 Income 字段的数据分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>].hist(bins=<span class="number">30</span>, color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Income Histogram'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Developer Income'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f583631eff.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p><strong>描述开发者收入分布的直方图</strong></p>
<p>上述的分布描述了一个在收入上右歪斜的分布，少数人赚更多的钱，多数人赚更少的钱。让我们基于自适应分箱方式做一个 4-分位数或分位数。我们可以很容易地得到如下的分位数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">quantile_list = [<span class="number">0</span>, <span class="number">.25</span>, <span class="number">.5</span>, <span class="number">.75</span>, <span class="number">1.</span>]</span><br><span class="line"></span><br><span class="line">quantiles =</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>].quantile(quantile_list)</span><br><span class="line"></span><br><span class="line">quantiles</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**Output**</span><br><span class="line"></span><br><span class="line">**------**</span><br><span class="line"></span><br><span class="line"><span class="number">0.00</span> <span class="number">6000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">0.25</span> <span class="number">20000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">0.50</span> <span class="number">37000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">0.75</span> <span class="number">60000.0</span></span><br><span class="line"></span><br><span class="line"><span class="number">1.00</span> <span class="number">200000.0</span></span><br><span class="line"></span><br><span class="line">Name: Income, dtype: float64</span><br></pre></td></tr></table></figure>

<p>现在让我们在原始的分布直方图中可视化下这些分位数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>].hist(bins=<span class="number">30</span>, color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> quantile <span class="keyword">in</span> quantiles:</span><br><span class="line"></span><br><span class="line">qvl = plt.axvline(quantile, color=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">ax.legend([qvl], [<span class="string">'Quantiles'</span>], fontsize=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Income Histogram with Quantiles'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Developer Income'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f5853f1a2c.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="带分位数形式描述开发者收入分布的直方图"><a href="#带分位数形式描述开发者收入分布的直方图" class="headerlink" title="带分位数形式描述开发者收入分布的直方图"></a>带分位数形式描述开发者收入分布的直方图</h5><p>上面描述的分布中红色线代表了分位数值和我们潜在的区间。让我们利用这些知识来构建我们基于分区间策略的分位数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">quantile_labels = [<span class="string">'0-25Q'</span>, <span class="string">'25-50Q'</span>, <span class="string">'50-75Q'</span>, <span class="string">'75-100Q'</span>]</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_quantile_range'</span>] = pd.qcut(</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>],q=quantile_list)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_quantile_label'</span>] = pd.qcut(</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income'</span>],q=quantile_list,labels=quantile_labels)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Income'</span>,<span class="string">'Income_quantile_range'</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">'Income_quantile_label'</span>]].iloc[<span class="number">4</span>:<span class="number">9</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f586dbd8f4.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="基于分位数的开发者收入的区间范围和标签"><a href="#基于分位数的开发者收入的区间范围和标签" class="headerlink" title="基于分位数的开发者收入的区间范围和标签"></a>基于分位数的开发者收入的区间范围和标签</h5><p>通过这个例子，你应该对如何做基于分位数的自适应分区间法有了一个很好的认识。一个需要重点记住的是，分区间的结果是离散值类型的分类特征，当你在模型中使用分类数据之前，可能需要额外的特征工程相关步骤。我们将在接下来的部分简要地讲述分类数据的特征工程技巧。</p>
<h4 id="统计变换"><a href="#统计变换" class="headerlink" title="统计变换"></a>统计变换</h4><p>我们讨论下先前简单提到过的数据分布倾斜的负面影响。现在我们可以考虑另一个特征工程技巧，即利用统计或数学变换。我们试试看 Log 变换和 Box-Cox 变换。这两种变换函数都属于幂变换函数簇，通常用来创建单调的数据变换。它们的主要作用在于它能帮助稳定方差，始终保持分布接近于正态分布并使得数据与分布的平均值无关。</p>
<h4 id="Log变换"><a href="#Log变换" class="headerlink" title="Log变换"></a>Log变换</h4><p>log 变换属于幂变换函数簇。该函数用数学表达式表示为</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f588a0f6a5.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>读为以 b 为底 x 的对数等于 y。这可以变换为</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f589e77242.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>表示以b为底指数必须达到多少才等于x。自然对数使用 b=e，e=2.71828，通常叫作欧拉常数。你可以使用通常在十进制系统中使用的 b=10 作为底数。</p>
<p><strong>当应用于倾斜分布时 Log 变换是很有用的，因为他们倾向于拉伸那些落在较低的幅度范围内自变量值的范围，倾向于压缩或减少更高幅度范围内的自变量值的范围</strong>。从而使得倾斜分布尽可能的接近正态分布。让我们对先前使用的开发者数据集的 Income 特征上使用log变换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df[<span class="string">'Income_log'</span>] = np.log((<span class="number">1</span>+fcc_survey_df[<span class="string">'Income'</span>]))</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Income'</span>,<span class="string">'Income_log'</span>]].iloc[<span class="number">4</span>:<span class="number">9</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58b3ed249.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="开发者收入log变换后结构"><a href="#开发者收入log变换后结构" class="headerlink" title="开发者收入log变换后结构"></a>开发者收入log变换后结构</h5><p>Income_log 字段描述了经过 log 变换后的特征。现在让我们来看看字段变换后数据的分布。</p>
<p>基于上面的图，我们可以清楚地看到与先前倾斜分布相比，该分布更加像正态分布或高斯分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">income_log_mean =np.round(np.mean(fcc_survey_df[<span class="string">'Income_log'</span>]), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_log'</span>].hist(bins=<span class="number">30</span>,color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>,grid=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">plt.axvline(income_log_mean, color=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">'Developer Income Histogram after Log Transform'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'Developer Income (log scale)'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">ax.text(<span class="number">11.5</span>, <span class="number">450</span>, <span class="string">r'$\mu$='</span>+str(income_log_mean),fontsize=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58cdaf02a.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p><strong>经过log变换后描述开发者收入分布的直方图</strong></p>
<h4 id="Box-Cox变换"><a href="#Box-Cox变换" class="headerlink" title="Box-Cox变换"></a>Box-Cox变换</h4><p>Box-Cox 变换是另一个流行的幂变换函数簇中的一个函数。该函数有一个前提条件，即数值型值必须先变换为正数（与 log 变换所要求的一样）。万一出现数值是负的，使用一个常数对数值进行偏移是有帮助的。数学上，Box-Cox 变换函数可以表示如下。</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58e556c08.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p>生成的变换后的输出y是输入 x 和变换参数的函数；当 λ=0 时，该变换就是自然对数 log 变换，前面我们已经提到过了。λ 的最佳取值通常由最大似然或最大对数似然确定。现在让我们在开发者数据集的收入特征上应用 Box-Cox 变换。首先我们从数据分布中移除非零值得到最佳的值，结果如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">income = np.array(fcc_survey_df[<span class="string">'Income'</span>])</span><br><span class="line"></span><br><span class="line">income_clean = income[~np.isnan(income)]</span><br><span class="line"></span><br><span class="line">l, opt_lambda = spstats.boxcox(income_clean)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Optimal lambda value:'</span>, opt_lambda)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**Output**</span><br><span class="line"></span><br><span class="line">**------**</span><br><span class="line"></span><br><span class="line">Optimal <span class="keyword">lambda</span> value: <span class="number">0.117991239456</span></span><br></pre></td></tr></table></figure>

<p>现在我们得到了最佳的值，让我们在取值为 0 和 λ（最佳取值 λ ）时使用 Box-Cox 变换对开发者收入特征进行变换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fcc_survey_df[<span class="string">'Income_boxcox_lambda_0'</span>] = spstats.boxcox((<span class="number">1</span>+fcc_survey_df[<span class="string">'Income'</span>]),lmbda=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[<span class="string">'Income_boxcox_lambda_opt'</span>] = spstats.boxcox(fcc_survey_df[<span class="string">'Income'</span>],lmbda=opt_lambda)</span><br><span class="line"></span><br><span class="line">fcc_survey_df[[<span class="string">'ID.x'</span>, <span class="string">'Age'</span>, <span class="string">'Income'</span>, <span class="string">'Income_log'</span>,<span class="string">'Income_boxcox_lambda_0'</span>,<span class="string">'Income_boxcox_lambda_opt'</span>]].iloc[<span class="number">4</span>:<span class="number">9</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f58fd7fd5e.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<h5 id="经过-Box-Cox-变换后开发者的收入分布"><a href="#经过-Box-Cox-变换后开发者的收入分布" class="headerlink" title="经过 Box-Cox 变换后开发者的收入分布"></a>经过 Box-Cox 变换后开发者的收入分布</h5><p>变换后的特征在上述数据框中描述了。就像我们期望的那样，Income_log 和 Income_boxcox_lamba_0具有相同的取值。让我们看看经过最佳λ变换后 Income 特征的分布。</p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;income_boxcox_mean = np.round(np.mean(fcc_survey_df[<span class="string">'Income_boxcox_lambda_opt'</span>]),<span class="number">2</span>)</span><br><span class="line">&gt; </span><br><span class="line">&gt;fig, ax = plt.subplots()</span><br><span class="line">&gt; </span><br><span class="line">&gt;fcc_survey_df[<span class="string">'Income_boxcox_lambda_opt'</span>].hist(bins=<span class="number">30</span>,  color=<span class="string">'#A9C5D3'</span>,edgecolor=<span class="string">'black'</span>, grid=<span class="literal">False</span>)</span><br><span class="line">&gt;    plt.axvline(income_boxcox_mean, color=<span class="string">'r'</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.set_title(<span class="string">'Developer Income Histogram after Box–Cox Transform'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.set_xlabel(<span class="string">'Developer Income (Box–Cox transform)'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.set_ylabel(<span class="string">'Frequency'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; ax.text(<span class="number">24</span>, <span class="number">450</span>, <span class="string">r'$\mu$='</span>+str(income_boxcox_mean),fontsize=<span class="number">10</span>)       </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5f591679bfb.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！上篇 - 连续数据的处理方法"></p>
<p><strong>经过Box-Cox变换后描述开发者收入分布的直方图</strong></p>
<p> 分布看起来更像是正态分布，与我们经过 log 变换后的分布相似。</p>
<h3 id="类别型数据上的特征工程"><a href="#类别型数据上的特征工程" class="headerlink" title="类别型数据上的特征工程"></a>类别型数据上的特征工程</h3><p>在深入研究特征工程之前，让我们先了解一下分类数据。通常，在<strong>自然界中可分类的任意数据属性都是离散值，这意味着它们属于某一特定的有限类别</strong>。在模型预测的属性或者变量（通常被称为<strong>响应变量 response variables</strong>）中，这些也经常被称为类别或者标签。这些离散值在自然界中可以是文本或者数字（甚至是诸如图像这样的非结构化数据）。分类数据有两大类——<strong>定类（Nominal）和定序（Ordinal）</strong>。</p>
<p>在任意定类分类数据属性中，这些属性值之间<strong>没有顺序的概念</strong>。如下图所示，举个简单的例子，天气分类。我们可以看到，在这个特定的场景中，主要有六个大类，而这些类之间没有任何顺序上的关系（刮风天并不总是发生在晴天之前，并且也不能说比晴天来的更小或者更大）</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af6bc87b4e.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>将天气作为分类属性</p>
<p>与天气相类似的属性还有很多，比如电影、音乐、电子游戏、国家、食物和美食类型等等，这些都属于定类分类属性。</p>
<p>定序分类的属性值则存在着一定的顺序意义或概念。例如，下图中的字母标识了衬衫的大小。显而易见的是，当我们考虑衬衫的时候，它的“大小”属性是很重要的（S 码比 M 码来的小，而 M 码又小于 L 码等等）。</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af6d3b83ac.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>衬衫大小作为定序分类属性</p>
<p>鞋号、受教育水平和公司职位则是定序分类属性的一些其它例子。既然已经对分类数据有了一个大致的理解之后，接下来我们来看看一些特征工程的策略。</p>
<p>在接受像文本标签这样复杂的分类数据类型问题上，各种机器学习框架均已取得了许多的进步。通常，特征工程中的任意标准工作流都涉及将这些分类值转换为数值标签的某种形式，然后对这些值应用一些<strong>编码方案</strong>。我们将在开始之前导入必要的工具包。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<h4 id="定类属性转换-LabelEncoding"><a href="#定类属性转换-LabelEncoding" class="headerlink" title="定类属性转换(LabelEncoding)"></a>定类属性转换(LabelEncoding)</h4><p><strong>定类属性由离散的分类值组成，它们没有先后顺序概念</strong>。这里的思想是将这些属性转换成更具代表性的数值格式，这样可以很容易被下游的代码和流水线所理解。我们来看一个关于视频游戏销售的新数据集。这个数据集也可以在 <a href="https://www.kaggle.com/gregorut/videogamesales" target="_blank" rel="noopener">Kaggle</a> 和我的 <a href="https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/notebooks/Ch04_Feature_Engineering_and_Selection" target="_blank" rel="noopener">GitHub</a> 仓库中找到。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vg_df = pd.read_csv(<span class="string">'datasets/vgsales.csv'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">vg_df[[<span class="string">'Name'</span>, <span class="string">'Platform'</span>, <span class="string">'Year'</span>, <span class="string">'Genre'</span>, <span class="string">'Publisher'</span>]].iloc[<span class="number">1</span>:<span class="number">7</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af756b687d.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>游戏销售数据</p>
<p>让我们首先专注于上面数据框中“视频游戏风格（Genre）”属性。显而易见的是，这是一个类似于“发行商（Publisher）”和“平台（Platform）”属性一样的定类分类属性。我们可以很容易得到一个独特的视频游戏风格列表，如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">genres = np.unique(vg_df[<span class="string">'Genre'</span>])</span><br><span class="line"></span><br><span class="line">genres</span><br><span class="line"></span><br><span class="line">Output</span><br><span class="line"></span><br><span class="line">\------</span><br><span class="line"></span><br><span class="line">array([<span class="string">'Action'</span>, <span class="string">'Adventure'</span>, <span class="string">'Fighting'</span>, <span class="string">'Misc'</span>, <span class="string">'Platform'</span>, <span class="string">'Puzzle'</span>, <span class="string">'Racing'</span>, <span class="string">'Role-Playing'</span>, <span class="string">'Shooter'</span>, <span class="string">'Simulation'</span>, <span class="string">'Sports'</span>, <span class="string">'Strategy'</span>], dtype=object)</span><br></pre></td></tr></table></figure>

<p>输出结果表明，我们有 12 种不同的视频游戏风格。我们现在可以生成一个标签编码方法，即利用 scikit-learn 将每个类别映射到一个数值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line">gle = LabelEncoder()</span><br><span class="line"></span><br><span class="line">genre_labels = gle.fit_transform(vg_df[<span class="string">'Genre'</span>])</span><br><span class="line"></span><br><span class="line">genre_mappings = &#123;index: label <span class="keyword">for</span> index, label <span class="keyword">in</span> enumerate(gle.classes_)&#125;</span><br><span class="line"></span><br><span class="line">genre_mappings</span><br><span class="line"></span><br><span class="line">Output</span><br><span class="line"></span><br><span class="line">\------</span><br><span class="line"></span><br><span class="line">&#123;<span class="number">0</span>: <span class="string">'Action'</span>, <span class="number">1</span>: <span class="string">'Adventure'</span>, <span class="number">2</span>: <span class="string">'Fighting'</span>, <span class="number">3</span>: <span class="string">'Misc'</span>, <span class="number">4</span>: <span class="string">'Platform'</span>, <span class="number">5</span>: <span class="string">'Puzzle'</span>, <span class="number">6</span>: <span class="string">'Racing'</span>, <span class="number">7</span>: <span class="string">'Role-Playing'</span>, <span class="number">8</span>: <span class="string">'Shooter'</span>, <span class="number">9</span>: <span class="string">'Simulation'</span>, <span class="number">10</span>: <span class="string">'Sports'</span>, <span class="number">11</span>: <span class="string">'Strategy'</span>&#125;</span><br></pre></td></tr></table></figure>

<p>因此，在 <em>LabelEncoder</em> 类的实例对象 <em>gle</em> 的帮助下生成了一个映射方案，成功地将每个风格属性映射到一个数值。转换后的标签存储在 <em>genre_labels</em> 中，该变量允许我们将其写回数据表中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vg_df[<span class="string">'GenreLabel'</span>] = genre_labels</span><br><span class="line"></span><br><span class="line">vg_df[[<span class="string">'Name'</span>, <span class="string">'Platform'</span>, <span class="string">'Year'</span>, <span class="string">'Genre'</span>, <span class="string">'GenreLabel'</span>]].iloc[<span class="number">1</span>:<span class="number">7</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af8164e6db.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>视频游戏风格及其编码标签</p>
<p>如果你打算将它们用作预测的响应变量，那么这些标签通常可以直接用于诸如 sikit-learn 这样的框架。但是如前所述，我们还需要额外的编码步骤才能将它们用作特征。</p>
<h4 id="定序属性编码"><a href="#定序属性编码" class="headerlink" title="定序属性编码"></a>定序属性编码</h4><p><strong>定序属性是一种带有先后顺序概念的分类属性</strong>。这里我将以本系列文章第一部分所使用的<a href="https://www.kaggle.com/abcsds/pokemon/data" target="_blank" rel="noopener">神奇宝贝数据集</a>进行说明。让我们先专注于 「世代（Generation）」 属性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; poke_df = pd.read_csv(<span class="string">'datasets/Pokemon.csv'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; poke_df = poke_df.sample(random_state=<span class="number">1</span>, frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">&gt;</span><br><span class="line">&gt; np.unique(poke_df[<span class="string">'Generation'</span>])</span><br><span class="line">&gt;</span><br><span class="line">&gt; Output</span><br><span class="line">&gt;</span><br><span class="line">&gt; \------</span><br><span class="line">&gt;</span><br><span class="line">&gt; array([<span class="string">'Gen 1'</span>, <span class="string">'Gen 2'</span>, <span class="string">'Gen 3'</span>, <span class="string">'Gen 4'</span>, <span class="string">'Gen 5'</span>, <span class="string">'Gen 6'</span>], dtype=object)</span><br></pre></td></tr></table></figure>

<p>根据上面的输出，我们可以看到一共有 6 代，并且每个神奇宝贝通常属于视频游戏的特定世代（依据发布顺序），而且电视系列也遵循了相似的时间线。这个属性通常是定序的（需要相关的领域知识才能理解），因为属于第一代的大多数神奇宝贝在第二代的视频游戏或者电视节目中也会被更早地引入。神奇宝贝的粉丝们可以看下下图，然后记住每一代中一些比较受欢迎的神奇宝贝（不同的粉丝可能有不同的看法）。</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af8f58f535.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>基于不同类型和世代选出的一些受欢迎的神奇宝贝</p>
<p>因此，它们之间存在着先后顺序。一般来说，没有通用的模块或者函数可以根据这些顺序自动将这些特征转换和映射到数值表示。因此，我们可以使用自定义的编码\映射方案。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">gen_ord_map = &#123;<span class="string">'Gen 1'</span>: <span class="number">1</span>, <span class="string">'Gen 2'</span>: <span class="number">2</span>, <span class="string">'Gen 3'</span>: <span class="number">3</span>, <span class="string">'Gen 4'</span>: <span class="number">4</span>, <span class="string">'Gen 5'</span>: <span class="number">5</span>, <span class="string">'Gen 6'</span>: <span class="number">6</span>&#125; </span><br><span class="line"></span><br><span class="line">poke_df[<span class="string">'GenerationLabel'</span>] = poke_df[<span class="string">'Generation'</span>].map(gen_ord_map)</span><br><span class="line"></span><br><span class="line">poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'GenerationLabel'</span>]].iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af95f94dc8.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>神奇宝贝世代编码</p>
<p>从上面的代码中可以看出，来自 <em>pandas</em> 库的 <em>map(…)</em> 函数在转换这种定序特征的时候非常有用。</p>
<h4 id="编码分类属性–独热编码方案（One-hot-Encoding-Scheme）"><a href="#编码分类属性–独热编码方案（One-hot-Encoding-Scheme）" class="headerlink" title="编码分类属性–独热编码方案（One-hot Encoding Scheme）"></a>编码分类属性–独热编码方案（One-hot Encoding Scheme）</h4><p>如果你还记得我们之前提到过的内容，通常对分类数据进行特征工程就涉及到一个转换过程，我们在前一部分描述了一个转换过程，还有一个强制编码过程，我们应用特定的编码方案为特定的每个类别创建虚拟变量或特征分类属性。</p>
<p>你可能想知道，我们刚刚在上一节说到将类别转换为数字标签，为什么现在我们又需要这个？原因很简单。考虑到视频游戏风格，如果我们直接将 <em>GenereLabel</em> 作为属性特征提供给机器学习模型，则模型会认为它是一个连续的数值特征，从而认为值 10 （体育）要大于值 6 （赛车），然而事实上这种信息是毫无意义的，因为<em>体育类型</em>显然并不大于或者小于<em>赛车类型</em>，这些不同值或者类别无法直接进行比较。因此我们需要另一套编码方案层，它要能为每个属性的所有不同类别中的每个唯一值或类别创建虚拟特征。</p>
<p>考虑到任意具有 m 个标签的分类属性（变换之后）的数字表示，独热编码方案将该属性编码或变换成 m 个二进制特征向量（向量中的每一维的值只能为 0 或 1）。那么在这个分类特征中每个属性值都被转换成一个 m 维的向量，其中只有某一维的值为 1。让我们来看看神奇宝贝数据集的一个子集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Legendary'</span>]].iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5af9b37bf97.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>神奇宝贝数据集子集</p>
<p>这里关注的属性是神奇宝贝的「世代（Generation）」和「传奇（Legendary）」状态。第一步是根据之前学到的将这些属性转换为数值表示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder, LabelEncoder</span><br><span class="line"></span><br><span class="line">\<span class="comment"># transform and map pokemon generations</span></span><br><span class="line"></span><br><span class="line">gen_le = LabelEncoder()</span><br><span class="line"></span><br><span class="line">gen_labels = gen_le.fit_transform(poke_df[<span class="string">'Generation'</span>])</span><br><span class="line"></span><br><span class="line">poke_df[<span class="string">'Gen_Label'</span>] = gen_labels</span><br><span class="line"></span><br><span class="line">\<span class="comment"># transform and map pokemon legendary status</span></span><br><span class="line"></span><br><span class="line">leg_le = LabelEncoder()</span><br><span class="line"></span><br><span class="line">leg_labels = leg_le.fit_transform(poke_df[<span class="string">'Legendary'</span>])</span><br><span class="line"></span><br><span class="line">poke_df[<span class="string">'Lgnd_Label'</span>] = leg_labels</span><br><span class="line"></span><br><span class="line">poke_df_sub = poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>, <span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>]]</span><br><span class="line"></span><br><span class="line">poke_df_sub.iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afa18d27fc.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>转换后的标签属性</p>
<p><em>Gen_Label</em> 和 <em>Lgnd_Label</em> 特征描述了我们分类特征的数值表示。现在让我们在这些特征上应用独热编码方案。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encode generation labels using one-hot encoding scheme</span></span><br><span class="line"></span><br><span class="line">gen_ohe = OneHotEncoder()</span><br><span class="line"></span><br><span class="line">gen_feature_arr = gen_ohe.fit_transform(poke_df[[<span class="string">'Gen_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">gen_feature_labels = list(gen_le.classes_)</span><br><span class="line"></span><br><span class="line">gen_features = pd.DataFrame(gen_feature_arr, columns=gen_feature_labels)</span><br><span class="line"></span><br><span class="line">\<span class="comment"># encode legendary status labels using one-hot encoding scheme</span></span><br><span class="line"></span><br><span class="line">leg_ohe = OneHotEncoder()</span><br><span class="line"></span><br><span class="line">leg_feature_arr = leg_ohe.fit_transform(poke_df[[<span class="string">'Lgnd_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">leg_feature_labels = [<span class="string">'Legendary_'</span>+str(cls_label) <span class="keyword">for</span> cls_label <span class="keyword">in</span> leg_le.classes_]</span><br><span class="line"></span><br><span class="line">leg_features = pd.DataFrame(leg_feature_arr, columns=leg_feature_labels)</span><br></pre></td></tr></table></figure>

<p>通常来说，你可以使用 <em>fit_transform</em> 函数将两个特征一起编码（通过将两个特征的二维数组一起传递给函数，详情<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" target="_blank" rel="noopener">查看文档</a>）。但是我们分开编码每个特征，这样可以更易于理解。除此之外，我们还可以创建单独的数据表并相应地标记它们。现在让我们链接这些特征表（Feature frames）然后看看最终的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">poke_df_ohe = pd.concat([poke_df_sub, gen_features, leg_features], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">columns = sum([[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>], gen_feature_labels, [<span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>], leg_feature_labels], [])</span><br><span class="line"></span><br><span class="line">poke_df_ohe[columns].iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afab9940ae.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>神奇宝贝世代和传奇状态的独热编码特征</p>
<p>此时可以看到已经为「世代（Generation）」生成 6 个虚拟变量或者二进制特征，并为「传奇（Legendary）」生成了 2 个特征。这些特征数量是这些属性中不同类别的总数。<strong>某一类别的激活状态通过将对应的虚拟变量置 1 来表示</strong>，这从上面的数据表中可以非常明显地体现出来。</p>
<p>考虑你在训练数据上建立了这个编码方案，并建立了一些模型，现在你有了一些新的数据，这些数据必须在预测之前进行如下设计。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_poke_df = pd.DataFrame([[<span class="string">'PikaZoom'</span>, <span class="string">'Gen 3'</span>, <span class="literal">True</span>], [<span class="string">'CharMyToast'</span>, <span class="string">'Gen 4'</span>, <span class="literal">False</span>]], columns=[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Legendary'</span>])</span><br><span class="line"></span><br><span class="line">new_poke_df</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afaf0cb42e.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>新数据</p>
<p>你可以通过调用之前构建的 <em>LabelEncoder</em> 和 <em>OneHotEncoder</em> 对象的 <em>transform()</em> 方法来处理新数据。请记得我们的工作流程，首先我们要做转换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">new_gen_labels = gen_le.transform(new_poke_df[<span class="string">'Generation'</span>])</span><br><span class="line"></span><br><span class="line">new_poke_df[<span class="string">'Gen_Label'</span>] = new_gen_labels</span><br><span class="line"></span><br><span class="line">new_leg_labels = leg_le.transform(new_poke_df[<span class="string">'Legendary'</span>])</span><br><span class="line"></span><br><span class="line">new_poke_df[<span class="string">'Lgnd_Label'</span>] = new_leg_labels</span><br><span class="line"></span><br><span class="line">new_poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>, <span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>]]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afb428f0dc.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>转换之后的分类属性</p>
<p>在得到了数值标签之后，接下来让我们应用编码方案吧！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">new_gen_feature_arr = gen_ohe.transform(new_poke_df[[<span class="string">'Gen_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">new_gen_features = pd.DataFrame(new_gen_feature_arr, columns=gen_feature_labels)</span><br><span class="line"></span><br><span class="line">new_leg_feature_arr = leg_ohe.transform(new_poke_df[[<span class="string">'Lgnd_Label'</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">new_leg_features = pd.DataFrame(new_leg_feature_arr, columns=leg_feature_labels)</span><br><span class="line"></span><br><span class="line">new_poke_ohe = pd.concat([new_poke_df, new_gen_features, new_leg_features], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">columns = sum([[<span class="string">'Name'</span>, <span class="string">'Generation'</span>, <span class="string">'Gen_Label'</span>], gen_feature_labels, [<span class="string">'Legendary'</span>, <span class="string">'Lgnd_Label'</span>], leg_feature_labels], [])</span><br><span class="line"></span><br><span class="line">new_poke_ohe[columns]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afb91bb3be.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>独热编码之后的分类属性</p>
<p>因此，通过利用 scikit-learn 强大的 API，我们可以很容易将编码方案应用于新数据。</p>
<p>你也可以通过利用来自 pandas 的 <em>to_dummies()</em> 函数轻松应用独热编码方案。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gen_onehot_features = pd.get_dummies(poke_df[<span class="string">'Generation'</span>])</span><br><span class="line"></span><br><span class="line">pd.concat([poke_df[[<span class="string">'Name'</span>, <span class="string">'Generation'</span>]], gen_onehot_features], axis=<span class="number">1</span>).iloc[<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afbcc0ff2b.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>使用 pandas 实现的独热编码特征</p>
<p>上面的数据表描述了应用在「世代（Generation）」属性上的独热编码方案，结果与之前的一致。</p>
<h4 id="区间计数方案（Bin-counting-Scheme）"><a href="#区间计数方案（Bin-counting-Scheme）" class="headerlink" title="区间计数方案（Bin-counting Scheme）"></a>区间计数方案（Bin-counting Scheme）</h4><p>到目前为止，我们所讨论的编码方案在分类数据方面效果还不错，但是当任意特征的不同类别数量变得很大的时候，问题开始出现。对于具有 m 个不同标签的任意分类特征这点非常重要，你将得到 m 个独立的特征。这会很容易地增加特征集的大小，从而导致在时间、空间和内存方面出现存储问题或者模型训练问题。除此之外，我们还必须处理“<a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" target="_blank" rel="noopener">维度诅咒</a>”问题，通常指的是拥有大量的特征，却缺乏足够的代表性样本，然后模型的性能开始受到影响并导致过拟合。</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afd0459749.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>因此，我们需要针对那些可能具有非常多种类别的特征（如 IP 地址），研究其它分类数据特征工程方案。区间计数方案是处理具有多个类别的分类变量的有效方案。在这个方案中，我们使用<strong>基于概率的统计信息和在建模过程中所要预测的实际目标或者响应值</strong>，而不是使用实际的标签值进行编码。一个简单的例子是，基于过去的 IP 地址历史数据和 DDOS 攻击中所使用的历史数据，我们可以为任一 IP 地址会被 DDOS 攻击的可能性建立概率模型。使用这些信息，我们可以对输入特征进行编码，该输入特征描述了如果将来出现相同的 IP 地址，则引起 DDOS 攻击的概率值是多少。<strong>这个方案需要历史数据作为先决条件，并且要求数据非常详尽。</strong></p>
<h4 id="特征哈希方案"><a href="#特征哈希方案" class="headerlink" title="特征哈希方案"></a>特征哈希方案</h4><p>特征哈希方案（Feature Hashing Scheme）是处理大规模分类特征的另一个有用的特征工程方案。在该方案中，哈希函数通常与预设的编码特征的数量（作为预定义长度向量）一起使用，使得特征的哈希值被用作这个预定义向量中的索引，并且值也要做相应的更新。由于哈希函数将大量的值映射到一个小的有限集合中，因此<strong>多个不同值可能会创建相同的哈希</strong>，这一现象称为<strong>冲突</strong>。典型地，使用带符号的哈希函数，使得从哈希获得的值的符号被用作那些在适当的索引处存储在最终特征向量中的值的符号。这样能够确保实现较少的冲突和由于冲突导致的误差累积。</p>
<p>哈希方案适用于字符串、数字和其它结构（如向量）。你可以将哈希输出看作一个有限的 <em>b bins</em> 集合，以便于当将哈希函数应用于相同的值\类别时，哈希函数能根据哈希值将其分配到 <em>b bins</em> 中的同一个 bin（或者 bins 的子集）。我们可以预先定义 <em>b</em> 的值，它成为我们使用特征哈希方案编码的每个分类属性的编码特征向量的最终尺寸。</p>
<p>因此，即使我们有一个特征拥有超过 <strong>1000</strong> 个不同的类别，我们设置 <strong>b = 10</strong> 作为最终的特征向量长度，那么最终输出的特征将只有 10 个特征。而采用独热编码方案则有 1000 个二进制特征。我们来考虑下视频游戏数据集中的「风格（Genre）」属性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">unique_genres = np.unique(vg_df[[<span class="string">'Genre'</span>]])</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Total game genres:"</span>, len(unique_genres))</span><br><span class="line"></span><br><span class="line">print(unique_genres)</span><br><span class="line"></span><br><span class="line">Output</span><br><span class="line"></span><br><span class="line">\------</span><br><span class="line"></span><br><span class="line">Total game genres: <span class="number">12</span></span><br><span class="line"></span><br><span class="line">[<span class="string">'Action'</span> <span class="string">'Adventure'</span> <span class="string">'Fighting'</span> <span class="string">'Misc'</span> <span class="string">'Platform'</span> <span class="string">'Puzzle'</span> <span class="string">'Racing'</span> <span class="string">'Role-Playing'</span> <span class="string">'Shooter'</span> <span class="string">'Simulation'</span> <span class="string">'Sports'</span> <span class="string">'Strategy'</span>]</span><br></pre></td></tr></table></figure>

<p>我们可以看到，总共有 12 中风格的游戏。如果我们在“风格”特征中采用独热编码方案，则将得到 12 个二进制特征。而这次，我们将通过 scikit-learn 的 <em>FeatureHasher</em> 类来使用特征哈希方案，该类使用了一个有符号的 32 位版本的 <em>Murmurhash3</em> 哈希函数。在这种情况下，我们将预先定义最终的特征向量大小为 6。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> FeatureHasher</span><br><span class="line"></span><br><span class="line">fh = FeatureHasher(n_features=<span class="number">6</span>, input_type=<span class="string">'string'</span>)</span><br><span class="line"></span><br><span class="line">hashed_features = fh.fit_transform(vg_df[<span class="string">'Genre'</span>])</span><br><span class="line"></span><br><span class="line">hashed_features = hashed_features.toarray()pd.concat([vg_df[[<span class="string">'Name'</span>, <span class="string">'Genre'</span>]], pd.DataFrame(hashed_features)], axis=<span class="number">1</span>).iloc[<span class="number">1</span>:<span class="number">7</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201801/5a5afd62f2a51.png?imageMogr2/format/jpg/quality/90" alt="不会做特征工程的 AI 研究员不是好数据科学家！下篇 - 离散数据的处理方法"></p>
<p>风格属性的特征哈希</p>
<p>基于上述输出，「风格（Genre）」属性已经使用哈希方案编码成 6 个特征而不是 12 个。我们还可以看到，第 1 行和第 6 行表示相同风格的游戏「平台（Platform）」，而它们也被正确编码成了相同的特征向量。</p>
<h3 id="时间型"><a href="#时间型" class="headerlink" title="时间型"></a>时间型</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBQ8OS.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQrOU.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQcTJ.jpg" alt="avatar"></p>
<h3 id="文本型"><a href="#文本型" class="headerlink" title="文本型"></a>文本型</h3><p><img src="https://s1.ax1x.com/2020/04/24/JBQhSx.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQIOO.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBQzX8.jpg" alt="avatar"><br><img src="https://s1.ax1x.com/2020/04/24/JBlC7Q.jpg" alt="avatar"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/特征工程/">特征工程</a><a class="article-category-link" href="/categories/特征工程/模型调优/">模型调优</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/模型调优/">模型调优</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/特征工程/">特征工程</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-语言模型" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/04/18/语言模型/" class="article-date">
      <time datetime="2020-04-18T11:00:36.000Z" itemprop="datePublished">2020-04-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/18/语言模型/">语言模型</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h1><p>学习目标</p>
<ul>
<li>学习语言模型，以及如何训练一个语言模型</li>
<li>学习torchtext的基本使用方法<ul>
<li>构建 vocabulary</li>
<li>word to inde 和 index to word</li>
</ul>
</li>
<li>学习torch.nn的一些基本模型<ul>
<li>Linear</li>
<li>RNN</li>
<li>LSTM</li>
<li>GRU</li>
</ul>
</li>
<li>RNN的训练技巧<ul>
<li>Gradient Clipping</li>
</ul>
</li>
<li>如何保存和读取模型</li>
</ul>
<p>我们会使用 <a href="https://github.com/pytorch/text" target="_blank" rel="noopener">torchtext</a> 来创建vocabulary, 然后把数据读成batch的格式。请大家自行阅读README来学习torchtext。</p>
<p><strong>先了解下torchtext库：<a href="https://blog.csdn.net/u012436149/article/details/79310176" target="_blank" rel="noopener">torchtext介绍和使用教程</a></strong></p>
<p>In [1]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">USE_CUDA = torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了保证实验结果可以复现，我们经常会把各种random seed固定在某一个值</span></span><br><span class="line">random.seed(<span class="number">53113</span>)</span><br><span class="line">np.random.seed(<span class="number">53113</span>)</span><br><span class="line">torch.manual_seed(<span class="number">53113</span>)</span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    torch.cuda.manual_seed(<span class="number">53113</span>)</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">32</span> <span class="comment">#一个batch多少个句子</span></span><br><span class="line">EMBEDDING_SIZE = <span class="number">650</span>  <span class="comment">#每个单词多少维</span></span><br><span class="line">MAX_VOCAB_SIZE = <span class="number">50000</span>  <span class="comment">#单词总数</span></span><br></pre></td></tr></table></figure>

<ul>
<li>我们会继续使用上次的text8作为我们的训练，验证和测试数据</li>
<li>torchtext提供了LanguageModelingDataset这个class来帮助我们处理语言模型数据集</li>
<li>BPTTIterator可以连续地得到连贯的句子</li>
</ul>
<p>In [2]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">TEXT = torchtext.data.Field(lower=<span class="literal">True</span>) </span><br><span class="line"><span class="comment"># .Field这个对象包含了我们打算如何预处理文本数据的信息，这里定义单词全部小写</span></span><br><span class="line"></span><br><span class="line">train, val, test = \</span><br><span class="line">torchtext.datasets.LanguageModelingDataset.splits(</span><br><span class="line">    path=<span class="string">"."</span>, </span><br><span class="line">    train=<span class="string">"text8.train.txt"</span>, </span><br><span class="line">    validation=<span class="string">"text8.dev.txt"</span>, </span><br><span class="line">    test=<span class="string">"text8.test.txt"</span>, </span><br><span class="line">    text_field=TEXT)</span><br><span class="line"><span class="comment"># torchtext提供了LanguageModelingDataset这个class来帮助我们处理语言模型数据集</span></span><br><span class="line"></span><br><span class="line">TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)</span><br><span class="line"><span class="comment"># build_vocab可以根据我们提供的训练数据集来创建最高频单词的单词表，max_size帮助我们限定单词总量。</span></span><br><span class="line">print(<span class="string">"vocabulary size: &#123;&#125;"</span>.format(len(TEXT.vocab)))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vocabulary size: 50002</span><br></pre></td></tr></table></figure>

<p>In [4]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test</span><br></pre></td></tr></table></figure>

<p>Out[4]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;torchtext.data.example.Example at 0x121738b00&gt;</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>In [9]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(TEXT.vocab.itos[<span class="number">0</span>:<span class="number">50</span>]) </span><br><span class="line"><span class="comment"># 这里越靠前越常见，增加了两个特殊的token，&lt;unk&gt;表示未知的单词，&lt;pad&gt;表示padding。</span></span><br><span class="line">print(<span class="string">"------"</span>*<span class="number">10</span>)</span><br><span class="line">print(list(TEXT.vocab.stoi.items())[<span class="number">0</span>:<span class="number">50</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&apos;&lt;unk&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;the&apos;, &apos;of&apos;, &apos;and&apos;, &apos;one&apos;, &apos;in&apos;, &apos;a&apos;, &apos;to&apos;, &apos;zero&apos;, &apos;nine&apos;, &apos;two&apos;, &apos;is&apos;, &apos;as&apos;, &apos;eight&apos;, &apos;for&apos;, &apos;s&apos;, &apos;five&apos;, &apos;three&apos;, &apos;was&apos;, &apos;by&apos;, &apos;that&apos;, &apos;four&apos;, &apos;six&apos;, &apos;seven&apos;, &apos;with&apos;, &apos;on&apos;, &apos;are&apos;, &apos;it&apos;, &apos;from&apos;, &apos;or&apos;, &apos;his&apos;, &apos;an&apos;, &apos;be&apos;, &apos;this&apos;, &apos;he&apos;, &apos;at&apos;, &apos;which&apos;, &apos;not&apos;, &apos;also&apos;, &apos;have&apos;, &apos;were&apos;, &apos;has&apos;, &apos;but&apos;, &apos;other&apos;, &apos;their&apos;, &apos;its&apos;, &apos;first&apos;, &apos;they&apos;, &apos;had&apos;]</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">[(&apos;&lt;unk&gt;&apos;, 0), (&apos;&lt;pad&gt;&apos;, 1), (&apos;the&apos;, 2), (&apos;of&apos;, 3), (&apos;and&apos;, 4), (&apos;one&apos;, 5), (&apos;in&apos;, 6), (&apos;a&apos;, 7), (&apos;to&apos;, 8), (&apos;zero&apos;, 9), (&apos;nine&apos;, 10), (&apos;two&apos;, 11), (&apos;is&apos;, 12), (&apos;as&apos;, 13), (&apos;eight&apos;, 14), (&apos;for&apos;, 15), (&apos;s&apos;, 16), (&apos;five&apos;, 17), (&apos;three&apos;, 18), (&apos;was&apos;, 19), (&apos;by&apos;, 20), (&apos;that&apos;, 21), (&apos;four&apos;, 22), (&apos;six&apos;, 23), (&apos;seven&apos;, 24), (&apos;with&apos;, 25), (&apos;on&apos;, 26), (&apos;are&apos;, 27), (&apos;it&apos;, 28), (&apos;from&apos;, 29), (&apos;or&apos;, 30), (&apos;his&apos;, 31), (&apos;an&apos;, 32), (&apos;be&apos;, 33), (&apos;this&apos;, 34), (&apos;he&apos;, 35), (&apos;at&apos;, 36), (&apos;which&apos;, 37), (&apos;not&apos;, 38), (&apos;also&apos;, 39), (&apos;have&apos;, 40), (&apos;were&apos;, 41), (&apos;has&apos;, 42), (&apos;but&apos;, 43), (&apos;other&apos;, 44), (&apos;their&apos;, 45), (&apos;its&apos;, 46), (&apos;first&apos;, 47), (&apos;they&apos;, 48), (&apos;had&apos;, 49)]</span><br></pre></td></tr></table></figure>

<p>In [10]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">VOCAB_SIZE = len(TEXT.vocab) <span class="comment"># 50002</span></span><br><span class="line">train_iter, val_iter, test_iter = \</span><br><span class="line">torchtext.data.BPTTIterator.splits(</span><br><span class="line">    (train, val, test), </span><br><span class="line">    batch_size=BATCH_SIZE, </span><br><span class="line">    device=<span class="number">-1</span>, </span><br><span class="line">    bptt_len=<span class="number">50</span>, <span class="comment"># 反向传播往回传的长度，这里我暂时理解为一个样本有多少个单词传入模型</span></span><br><span class="line">    repeat=<span class="literal">False</span>, </span><br><span class="line">    shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># BPTTIterator可以连续地得到连贯的句子，BPTT的全称是back propagation through time。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Iterator：标准迭代器</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">BucketIerator：相比于标准迭代器，会将类似长度的样本当做一批来处理，</span></span><br><span class="line"><span class="string">因为在文本处理中经常会需要将每一批样本长度补齐为当前批中最长序列的长度，</span></span><br><span class="line"><span class="string">因此当样本长度差别较大时，使用BucketIerator可以带来填充效率的提高。</span></span><br><span class="line"><span class="string">除此之外，我们还可以在Field中通过fix_length参数来对样本进行截断补齐操作。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">BPTTIterator: 基于BPTT(基于时间的反向传播算法)的迭代器，一般用于语言模型中。</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.</span><br><span class="line">The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.</span><br><span class="line">The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.</span><br></pre></td></tr></table></figure>

<p>Out[10]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;\nIterator：标准迭代器\n\nBucketIerator：相比于标准迭代器，会将类似长度的样本当做一批来处理，\n因为在文本处理中经常会需要将每一批样本长度补齐为当前批中最长序列的长度，\n因此当样本长度差别较大时，使用BucketIerator可以带来填充效率的提高。\n除此之外，我们还可以在Field中通过fix_length参数来对样本进行截断补齐操作。\n\nBPTTIterator: 基于BPTT(基于时间的反向传播算法)的迭代器，一般用于语言模型中。\n&apos;</span><br></pre></td></tr></table></figure>

<p>In [11]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(next(iter(train_iter))) <span class="comment"># 一个batch训练集维度</span></span><br><span class="line">print(next(iter(val_iter))) <span class="comment"># 一个batch验证集维度</span></span><br><span class="line">print(next(iter(test_iter))) <span class="comment"># 一个batch测试集维度</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[torchtext.data.batch.Batch of size <span class="number">32</span>]</span><br><span class="line">	[.text]:[torch.LongTensor of size <span class="number">50</span>x32]</span><br><span class="line">	[.target]:[torch.LongTensor of size <span class="number">50</span>x32]</span><br><span class="line"></span><br><span class="line">[torchtext.data.batch.Batch of size <span class="number">32</span>]</span><br><span class="line">	[.text]:[torch.LongTensor of size <span class="number">50</span>x32]</span><br><span class="line">	[.target]:[torch.LongTensor of size <span class="number">50</span>x32]</span><br><span class="line"></span><br><span class="line">[torchtext.data.batch.Batch of size <span class="number">32</span>]</span><br><span class="line">	[.text]:[torch.LongTensor of size <span class="number">50</span>x32]</span><br><span class="line">	[.target]:[torch.LongTensor of size <span class="number">50</span>x32]</span><br></pre></td></tr></table></figure>

<p>模型的输入是一串文字，模型的输出也是一串文字，他们之间相差一个位置，因为语言模型的目标是根据之前的单词预测下一个单词。</p>
<p>In [12]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">it = iter(train_iter)</span><br><span class="line">batch = next(it)</span><br><span class="line">print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.text[:,<span class="number">1</span>].data])) <span class="comment"># 打印一个输入的句子</span></span><br><span class="line">print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.target[:,<span class="number">1</span>].data])) <span class="comment"># 打印一个输出的句子</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">combine in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most typical forms of matter they had also suggested the possibility</span><br><span class="line">in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most typical forms of matter they had also suggested the possibility of</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>In [13]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">5</span>): <span class="comment"># 这种取法是在一个固定的batch里取数据，发现一个batch里的数据是连不起来的。</span></span><br><span class="line">    print(j)</span><br><span class="line">    print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.text[:,j].data]))</span><br><span class="line">    print(j)</span><br><span class="line">    print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.target[:,j].data]))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">0</span><br><span class="line">anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans &lt;unk&gt; of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the</span><br><span class="line">0</span><br><span class="line">originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans &lt;unk&gt; of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization</span><br><span class="line">1</span><br><span class="line">combine in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most typical forms of matter they had also suggested the possibility</span><br><span class="line">1</span><br><span class="line">in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most typical forms of matter they had also suggested the possibility of</span><br><span class="line">2</span><br><span class="line">culture few living ainu settlements exist many authentic ainu villages advertised in hokkaido are simply tourist attractions language the ainu language is significantly different from japanese in its syntax phonology morphology and vocabulary although there have been attempts to show that they are related the vast majority of modern scholars</span><br><span class="line">2</span><br><span class="line">few living ainu settlements exist many authentic ainu villages advertised in hokkaido are simply tourist attractions language the ainu language is significantly different from japanese in its syntax phonology morphology and vocabulary although there have been attempts to show that they are related the vast majority of modern scholars reject</span><br><span class="line">3</span><br><span class="line">zero the apple iie card an expansion card for the lc line of macintosh computers was released essentially a miniaturized apple iie computer on a card utilizing the mega ii chip from the apple iigs it allowed the macintosh to run eight bit apple iie software through hardware emulation although</span><br><span class="line">3</span><br><span class="line">the apple iie card an expansion card for the lc line of macintosh computers was released essentially a miniaturized apple iie computer on a card utilizing the mega ii chip from the apple iigs it allowed the macintosh to run eight bit apple iie software through hardware emulation although video</span><br><span class="line">4</span><br><span class="line">in papers have been written arguing that the anthropic principle would explain the physical constants such as the fine structure constant the number of dimensions in the universe and the cosmological constant the three primary versions of the principle as stated by john d barrow and frank j &lt;unk&gt; one</span><br><span class="line">4</span><br><span class="line">papers have been written arguing that the anthropic principle would explain the physical constants such as the fine structure constant the number of dimensions in the universe and the cosmological constant the three primary versions of the principle as stated by john d barrow and frank j &lt;unk&gt; one nine</span><br></pre></td></tr></table></figure>

<p>In [14]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>): <span class="comment"># 这种取法是在每个batch里取某一个相同位置数据，发现不同batch间相同位置的数据是可以连起来的。这里有点小疑问。</span></span><br><span class="line">    batch = next(it)</span><br><span class="line">    print(i)</span><br><span class="line">    print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.text[:,<span class="number">2</span>].data]))</span><br><span class="line">    print(i)</span><br><span class="line">    print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.target[:,<span class="number">2</span>].data]))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">0</span><br><span class="line">reject that the relationship goes beyond contact i e mutual borrowing of words between japanese and ainu in fact no attempt to show a relationship with ainu to any other language has gained wide acceptance and ainu is currently considered to be a language isolate culture traditional ainu culture is</span><br><span class="line">0</span><br><span class="line">that the relationship goes beyond contact i e mutual borrowing of words between japanese and ainu in fact no attempt to show a relationship with ainu to any other language has gained wide acceptance and ainu is currently considered to be a language isolate culture traditional ainu culture is quite</span><br><span class="line">1</span><br><span class="line">quite different from japanese culture never shaving after a certain age the men had full beards and &lt;unk&gt; men and women alike cut their hair level with the shoulders at the sides of the head but trimmed it &lt;unk&gt; behind the women tattooed their mouths arms &lt;unk&gt; and sometimes their</span><br><span class="line">1</span><br><span class="line">different from japanese culture never shaving after a certain age the men had full beards and &lt;unk&gt; men and women alike cut their hair level with the shoulders at the sides of the head but trimmed it &lt;unk&gt; behind the women tattooed their mouths arms &lt;unk&gt; and sometimes their &lt;unk&gt;</span><br><span class="line">2</span><br><span class="line">&lt;unk&gt; starting at the onset of puberty the soot deposited on a pot hung over a fire of birch bark was used for colour their traditional dress is a robe spun from the bark of the elm tree it has long sleeves reaches nearly to the feet is folded round</span><br><span class="line">2</span><br><span class="line">starting at the onset of puberty the soot deposited on a pot hung over a fire of birch bark was used for colour their traditional dress is a robe spun from the bark of the elm tree it has long sleeves reaches nearly to the feet is folded round the</span><br><span class="line">3</span><br><span class="line">the body and is tied with a girdle of the same material women also wear an &lt;unk&gt; of japanese cloth in winter the skins of animals were worn with &lt;unk&gt; of &lt;unk&gt; and boots made from the skin of dogs or salmon both sexes are fond of earrings which are</span><br><span class="line">3</span><br><span class="line">body and is tied with a girdle of the same material women also wear an &lt;unk&gt; of japanese cloth in winter the skins of animals were worn with &lt;unk&gt; of &lt;unk&gt; and boots made from the skin of dogs or salmon both sexes are fond of earrings which are said</span><br><span class="line">4</span><br><span class="line">said to have been made of grapevine in former times as also are bead necklaces called &lt;unk&gt; which the women prized highly their traditional cuisine consists of the flesh of bear fox wolf badger ox or horse as well as fish fowl millet vegetables herbs and roots they never ate</span><br><span class="line">4</span><br><span class="line">to have been made of grapevine in former times as also are bead necklaces called &lt;unk&gt; which the women prized highly their traditional cuisine consists of the flesh of bear fox wolf badger ox or horse as well as fish fowl millet vegetables herbs and roots they never ate raw</span><br></pre></td></tr></table></figure>

<h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><ul>
<li>继承nn.Module</li>
<li>初始化函数</li>
<li>forward函数</li>
<li>其余可以根据模型需要定义相关的函数</li>
</ul>
<p>In [15]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" 一个简单的循环神经网络"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        <span class="comment"># rnn_type；有两个层供选择'LSTM', 'GRU'</span></span><br><span class="line">        <span class="comment"># ntoken：VOCAB_SIZE=50002</span></span><br><span class="line">        <span class="comment"># ninp：EMBEDDING_SIZE = 650，输入层维度</span></span><br><span class="line">        <span class="comment"># nhid：EMBEDDING_SIZE = 1000，隐藏层维度，这里是我自己设置的，用于区分ninp层。</span></span><br><span class="line">        <span class="comment"># nlayers：纵向有多少层神经网络</span></span><br><span class="line"></span><br><span class="line">        <span class="string">''' 该模型包含以下几层:</span></span><br><span class="line"><span class="string">            - 词嵌入层</span></span><br><span class="line"><span class="string">            - 一个循环神经网络层(RNN, LSTM, GRU)</span></span><br><span class="line"><span class="string">            - 一个线性层，从hidden state到输出单词表</span></span><br><span class="line"><span class="string">            - 一个dropout层，用来做regularization</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super(RNNModel, self).__init__()</span><br><span class="line">        self.drop = nn.Dropout(dropout)</span><br><span class="line">        self.encoder = nn.Embedding(ntoken, ninp)</span><br><span class="line">        <span class="comment"># 定义输入的Embedding层，用来把每个单词转化为词向量</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> rnn_type <span class="keyword">in</span> [<span class="string">'LSTM'</span>, <span class="string">'GRU'</span>]: <span class="comment"># 下面代码以LSTM举例</span></span><br><span class="line">            </span><br><span class="line">            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)</span><br><span class="line">            <span class="comment"># getattr(nn, rnn_type) 相当于 nn.rnn_type</span></span><br><span class="line">            <span class="comment"># nlayers代表纵向有多少层。还有个参数是bidirectional: 是否是双向LSTM，默认false</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                nonlinearity = &#123;<span class="string">'RNN_TANH'</span>: <span class="string">'tanh'</span>, <span class="string">'RNN_RELU'</span>: <span class="string">'relu'</span>&#125;[rnn_type]</span><br><span class="line">            <span class="keyword">except</span> KeyError:</span><br><span class="line">                <span class="keyword">raise</span> ValueError( <span class="string">"""An invalid option for `--model` was supplied,</span></span><br><span class="line"><span class="string">                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']"""</span>)</span><br><span class="line">            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)</span><br><span class="line">        self.decoder = nn.Linear(nhid, ntoken)</span><br><span class="line">        <span class="comment"># 最后线性全连接隐藏层的维度(1000,50002)</span></span><br><span class="line">      </span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">        self.rnn_type = rnn_type</span><br><span class="line">        self.nhid = nhid</span><br><span class="line">        self.nlayers = nlayers</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        initrange = <span class="number">0.1</span></span><br><span class="line">        self.encoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        self.decoder.bias.data.zero_()</span><br><span class="line">        self.decoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span> </span><br><span class="line">        </span><br><span class="line">        <span class="string">''' Forward pass:</span></span><br><span class="line"><span class="string">            - word embedding</span></span><br><span class="line"><span class="string">            - 输入循环神经网络</span></span><br><span class="line"><span class="string">            - 一个线性层从hidden state转化为输出单词表</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># input.shape = seg_length * batch = torch.Size([50, 32])</span></span><br><span class="line">        <span class="comment"># 如果觉得想变成32*50格式，可以在LSTM里定义batch_first = True</span></span><br><span class="line">        <span class="comment"># hidden = (nlayers * 32 * hidden_size, nlayers * 32 * hidden_size)</span></span><br><span class="line">        <span class="comment"># hidden是个元组，输入有两个参数，一个是刚开始的隐藏层h的维度，一个是刚开始的用于记忆的c的维度，</span></span><br><span class="line">        <span class="comment"># 这两个层的维度一样，并且需要先初始化，hidden_size的维度和上面nhid的维度一样 =1000，我理解这两个是同一个东西。</span></span><br><span class="line">        emb = self.drop(self.encoder(input)) <span class="comment"># </span></span><br><span class="line">        <span class="comment"># emb.shape=torch.Size([50, 32, 650]) # 输入数据的维度</span></span><br><span class="line">        <span class="comment"># 这里进行了运算（50，50002，650）*(50, 32，50002)</span></span><br><span class="line">        output, hidden = self.rnn(emb, hidden)</span><br><span class="line">        <span class="comment"># output.shape = 50 * 32 * hidden_size # 最终输出数据的维度，</span></span><br><span class="line">        <span class="comment"># hidden是个元组，输出有两个参数，一个是最后的隐藏层h的维度，一个是最后的用于记忆的c的维度，这两个层维度相同 </span></span><br><span class="line">        <span class="comment"># hidden = (h层维度：nlayers * 32 * hidden_size, c层维度：nlayers * 32 * hidden_size)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        output = self.drop(output)</span><br><span class="line">        decoded = self.decoder(output.view(output.size(<span class="number">0</span>)*output.size(<span class="number">1</span>), output.size(<span class="number">2</span>)))</span><br><span class="line">        <span class="comment"># output最后的输出层一定要是二维的，只是为了能进行全连接层的运算，所以把前两个维度拼到一起，（50*32,hidden_size)</span></span><br><span class="line">        <span class="comment"># decoded.shape=（50*32,hidden_size)*(hidden_size,50002)=torch.Size([1600, 50002])</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> decoded.view(output.size(<span class="number">0</span>), output.size(<span class="number">1</span>), decoded.size(<span class="number">1</span>)), hidden</span><br><span class="line">               <span class="comment"># 我们要知道每一个位置预测的是哪个单词，所以最终输出要恢复维度 = (50,32,50002)</span></span><br><span class="line">               <span class="comment"># hidden = (h层维度：2 * 32 * 1000, c层维度：2 * 32 * 1000)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, bsz, requires_grad=True)</span>:</span></span><br><span class="line">        <span class="comment"># 这步我们初始化下隐藏层参数</span></span><br><span class="line">        weight = next(self.parameters())</span><br><span class="line">        <span class="comment"># weight = torch.Size([50002, 650])是所有参数的第一个参数</span></span><br><span class="line">        <span class="comment"># 所有参数self.parameters()，是个生成器，LSTM所有参数维度种类如下：</span></span><br><span class="line">        <span class="comment"># print(list(iter(self.parameters())))</span></span><br><span class="line">        <span class="comment"># torch.Size([50002, 650])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000, 650])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000, 1000])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000]) # 偏置项</span></span><br><span class="line">        <span class="comment"># torch.Size([4000])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000, 1000])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000, 1000])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000])</span></span><br><span class="line">        <span class="comment"># torch.Size([4000])</span></span><br><span class="line">        <span class="comment"># torch.Size([50002, 1000])</span></span><br><span class="line">        <span class="comment"># torch.Size([50002])</span></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type == <span class="string">'LSTM'</span>:</span><br><span class="line">            <span class="keyword">return</span> (weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad),</span><br><span class="line">                    weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad))</span><br><span class="line">                   <span class="comment"># return = (2 * 32 * 1000, 2 * 32 * 1000)</span></span><br><span class="line">                   <span class="comment"># 这里不明白为什么需要weight.new_zeros，我估计是想整个计算图能链接起来</span></span><br><span class="line">                   <span class="comment"># 这里特别注意hidden的输入不是model的参数，不参与更新，就跟输入数据x一样</span></span><br><span class="line">                   </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad)</span><br><span class="line">            <span class="comment"># GRU神经网络把h层和c层合并了，所以这里只有一层。</span></span><br></pre></td></tr></table></figure>

<p>初始化一个模型</p>
<p>In [16]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nhid = <span class="number">1000</span> <span class="comment"># 我自己设置的维度，用于区分embeding_size=650</span></span><br><span class="line">model = RNNModel(<span class="string">"LSTM"</span>, VOCAB_SIZE, EMBEDDING_SIZE, nhid, <span class="number">2</span>, dropout=<span class="number">0.5</span>)</span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    model = model.cuda()</span><br></pre></td></tr></table></figure>

<p>In [17]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model</span><br></pre></td></tr></table></figure>

<p>Out[17]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">RNNModel(</span><br><span class="line">  (drop): Dropout(p=<span class="number">0.5</span>)</span><br><span class="line">  (encoder): Embedding(<span class="number">50002</span>, <span class="number">650</span>)</span><br><span class="line">  (rnn): LSTM(<span class="number">650</span>, <span class="number">1000</span>, num_layers=<span class="number">2</span>, dropout=<span class="number">0.5</span>)</span><br><span class="line">  (decoder): Linear(in_features=<span class="number">1000</span>, out_features=<span class="number">50002</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>In [23]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list(model.parameters())[0].shape</span><br></pre></td></tr></table></figure>

<p>Out[23]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([50002, 650])</span><br></pre></td></tr></table></figure>

<ul>
<li>我们首先定义评估模型的代码。</li>
<li>模型的评估和模型的训练逻辑基本相同，唯一的区别是我们只需要forward pass，不需要backward pass</li>
</ul>
<p>In [68]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先从下面训练模式看起，在看evaluate</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, data)</span>:</span></span><br><span class="line">    model.eval() <span class="comment"># 预测模式</span></span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    it = iter(data)</span><br><span class="line">    total_count = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        hidden = model.init_hidden(BATCH_SIZE, requires_grad=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 这里不管是训练模式还是预测模式，h层的输入都是初始化为0，hidden的输入不是model的参数</span></span><br><span class="line"><span class="comment"># 这里model里的model.parameters()已经是训练过的参数。</span></span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(it):</span><br><span class="line">            data, target = batch.text, batch.target</span><br><span class="line">            <span class="comment"># # 取出验证集的输入的数据和输出的数据，相当于特征和标签</span></span><br><span class="line">            <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">                data, target = data.cuda(), target.cuda()</span><br><span class="line">            hidden = repackage_hidden(hidden) <span class="comment"># 截断计算图</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad(): <span class="comment"># 验证阶段不需要更新梯度</span></span><br><span class="line">                output, hidden = model(data, hidden)</span><br><span class="line">                <span class="comment">#调用model的forward方法进行一次前向传播，得到return输出值</span></span><br><span class="line">            loss = loss_fn(output.view(<span class="number">-1</span>, VOCAB_SIZE), target.view(<span class="number">-1</span>))</span><br><span class="line">            <span class="comment"># 计算交叉熵损失</span></span><br><span class="line">            </span><br><span class="line">            total_count += np.multiply(*data.size()) </span><br><span class="line"><span class="comment"># 上面计算交叉熵的损失是平均过的，这里需要计算下总的损失</span></span><br><span class="line"><span class="comment"># total_count先计算验证集样本的单词总数，一个样本有50个单词，一个batch32个样本</span></span><br><span class="line"><span class="comment"># np.multiply(*data.size()) =50*32=1600</span></span><br><span class="line">            total_loss += loss.item()*np.multiply(*data.size())</span><br><span class="line"><span class="comment"># 每次batch平均后的损失乘以每次batch的样本的总的单词数 = 一次batch总的损失</span></span><br><span class="line">            </span><br><span class="line">    loss = total_loss / total_count <span class="comment"># 整个验证集总的损失除以总的单词数</span></span><br><span class="line">    model.train() <span class="comment"># 训练模式</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>

<p>In [9]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = torch.ones((<span class="number">5</span>,<span class="number">3</span>))</span><br><span class="line">print(a.size())</span><br><span class="line">np.multiply(*a.size())</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([5, 3])</span><br></pre></td></tr></table></figure>

<p>Out[9]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">15</span><br></pre></td></tr></table></figure>

<p>我们需要定义下面的一个function，帮助我们把一个hidden state和计算图之前的历史分离。</p>
<p>In [69]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Remove this part</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">repackage_hidden</span><span class="params">(h)</span>:</span></span><br><span class="line">    <span class="string">"""Wraps hidden states in new Tensors, to detach them from their history."""</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(h, torch.Tensor): </span><br><span class="line">        <span class="comment"># 这个是GRU的截断，因为只有一个隐藏层</span></span><br><span class="line">        <span class="comment"># 判断h是不是torch.Tensor</span></span><br><span class="line">        <span class="keyword">return</span> h.detach() <span class="comment"># 截断计算图，h是全的计算图的开始，只是保留了h的值</span></span><br><span class="line">    <span class="keyword">else</span>: <span class="comment"># 这个是LSTM的截断，有两个隐藏层，格式是元组</span></span><br><span class="line">        <span class="keyword">return</span> tuple(repackage_hidden(v) <span class="keyword">for</span> v <span class="keyword">in</span> h)</span><br></pre></td></tr></table></figure>

<p>定义loss function和optimizer</p>
<p>In [70]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss() <span class="comment"># 交叉熵损失</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, <span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># 每调用一次这个函数，lenrning_rate就降一半，0.5就是一半的意思</span></span><br></pre></td></tr></table></figure>

<p>训练模型：</p>
<ul>
<li>模型一般需要训练若干个epoch</li>
<li>每个epoch我们都把所有的数据分成若干个batch</li>
<li>把每个batch的输入和输出都包装成cuda tensor</li>
<li>forward pass，通过输入的句子预测每个单词的下一个单词</li>
<li>用模型的预测和正确的下一个单词计算cross entropy loss</li>
<li>清空模型当前gradient</li>
<li>backward pass</li>
<li>gradient clipping，防止梯度爆炸</li>
<li>更新模型参数</li>
<li>每隔一定的iteration输出模型在当前iteration的loss，以及在验证集上做模型的评估</li>
</ul>
<p>In [13]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">GRAD_CLIP = <span class="number">1.</span></span><br><span class="line">NUM_EPOCHS = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">val_losses = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    model.train() <span class="comment"># 训练模式</span></span><br><span class="line">    it = iter(train_iter) </span><br><span class="line">    <span class="comment"># iter,生成迭代器,这里train_iter也是迭代器，不用iter也可以</span></span><br><span class="line">    hidden = model.init_hidden(BATCH_SIZE) </span><br><span class="line">    <span class="comment"># 得到hidden初始化后的维度</span></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(it):</span><br><span class="line">        data, target = batch.text, batch.target</span><br><span class="line">        <span class="comment"># 取出训练集的输入的数据和输出的数据，相当于特征和标签</span></span><br><span class="line">        <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">            data, target = data.cuda(), target.cuda()</span><br><span class="line">        hidden = repackage_hidden(hidden)</span><br><span class="line"><span class="comment"># 语言模型每个batch的隐藏层的输出值是要继续作为下一个batch的隐藏层的输入的</span></span><br><span class="line"><span class="comment"># 因为batch数量很多，如果一直往后传，会造成整个计算图很庞大，反向传播会内存崩溃。</span></span><br><span class="line"><span class="comment"># 所有每次一个batch的计算图迭代完成后，需要把计算图截断，只保留隐藏层的输出值。</span></span><br><span class="line"><span class="comment"># 不过只有语言模型才这么干，其他比如翻译模型不需要这么做。</span></span><br><span class="line"><span class="comment"># repackage_hidden自定义函数用来截断计算图的。</span></span><br><span class="line">        model.zero_grad() <span class="comment"># 梯度归零，不然每次迭代梯度会累加</span></span><br><span class="line">        output, hidden = model(data, hidden)</span><br><span class="line">        <span class="comment"># output = (50,32,50002)</span></span><br><span class="line">        loss = loss_fn(output.view(<span class="number">-1</span>, VOCAB_SIZE), target.view(<span class="number">-1</span>))</span><br><span class="line"><span class="comment"># output.view(-1, VOCAB_SIZE) = (1600,50002)</span></span><br><span class="line"><span class="comment"># target.view(-1) =(1600),关于pytorch中交叉熵的计算公式请看下面链接。</span></span><br><span class="line"><span class="comment"># https://blog.csdn.net/geter_CS/article/details/84857220</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)</span><br><span class="line">        <span class="comment"># 防止梯度爆炸，设定阈值，当梯度大于阈值时，更新的梯度为阈值</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"epoch"</span>, epoch, <span class="string">"iter"</span>, i, <span class="string">"loss"</span>, loss.item())</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            val_loss = evaluate(model, val_iter)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> len(val_losses) == <span class="number">0</span> <span class="keyword">or</span> val_loss &lt; min(val_losses):</span><br><span class="line">                <span class="comment"># 如果比之前的loss要小，就保存模型</span></span><br><span class="line">                print(<span class="string">"best model, val loss: "</span>, val_loss)</span><br><span class="line">                torch.save(model.state_dict(), <span class="string">"lm-best.th"</span>)</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 否则loss没有降下来，需要优化</span></span><br><span class="line">                scheduler.step() <span class="comment"># 自动调整学习率</span></span><br><span class="line">                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line">                <span class="comment"># 学习率调整后需要更新optimizer，下次训练就用更新后的</span></span><br><span class="line">            val_losses.append(val_loss) <span class="comment"># 保存每10000次迭代后的验证集损失损失</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">epoch 0 iter 0 loss 10.821578979492188</span><br><span class="line">best model, val loss:  10.782116411285918</span><br><span class="line">epoch 0 iter 1000 loss 6.5122528076171875</span><br><span class="line">epoch 0 iter 2000 loss 6.3599748611450195</span><br><span class="line">epoch 0 iter 3000 loss 6.13856315612793</span><br><span class="line">epoch 0 iter 4000 loss 5.473214626312256</span><br><span class="line">epoch 0 iter 5000 loss 5.901871204376221</span><br><span class="line">epoch 0 iter 6000 loss 5.85321569442749</span><br><span class="line">epoch 0 iter 7000 loss 5.636535167694092</span><br><span class="line">epoch 0 iter 8000 loss 5.7489800453186035</span><br><span class="line">epoch 0 iter 9000 loss 5.464158058166504</span><br><span class="line">epoch 0 iter 10000 loss 5.554863452911377</span><br><span class="line">best model, val loss:  5.264891533569864</span><br><span class="line">epoch 0 iter 11000 loss 5.703625202178955</span><br><span class="line">epoch 0 iter 12000 loss 5.6448974609375</span><br><span class="line">epoch 0 iter 13000 loss 5.372857570648193</span><br><span class="line">epoch 0 iter 14000 loss 5.2639479637146</span><br><span class="line">epoch 1 iter 0 loss 5.696778297424316</span><br><span class="line">best model, val loss:  5.124550380139679</span><br><span class="line">epoch 1 iter 1000 loss 5.534722805023193</span><br><span class="line">epoch 1 iter 2000 loss 5.599489212036133</span><br><span class="line">epoch 1 iter 3000 loss 5.459986686706543</span><br><span class="line">epoch 1 iter 4000 loss 4.927192211151123</span><br><span class="line">epoch 1 iter 5000 loss 5.435710906982422</span><br><span class="line">epoch 1 iter 6000 loss 5.4059576988220215</span><br><span class="line">epoch 1 iter 7000 loss 5.308575630187988</span><br><span class="line">epoch 1 iter 8000 loss 5.405811786651611</span><br><span class="line">epoch 1 iter 9000 loss 5.1389055252075195</span><br><span class="line">epoch 1 iter 10000 loss 5.226413726806641</span><br><span class="line">best model, val loss:  4.946829228873176</span><br><span class="line">epoch 1 iter 11000 loss 5.379891395568848</span><br><span class="line">epoch 1 iter 12000 loss 5.360724925994873</span><br><span class="line">epoch 1 iter 13000 loss 5.176026344299316</span><br><span class="line">epoch 1 iter 14000 loss 5.110936641693115</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载保存好的模型参数</span></span><br><span class="line">best_model = RNNModel(<span class="string">"LSTM"</span>, VOCAB_SIZE, EMBEDDING_SIZE, nhid, <span class="number">2</span>, dropout=<span class="number">0.5</span>)</span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    best_model = best_model.cuda()</span><br><span class="line">best_model.load_state_dict(torch.load(<span class="string">"lm-best.th"</span>))</span><br><span class="line"><span class="comment"># 把模型参数load到best_model里</span></span><br></pre></td></tr></table></figure>

<h3 id="使用最好的模型在valid数据上计算perplexity"><a href="#使用最好的模型在valid数据上计算perplexity" class="headerlink" title="使用最好的模型在valid数据上计算perplexity"></a>使用最好的模型在valid数据上计算perplexity</h3><p>In [15]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val_loss = evaluate(best_model, val_iter)</span><br><span class="line">print(<span class="string">"perplexity: "</span>, np.exp(val_loss))</span><br><span class="line"><span class="comment"># 这里不清楚语言模型的评估指标perplexity = np.exp(val_loss)</span></span><br><span class="line"><span class="comment"># 清楚的朋友欢迎交流下</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">perplexity:  140.72803934425724</span><br></pre></td></tr></table></figure>

<h3 id="使用最好的模型在测试数据上计算perplexity"><a href="#使用最好的模型在测试数据上计算perplexity" class="headerlink" title="使用最好的模型在测试数据上计算perplexity"></a>使用最好的模型在测试数据上计算perplexity</h3><p>In [16]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_loss = evaluate(best_model, test_iter)</span><br><span class="line">print(<span class="string">"perplexity: "</span>, np.exp(test_loss))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">perplexity:  178.54742013696125</span><br></pre></td></tr></table></figure>

<p>使用训练好的模型生成一些句子。</p>
<p>In [18]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">hidden = best_model.init_hidden(<span class="number">1</span>) <span class="comment"># batch_size = 1</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">input = torch.randint(VOCAB_SIZE, (<span class="number">1</span>, <span class="number">1</span>), dtype=torch.long).to(device)</span><br><span class="line"><span class="comment"># (1,1)表示输出格式是1行1列的2维tensor，VOCAB_SIZE表示随机取的值小于VOCAB_SIZE=50002</span></span><br><span class="line"><span class="comment"># 我们input相当于取的是一个单词</span></span><br><span class="line">words = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    output, hidden = best_model(input, hidden)</span><br><span class="line">    <span class="comment"># output.shape = 1 * 1 * 50002</span></span><br><span class="line">    <span class="comment"># hidden = (2 * 1 * 1000, 2 * 1 * 1000)</span></span><br><span class="line">    word_weights = output.squeeze().exp().cpu()</span><br><span class="line">    <span class="comment"># .exp()的两个作用：一是把概率更大的变得更大，二是把负数经过e后变成正数，下面.multinomial参数需要正数</span></span><br><span class="line">    word_idx = torch.multinomial(word_weights, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 按照word_weights里面的概率随机的取值，概率大的取到的机会大。</span></span><br><span class="line">    <span class="comment"># torch.multinomial看这个博客理解：https://blog.csdn.net/monchin/article/details/79787621</span></span><br><span class="line">    <span class="comment"># 这里如果选择概率最大的，会每次生成重复的句子。</span></span><br><span class="line">    input.fill_(word_idx) <span class="comment"># 预测的单词index是word_idx，然后把word_idx作为下一个循环预测的input输入</span></span><br><span class="line">    word = TEXT.vocab.itos[word_idx] <span class="comment"># 根据word_idx取出对应的单词</span></span><br><span class="line">    words.append(word) </span><br><span class="line">print(<span class="string">" "</span>.join(words))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s influence clinton decision de gaulle is himself sappho s iv one family banquet was made published by paul &lt;unk&gt; and by a persuaded to prevent arcane of animate poverty based at copernicus bachelor in search services and in a cruise corps references eds the robin series july four one nine zero eight summer gutenberg one nine six four births one nine two eight deaths timeline of this method by the fourth amendment the german ioc known for his &lt;unk&gt; from &lt;unk&gt; one eight nine eight one seven eight nine management was established in one nine seven zero they had</span><br></pre></td></tr></table></figure>

<p>In [42]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.randint(50002, (1, 1))</span><br></pre></td></tr></table></figure>

<p>Out[42]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[11293]])</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Gradient-Clipping/">Gradient Clipping</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linear/">Linear</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RNN/">RNN</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-SQuAD-BiDAF" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/04/17/SQuAD-BiDAF/" class="article-date">
      <time datetime="2020-04-16T23:32:02.000Z" itemprop="datePublished">2020-04-17</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/17/SQuAD-BiDAF/">SQuAD-BiDAF</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>代码是在github<a href="https://github.com/galsang/BiDAF-pytorch" target="_blank" rel="noopener">BiDAF-pytorch</a>上下载的，我把代码弄成了下面jupyter notebook格式，代码是在kaggle GPU跑的，</p>
<p>数据集如果不能下载的可以到我的网盘下载，包括数据集和训练好的模型，比较大：<a href="https://pan.baidu.com/s/1XCEUG6E2biCdqFyaxIGtBw" target="_blank" rel="noopener">百度网盘下载地址</a></p>
<p>整个代码跑下来，训练集可以跑通，测试集当时跑的时候kaggle内存不够了，报错了，有兴趣可以试下最终的效果。</p>
<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This Python 3 environment comes with many helpful analytics libraries installed</span></span><br><span class="line"><span class="comment"># It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python</span></span><br><span class="line"><span class="comment"># For example, here's several helpful packages to load in </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># linear algebra</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># data processing, CSV file I/O (e.g. pd.read_csv)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Input data files are available in the "../input/" directory.</span></span><br><span class="line"><span class="comment"># For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">print(os.listdir(<span class="string">"../input"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Any results you write to the current directory are saved as output.</span></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!cp -r /kaggle/input/bidaf-pytorch-master/BiDAF-pytorch-master /kaggle/working</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.chdir(&quot;BiDAF-pytorch-master&quot;)</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.chdir(&quot;..&quot;)</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!ls</span><br><span class="line">!pwd</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> copy, json, os</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> gmtime, strftime</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> GloVe</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br></pre></td></tr></table></figure>

<h1 id="一、定义初始变量参数"><a href="#一、定义初始变量参数" class="headerlink" title="一、定义初始变量参数"></a>一、定义初始变量参数</h1><p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有关argparse的官方文档操作请查看：https://docs.python.org/3/library/argparse.html#module-argparse，</span></span><br><span class="line"><span class="comment"># 下面的参数代码注释，我也不是特别懂，仅供参考</span></span><br><span class="line"><span class="comment"># 关于parser.add_argument(）的详解请查看：https://blog.csdn.net/u013177568/article/details/62432761/</span></span><br><span class="line"><span class="comment"># 对于下面函数add_argument()第一个是选项是必须写的参数，该参数接受选项参数或者是位置参数（一串文件名）</span></span><br><span class="line"><span class="comment"># 第二个是default默认值，如果第一个选项参数没有单独指定，那选项参数的值就是默认值</span></span><br><span class="line"><span class="comment"># 第三个是参数数据类型，代表你的选项参数必须是是int还是float字符型数据。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_args</span><span class="params">()</span>:</span></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">'--char-dim'</span>, default=<span class="number">8</span>, type=int)</span><br><span class="line">    <span class="comment"># char-dim 默认值是8</span></span><br><span class="line">    parser.add_argument(<span class="string">'--char-channel-width'</span>, default=<span class="number">5</span>, type=int)</span><br><span class="line">    <span class="comment"># char-channel-width 默认值是5 以下类似</span></span><br><span class="line">    parser.add_argument(<span class="string">'--char-channel-size'</span>, default=<span class="number">100</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--context-threshold'</span>, default=<span class="number">400</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--dev-batch-size'</span>, default=<span class="number">100</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--dev-file'</span>, default=<span class="string">'dev-v1.1.json'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--dropout'</span>, default=<span class="number">0.2</span>, type=float)</span><br><span class="line">    parser.add_argument(<span class="string">'--epoch'</span>, default=<span class="number">12</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--exp-decay-rate'</span>, default=<span class="number">0.999</span>, type=float)</span><br><span class="line">    parser.add_argument(<span class="string">'--gpu'</span>, default=<span class="number">0</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--hidden-size'</span>, default=<span class="number">100</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--learning-rate'</span>, default=<span class="number">0.5</span>, type=float)</span><br><span class="line">    parser.add_argument(<span class="string">'--print-freq'</span>, default=<span class="number">250</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--train-batch-size'</span>, default=<span class="number">60</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--train-file'</span>, default=<span class="string">'train-v1.1.json'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--word-dim'</span>, default=<span class="number">100</span>, type=int)</span><br><span class="line">    args = parser.parse_args(args=[]) </span><br><span class="line"><span class="comment"># .parse_args()是将之前所有add_argument定义的参数在括号里进行赋值，没有赋值(args=[])，就返回参数各自default的默认值。</span></span><br><span class="line"><span class="comment"># 返回值args相当于是个参数命名空间的集合，可以调用上面第一项选项参数的名字，就可以得到default值了。</span></span><br><span class="line"><span class="comment"># 比如调用上面参数方式：args.char_dim,args.char_channel_width....默认情况下，中划线会转换为下划线.</span></span><br><span class="line">    <span class="keyword">return</span> args</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args = parse_args()</span><br></pre></td></tr></table></figure>

<h1 id="二、SQuAD问答数据预处理"><a href="#二、SQuAD问答数据预处理" class="headerlink" title="二、SQuAD问答数据预处理"></a>二、SQuAD问答数据预处理</h1><h2 id="1、查看数据集结构"><a href="#1、查看数据集结构" class="headerlink" title="1、查看数据集结构"></a>1、查看数据集结构</h2><p>SQuAD问答数据介绍：<a href="https://rajpurkar.github.io/SQuAD-explorer/" target="_blank" rel="noopener">https://rajpurkar.github.io/SQuAD-explorer/</a> 这个数据集有两个文件，验证集和测试集：train-v1.1.json，dev-v1.1.json</p>
<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'data/squad/dev-v1.1.json'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                line = f.readline() <span class="comment"># 该方法每次读出一行内容</span></span><br><span class="line">                <span class="keyword">if</span> line:</span><br><span class="line">                    print(<span class="string">"type(line)"</span>,type(line)) <span class="comment"># 直接打印就是字符串格式</span></span><br><span class="line">                    r = json.loads(line)</span><br><span class="line">                    print(<span class="string">"type(r)"</span>,type(r)) <span class="comment"># 使用json.loads将字符串转化为字典</span></span><br><span class="line">                    print(r)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            f.close()</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据架构如下</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"data"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"title"</span>: <span class="string">"Super_Bowl_50"</span>, <span class="comment"># 第一个主题</span></span><br><span class="line">            <span class="string">"paragraphs"</span>: [</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">"context"</span>: <span class="string">" numerals 50......."</span>, <span class="comment"># 每个主题会有很多context短文,这里只列出一个</span></span><br><span class="line">                    <span class="string">"qas"</span>: [  <span class="comment"># 这个列表里放问题和答案的位置，每篇context会有很有很多answer和question，这里只列出一个</span></span><br><span class="line">                        &#123;</span><br><span class="line">                            <span class="string">"answers"</span>: [  <span class="comment"># 一个问题会有三个答案，三个答案都是对的，只是在context不同或相同位置</span></span><br><span class="line">                                &#123;         <span class="comment"># 下面三个答案都在相同的位置</span></span><br><span class="line">                                    <span class="string">"answer_start"</span>: <span class="number">177</span>,  <span class="comment"># 答案在文中的起始位置是第177的字符。</span></span><br><span class="line">                                    <span class="string">"text"</span>: <span class="string">"Denver Broncos"</span></span><br><span class="line">                                &#125;,</span><br><span class="line">                                &#123;</span><br><span class="line">                                    <span class="string">"answer_start"</span>: <span class="number">177</span>,</span><br><span class="line">                                    <span class="string">"text"</span>: <span class="string">"Denver Broncos"</span></span><br><span class="line">                                &#125;,</span><br><span class="line">                                &#123;</span><br><span class="line">                                    <span class="string">"answer_start"</span>: <span class="number">177</span>,</span><br><span class="line">                                    <span class="string">"text"</span>: <span class="string">"Denver Broncos"</span></span><br><span class="line">                                &#125;</span><br><span class="line">                            ],</span><br><span class="line">                            <span class="string">"question"</span>: <span class="string">"Which NFL team represented the AFC at Super Bowl 50?"</span>,</span><br><span class="line">                            <span class="string">"id"</span>: <span class="string">"56be4db0acb8001400a502ec"</span></span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                    ]</span><br><span class="line">                &#125;</span><br><span class="line">                </span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"title"</span>: <span class="string">"Warsaw"</span>, <span class="comment"># 第二个主题</span></span><br><span class="line">            <span class="string">"paragraphs"</span>:   </span><br><span class="line">        &#125;,</span><br><span class="line">        </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"title"</span>: <span class="string">"Normans"</span>, <span class="comment"># 第三个主题</span></span><br><span class="line">            <span class="string">"paragraphs"</span>: </span><br><span class="line">        &#125;,</span><br><span class="line">        </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"title"</span>: <span class="string">"Nikola_Tesla"</span>, <span class="comment"># 第四个主题</span></span><br><span class="line">            <span class="string">"paragraphs"</span>: </span><br><span class="line">        &#125;,</span><br><span class="line">        ........... <span class="comment"># 还有很多</span></span><br><span class="line">        </span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"version"</span>: <span class="string">"1.1"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="2、定义分词方法"><a href="#2、定义分词方法" class="headerlink" title="2、定义分词方法"></a>2、定义分词方法</h2><p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_tokenize</span><span class="params">(tokens)</span>:</span></span><br><span class="line">    tokens = [token.replace(<span class="string">"''"</span>, <span class="string">'"'</span>).replace(<span class="string">"``"</span>, <span class="string">'"'</span>) <span class="keyword">for</span> token <span class="keyword">in</span> nltk.word_tokenize(tokens)]</span><br><span class="line">    <span class="comment"># nltk.word_tokenize(tokens)分词，replace规范化引号，方便后面处理</span></span><br><span class="line">    <span class="keyword">return</span> tokens</span><br></pre></td></tr></table></figure>

<h2 id="3、清洗数据，并生成数据迭代器"><a href="#3、清洗数据，并生成数据迭代器" class="headerlink" title="3、清洗数据，并生成数据迭代器"></a>3、清洗数据，并生成数据迭代器</h2><p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SQuAD</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 以下定好中间输出缓存文件的路径</span></span><br><span class="line">        path = <span class="string">'data/squad'</span> </span><br><span class="line">        dataset_path = path + <span class="string">'/torch_text/'</span> </span><br><span class="line">        train_examples_path = dataset_path + <span class="string">'train_examples.pt'</span></span><br><span class="line">        dev_examples_path = dataset_path + <span class="string">'dev_examples.pt'</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">"preprocessing data files..."</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.train_file&#125;</span>l'</span>):</span><br><span class="line">            <span class="comment"># 字符串前以f开头表示在字符串内支持大括号内的 python 表达式</span></span><br><span class="line">            <span class="comment"># args.train_file = 'train-v1.1.json'</span></span><br><span class="line">            print(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.train_file&#125;</span>'</span>)</span><br><span class="line">            self.preprocess_file(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.train_file&#125;</span>'</span>)  </span><br><span class="line">            <span class="comment"># preprocess_file下面函数有定义，完成文件的预处理</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.dev_file&#125;</span>l'</span>):</span><br><span class="line">            <span class="comment"># args.dev_file = 'dev-v1.1.json'</span></span><br><span class="line">            self.preprocess_file(<span class="string">f'<span class="subst">&#123;path&#125;</span>/<span class="subst">&#123;args.dev_file&#125;</span>'</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 下面是用torchtext处理数据的步骤看不懂了，有知道的可以交流下   </span></span><br><span class="line">        self.RAW = data.RawField()<span class="comment"># 这个是完全空白的field，意味着不经过任何处理</span></span><br><span class="line">        <span class="comment"># explicit declaration for torchtext compatibility</span></span><br><span class="line">        self.RAW.is_target = <span class="literal">False</span></span><br><span class="line">        self.CHAR_NESTING = data.Field(batch_first=<span class="literal">True</span>, tokenize=list, lower=<span class="literal">True</span>)</span><br><span class="line">        self.CHAR = data.NestedField(self.CHAR_NESTING, tokenize=word_tokenize)</span><br><span class="line">        self.WORD = data.Field(batch_first=<span class="literal">True</span>, tokenize=word_tokenize, lower=<span class="literal">True</span>, include_lengths=<span class="literal">True</span>)</span><br><span class="line">        self.LABEL = data.Field(sequential=<span class="literal">False</span>, unk_token=<span class="literal">None</span>, use_vocab=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        dict_fields = &#123;<span class="string">'id'</span>: (<span class="string">'id'</span>, self.RAW),</span><br><span class="line">                       <span class="string">'s_idx'</span>: (<span class="string">'s_idx'</span>, self.LABEL),</span><br><span class="line">                       <span class="string">'e_idx'</span>: (<span class="string">'e_idx'</span>, self.LABEL),</span><br><span class="line">                       <span class="string">'context'</span>: [(<span class="string">'c_word'</span>, self.WORD), (<span class="string">'c_char'</span>, self.CHAR)],</span><br><span class="line">                       <span class="string">'question'</span>: [(<span class="string">'q_word'</span>, self.WORD), (<span class="string">'q_char'</span>, self.CHAR)]&#125;</span><br><span class="line"></span><br><span class="line">        list_fields = [(<span class="string">'id'</span>, self.RAW), (<span class="string">'s_idx'</span>, self.LABEL), (<span class="string">'e_idx'</span>, self.LABEL),</span><br><span class="line">                       (<span class="string">'c_word'</span>, self.WORD), (<span class="string">'c_char'</span>, self.CHAR),</span><br><span class="line">                       (<span class="string">'q_word'</span>, self.WORD), (<span class="string">'q_char'</span>, self.CHAR)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(dataset_path):</span><br><span class="line">            print(<span class="string">"loading splits..."</span>)</span><br><span class="line">            train_examples = torch.load(train_examples_path)</span><br><span class="line">            dev_examples = torch.load(dev_examples_path)</span><br><span class="line"></span><br><span class="line">            self.train = data.Dataset(examples=train_examples, fields=list_fields)</span><br><span class="line">            self.dev = data.Dataset(examples=dev_examples, fields=list_fields)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"building splits..."</span>)</span><br><span class="line">             <span class="comment"># 划分训练集和验证集</span></span><br><span class="line">            self.train, self.dev = data.TabularDataset.splits(</span><br><span class="line">                path=path,</span><br><span class="line">                train=<span class="string">f'<span class="subst">&#123;args.train_file&#125;</span>l'</span>,</span><br><span class="line">                validation=<span class="string">f'<span class="subst">&#123;args.dev_file&#125;</span>l'</span>,</span><br><span class="line">                format=<span class="string">'json'</span>,</span><br><span class="line">                fields=dict_fields)</span><br><span class="line"></span><br><span class="line">            os.makedirs(dataset_path)</span><br><span class="line">            torch.save(self.train.examples, train_examples_path)</span><br><span class="line">            torch.save(self.dev.examples, dev_examples_path)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#cut too long context in the training set for efficiency.</span></span><br><span class="line">        <span class="keyword">if</span> args.context_threshold &gt; <span class="number">0</span>:</span><br><span class="line">            self.train.examples = [e <span class="keyword">for</span> e <span class="keyword">in</span> self.train.examples <span class="keyword">if</span> len(e.c_word) &lt;= args.context_threshold]</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"building vocab..."</span>)</span><br><span class="line">        self.CHAR.build_vocab(self.train, self.dev) <span class="comment"># 字符向量没有设置vector</span></span><br><span class="line">        self.WORD.build_vocab(self.train, self.dev, vectors=GloVe(name=<span class="string">'6B'</span>, dim=args.word_dim))</span><br><span class="line">        <span class="comment"># 加载Glove向量，args.word_dim = 100</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">"building iterators..."</span>)</span><br><span class="line">        device = torch.device(<span class="string">f"cuda:<span class="subst">&#123;args.gpu&#125;</span>"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">        <span class="comment"># 生成迭代器</span></span><br><span class="line">        self.train_iter, self.dev_iter = \</span><br><span class="line">            data.BucketIterator.splits((self.train, self.dev),</span><br><span class="line">                                       batch_sizes=[args.train_batch_size, args.dev_batch_size],</span><br><span class="line">                                       device=device,</span><br><span class="line">                                       sort_key=<span class="keyword">lambda</span> x: len(x.c_word))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preprocess_file</span><span class="params">(self,path)</span>:</span></span><br><span class="line">        dump = []</span><br><span class="line">        abnormals = [<span class="string">' '</span>, <span class="string">'\n'</span>, <span class="string">'\u3000'</span>, <span class="string">'\u202f'</span>, <span class="string">'\u2009'</span>]</span><br><span class="line">        <span class="comment"># 空白无效字符列表</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> open(path, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            data = json.load(f) <span class="comment"># 直接文件句柄转化为字典</span></span><br><span class="line">            data = data[<span class="string">'data'</span>] <span class="comment"># 返回值data是个列表，字典是列表的元素</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> article <span class="keyword">in</span> data:</span><br><span class="line">                <span class="comment"># 每个article是一个字典，一个字典包含一个title的信息</span></span><br><span class="line">                <span class="keyword">for</span> paragraph <span class="keyword">in</span> article[<span class="string">'paragraphs'</span>]:</span><br><span class="line">                    <span class="comment"># 每个paragraph是一个字典，一个字典里有一个context和qas的信息，qas是问题和答案。</span></span><br><span class="line">                    context = paragraph[<span class="string">'context'</span>]</span><br><span class="line">                    <span class="comment"># context的内容，是字符串，如：" numerals 50............."</span></span><br><span class="line">                    tokens = word_tokenize(context) <span class="comment"># 对context进行分词</span></span><br><span class="line">                    <span class="keyword">for</span> qa <span class="keyword">in</span> paragraph[<span class="string">'qas'</span>]:</span><br><span class="line">                        <span class="comment"># 每个qa是一个字典，一个字典包含一对answers和question的信息</span></span><br><span class="line">                        id = qa[<span class="string">'id'</span>]</span><br><span class="line">                        <span class="comment"># 取出这对answers和question的id信息，如："56be4db0acb8001400a502ec"</span></span><br><span class="line">                        question = qa[<span class="string">'question'</span>]</span><br><span class="line">                        <span class="comment"># 取出question，如："Which NFL team represented the AFC at Super Bowl 50?"</span></span><br><span class="line">                        <span class="keyword">for</span> ans <span class="keyword">in</span> qa[<span class="string">'answers'</span>]:</span><br><span class="line">                            <span class="comment"># ans为每个答案，共有三个标准答案，可以相同，可以不同，统一为3个。</span></span><br><span class="line">                            answer = ans[<span class="string">'text'</span>]</span><br><span class="line">                            <span class="comment"># 问题的每个回答，如："Denver Broncos"</span></span><br><span class="line">                            s_idx = ans[<span class="string">'answer_start'</span>]</span><br><span class="line">                            <span class="comment"># 每个回答的start位置，数值代表context中第几个字符，如：177</span></span><br><span class="line">                            e_idx = s_idx + len(answer)</span><br><span class="line">                            <span class="comment"># 每个回答的end位置</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                            <span class="comment"># 下面重新更新字符的起始位置，使用字符计算位置改为使用单词计算位置</span></span><br><span class="line">                            <span class="comment"># 请看下面单元格的示例输出有助理解。</span></span><br><span class="line">                            l = <span class="number">0</span></span><br><span class="line">                            s_found = <span class="literal">False</span></span><br><span class="line">                            <span class="keyword">for</span> i, t <span class="keyword">in</span> enumerate(tokens):</span><br><span class="line">                                <span class="comment"># 循环t次，t为分词后的单词数量</span></span><br><span class="line">                                <span class="keyword">while</span> l &lt; len(context):</span><br><span class="line">                                    <span class="keyword">if</span> context[l] <span class="keyword">in</span> abnormals:</span><br><span class="line">                                        <span class="comment"># context中有空白无效字符，就计数</span></span><br><span class="line">                                        l += <span class="number">1</span></span><br><span class="line">                                    <span class="keyword">else</span>:    <span class="comment"># 一碰到不是空白字符的就break</span></span><br><span class="line">                                        <span class="keyword">break</span></span><br><span class="line">                                <span class="comment"># exceptional cases</span></span><br><span class="line">                                <span class="keyword">if</span> t[<span class="number">0</span>] == <span class="string">'"'</span> <span class="keyword">and</span> context[l:l + <span class="number">2</span>] == <span class="string">'\'\''</span>:</span><br><span class="line">                                    <span class="comment"># 专门计算context=''an 这种长度，这个长度为4</span></span><br><span class="line">                                    t = <span class="string">'\'\''</span> + t[<span class="number">1</span>:] </span><br><span class="line">                                <span class="keyword">elif</span> t == <span class="string">'"'</span> <span class="keyword">and</span> context[l:l + <span class="number">2</span>] == <span class="string">'\'\''</span>:</span><br><span class="line">                                    <span class="comment"># 专门计算context='' 这种长度</span></span><br><span class="line">                                    <span class="comment"># 上面t[0] == '"'表达式包含了这种，所以我认为这个表达式没用上</span></span><br><span class="line">                                    t = <span class="string">'\'\''</span></span><br><span class="line"></span><br><span class="line">                                l += len(t)</span><br><span class="line">                                <span class="keyword">if</span> l &gt; s_idx <span class="keyword">and</span> s_found == <span class="literal">False</span>:</span><br><span class="line">                                    <span class="comment"># 只要计数超过起始位置值，这个单词就是start的单词</span></span><br><span class="line">                                    s_idx = i</span><br><span class="line">                                    s_found = <span class="literal">True</span></span><br><span class="line">                                <span class="keyword">if</span> l &gt;= e_idx:</span><br><span class="line">                                    <span class="comment"># 这里不出错的话，等于e_idx就是end的单词</span></span><br><span class="line">                                    e_idx = i</span><br><span class="line">                                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">                            <span class="comment"># 这里把三个answer分开，每个answer都放进字典中,并作为一个样本</span></span><br><span class="line">                            dump.append(dict([(<span class="string">'id'</span>, id),</span><br><span class="line">                                              (<span class="string">'context'</span>, context),</span><br><span class="line">                                              (<span class="string">'question'</span>, question),</span><br><span class="line">                                              (<span class="string">'answer'</span>, answer),</span><br><span class="line">                                              (<span class="string">'s_idx'</span>, s_idx),</span><br><span class="line">                                              (<span class="string">'e_idx'</span>, e_idx)]))</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">f'<span class="subst">&#123;path&#125;</span>l'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> dump:</span><br><span class="line">                <span class="comment"># line为字典，一个样本存储</span></span><br><span class="line">                json.dump(line, f)</span><br><span class="line">                <span class="comment">#dump：将dict类型转换为json字符串格式，写入到文件</span></span><br><span class="line">                print(<span class="string">''</span>, file=f) <span class="comment"># 这里print的作用就是换行用的。</span></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = SQuAD(args)</span><br></pre></td></tr></table></figure>

<h3 id="上面不太明白的举例子"><a href="#上面不太明白的举例子" class="headerlink" title="上面不太明白的举例子"></a>上面不太明白的举例子</h3><p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 举例子</span></span><br><span class="line">a = <span class="string">" \u2009\n\u3000Super Bowl 50 was ''an'' American football     \u3000game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50."</span></span><br><span class="line">tokens = word_tokenize(a)</span><br><span class="line">print(nltk.word_tokenize(a)) <span class="comment"># 所有的“\u2009”，“\n”，“\u3000”等空白字符都去掉了</span></span><br><span class="line">print(<span class="string">"----"</span>*<span class="number">20</span>)</span><br><span class="line">print(tokens)</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># </span></span><br><span class="line">print(a[<span class="number">0</span>]) <span class="comment"># 空白字符打印不出来</span></span><br><span class="line">print(a[<span class="number">1</span>]) <span class="comment"># 空白字符打印不出来</span></span><br><span class="line">print(a[<span class="number">2</span>]) <span class="comment"># 空白字符打印不出来</span></span><br><span class="line">print(a[<span class="number">3</span>]) <span class="comment"># 空白字符打印不出来</span></span><br><span class="line">print(a[<span class="number">4</span>]) </span><br><span class="line">print(a[<span class="number">5</span>])</span><br><span class="line">a[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面特别注意</span></span><br><span class="line">print(tokens[<span class="number">4</span>][<span class="number">0</span>]== <span class="string">'"'</span>) <span class="comment"># 虽然切分后看起来是"''"，但实际上是'"'</span></span><br><span class="line">print(tokens[<span class="number">4</span>][<span class="number">0</span>]== <span class="string">"''"</span>) </span><br><span class="line">print(tokens[<span class="number">5</span>]== <span class="string">'"'</span>)</span><br><span class="line">print(len(<span class="string">'"'</span>)) <span class="comment"># 这种长度为1</span></span><br><span class="line">print(len(<span class="string">"''"</span>)) <span class="comment"># 这种长度为2</span></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看输出理解</span></span><br><span class="line">s_idx = <span class="number">177</span></span><br><span class="line">e_idx = s_idx + len(<span class="string">"Denver Broncos"</span>)</span><br><span class="line">l=<span class="number">0</span></span><br><span class="line">context = <span class="string">" \u2009\n\u3000Super Bowl 50 was ''an'' American football     \u3000game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50."</span></span><br><span class="line">tokens = word_tokenize(context)</span><br><span class="line">abnormals = [<span class="string">' '</span>, <span class="string">'\n'</span>, <span class="string">'\u3000'</span>, <span class="string">'\u202f'</span>, <span class="string">'\u2009'</span>]</span><br><span class="line">s_found = <span class="literal">False</span></span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> enumerate(tokens):</span><br><span class="line">    print(<span class="string">"t="</span>,t)</span><br><span class="line">    <span class="keyword">while</span> l &lt; len(context):</span><br><span class="line">        <span class="keyword">if</span> context[l] <span class="keyword">in</span> abnormals:</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    print(<span class="string">"l"</span>,l)</span><br><span class="line">    <span class="comment"># exceptional cases</span></span><br><span class="line">    <span class="keyword">if</span> t[<span class="number">0</span>] == <span class="string">'"'</span> <span class="keyword">and</span> context[l:l + <span class="number">2</span>] == <span class="string">'\'\''</span>:</span><br><span class="line">        print(<span class="string">"1111111111111111111"</span>)</span><br><span class="line">        print(t)</span><br><span class="line">        print(t[<span class="number">1</span>:])</span><br><span class="line">        t = <span class="string">'\'\''</span> + t[<span class="number">1</span>:]</span><br><span class="line">        print(t)</span><br><span class="line">    <span class="keyword">elif</span> t == <span class="string">'"'</span> <span class="keyword">and</span> context[l:l + <span class="number">2</span>] == <span class="string">'\'\''</span>:</span><br><span class="line">        <span class="comment"># 看输出结果，这个表达式没有用到</span></span><br><span class="line">        print(<span class="string">"22222222222222222222"</span>)</span><br><span class="line">        print(t)</span><br><span class="line">        t = <span class="string">'\'\''</span></span><br><span class="line">    print(<span class="string">"len(t)"</span>,len(t))</span><br><span class="line">    l += len(t)</span><br><span class="line">    print(<span class="string">"l"</span>,l)</span><br><span class="line">    <span class="keyword">if</span> l &gt; s_idx <span class="keyword">and</span> s_found == <span class="literal">False</span>:</span><br><span class="line">        s_idx = i</span><br><span class="line">        print(<span class="string">"s_idx"</span>,s_idx)</span><br><span class="line">        s_found = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> l &gt;= e_idx:</span><br><span class="line">        e_idx = i</span><br><span class="line">        print(<span class="string">"e_idx"</span>,e_idx)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch = next(iter(data.train_iter)) <span class="comment">#一个batch的信息</span></span><br><span class="line">print(batch)</span><br><span class="line"><span class="comment"># 训练集的batch_sizes=60</span></span><br><span class="line"><span class="comment"># batch.c_word = 60x293，293是60个样本中最长样本token的单词数</span></span><br><span class="line"><span class="comment"># batch.c_char = 60x293x25，25是某个单词字符的最大的数量</span></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(batch.q_word)</span><br><span class="line">print(batch.q_char[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面为args新增参数，并赋值</span></span><br><span class="line"><span class="comment"># hasattr() getattr() setattr() 函数使用方法详解https://www.cnblogs.com/cenyu/p/5713686.html</span></span><br><span class="line">setattr(args, <span class="string">'char_vocab_size'</span>, len(data.CHAR.vocab)) <span class="comment"># 设置属性args.char_vocab_size的值 = len(data.CHAR.vocab)</span></span><br><span class="line">setattr(args, <span class="string">'word_vocab_size'</span>, len(data.WORD.vocab))</span><br><span class="line">setattr(args, <span class="string">'dataset_file'</span>, <span class="string">f'data/squad/<span class="subst">&#123;args.dev_file&#125;</span>'</span>)</span><br><span class="line">setattr(args, <span class="string">'prediction_file'</span>, <span class="string">f'prediction<span class="subst">&#123;args.gpu&#125;</span>.out'</span>)</span><br><span class="line">setattr(args, <span class="string">'model_time'</span>, strftime(<span class="string">'%H:%M:%S'</span>, gmtime())) <span class="comment"># 时间</span></span><br><span class="line">print(<span class="string">'data loading complete!'</span>)</span><br></pre></td></tr></table></figure>

<h2 id="BIDAF"><a href="#BIDAF" class="headerlink" title="BIDAF"></a>BIDAF</h2><p><img src="https://s1.ax1x.com/2020/06/09/t4C2yd.jpg" alt="avatar"></p>
<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTM</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, batch_first=False, num_layers=<span class="number">1</span>, bidirectional=False, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        <span class="comment"># input_size=args.hidden_size * 2 = 200,</span></span><br><span class="line">        <span class="comment"># hidden_size=args.hidden_size = 100,</span></span><br><span class="line">        <span class="comment"># bidirectional=True,</span></span><br><span class="line">        <span class="comment"># batch_first=True, </span></span><br><span class="line">        <span class="comment"># dropout=args.dropout = 0.2</span></span><br><span class="line">        super(LSTM, self).__init__()</span><br><span class="line">        self.rnn = nn.LSTM(input_size=input_size,</span><br><span class="line">                           hidden_size=hidden_size,</span><br><span class="line">                           num_layers=num_layers,</span><br><span class="line">                           bidirectional=bidirectional,</span><br><span class="line">                           batch_first=batch_first)</span><br><span class="line">        self.reset_params() <span class="comment"># 重置参数</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_params</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.rnn.num_layers):</span><br><span class="line">            nn.init.orthogonal_(getattr(self.rnn, <span class="string">f'weight_hh_l<span class="subst">&#123;i&#125;</span>'</span>)) <span class="comment"># hidden-hidden weights</span></span><br><span class="line">            <span class="comment"># weight_hh_l&#123;i&#125;、weight_ih_l&#123;i&#125;、bias_hh_l&#123;i&#125;、bias_ih_l&#123;i&#125; 都是nn.LSTM源码里的参数</span></span><br><span class="line">            <span class="comment"># getattr取出源码里参数的值，用nn.init.orthogonal_正交进行重新初始化</span></span><br><span class="line">            <span class="comment"># nn.init初始化方法看这个链接：https://www.aiuai.cn/aifarm613.html</span></span><br><span class="line">            nn.init.kaiming_normal_(getattr(self.rnn, <span class="string">f'weight_ih_l<span class="subst">&#123;i&#125;</span>'</span>)) <span class="comment"># input-hidden weights</span></span><br><span class="line">            nn.init.constant_(getattr(self.rnn, <span class="string">f'bias_hh_l<span class="subst">&#123;i&#125;</span>'</span>), val=<span class="number">0</span>) <span class="comment"># hidden-hidden bias</span></span><br><span class="line">            nn.init.constant_(getattr(self.rnn, <span class="string">f'bias_ih_l<span class="subst">&#123;i&#125;</span>'</span>), val=<span class="number">0</span>) <span class="comment"># input-hidden bias</span></span><br><span class="line">            getattr(self.rnn, <span class="string">f'bias_hh_l<span class="subst">&#123;i&#125;</span>'</span>).chunk(<span class="number">4</span>)[<span class="number">1</span>].fill_(<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># .chunk看下这个链接：https://blog.csdn.net/XuM222222/article/details/92380538</span></span><br><span class="line">            <span class="comment"># .fill_(1),下划线代表直接替换，看链接：https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.fill.html</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.rnn.bidirectional: <span class="comment"># 双向，需要初始化反向的参数</span></span><br><span class="line">                nn.init.orthogonal_(getattr(self.rnn, <span class="string">f'weight_hh_l<span class="subst">&#123;i&#125;</span>_reverse'</span>))</span><br><span class="line">                nn.init.kaiming_normal_(getattr(self.rnn, <span class="string">f'weight_ih_l<span class="subst">&#123;i&#125;</span>_reverse'</span>))</span><br><span class="line">                nn.init.constant_(getattr(self.rnn, <span class="string">f'bias_hh_l<span class="subst">&#123;i&#125;</span>_reverse'</span>), val=<span class="number">0</span>)</span><br><span class="line">                nn.init.constant_(getattr(self.rnn, <span class="string">f'bias_ih_l<span class="subst">&#123;i&#125;</span>_reverse'</span>), val=<span class="number">0</span>)</span><br><span class="line">                getattr(self.rnn, <span class="string">f'bias_hh_l<span class="subst">&#123;i&#125;</span>_reverse'</span>).chunk(<span class="number">4</span>)[<span class="number">1</span>].fill_(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># x是一个元组(c, c_lens)</span></span><br><span class="line">        x, x_len = x</span><br><span class="line">        <span class="comment"># x = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        <span class="comment"># x_len = (batch) 一个batch中所有context或question的样本长度</span></span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 下面一顿操作和第七课机器翻译的一样，</span></span><br><span class="line">        <span class="comment"># 看下这篇博客理解：https://www.cnblogs.com/sbj123456789/p/9834018.html</span></span><br><span class="line">        x_len_sorted, x_idx = torch.sort(x_len, descending=<span class="literal">True</span>)</span><br><span class="line">        x_sorted = x.index_select(dim=<span class="number">0</span>, index=x_idx)</span><br><span class="line">        _, x_ori_idx = torch.sort(x_idx)</span><br><span class="line"></span><br><span class="line">        x_packed = nn.utils.rnn.pack_padded_sequence(x_sorted, x_len_sorted, batch_first=<span class="literal">True</span>)</span><br><span class="line">        x_packed, (h, c) = self.rnn(x_packed)</span><br><span class="line"></span><br><span class="line">        x = nn.utils.rnn.pad_packed_sequence(x_packed, batch_first=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">        x = x.index_select(dim=<span class="number">0</span>, index=x_ori_idx)</span><br><span class="line">        h = h.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>).contiguous().view(<span class="number">-1</span>, h.size(<span class="number">0</span>) * h.size(<span class="number">2</span>)).squeeze()</span><br><span class="line">        h = h.index_select(dim=<span class="number">0</span>, index=x_ori_idx)</span><br><span class="line">        <span class="comment"># x = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        <span class="comment"># h = (1, batch, hidden_size * 2) 这个维度不用管</span></span><br><span class="line">        <span class="keyword">return</span> x, h</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features, dropout=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(Linear, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.linear = nn.Linear(in_features=in_features, out_features=out_features)</span><br><span class="line">        <span class="comment"># in_features = hidden_size * 2</span></span><br><span class="line">        <span class="comment"># out_features = hidden_size * 2</span></span><br><span class="line">        <span class="keyword">if</span> dropout &gt; <span class="number">0</span>:</span><br><span class="line">            self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        self.reset_params()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_params</span><span class="params">(self)</span>:</span></span><br><span class="line">        nn.init.kaiming_normal_(self.linear.weight)</span><br><span class="line">        nn.init.constant_(self.linear.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> hasattr(self, <span class="string">'dropout'</span>): <span class="comment"># 判断self有没有'dropout'这个参数，返回bool值</span></span><br><span class="line">            x = self.dropout(x)</span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args.char_dim</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 看英文论文或这篇博客理解模型：https://blog.csdn.net/u014665013/article/details/79793395</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiDAF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args, pretrained)</span>:</span></span><br><span class="line">        <span class="comment"># pretrained = data.WORD.vocab.vectors = (108777, 100)</span></span><br><span class="line">        super(BiDAF, self).__init__()</span><br><span class="line">        self.args = args</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. Character Embedding Layer 是模型示意图左边的层的名字，从下往上</span></span><br><span class="line">        <span class="comment"># 字符编码层</span></span><br><span class="line">        self.char_emb = nn.Embedding(args.char_vocab_size, args.char_dim, padding_idx=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># args.char_vocab_size = 1307，args.char_dim = 8</span></span><br><span class="line">        nn.init.uniform_(self.char_emb.weight, <span class="number">-0.001</span>, <span class="number">0.001</span>)</span><br><span class="line">        <span class="comment"># 初始化权重</span></span><br><span class="line"></span><br><span class="line">        self.char_conv = nn.Conv2d(<span class="number">1</span>, args.char_channel_size, (args.char_dim, args.char_channel_width))</span><br><span class="line">        <span class="comment"># args.char_channel_size = 100 卷积核数量 </span></span><br><span class="line">        <span class="comment"># (args.char_dim, args.char_channel_width) = (8,5) 过滤器大小</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. Word Embedding Layer</span></span><br><span class="line">        <span class="comment"># 单词编码层</span></span><br><span class="line">        <span class="comment"># initialize word embedding with GloVe</span></span><br><span class="line">        self.word_emb = nn.Embedding.from_pretrained(pretrained, freeze=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 初始化词向量权重，用的Glove向量</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># highway network</span></span><br><span class="line">        <span class="keyword">assert</span> self.args.hidden_size * <span class="number">2</span> == (self.args.char_channel_size + self.args.word_dim)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            setattr(self, <span class="string">f'highway_linear<span class="subst">&#123;i&#125;</span>'</span>,</span><br><span class="line">                    nn.Sequential(Linear(args.hidden_size * <span class="number">2</span>, args.hidden_size * <span class="number">2</span>),</span><br><span class="line">                                  nn.ReLU()))</span><br><span class="line">            <span class="comment"># 设置highway_linear0 = nn.Sequential(Linear(args.hidden_size * 2, args.hidden_size * 2)</span></span><br><span class="line">            <span class="comment"># 设置highway_linear1 = nn.Sequential(Linear(args.hidden_size * 2, args.hidden_size * 2)</span></span><br><span class="line">            <span class="comment"># args.hidden_size = 100</span></span><br><span class="line">                                </span><br><span class="line">            setattr(self, <span class="string">f'highway_gate<span class="subst">&#123;i&#125;</span>'</span>,</span><br><span class="line">                    nn.Sequential(Linear(args.hidden_size * <span class="number">2</span>, args.hidden_size * <span class="number">2</span>),</span><br><span class="line">                                  nn.Sigmoid()))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. Contextual Embedding Layer</span></span><br><span class="line">        <span class="comment"># 上下文，和答案嵌入层，用的LSTM</span></span><br><span class="line">        <span class="comment"># 下面LSTM定位到了自定义的class LSTM(nn.Module)。</span></span><br><span class="line">        self.context_LSTM = LSTM(input_size=args.hidden_size * <span class="number">2</span>,</span><br><span class="line">                                 hidden_size=args.hidden_size,</span><br><span class="line">                                 bidirectional=<span class="literal">True</span>,</span><br><span class="line">                                 batch_first=<span class="literal">True</span>,</span><br><span class="line">                                 dropout=args.dropout) </span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. Attention Flow Layer</span></span><br><span class="line">        <span class="comment"># 注意力层</span></span><br><span class="line">        self.att_weight_c = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.att_weight_q = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.att_weight_cq = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5. Modeling Layer</span></span><br><span class="line">        self.modeling_LSTM1 = LSTM(input_size=args.hidden_size * <span class="number">8</span>,</span><br><span class="line">                                   hidden_size=args.hidden_size,</span><br><span class="line">                                   bidirectional=<span class="literal">True</span>,</span><br><span class="line">                                   batch_first=<span class="literal">True</span>,</span><br><span class="line">                                   dropout=args.dropout)</span><br><span class="line"></span><br><span class="line">        self.modeling_LSTM2 = LSTM(input_size=args.hidden_size * <span class="number">2</span>,</span><br><span class="line">                                   hidden_size=args.hidden_size,</span><br><span class="line">                                   bidirectional=<span class="literal">True</span>,</span><br><span class="line">                                   batch_first=<span class="literal">True</span>,</span><br><span class="line">                                   dropout=args.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 6. Output Layer</span></span><br><span class="line">        self.p1_weight_g = Linear(args.hidden_size * <span class="number">8</span>, <span class="number">1</span>, dropout=args.dropout)</span><br><span class="line">        self.p1_weight_m = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>, dropout=args.dropout)</span><br><span class="line">        self.p2_weight_g = Linear(args.hidden_size * <span class="number">8</span>, <span class="number">1</span>, dropout=args.dropout)</span><br><span class="line">        self.p2_weight_m = Linear(args.hidden_size * <span class="number">2</span>, <span class="number">1</span>, dropout=args.dropout)</span><br><span class="line"></span><br><span class="line">        self.output_LSTM = LSTM(input_size=args.hidden_size * <span class="number">2</span>,</span><br><span class="line">                                hidden_size=args.hidden_size,</span><br><span class="line">                                bidirectional=<span class="literal">True</span>,</span><br><span class="line">                                batch_first=<span class="literal">True</span>,</span><br><span class="line">                                dropout=args.dropout)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(p=args.dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, batch)</span>:</span></span><br><span class="line">        <span class="comment"># batch里面有'id','s_idx','e_idx', 'c_word','c_char','q_word', 'q_char'数据</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> More memory-efficient architecture</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">char_emb_layer</span><span class="params">(x)</span>:</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            :param x: (batch, seq_len, word_len)</span></span><br><span class="line"><span class="string">            :return: (batch, seq_len, char_channel_size)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            <span class="comment"># x = (batch_sizes,seq_len,word_len)</span></span><br><span class="line">            batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">            x = self.dropout(self.char_emb(x))</span><br><span class="line">            <span class="comment"># (batch, seq_len, word_len, char_dim)</span></span><br><span class="line">            x = x.view(<span class="number">-1</span>, self.args.char_dim, x.size(<span class="number">2</span>)).unsqueeze(<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># (batch * seq_len, 1, char_dim, word_len) 1是输入的channel的维度</span></span><br><span class="line">            x = self.char_conv(x).squeeze()</span><br><span class="line">            <span class="comment"># (batch * seq_len, char_channel_size, 1, conv_len) -&gt; </span></span><br><span class="line">            <span class="comment"># (batch * seq_len, char_channel_size, conv_len) conv_len不用管，下一步都会pool掉</span></span><br><span class="line">            x = F.max_pool1d(x, x.size(<span class="number">2</span>)).squeeze()</span><br><span class="line">            <span class="comment"># (batch * seq_len, char_channel_size, 1) -&gt; (batch * seq_len, char_channel_size)</span></span><br><span class="line">            x = x.view(batch_size, <span class="number">-1</span>, self.args.char_channel_size)</span><br><span class="line">            <span class="comment"># (batch, seq_len, char_channel_size)</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">highway_network</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            :param x1: (batch, seq_len, char_channel_size)</span></span><br><span class="line"><span class="string">            :param x2: (batch, seq_len, word_dim)</span></span><br><span class="line"><span class="string">            :return: (batch, seq_len, hidden_size * 2)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            </span><br><span class="line">            x = torch.cat([x1, x2], dim=<span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># x = (batch, seq_len, char_channel_size + word_dim)</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">                h = getattr(self, <span class="string">f'highway_linear<span class="subst">&#123;i&#125;</span>'</span>)(x) <span class="comment"># 调用Linear的forward方法</span></span><br><span class="line">                <span class="comment"># h = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">                g = getattr(self, <span class="string">f'highway_gate<span class="subst">&#123;i&#125;</span>'</span>)(x)</span><br><span class="line">                <span class="comment"># g = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">                x = g * h + (<span class="number">1</span> - g) * x</span><br><span class="line">            <span class="comment"># (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">att_flow_layer</span><span class="params">(c, q)</span>:</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            :param c: (batch, c_len, hidden_size * 2)</span></span><br><span class="line"><span class="string">            :param q: (batch, q_len, hidden_size * 2)</span></span><br><span class="line"><span class="string">            :return: (batch, c_len, q_len)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            c_len = c.size(<span class="number">1</span>)</span><br><span class="line">            q_len = q.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (batch, c_len, q_len, hidden_size * 2)</span></span><br><span class="line">            <span class="comment">#c_tiled = c.unsqueeze(2).expand(-1, -1, q_len, -1)</span></span><br><span class="line">            <span class="comment"># (batch, c_len, q_len, hidden_size * 2)</span></span><br><span class="line">            <span class="comment">#q_tiled = q.unsqueeze(1).expand(-1, c_len, -1, -1)</span></span><br><span class="line">            <span class="comment"># (batch, c_len, q_len, hidden_size * 2)</span></span><br><span class="line">            <span class="comment">#cq_tiled = c_tiled * q_tiled</span></span><br><span class="line">            <span class="comment">#cq_tiled = c.unsqueeze(2).expand(-1, -1, q_len, -1) * q.unsqueeze(1).expand(-1, c_len, -1, -1)</span></span><br><span class="line"><span class="comment">#        # 4. Attention Flow Layer</span></span><br><span class="line"><span class="comment">#         # 注意力层</span></span><br><span class="line"><span class="comment">#         self.att_weight_c = Linear(args.hidden_size * 2, 1)</span></span><br><span class="line"><span class="comment">#         self.att_weight_q = Linear(args.hidden_size * 2, 1)</span></span><br><span class="line"><span class="comment">#         self.att_weight_cq = Linear(args.hidden_size * 2, 1)</span></span><br><span class="line">            cq = []</span><br><span class="line">            <span class="comment"># 1、相似度计算方式，看下这篇博客理解：https://blog.csdn.net/u014665013/article/details/79793395</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(q_len):</span><br><span class="line">                qi = q.select(<span class="number">1</span>, i).unsqueeze(<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># (batch, 1, hidden_size * 2)</span></span><br><span class="line">                <span class="comment"># .select看这个：https://blog.csdn.net/hungryof/article/details/51802829</span></span><br><span class="line">                ci = self.att_weight_cq(c * qi).squeeze()</span><br><span class="line">                <span class="comment"># (batch, c_len, 1)</span></span><br><span class="line">                cq.append(ci)</span><br><span class="line">            cq = torch.stack(cq, dim=<span class="number">-1</span>) </span><br><span class="line">            <span class="comment"># (batch, c_len, q_len) cp是共享相似度矩阵</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 2、计算对每一个 context word 而言哪些 query words 和它最相关。</span></span><br><span class="line">            <span class="comment"># context-to-query attention(C2Q):</span></span><br><span class="line">            s = self.att_weight_c(c).expand(<span class="number">-1</span>, <span class="number">-1</span>, q_len) + \</span><br><span class="line">                self.att_weight_q(q).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).expand(<span class="number">-1</span>, c_len, <span class="number">-1</span>) + cq</span><br><span class="line">            <span class="comment"># (batch, c_len, q_len) </span></span><br><span class="line">            a = F.softmax(s, dim=<span class="number">2</span>) </span><br><span class="line">            <span class="comment"># (batch, c_len, q_len)</span></span><br><span class="line">            c2q_att = torch.bmm(a, q) </span><br><span class="line">            <span class="comment"># (batch, c_len, q_len) * (batch, q_len, hidden_size * 2) -&gt; (batch, c_len, hidden_size * 2)</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 3、计算对每一个 query word 而言哪些 context words 和它最相关</span></span><br><span class="line">            <span class="comment"># query-to-context attention(Q2C):</span></span><br><span class="line">            b = F.softmax(torch.max(s, dim=<span class="number">2</span>)[<span class="number">0</span>], dim=<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># (batch, 1, c_len)</span></span><br><span class="line">            q2c_att = torch.bmm(b, c).squeeze()</span><br><span class="line">            <span class="comment"># (batch, 1, c_len) * (batch, c_len, hidden_size * 2) -&gt; (batch, hidden_size * 2)</span></span><br><span class="line">            q2c_att = q2c_att.unsqueeze(<span class="number">1</span>).expand(<span class="number">-1</span>, c_len, <span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># (batch, c_len, hidden_size * 2) (tiled)</span></span><br><span class="line">            <span class="comment"># q2c_att = torch.stack([q2c_att] * c_len, dim=1)</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 4、最后将context embedding和C2Q、Q2C的结果（三个矩阵）拼接起来</span></span><br><span class="line">            x = torch.cat([c, c2q_att, c * c2q_att, c * q2c_att], dim=<span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># (batch, c_len, hidden_size * 8)</span></span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">output_layer</span><span class="params">(g, m, l)</span>:</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            :param g: (batch, c_len, hidden_size * 8)</span></span><br><span class="line"><span class="string">            :param m: (batch, c_len ,hidden_size * 2)</span></span><br><span class="line"><span class="string">             #  l = c_lens</span></span><br><span class="line"><span class="string">            :return: p1: (batch, c_len), p2: (batch, c_len)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            p1 = (self.p1_weight_g(g) + self.p1_weight_m(m)).squeeze()</span><br><span class="line">            <span class="comment"># (batch, c_len)</span></span><br><span class="line">            m2 = self.output_LSTM((m, l))[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># (batch, c_len, hidden_size * 2)</span></span><br><span class="line">            p2 = (self.p2_weight_g(g) + self.p2_weight_m(m2)).squeeze()</span><br><span class="line">            <span class="comment"># (batch, c_len)</span></span><br><span class="line">            <span class="keyword">return</span> p1, p2</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. Character Embedding Layer</span></span><br><span class="line">        <span class="comment"># 令:一个batch中单词数量最多的样本长度为seq_len</span></span><br><span class="line">        <span class="comment"># 令:一个batch中某个单词长度最长的单词长度为word_len</span></span><br><span class="line">        </span><br><span class="line">        c_char = char_emb_layer(batch.c_char) </span><br><span class="line">        <span class="comment"># batch.c_char = (batch,seq_len,word_len) 后两个维度对应context</span></span><br><span class="line">        <span class="comment"># c_char = (batch, seq_len, char_channel_size)</span></span><br><span class="line"></span><br><span class="line">        q_char = char_emb_layer(batch.q_char)</span><br><span class="line">        <span class="comment"># batch.c_char = (batch,seq_len,word_len) 后两个维度对应question</span></span><br><span class="line">        <span class="comment"># c_char = (batch, seq_len, char_channel_size)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. Word Embedding Layer</span></span><br><span class="line">        c_word = self.word_emb(batch.c_word[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># batch.c_word[0] = (batch,seq_len) 后一个维度对应context</span></span><br><span class="line">        <span class="comment"># c_word = (batch, seq_len, word_dim) word_dim是Glove词向量维度</span></span><br><span class="line">        q_word = self.word_emb(batch.q_word[<span class="number">0</span>]) </span><br><span class="line">        <span class="comment"># batch.q_word[0] = (batch,seq_len) 后一个维度对应question</span></span><br><span class="line">        <span class="comment"># q_word = (batch, seq_len, word_dim)</span></span><br><span class="line">        c_lens = batch.c_word[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># c_lens：一个batch中所有context的样本长度</span></span><br><span class="line">        q_lens = batch.q_word[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># q_lens：一个batch中所有question的样本长度</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Highway network</span></span><br><span class="line">        c = highway_network(c_char, c_word)</span><br><span class="line">        <span class="comment"># c = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        q = highway_network(q_char, q_word)</span><br><span class="line">        <span class="comment"># q = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. Contextual Embedding Layer</span></span><br><span class="line">        c = self.context_LSTM((c, c_lens))[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># c = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        q = self.context_LSTM((q, q_lens))[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># q = (batch, seq_len, hidden_size * 2)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 4. Attention Flow Layer</span></span><br><span class="line">        g = att_flow_layer(c, q)</span><br><span class="line">        <span class="comment"># (batch, c_len, hidden_size * 8)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 5. Modeling Layer</span></span><br><span class="line">        m = self.modeling_LSTM2((self.modeling_LSTM1((g, c_lens))[<span class="number">0</span>], c_lens))[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># self.modeling_LSTM1((g, c_lens))[0] = (batch, c_len, hidden_size * 2) # 2因为是双向</span></span><br><span class="line">        <span class="comment"># m = (batch, c_len, hidden_size * 2) 2因为是双向</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 6. Output Layer</span></span><br><span class="line">        p1, p2 = output_layer(g, m, c_lens) <span class="comment"># 预测开始位置和结束位置</span></span><br><span class="line">        <span class="comment"># (batch, c_len), (batch, c_len)</span></span><br><span class="line">        <span class="keyword">return</span> p1, p2</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand((<span class="number">2</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line">print(x)</span><br><span class="line">y = x.select(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(len(data.WORD.vocab)) <span class="comment"># 108777个单词</span></span><br><span class="line">print(data.WORD.vocab.vectors.shape) <span class="comment"># 词向量维度</span></span><br><span class="line"></span><br><span class="line">print(data.WORD.vocab.itos[:<span class="number">50</span>]) <span class="comment"># 前50个词频最高的单词</span></span><br><span class="line">print(<span class="string">"------"</span>*<span class="number">10</span>)</span><br><span class="line">print(list(data.WORD.vocab.stoi.items())[<span class="number">0</span>:<span class="number">50</span>]) <span class="comment"># 对应的索引</span></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(len(data.CHAR.vocab)) <span class="comment"># 1307个单词</span></span><br><span class="line">print(data.CHAR.vocab.itos[:<span class="number">50</span>]) <span class="comment"># 108777个单词</span></span><br><span class="line">print(<span class="string">"------"</span>*<span class="number">10</span>)</span><br><span class="line">print(list(data.CHAR.vocab.stoi.items())[<span class="number">0</span>:<span class="number">50</span>]) <span class="comment"># 对应的索引</span></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">f"cuda:<span class="subst">&#123;args.gpu&#125;</span>"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">model = BiDAF(args, data.WORD.vocab.vectors).to(device)</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EMA</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mu)</span>:</span></span><br><span class="line">        <span class="comment"># mu = args.exp_decay_rate = 0.999</span></span><br><span class="line">        self.mu = mu</span><br><span class="line">        self.shadow = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">register</span><span class="params">(self, name, val)</span>:</span></span><br><span class="line">        <span class="comment"># name:各个参数层的名字, param.data；参数层的数据</span></span><br><span class="line">        self.shadow[name] = val.clone() <span class="comment"># 建立字典</span></span><br><span class="line">        <span class="comment"># clone()得到的Tensor不仅拷贝了原始的value，而且会计算梯度传播信息，copy_()只拷贝数值</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.shadow[name]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, name, x)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> name <span class="keyword">in</span> self.shadow</span><br><span class="line">        new_average = (<span class="number">1.0</span> - self.mu) * x + self.mu * self.shadow[name]</span><br><span class="line">        self.shadow[name] = new_average.clone()</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(model, ema, args, data)</span>:</span></span><br><span class="line">    device = torch.device(<span class="string">f"cuda:<span class="subst">&#123;args.gpu&#125;</span>"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    answers = dict()</span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    backup_params = EMA(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">            backup_params.register(name, param.data) <span class="comment"># 重新建立字典</span></span><br><span class="line">            param.data.copy_(ema.get(name))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.set_grad_enabled(<span class="literal">False</span>):</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> iter(data.dev_iter):</span><br><span class="line">            p1, p2 = model(batch)</span><br><span class="line">            print(p1.shape,p2.shape)</span><br><span class="line">            print(batch.s_idx,batch.e_idx)</span><br><span class="line">            batch_loss = criterion(p1, batch.s_idx<span class="number">-1</span>) + criterion(p2, batch.e_idx<span class="number">-1</span>)</span><br><span class="line">            print(<span class="string">"batch_loss"</span>,batch_loss)</span><br><span class="line">            print(<span class="string">"----"</span>*<span class="number">40</span>)</span><br><span class="line">            loss += batch_loss.item()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (batch, c_len, c_len)</span></span><br><span class="line">            batch_size, c_len = p1.size()</span><br><span class="line">            ls = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">            mask = (torch.ones(c_len, c_len) * float(<span class="string">'-inf'</span>)).to(device).tril(<span class="number">-1</span>).unsqueeze(<span class="number">0</span>).expand(batch_size, <span class="number">-1</span>, <span class="number">-1</span>)</span><br><span class="line">            score = (ls(p1).unsqueeze(<span class="number">2</span>) + ls(p2).unsqueeze(<span class="number">1</span>)) + mask</span><br><span class="line">            score, s_idx = score.max(dim=<span class="number">1</span>)</span><br><span class="line">            score, e_idx = score.max(dim=<span class="number">1</span>)</span><br><span class="line">            s_idx = torch.gather(s_idx, <span class="number">1</span>, e_idx.view(<span class="number">-1</span>, <span class="number">1</span>)).squeeze()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">                id = batch.id[i]</span><br><span class="line">                answer = batch.c_word[<span class="number">0</span>][i][s_idx[i]:e_idx[i]+<span class="number">1</span>]</span><br><span class="line">                answer = <span class="string">' '</span>.join([data.WORD.vocab.itos[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> answer])</span><br><span class="line">                answers[id] = answer</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                param.data.copy_(backup_params.get(name))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(args.prediction_file, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        print(json.dumps(answers), file=f)</span><br><span class="line"></span><br><span class="line">    results = evaluate.main(args)</span><br><span class="line">    <span class="keyword">return</span> loss, results[<span class="string">'exact_match'</span>], results[<span class="string">'f1'</span>]</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    print(name)</span><br><span class="line">    print(param.requires_grad)</span><br><span class="line">    print(param.data.shape)</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">strftime(&apos;%H:%M:%S&apos;, gmtime())</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">iterator = data.train_iter</span><br><span class="line">n= <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    print(<span class="string">"j="</span>,j)</span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(iterator):</span><br><span class="line">        print(<span class="string">"当前epoch"</span>,int(iterator.epoch))</span><br><span class="line">        print(<span class="string">"-----"</span>*<span class="number">10</span>)</span><br><span class="line">        print(i)</span><br><span class="line">        print(batch)</span><br><span class="line">        n+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> n&gt;<span class="number">3</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(args, data)</span>:</span></span><br><span class="line">    device = torch.device(<span class="string">f"cuda:<span class="subst">&#123;args.gpu&#125;</span>"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">    model = BiDAF(args, data.WORD.vocab.vectors).to(device) <span class="comment"># 定义主模型类实例</span></span><br><span class="line"></span><br><span class="line">    ema = EMA(args.exp_decay_rate) <span class="comment"># args.exp_decay_rate = 0.999</span></span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters(): </span><br><span class="line">        <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">            ema.register(name, param.data) <span class="comment"># 参数名字和对应的参数数据形成字典</span></span><br><span class="line">    parameters = filter(<span class="keyword">lambda</span> p: p.requires_grad, model.parameters())</span><br><span class="line">    <span class="comment"># p.requires_grad = True or False 保留有梯度的参数</span></span><br><span class="line">    optimizer = optim.Adadelta(parameters, lr=args.learning_rate)</span><br><span class="line">    <span class="comment"># args.learning_rate = 0.5,优化器选用Adadelta</span></span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    <span class="comment"># 交叉熵损失</span></span><br><span class="line"></span><br><span class="line">    writer = SummaryWriter(log_dir=<span class="string">'runs/'</span> + args.model_time)</span><br><span class="line">    <span class="comment"># args.model_time = strftime('%H:%M:%S', gmtime()) 文件夹命名为写入文件的当地时间</span></span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line">    loss, last_epoch = <span class="number">0</span>, <span class="number">-1</span></span><br><span class="line">    max_dev_exact, max_dev_f1 = <span class="number">-1</span>, <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    iterator = data.train_iter</span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(iterator):</span><br><span class="line">        present_epoch = int(iterator.epoch) </span><br><span class="line">        <span class="comment">#print("当前epoch",present_epoch)# 这个我打印了下，一直是0，觉得有问题</span></span><br><span class="line">        <span class="keyword">if</span> present_epoch == args.epoch:</span><br><span class="line">            <span class="comment"># args.epoch=12</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> present_epoch &gt; last_epoch:</span><br><span class="line">            print(<span class="string">'epoch:'</span>, present_epoch + <span class="number">1</span>)</span><br><span class="line">        last_epoch = present_epoch</span><br><span class="line"></span><br><span class="line">        p1, p2 = model(batch)</span><br><span class="line">        <span class="comment"># (batch, c_len), (batch, c_len)</span></span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        batch_loss = criterion(p1, batch.s_idx) + criterion(p2, batch.e_idx)</span><br><span class="line">        <span class="comment"># 最后的目标函数：batch.s_idx是答案开始的位置，batch.e_idx是答案结束的位置</span></span><br><span class="line">        loss += batch_loss.item()</span><br><span class="line">        batch_loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                ema.update(name, param.data) <span class="comment"># 更新训练完后的的参数数据</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % args.print_freq == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"i"</span>,i)</span><br><span class="line">            dev_loss, dev_exact, dev_f1 = test(model, ema, args, data)</span><br><span class="line">            c = (i + <span class="number">1</span>) // args.print_freq</span><br><span class="line"></span><br><span class="line">            writer.add_scalar(<span class="string">'loss/train'</span>, loss, c)</span><br><span class="line">            writer.add_scalar(<span class="string">'loss/dev'</span>, dev_loss, c)</span><br><span class="line">            writer.add_scalar(<span class="string">'exact_match/dev'</span>, dev_exact, c)</span><br><span class="line">            writer.add_scalar(<span class="string">'f1/dev'</span>, dev_f1, c)</span><br><span class="line">            print(<span class="string">f'train loss: <span class="subst">&#123;loss:<span class="number">.3</span>f&#125;</span> / dev loss: <span class="subst">&#123;dev_loss:<span class="number">.3</span>f&#125;</span>'</span></span><br><span class="line">                  <span class="string">f' / dev EM: <span class="subst">&#123;dev_exact:<span class="number">.3</span>f&#125;</span> / dev F1: <span class="subst">&#123;dev_f1:<span class="number">.3</span>f&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> dev_f1 &gt; max_dev_f1:</span><br><span class="line">                max_dev_f1 = dev_f1</span><br><span class="line">                max_dev_exact = dev_exact</span><br><span class="line">                best_model = copy.deepcopy(model)</span><br><span class="line"></span><br><span class="line">            loss = <span class="number">0</span></span><br><span class="line">            model.train()</span><br><span class="line"></span><br><span class="line">    writer.close()</span><br><span class="line"><span class="comment">#     print(f'max dev EM: &#123;max_dev_exact:.3f&#125; / max dev F1: &#123;max_dev_f1:.3f&#125;')</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> best_model</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(&apos;training start!&apos;)</span><br><span class="line">best_model = train(args, data)</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/pytorch/pytorch/issues/4144</span><br></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SQuAD-BiDAF/">SQuAD-BiDAF</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-NLP中的ConvNet" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/04/15/NLP中的ConvNet/" class="article-date">
      <time datetime="2020-04-15T00:21:14.000Z" itemprop="datePublished">2020-04-15</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/15/NLP中的ConvNet/">NLP中的ConvNet</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>​        NLP/AI是近几年来飞速发展的领域，很多的模型和算法只能在论文、讲义和博客中找到，而不会出现在任何的教科书中。凡是课程中提到的论文，大家都能够阅读一遍。对于重要的论文（我会特别标明或者在课上强调，例如BERT, transformer等），建议认真阅读，搞清楚模型的细节。其余的论文，建议至少能够阅读，了解论文的创新点和中心思想。</p>
<h3 id="如何读论文？"><a href="#如何读论文？" class="headerlink" title="如何读论文？"></a>如何读论文？</h3><p>对于如何读论文，每个人有自己不同的方法。我的建议是：</p>
<ul>
<li><p>最快读论文的方法：上各大中文网站（知乎，CSDN，微信公众号等）寻找该论文的中文解读，大部分有名的论文都会有很多的解读文章。</p>
</li>
<li><p>读论文时候的重点章节：大部分NLP的论文的主要两个章节是，Model, Experiments。基本上看完这两个章节就了解了论文的核心思想。另外我也会特别关注论文使用的<strong>数据</strong>，因为这些数据我们可能可以拿来用在自己的项目上。</p>
</li>
<li><p>如果想要更加深入地学习该论文的内容，可以上网去寻找与该论文相关的资料，包括作者的个人主页，他/她发布的论文slides，论文代码等等。顺便说一下，如果你想要复现论文的结果，但是在网上找不到代码，不要急于自己实现，可以写邮件给论文的第一作者与通讯作者（最后一位），礼貌地询问对方是否可以将源码和数据提供给你，理论上论文作者有义务公开自己的代码和数据。如果没有代码可以公开，要不然可能是论文太新，还没有公开代码，要不然可能是论文中某些部分的实现有困难，不那么容易复现。</p>
</li>
<li><p>另外如果你想要更深入地学习这个论文相关的领域，可以读一下Related Work中提到的一些文章。</p>
</li>
</ul>
<h1 id="NLP中的-ConvNet-精选论文"><a href="#NLP中的-ConvNet-精选论文" class="headerlink" title="NLP中的 ConvNet 精选论文"></a>NLP中的 ConvNet 精选论文</h1><p>MNIST</p>
<p>convolutional kernel: local feature detector</p>
<p>图像：</p>
<ul>
<li><p>平移不变性</p>
</li>
<li><p>pixel features</p>
</li>
</ul>
<p>Hinton</p>
<ul>
<li><p>Capsule Network</p>
</li>
<li><p>ConvNet的缺陷：</p>
</li>
<li><p>没有处理旋转不变性</p>
</li>
<li><p>图片大小发生改变</p>
</li>
</ul>
<p>文本</p>
<ul>
<li><p>ngram</p>
</li>
<li><p>ngram 之间的联系 n-n-gram</p>
</li>
</ul>
<p>曾经有一段时间由于<strong>Yann Lecun</strong>加入Facebook AI Research担任Director的关系，FB投入了很多的精力研发把ConvNet用在Text问题上。ConvNet主打的一个强项就是速度比RNN快，Encoder可以并行。后来可能是由于Google的Transformer开始统治这个领域，导致大家慢慢在ConvNet上的关注度越来越小。</p>
<p>transformer (BERT) 就是 filter size 为 1 的 convolutional neural network 。</p>
<p>不过这一系列以ConvNet为核心的NLP模型依然非常值得学习。ConvNet的一个长处在于它可以很自然地得到 <strong>ngram</strong> 的表示。由于NLP最近的进展日新月异，可能几天或者几个月之后又有一系列基于ConvNet的模型重登SOTA，谁知道呢。</p>
<p>对于不了解什么是Convolutional Neural Network的同学，建议阅读斯坦福cs231的课程资料 <a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="noopener">http://cs231n.github.io/convolutional-networks/</a> 网上的中文翻译很多，例如：<a href="https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit</a></p>
<h2 id="Yoon-Kim-Convolutional-Neural-Networks-for-Sentence-Classification"><a href="#Yoon-Kim-Convolutional-Neural-Networks-for-Sentence-Classification" class="headerlink" title="Yoon Kim Convolutional Neural Networks for Sentence Classification"></a>Yoon Kim <a href="https://aclweb.org/anthology/D14-1181" target="_blank" rel="noopener">Convolutional Neural Networks for Sentence Classification</a></h2><p><a href="https://aclweb.org/anthology/D14-1181" target="_blank" rel="noopener">https://aclweb.org/anthology/D14-1181</a></p>
<p>这篇文章首次提出了在text上使用convolutional network，并且取得了不错的效果。后续很多把ConvNet用在NLP任务上都是基于这篇论文的模型改进。</p>
<h3 id="模型架构图"><a href="#模型架构图" class="headerlink" title="模型架构图"></a>模型架构图</h3><p><img src="https://uploader.shimo.im/f/bAD5TU2kjCQipLid.png!thumbnail" alt="img"></p>
<h3 id="embedding层"><a href="#embedding层" class="headerlink" title="embedding层"></a>embedding层</h3><p><img src="https://uploader.shimo.im/f/pOmG3eS8ntYi0dSQ.png!thumbnail" alt="img"></p>
<h3 id="convolution层"><a href="#convolution层" class="headerlink" title="convolution层"></a>convolution层</h3><p><img src="https://uploader.shimo.im/f/MlEd8ePXgDs7gaLg.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/T3pionftmyImwzPH.png!thumbnail" alt="img"></p>
<h3 id="Max-over-time-pooling"><a href="#Max-over-time-pooling" class="headerlink" title="Max over time pooling"></a>Max over time pooling</h3><p><img src="https://uploader.shimo.im/f/cM7DZvGSt3gNz8uL.png!thumbnail" alt="img"></p>
<h3 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h3><p>一个affine transformation加上dropout</p>
<p><img src="https://uploader.shimo.im/f/5ZrPtIuh6X4P9lQJ.png!thumbnail" alt="img"></p>
<h3 id="模型的效果"><a href="#模型的效果" class="headerlink" title="模型的效果"></a>模型的效果</h3><p>可以媲美当时的众多传统模型。从今天的眼光来看这个模型的思路还是挺简单的，不过当时大家开始探索把CNN用到text问题上的时候，这一系列模型架构的想法还是很新颖的。</p>
<p><img src="https://uploader.shimo.im/f/BxnIovJf5Pwv8RHZ.png!thumbnail" alt="img"></p>
<h3 id="我们的代码实现"><a href="#我们的代码实现" class="headerlink" title="我们的代码实现"></a>我们的代码实现</h3><p>用ConvNet做文本分类的部分代码。有些部分可能的实现可能和模型有一定出入，不过我的模型实现效果也很不错，仅供参考。</p>
<p><a href="https://github.com/ZeweiChu/PyTorch-Course/blob/master/notebooks/4.sentiment_with_mask.ipynb" target="_blank" rel="noopener">https://github.com/ZeweiChu/PyTorch-Course/blob/master/notebooks/4.sentiment_with_mask.ipynb</a></p>
<p>感兴趣的同学可以参考更多Yoon Kim的工作</p>
<p><a href="http://www.people.fas.harvard.edu/~yoonkim/" target="_blank" rel="noopener">http://www.people.fas.harvard.edu/~yoonkim/</a></p>
<p>Yoon Kim的导师Alex Rush</p>
<p><a href="http://nlp.seas.harvard.edu/rush.html" target="_blank" rel="noopener">http://nlp.seas.harvard.edu/rush.html</a></p>
<p>他们的一项工作OpenNMT-py</p>
<p><a href="https://github.com/OpenNMT/OpenNMT-py" target="_blank" rel="noopener">https://github.com/OpenNMT/OpenNMT-py</a></p>
<p>Alex Rush的一些优秀学生</p>
<p>Sam Wiseman <a href="https://swiseman.github.io/" target="_blank" rel="noopener">https://swiseman.github.io/</a> 他做了很多VAE的工作</p>
<h2 id="Zhang-et-al-Character-level-Convolutional-Networks-for-Text-Classification"><a href="#Zhang-et-al-Character-level-Convolutional-Networks-for-Text-Classification" class="headerlink" title="Zhang et. al., Character-level Convolutional Networks for Text Classification "></a>Zhang et. al., <a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf" target="_blank" rel="noopener">Character-level Convolutional Networks for Text Classification </a></h2><p><a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf</a></p>
<p>这篇文章在char层面上使用ConvNet，当时在分类任务上取得了SOTA的效果。后来人们经常把这套方法用来做单词表示的学习，例如ELMo就是用CharCNN来encode单词的。</p>
<h3 id="关键Modules"><a href="#关键Modules" class="headerlink" title="关键Modules"></a>关键Modules</h3><p>Convolutional Module</p>
<p><img src="https://uploader.shimo.im/f/24t3sOep2m8g4ls6.png!thumbnail" alt="img"></p>
<p>k是kernel size。</p>
<p>max pooling</p>
<p><img src="https://uploader.shimo.im/f/NzpvIEElx3UVKJyo.png!thumbnail" alt="img"></p>
<h3 id="模型架构图-1"><a href="#模型架构图-1" class="headerlink" title="模型架构图"></a>模型架构图</h3><h2 id><a href="#" class="headerlink" title></a><img src="https://uploader.shimo.im/f/WDfJgndz6HMQ5CTF.png!thumbnail" alt="img"></h2><p>在ELMo上的character embedding</p>
<p><img src="https://uploader.shimo.im/f/3aLnMpCpyUUQSGcQ.png!thumbnail" alt="img"></p>
<h3 id="模型代码"><a href="#模型代码" class="headerlink" title="模型代码"></a>模型代码</h3><p><a href="https://github.com/srviest/char-cnn-text-classification-pytorch/blob/master/model.py" target="_blank" rel="noopener">https://github.com/srviest/char-cnn-text-classification-pytorch/blob/master/model.py</a></p>
<h2 id="Gehring-et-al-Convolutional-Sequence-to-Sequence-Learning"><a href="#Gehring-et-al-Convolutional-Sequence-to-Sequence-Learning" class="headerlink" title="Gehring et. al., Convolutional Sequence to Sequence Learning"></a>Gehring et. al., <a href="https://arxiv.org/pdf/1705.03122.pdf" target="_blank" rel="noopener">Convolutional Sequence to Sequence Learning</a></h2><p><a href="https://arxiv.org/pdf/1705.03122.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1705.03122.pdf</a></p>
<p>参考博客资料</p>
<p><a href="https://ycts.github.io/weeklypapers/convSeq2seq/" target="_blank" rel="noopener">https://ycts.github.io/weeklypapers/convSeq2seq/</a></p>
<p>用ConvNet做Seq2Seq模型，其实这篇文章中有很多Transformer的影子，并且模型效果也很好。可能由于同时期的Transformer光芒过于耀眼，掩盖了这一篇同样非常重量级的文章。</p>
<p>我的建议是，这篇文章可以简要阅读，了解ConvNet可以怎么样被运用到Text Modeling问题上。由于现在学术界和工业界的主流是各种Transformer模型的变种，且Transformer的模型相对更简洁易懂，所以建议同学们在后面花更多的时间在Transformer上。最近很多NLP的面试都会问到一些与Transformer和BERT相关的问题，可能很多人不太了解这篇Conv Seq2Seq的论文。</p>
<h3 id="Positional-Embedddings"><a href="#Positional-Embedddings" class="headerlink" title="Positional Embedddings"></a>Positional Embedddings</h3><p><img src="https://uploader.shimo.im/f/3s1XVoEyWCA2091O.png!thumbnail" alt="img"></p>
<p>对每个单词分别做word embedding w_i和positional embedding p_i，然后单词的embedding的w_i + p_i。p_i是模型的参数，在训练中会被更新。</p>
<p>如果没有positional embedding，CNN是无法知晓单词的位置信息的。因为不同于LSTM，如果没有postional embedding，在CNN encoder中的单词位置其实没有区别。</p>
<h3 id="Convolutional-Block-Structure"><a href="#Convolutional-Block-Structure" class="headerlink" title="Convolutional Block Structure"></a>Convolutional Block Structure</h3><p>Encoder和Decoder第l层的输入</p>
<p><img src="https://uploader.shimo.im/f/8RwAjCP390sIoG1A.png!thumbnail" alt="img"></p>
<p>每一层都包含一个一维Convolution，以及一个non-linearity单元，其中conv block/layer的kernel宽度为k，其output包含k个输入元素的信息。参数为</p>
<p><img src="https://uploader.shimo.im/f/8tCfiuW0gHgwBKAi.png!thumbnail" alt="img"></p>
<p>输出为</p>
<p><img src="https://uploader.shimo.im/f/kDZ1ulm65h8m0dOX.png!thumbnail" alt="img"></p>
<p>然后使用一个Gated Linear Units作为non-linearity。</p>
<p><img src="https://uploader.shimo.im/f/MuogKhR9b48cABpI.png!thumbnail" alt="img"></p>
<p>encoder和decoder都有好多层，每一层都加上了residual connection。</p>
<p><img src="https://uploader.shimo.im/f/EdLJ369IWcQgqx0x.png!thumbnail" alt="img"></p>
<p>我们在encoder每一层的左右两边都添加padding，这样可以保证每一层经过convolution之后输出的长度和原来一样。decoder和encoder稍有不同，因为我们必须保证我们在decoder一个位置的单词的时候没有看到这个位置后面的单词。所以我们的做法是，在decoder每一层左右两边都加上k-1个padding，做完conv之后把右边的k个单位移除。</p>
<p>最后的一个标准套路是把hidden state做个affine transformation，然后Softmax变成单词表上的一个概率分布。</p>
<p><img src="https://uploader.shimo.im/f/yFXfmhqvzzcU8D7D.png!thumbnail" alt="img"></p>
<h3 id="Multi-step-Attention"><a href="#Multi-step-Attention" class="headerlink" title="Multi-step Attention"></a>Multi-step Attention</h3><p>Decoder的每一层都有单独的Attention。</p>
<p><img src="https://uploader.shimo.im/f/uQLvQ1erpmAJfLlP.png!thumbnail" alt="img"></p>
<p>g_i是当前单词的embedding，</p>
<p><img src="https://uploader.shimo.im/f/eHzdNjYIZBM93TQN.png!thumbnail" alt="img"></p>
<p>然后我们用这个新造的 d_i^l 对 encoder 的每个位置做attention。</p>
<p><img src="https://uploader.shimo.im/f/er1A7CMeiukGczjw.png!thumbnail" alt="img"></p>
<p>然后非常常规的，用attention score对encoder hidden states做加权平均。唯一不同的是，这里还直接加上了输入的embedding。</p>
<p><img src="https://uploader.shimo.im/f/BJXPoQQi9Xg4fJJa.png!thumbnail" alt="img"></p>
<p>作者说他们发现直接加上这个词向量的embedding还是很有用的。</p>
<h3 id="模型架构图-2"><a href="#模型架构图-2" class="headerlink" title="模型架构图"></a>模型架构图</h3><p><img src="https://uploader.shimo.im/f/r5pAK5SQWzQDyDFL.png!thumbnail" alt="img"></p>
<h3 id="Normalization策略"><a href="#Normalization策略" class="headerlink" title="Normalization策略"></a>Normalization策略</h3><p>为了保持模型训练的稳定性，我们希望模型中间的向量的variance不要太大。</p>
<ul>
<li>输出+residual之后乘以\sqrt{5}，这样可以让这些vector每个维度的variance减半。其实很多时候这些确保模型稳定度的细节挺关键的，大家可能也知道transformer中也增加了一些减少variance的方法。如果不是调模型专家就会忽视这些细节，然后模型就训练不好了。</li>
</ul>
<p><img src="https://uploader.shimo.im/f/IMLzC3rtvxkqLmuo.png!thumbnail" alt="img"></p>
<p>还有更多的模型参数初始化细节，感兴趣的同学可以自己去认真阅读paper。</p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="https://uploader.shimo.im/f/aNGZg3EjGyw1aWu7.png!thumbnail" alt="img"></p>
<p>在翻译任务上超越了GNMT (Google Neural Machine Translation)，其实这个比较能说明问题，因为当时的GNMT是State of the Art。</p>
<p><img src="https://uploader.shimo.im/f/VMfU3If3biw5286r.png!thumbnail" alt="img"></p>
<p>然后他们还展示了ConvS2S的速度比GNMT更快。</p>
<p>总结来说，ConvS2S其实是一篇很有价值的文章，Decoder的设计比较精致， 不知道这篇文章对后来的Transformer产生了多少的影响，当然他们可以说是同时期的作品。</p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>主要代码在Fairseq的下面这个文件中</p>
<p><a href="https://github.com/ZeweiChu/fairseq/blob/master/fairseq/models/fconv.py" target="_blank" rel="noopener">https://github.com/ZeweiChu/fairseq/blob/master/fairseq/models/fconv.py</a></p>
<p>Fairseq是一个值得关注一波的工具包，由Facebook开发，主要开发者有 </p>
<ul>
<li>Myle Ott <a href="https://myleott.com/" target="_blank" rel="noopener">https://myleott.com/</a></li>
</ul>
<h1 id="关于文本分类的更多参考资料"><a href="#关于文本分类的更多参考资料" class="headerlink" title="关于文本分类的更多参考资料"></a>关于文本分类的更多参考资料</h1><p>基于深度学习的文本分类</p>
<p><a href="https://zhuanlan.zhihu.com/p/34212945" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34212945</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ConvNet/">ConvNet</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-seq2seq" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/04/13/seq2seq/" class="article-date">
      <time datetime="2020-04-12T23:04:08.000Z" itemprop="datePublished">2020-04-13</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/13/seq2seq/">seq2seq</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="Seq2Seq-Attention"><a href="#Seq2Seq-Attention" class="headerlink" title="Seq2Seq, Attention"></a>Seq2Seq, Attention</h1><p>在这份notebook当中，我们会(尽可能)复现Luong的attention模型</p>
<p>由于我们的数据集非常小，只有一万多个句子的训练数据，所以训练出来的模型效果并不好。如果大家想训练一个好一点的模型，可以参考下面的资料。</p>
<h2 id="更多阅读"><a href="#更多阅读" class="headerlink" title="更多阅读"></a>更多阅读</h2><h4 id="课件"><a href="#课件" class="headerlink" title="课件"></a>课件</h4><ul>
<li><a href="http://cs224d.stanford.edu/lectures/CS224d-Lecture15.pdf" target="_blank" rel="noopener">cs224d</a></li>
</ul>
<h4 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h4><ul>
<li><a href="https://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></li>
<li><a href="https://arxiv.org/abs/1508.04025?context=cs" target="_blank" rel="noopener">Effective Approaches to Attention-based Neural Machine Translation</a></li>
<li><a href="https://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
</ul>
<h4 id="PyTorch代码"><a href="#PyTorch代码" class="headerlink" title="PyTorch代码"></a>PyTorch代码</h4><ul>
<li><a href="https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb" target="_blank" rel="noopener">seq2seq-tutorial</a></li>
<li><a href="https://github.com/bentrevett/pytorch-seq2seq" target="_blank" rel="noopener">Tutorial from Ben Trevett</a></li>
<li><a href="https://github.com/IBM/pytorch-seq2seq" target="_blank" rel="noopener">IBM seq2seq</a></li>
<li><a href="https://github.com/OpenNMT/OpenNMT-py" target="_blank" rel="noopener">OpenNMT-py</a></li>
</ul>
<h4 id="更多关于Machine-Translation"><a href="#更多关于Machine-Translation" class="headerlink" title="更多关于Machine Translation"></a>更多关于Machine Translation</h4><ul>
<li><a href="https://www.coursera.org/lecture/nlp-sequence-models/beam-search-4EtHZ" target="_blank" rel="noopener">Beam Search</a></li>
<li>Pointer network 文本摘要</li>
<li>Copy Mechanism 文本摘要</li>
<li>Converage Loss </li>
<li>ConvSeq2Seq</li>
<li>Transformer</li>
<li>Tensor2Tensor</li>
</ul>
<h4 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h4><ul>
<li>建议同学尝试对中文进行分词</li>
</ul>
<h4 id="NER"><a href="#NER" class="headerlink" title="NER"></a>NER</h4><ul>
<li><a href="https://github.com/allenai/allennlp/tree/master/allennlp" target="_blank" rel="noopener">https://github.com/allenai/allennlp/tree/master/allennlp</a></li>
</ul>
<p>In [137]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter <span class="comment">#计数器</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> nltk</span><br></pre></td></tr></table></figure>

<p>读入中英文数据</p>
<ul>
<li>英文我们使用nltk的word tokenizer来分词，并且使用小写字母</li>
<li>中文我们直接使用单个汉字作为基本单元</li>
</ul>
<p>In [138]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(in_file)</span>:</span></span><br><span class="line">    cn = []</span><br><span class="line">    en = []</span><br><span class="line">    num_examples = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> open(in_file, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            <span class="comment">#print(line) #Anyone can do that.	任何人都可以做到。</span></span><br><span class="line">            line = line.strip().split(<span class="string">"\t"</span>) <span class="comment">#分词后用逗号隔开</span></span><br><span class="line">            <span class="comment">#print(line) #['Anyone can do that.', '任何人都可以做到。']</span></span><br><span class="line">            en.append([<span class="string">"BOS"</span>] + nltk.word_tokenize(line[<span class="number">0</span>].lower()) + [<span class="string">"EOS"</span>])</span><br><span class="line">            <span class="comment">#BOS:beginning of sequence EOS:end of</span></span><br><span class="line">            <span class="comment"># split chinese sentence into characters</span></span><br><span class="line">            cn.append([<span class="string">"BOS"</span>] + [c <span class="keyword">for</span> c <span class="keyword">in</span> line[<span class="number">1</span>]] + [<span class="string">"EOS"</span>])</span><br><span class="line">            <span class="comment">#中文一个一个字分词，可以尝试用分词器分词</span></span><br><span class="line">    <span class="keyword">return</span> en, cn</span><br><span class="line"></span><br><span class="line">train_file = <span class="string">"nmt/en-cn/train.txt"</span></span><br><span class="line">dev_file = <span class="string">"nmt/en-cn/dev.txt"</span></span><br><span class="line">train_en, train_cn = load_data(train_file)</span><br><span class="line">dev_en, dev_cn = load_data(dev_file)</span><br></pre></td></tr></table></figure>

<p>In [0]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(train_en[:10])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[&apos;BOS&apos;, &apos;anyone&apos;, &apos;can&apos;, &apos;do&apos;, &apos;that&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;how&apos;, &apos;about&apos;, &apos;another&apos;, &apos;piece&apos;, &apos;of&apos;, &apos;cake&apos;, &apos;?&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;she&apos;, &apos;married&apos;, &apos;him&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;i&apos;, &apos;do&apos;, &quot;n&apos;t&quot;, &apos;like&apos;, &apos;learning&apos;, &apos;irregular&apos;, &apos;verbs&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;it&apos;, &quot;&apos;s&quot;, &apos;a&apos;, &apos;whole&apos;, &apos;new&apos;, &apos;ball&apos;, &apos;game&apos;, &apos;for&apos;, &apos;me&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;he&apos;, &quot;&apos;s&quot;, &apos;sleeping&apos;, &apos;like&apos;, &apos;a&apos;, &apos;baby&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;he&apos;, &apos;can&apos;, &apos;play&apos;, &apos;both&apos;, &apos;tennis&apos;, &apos;and&apos;, &apos;baseball&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;we&apos;, &apos;should&apos;, &apos;cancel&apos;, &apos;the&apos;, &apos;hike&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;he&apos;, &apos;is&apos;, &apos;good&apos;, &apos;at&apos;, &apos;dealing&apos;, &apos;with&apos;, &apos;children&apos;, &apos;.&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;she&apos;, &apos;will&apos;, &apos;do&apos;, &apos;her&apos;, &apos;best&apos;, &apos;to&apos;, &apos;be&apos;, &apos;here&apos;, &apos;on&apos;, &apos;time&apos;, &apos;.&apos;, &apos;EOS&apos;]]</span><br></pre></td></tr></table></figure>

<p>In [0]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(train_cn[:10])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[&apos;BOS&apos;, &apos;任&apos;, &apos;何&apos;, &apos;人&apos;, &apos;都&apos;, &apos;可&apos;, &apos;以&apos;, &apos;做&apos;, &apos;到&apos;, &apos;。&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;要&apos;, &apos;不&apos;, &apos;要&apos;, &apos;再&apos;, &apos;來&apos;, &apos;一&apos;, &apos;塊&apos;, &apos;蛋&apos;, &apos;糕&apos;, &apos;？&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;她&apos;, &apos;嫁&apos;, &apos;给&apos;, &apos;了&apos;, &apos;他&apos;, &apos;。&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;我&apos;, &apos;不&apos;, &apos;喜&apos;, &apos;欢&apos;, &apos;学&apos;, &apos;习&apos;, &apos;不&apos;, &apos;规&apos;, &apos;则&apos;, &apos;动&apos;, &apos;词&apos;, &apos;。&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;這&apos;, &apos;對&apos;, &apos;我&apos;, &apos;來&apos;, &apos;說&apos;, &apos;是&apos;, &apos;個&apos;, &apos;全&apos;, &apos;新&apos;, &apos;的&apos;, &apos;球&apos;, &apos;類&apos;, &apos;遊&apos;, &apos;戲&apos;, &apos;。&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;他&apos;, &apos;正&apos;, &apos;睡&apos;, &apos;着&apos;, &apos;，&apos;, &apos;像&apos;, &apos;个&apos;, &apos;婴&apos;, &apos;儿&apos;, &apos;一&apos;, &apos;样&apos;, &apos;。&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;他&apos;, &apos;既&apos;, &apos;会&apos;, &apos;打&apos;, &apos;网&apos;, &apos;球&apos;, &apos;，&apos;, &apos;又&apos;, &apos;会&apos;, &apos;打&apos;, &apos;棒&apos;, &apos;球&apos;, &apos;。&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;我&apos;, &apos;們&apos;, &apos;應&apos;, &apos;該&apos;, &apos;取&apos;, &apos;消&apos;, &apos;這&apos;, &apos;次&apos;, &apos;遠&apos;, &apos;足&apos;, &apos;。&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;他&apos;, &apos;擅&apos;, &apos;長&apos;, &apos;應&apos;, &apos;付&apos;, &apos;小&apos;, &apos;孩&apos;, &apos;子&apos;, &apos;。&apos;, &apos;EOS&apos;], [&apos;BOS&apos;, &apos;她&apos;, &apos;会&apos;, &apos;尽&apos;, &apos;量&apos;, &apos;按&apos;, &apos;时&apos;, &apos;赶&apos;, &apos;来&apos;, &apos;的&apos;, &apos;。&apos;, &apos;EOS&apos;]]</span><br></pre></td></tr></table></figure>

<p>In [0]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>构建单词表</p>
<p>In [139]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">UNK_IDX = <span class="number">0</span></span><br><span class="line">PAD_IDX = <span class="number">1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dict</span><span class="params">(sentences, max_words=<span class="number">50000</span>)</span>:</span></span><br><span class="line">    word_count = Counter()</span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> sentence:</span><br><span class="line">            word_count[s] += <span class="number">1</span>  <span class="comment">#word_count这里应该是个字典</span></span><br><span class="line">    ls = word_count.most_common(max_words) </span><br><span class="line">    <span class="comment">#按每个单词数量排序前50000个,这个数字自己定的，不重复单词数没有50000</span></span><br><span class="line">    print(len(ls)) <span class="comment">#train_en：5491</span></span><br><span class="line">    total_words = len(ls) + <span class="number">2</span></span><br><span class="line">    <span class="comment">#加的2是留给"unk"和"pad"</span></span><br><span class="line">    <span class="comment">#ls = [('BOS', 14533), ('EOS', 14533), ('.', 12521), ('i', 4045), .......</span></span><br><span class="line">    word_dict = &#123;w[<span class="number">0</span>]: index+<span class="number">2</span> <span class="keyword">for</span> index, w <span class="keyword">in</span> enumerate(ls)&#125;</span><br><span class="line">    <span class="comment">#加的2是留给"unk"和"pad",转换成字典格式。</span></span><br><span class="line">    word_dict[<span class="string">"UNK"</span>] = UNK_IDX</span><br><span class="line">    word_dict[<span class="string">"PAD"</span>] = PAD_IDX</span><br><span class="line">    <span class="keyword">return</span> word_dict, total_words</span><br><span class="line"></span><br><span class="line">en_dict, en_total_words = build_dict(train_en)</span><br><span class="line">cn_dict, cn_total_words = build_dict(train_cn)</span><br><span class="line">inv_en_dict = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> en_dict.items()&#125;</span><br><span class="line"><span class="comment">#en_dict.items()把字典转换成可迭代对象，取出键值，并调换键值的位置。</span></span><br><span class="line">inv_cn_dict = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> cn_dict.items()&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">5491</span><br><span class="line">3193</span><br></pre></td></tr></table></figure>

<p>In [1]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># print(en_dict)</span><br><span class="line"># print(en_total_words)</span><br></pre></td></tr></table></figure>

<p>In [3]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(cn_dict)</span><br><span class="line">print(cn_total_words)</span><br></pre></td></tr></table></figure>

<p>In [4]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(inv_en_dict)</span><br></pre></td></tr></table></figure>

<p>In [5]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(inv_cn_dict)</span><br></pre></td></tr></table></figure>

<p>把单词全部转变成数字</p>
<p>In [140]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        Encode the sequences. </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    length = len(en_sentences)</span><br><span class="line">    <span class="comment">#en_sentences=[['BOS', 'anyone', 'can', 'do', 'that', '.', 'EOS'],....</span></span><br><span class="line">    </span><br><span class="line">    out_en_sentences = [[en_dict.get(w, <span class="number">0</span>) <span class="keyword">for</span> w <span class="keyword">in</span> sent] <span class="keyword">for</span> sent <span class="keyword">in</span> en_sentences]</span><br><span class="line">    <span class="comment">#out_en_sentences=[[2, 328, 43, 14, 28, 4, 3], ....</span></span><br><span class="line">    <span class="comment">#.get(w, 0)，返回w对应的值，没有就为0.因题库比较小，这里所有的单词向量都有非零索引。</span></span><br><span class="line">    </span><br><span class="line"> </span><br><span class="line">    out_cn_sentences = [[cn_dict.get(w, <span class="number">0</span>) <span class="keyword">for</span> w <span class="keyword">in</span> sent] <span class="keyword">for</span> sent <span class="keyword">in</span> cn_sentences]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sort sentences by english lengths</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">len_argsort</span><span class="params">(seq)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> sorted(range(len(seq)), key=<span class="keyword">lambda</span> x: len(seq[x]))</span><br><span class="line">      <span class="comment">#sorted()排序,key参数可以自定义规则，按seq[x]的长度排序，seq[0]为第一句话长度</span></span><br><span class="line">       </span><br><span class="line">    <span class="comment"># 把中文和英文按照同样的顺序排序</span></span><br><span class="line">    <span class="keyword">if</span> sort_by_len:</span><br><span class="line">        sorted_index = len_argsort(out_en_sentences)</span><br><span class="line">    <span class="comment">#print(sorted_index)</span></span><br><span class="line">    <span class="comment">#sorted_index=[63, 1544, 1917, 2650, 3998, 6240, 6294, 6703, ....</span></span><br><span class="line">     <span class="comment">#前面的索引都是最短句子的索引</span></span><br><span class="line">      </span><br><span class="line">        out_en_sentences = [out_en_sentences[i] <span class="keyword">for</span> i <span class="keyword">in</span> sorted_index]</span><br><span class="line">     <span class="comment">#print(out_en_sentences)</span></span><br><span class="line">     <span class="comment">#out_en_sentences=[[2, 475, 4, 3], [2, 1318, 126, 3], [2, 1707, 126, 3], ......</span></span><br><span class="line">     </span><br><span class="line">        out_cn_sentences = [out_cn_sentences[i] <span class="keyword">for</span> i <span class="keyword">in</span> sorted_index]</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> out_en_sentences, out_cn_sentences</span><br><span class="line"></span><br><span class="line">train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)</span><br><span class="line">dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)</span><br></pre></td></tr></table></figure>

<p>In [6]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">k=10000</span><br><span class="line">print(&quot; &quot;.join([inv_cn_dict[i] for i in train_cn[k]])) #通过inv字典获取单词</span><br><span class="line">print(&quot; &quot;.join([inv_en_dict[i] for i in train_en[k]]))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BOS 他 来 这 里 的 目 的 是 什 么 ？ EOS</span><br><span class="line">BOS for what purpose did he come here ? EOS</span><br></pre></td></tr></table></figure>

<p>把全部句子分成batch</p>
<p>In [0]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(np.arange(0, 100, 15))</span><br><span class="line">print(np.arange(0, 15))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ 0 15 30 45 60 75 90]</span><br><span class="line">[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]</span><br></pre></td></tr></table></figure>

<p>In [141]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_minibatches</span><span class="params">(n, minibatch_size, shuffle=True)</span>:</span></span><br><span class="line">    idx_list = np.arange(<span class="number">0</span>, n, minibatch_size) <span class="comment"># [0, 1, ..., n-1]</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        np.random.shuffle(idx_list) <span class="comment">#打乱数据</span></span><br><span class="line">    minibatches = []</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> idx_list:</span><br><span class="line">        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))</span><br><span class="line">        <span class="comment">#所有batch放在一个大列表里</span></span><br><span class="line">    <span class="keyword">return</span> minibatches</span><br></pre></td></tr></table></figure>

<p>In [10]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_minibatches(<span class="number">100</span>,<span class="number">15</span>) <span class="comment">#随机打乱的</span></span><br></pre></td></tr></table></figure>

<p>Out[10]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[array([75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]),</span><br><span class="line"> array([45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]),</span><br><span class="line"> array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]),</span><br><span class="line"> array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),</span><br><span class="line"> array([15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),</span><br><span class="line"> array([60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]),</span><br><span class="line"> array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99])]</span><br></pre></td></tr></table></figure>

<p>In [142]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_data</span><span class="params">(seqs)</span>:</span></span><br><span class="line"><span class="comment">#seqs=[[2, 12, 167, 23, 114, 5, 27, 1755, 4, 3], ........</span></span><br><span class="line">    lengths = [len(seq) <span class="keyword">for</span> seq <span class="keyword">in</span> seqs]<span class="comment">#每个batch里语句的长度统计出来</span></span><br><span class="line">    n_samples = len(seqs) <span class="comment">#一个batch有多少语句</span></span><br><span class="line">    max_len = np.max(lengths) <span class="comment">#取出最长的的语句长度，后面用这个做padding基准</span></span><br><span class="line">    x = np.zeros((n_samples, max_len)).astype(<span class="string">'int32'</span>)</span><br><span class="line">    <span class="comment">#先初始化全零矩阵，后面依次赋值</span></span><br><span class="line">    <span class="comment">#print(x.shape) #64*最大句子长度</span></span><br><span class="line">    </span><br><span class="line">    x_lengths = np.array(lengths).astype(<span class="string">"int32"</span>)</span><br><span class="line">    <span class="comment">#print(x_lengths) </span></span><br><span class="line"><span class="comment">#这里看下面的输入语句发现英文句子长度都一样，中文句子长短不一。</span></span><br><span class="line"><span class="comment">#说明英文句子是特征，中文句子是标签。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, seq <span class="keyword">in</span> enumerate(seqs):</span><br><span class="line">      <span class="comment">#取出一个batch的每条语句和对应的索引</span></span><br><span class="line">        x[idx, :lengths[idx]] = seq</span><br><span class="line">        <span class="comment">#每条语句按行赋值给x，x会有一些零值没有被赋值。</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> x, x_lengths <span class="comment">#x_mask</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_examples</span><span class="params">(en_sentences, cn_sentences, batch_size)</span>:</span></span><br><span class="line">    minibatches = get_minibatches(len(en_sentences), batch_size)</span><br><span class="line">    all_ex = []</span><br><span class="line">    <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line">        mb_en_sentences = [en_sentences[t] <span class="keyword">for</span> t <span class="keyword">in</span> minibatch]</span><br><span class="line"><span class="comment">#按打乱的batch序号分数据，打乱只是batch打乱，一个batach里面的语句还是顺序的。</span></span><br><span class="line">        <span class="comment">#print(mb_en_sentences)</span></span><br><span class="line">        </span><br><span class="line">        mb_cn_sentences = [cn_sentences[t] <span class="keyword">for</span> t <span class="keyword">in</span> minibatch]</span><br><span class="line">        mb_x, mb_x_len = prepare_data(mb_en_sentences)</span><br><span class="line">        <span class="comment">#返回的维度为：mb_x=(64 * 最大句子长度）,mb_x_len=最大句子长度</span></span><br><span class="line">        mb_y, mb_y_len = prepare_data(mb_cn_sentences)</span><br><span class="line">        </span><br><span class="line">        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))</span><br><span class="line">  <span class="comment">#这里把所有batch数据集合到一起。</span></span><br><span class="line">  <span class="comment">#依次为英文句子，英文长度，中文句子翻译，中文句子长度，这四个放在一个列表中</span></span><br><span class="line">  <span class="comment">#一个列表为一个batch的数据，所有batch组成一个大列表数据</span></span><br><span class="line">  </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> all_ex</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_data = gen_examples(train_en, train_cn, batch_size)</span><br><span class="line">random.shuffle(train_data)</span><br><span class="line">dev_data = gen_examples(dev_en, dev_cn, batch_size)</span><br></pre></td></tr></table></figure>

<p>In [28]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[0]</span><br></pre></td></tr></table></figure>

<p>Out[28]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">(array([[   2,   12,  707,   23,    7,  295,    4,    3],</span><br><span class="line">        [   2,   12,  120, 1207,  517,  604,    4,    3],</span><br><span class="line">        [   2,    8,   90,  433,   64, 1470,  126,    3],</span><br><span class="line">        [   2,   12,  144,   46,    9,   94,    4,    3],</span><br><span class="line">        [   2,   25,   10,    9,  535,  639,    4,    3],</span><br><span class="line">        [   2,   25,   10,   64,  377, 2512,    4,    3],</span><br><span class="line">        [   2,   12,   43,  309,    9,   96,    4,    3],</span><br><span class="line">        [   2,   43,  328, 1475,   25,  469,   11,    3],</span><br><span class="line">        [   2,   82, 1043,   34, 1991, 2514,    4,    3],</span><br><span class="line">        [   2,    5,   54,    7,  181, 1694,    4,    3],</span><br><span class="line">        [   2,   30,   51,  472,    6,  294,   11,    3],</span><br><span class="line">        [   2,    5,  241,   16,   65,  551,    4,    3],</span><br><span class="line">        [   2,   14,    8,   36, 2516,  680,   11,    3],</span><br><span class="line">        [   2,    8,   30,    9,   66,  333,    4,    3],</span><br><span class="line">        [   2,   12,   10,   34,   40,  777,    4,    3],</span><br><span class="line">        [   2,   29,   54,    9,  138, 1633,    4,    3],</span><br><span class="line">        [   2,   43,    8,  309,    9,   96,   11,    3],</span><br><span class="line">        [   2,   47,   12,   39,   59,  190,   11,    3],</span><br><span class="line">        [   2,   29,   85,   14,  150,  221,    4,    3],</span><br><span class="line">        [   2,   12,   70,   37,   36,  242,    4,    3],</span><br><span class="line">        [   2,    5,  239,   64, 2521, 1696,    4,    3],</span><br><span class="line">        [   2,    5,   14,   13,   36,  314,    4,    3],</span><br><span class="line">        [   2,    5,  234,    7,   45,   44,    4,    3],</span><br><span class="line">        [   2,    5,   76,  226,   17,  621,    4,    3],</span><br><span class="line">        [   2,   29,  180,    9,  269,  266,    4,    3],</span><br><span class="line">        [   2,   85,    5,   22,    6,  708,   11,    3],</span><br><span class="line">        [   2,    6,  788,   48,   37,  889,    4,    3],</span><br><span class="line">        [   2,    8,   63,  124,   45,   95,    4,    3],</span><br><span class="line">        [   2,  921,   10,   21,  640,  350,    4,    3],</span><br><span class="line">        [   2,   52,   10,    6,  296,   44,   11,    3],</span><br><span class="line">        [   2,  681,   10,  190,   24,  146,   11,    3],</span><br><span class="line">        [   2,   19, 1480,  838,    7,  596,    4,    3],</span><br><span class="line">        [   2,   29,   90,  472, 2036,  132,    4,    3],</span><br><span class="line">        [   2,    8,   90,    9,   66,  645,    4,    3],</span><br><span class="line">        [   2,    5,  192,  257,    7,  684,    4,    3],</span><br><span class="line">        [   2,    5,   68,   36,  384, 1686,    4,    3],</span><br><span class="line">        [   2,   12,   10,  120,   38,   23,    4,    3],</span><br><span class="line">        [   2,   18,   47,  965,  106,  112,    4,    3],</span><br><span class="line">        [   2,    8,   30,   37,    9,  250,    4,    3],</span><br><span class="line">        [   2,   31,   20,  129,   20,  900,   11,    3],</span><br><span class="line">        [   2,   29,  519,  118, 2044, 1313,    4,    3],</span><br><span class="line">        [   2,   29,   22,    6,  294,  229,    4,    3],</span><br><span class="line">        [   2,   25,  189, 1056,  335,  151,    4,    3],</span><br><span class="line">        [   2,    8,   67,   89,   57,  887,    4,    3],</span><br><span class="line">        [   2,   41,    8,   72,   59,  362,   11,    3],</span><br><span class="line">        [   2,   51,  923, 2534,   26,  364,    4,    3],</span><br><span class="line">        [   2,   22,    8, 1209,  914,  834,   11,    3],</span><br><span class="line">        [   2,   19,   48,    9, 1127,  847,    4,    3],</span><br><span class="line">        [   2,   25,  224,   70,   13,  425,    4,    3],</span><br><span class="line">        [   2,   19,  949,   62, 1112,  657,    4,    3],</span><br><span class="line">        [   2,   87,   10,    6,  751,  443,   11,    3],</span><br><span class="line">        [   2,   19,  144,   99,    9,  539,    4,    3],</span><br><span class="line">        [   2,   19,  599,  242,  117,  103,    4,    3],</span><br><span class="line">        [   2,   14,    8,   22,    9,  386,   11,    3],</span><br><span class="line">        [   2,   16,   20,   60,    7,   45,    4,    3],</span><br><span class="line">        [   2,   25,  145,  133,   10, 1974,    4,    3],</span><br><span class="line">        [   2,   25,   10,  426,   17,  343,    4,    3],</span><br><span class="line">        [   2,    5,   22,  239,    6,  461,    4,    3],</span><br><span class="line">        [   2,   14,   13,    8,  162,  242,   11,    3],</span><br><span class="line">        [   2,    8,   67,   13,  159,   59,    4,    3],</span><br><span class="line">        [   2,  140, 3452, 1220,   33,  601,    4,    3],</span><br><span class="line">        [   2,    5,   79, 1937,   35,  232,    4,    3],</span><br><span class="line">        [   2,   18, 1612,   35,  779,  926,    4,    3],</span><br><span class="line">        [   2,   12,  197,  599,    6,  632,    4,    3]], dtype=int32),</span><br><span class="line"> array([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,</span><br><span class="line">        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,</span><br><span class="line">        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8],</span><br><span class="line">       dtype=int32),</span><br><span class="line"> array([[  2,   9, 793, ...,   0,   0,   0],</span><br><span class="line">        [  2,   9, 504, ...,   0,   0,   0],</span><br><span class="line">        [  2,   8, 114, ...,   0,   0,   0],</span><br><span class="line">        ...,</span><br><span class="line">        [  2,   5, 154, ...,   0,   0,   0],</span><br><span class="line">        [  2, 214, 171, ..., 838,   4,   3],</span><br><span class="line">        [  2,   9,  74, ...,   0,   0,   0]], dtype=int32),</span><br><span class="line"> array([10, 12,  9, 10,  8, 10,  7, 13, 17,  8, 11, 10, 11,  9,  9, 12,  8,</span><br><span class="line">        12, 10,  9, 14,  9,  9,  6,  9, 10,  9, 10, 13, 11, 14, 13, 14,  8,</span><br><span class="line">         8, 10, 10,  9,  8,  7, 14, 12, 13, 13, 13, 12, 13,  8, 11, 11, 10,</span><br><span class="line">        12, 10,  9,  6, 10,  8, 11,  9, 11, 10, 12, 21,  9], dtype=int32))</span><br></pre></td></tr></table></figure>

<h3 id="没有Attention的版本"><a href="#没有Attention的版本" class="headerlink" title="没有Attention的版本"></a>没有Attention的版本</h3><p>下面是一个更简单的没有Attention的encoder decoder模型</p>
<p>In [143]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PlainEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, hidden_size, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        <span class="comment">#以英文为例，vocab_size=5493, hidden_size=100, dropout=0.2</span></span><br><span class="line">        super(PlainEncoder, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, hidden_size)</span><br><span class="line">        <span class="comment">#这里的hidden_size为embedding_dim：一个单词的维度 </span></span><br><span class="line">        <span class="comment">#torch.nn.Embedding(num_embeddings, embedding_dim, .....)</span></span><br><span class="line">        <span class="comment">#这里的hidden_size = 100</span></span><br><span class="line">        </span><br><span class="line">        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=<span class="literal">True</span>)      </span><br><span class="line">        <span class="comment">#第一个参数为input_size ：输入特征数量</span></span><br><span class="line">        <span class="comment">#第二个参数为hidden_size ：隐藏层特征数量</span></span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, lengths)</span>:</span> </span><br><span class="line">        <span class="comment">#x是输入的batch的所有单词，lengths：batch里每个句子的长度</span></span><br><span class="line">        <span class="comment">#因为需要把最后一个hidden state取出来，需要知道长度，因为句子长度不一样</span></span><br><span class="line">        <span class="comment">##print(x.shape,lengths),x.sahpe = torch.Size([64, 10])</span></span><br><span class="line">        <span class="comment"># lengths= =tensor([10, 10, 10, ..... 10, 10, 10])</span></span><br><span class="line">        </span><br><span class="line">        sorted_len, sorted_idx = lengths.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">#按照长度排序，descending=True长的在前。</span></span><br><span class="line">        <span class="comment">#返回两个参数，句子长度和未排序前的索引</span></span><br><span class="line">        <span class="comment"># sorted_idx=tensor([41, 40, 46, 45,...... 19, 18, 63])</span></span><br><span class="line">        <span class="comment"># sorted_len=tensor([10, 10, 10, ..... 10, 10, 10])</span></span><br><span class="line">        </span><br><span class="line">        x_sorted = x[sorted_idx.long()] <span class="comment">#句子用新的idx，按长度排好序了</span></span><br><span class="line">        </span><br><span class="line">        embedded = self.dropout(self.embed(x_sorted))</span><br><span class="line">        <span class="comment">#print(embedded.shape)=torch.Size([64, 10, 100])</span></span><br><span class="line">        <span class="comment">#tensor([[[-0.6312, -0.9863, -0.3123,  ..., -0.7384,  0.9230, -0.4311],....</span></span><br><span class="line"></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">#这个函数就是用来处理不同长度的句子的，https: // www.cnblogs.com / sbj123456789 / p / 9834018. html</span></span><br><span class="line"></span><br><span class="line">        packed_out, hid = self.rnn(packed_embedded)</span><br><span class="line">        <span class="comment">#hid.shape = torch.Size([1, 64, 100])</span></span><br><span class="line">        </span><br><span class="line">        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">#out.shape = torch.Size([64, 10, 100]),</span></span><br><span class="line"></span><br><span class="line">        _, original_idx = sorted_idx.sort(<span class="number">0</span>, descending=<span class="literal">False</span>)</span><br><span class="line">        out = out[original_idx.long()].contiguous()</span><br><span class="line">        hid = hid[:, original_idx.long()].contiguous()</span><br><span class="line">        <span class="comment">#out.shape = torch.Size([64, 10, 100])</span></span><br><span class="line">        <span class="comment">#hid.shape = torch.Size([1, 64, 100])</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> out, hid[[<span class="number">-1</span>]] <span class="comment">#有时候num_layers层数多，需要取出最后一层</span></span><br></pre></td></tr></table></figure>

<p>In [124]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PlainDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, hidden_size, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        super(PlainDecoder, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, hidden_size)</span><br><span class="line">        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.out = nn.Linear(hidden_size, vocab_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, y, y_lengths, hid)</span>:</span></span><br><span class="line">        <span class="comment">#print(y.shape)=torch.Size([64, 12])</span></span><br><span class="line">        <span class="comment">#print(hid.shape)=torch.Size([1, 64, 100])</span></span><br><span class="line">        <span class="comment">#中文的y和y_lengths</span></span><br><span class="line">        sorted_len, sorted_idx = y_lengths.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">        y_sorted = y[sorted_idx.long()]</span><br><span class="line">        hid = hid[:, sorted_idx.long()] <span class="comment">#隐藏层也要排序</span></span><br><span class="line"></span><br><span class="line">        y_sorted = self.dropout(self.embed(y_sorted)) </span><br><span class="line">        <span class="comment"># batch_size, output_length, embed_size</span></span><br><span class="line"></span><br><span class="line">        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=<span class="literal">True</span>)</span><br><span class="line">        out, hid = self.rnn(packed_seq, hid) <span class="comment">#加上隐藏层</span></span><br><span class="line">        <span class="comment">#print(hid.shape)=torch.Size([1, 64, 100])</span></span><br><span class="line">        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=<span class="literal">True</span>)</span><br><span class="line">        _, original_idx = sorted_idx.sort(<span class="number">0</span>, descending=<span class="literal">False</span>)</span><br><span class="line">        output_seq = unpacked[original_idx.long()].contiguous()</span><br><span class="line">        <span class="comment">#print(output_seq.shape)=torch.Size([64, 12, 100])</span></span><br><span class="line">        hid = hid[:, original_idx.long()].contiguous()</span><br><span class="line">        <span class="comment">#print(hid.shape)=torch.Size([1, 64, 100])</span></span><br><span class="line">        output = F.log_softmax(self.out(output_seq), <span class="number">-1</span>)</span><br><span class="line">        <span class="comment">#print(output.shape)=torch.Size([64, 12, 3195])</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, hid</span><br></pre></td></tr></table></figure>

<p>In [144]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PlainSeq2Seq</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder)</span>:</span></span><br><span class="line">        <span class="comment">#encoder是上面PlainEncoder的实例</span></span><br><span class="line">        <span class="comment">#decoder是上面PlainDecoder的实例</span></span><br><span class="line">        super(PlainSeq2Seq, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">       </span><br><span class="line">    <span class="comment">#把两个模型串起来 </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, x_lengths, y, y_lengths)</span>:</span></span><br><span class="line">        encoder_out, hid = self.encoder(x, x_lengths)</span><br><span class="line">        <span class="comment">#self.encoder(x, x_lengths)调用PlainEncoder里面forward的方法</span></span><br><span class="line">        <span class="comment">#返回forward的out和hid</span></span><br><span class="line">        </span><br><span class="line">        output, hid = self.decoder(y=y,y_lengths=y_lengths,hid=hid)</span><br><span class="line">        <span class="comment">#self.dencoder()调用PlainDecoder里面forward的方法</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">translate</span><span class="params">(self, x, x_lengths, y, max_length=<span class="number">10</span>)</span>:</span></span><br><span class="line">        <span class="comment">#x是一个句子，用数值表示</span></span><br><span class="line">        <span class="comment">#y是句子的长度</span></span><br><span class="line">        <span class="comment">#y是“bos”的数值索引=2</span></span><br><span class="line">        </span><br><span class="line">        encoder_out, hid = self.encoder(x, x_lengths)</span><br><span class="line">        preds = []</span><br><span class="line">        batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">        attns = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(max_length):</span><br><span class="line">            output, hid = self.decoder(y=y,</span><br><span class="line">                    y_lengths=torch.ones(batch_size).long().to(y.device),</span><br><span class="line">                    hid=hid) </span><br><span class="line">            </span><br><span class="line"><span class="comment">#刚开始循环bos作为模型的首个输入单词，后续更新y，下个预测单词的输入是上个输出单词</span></span><br><span class="line">            y = output.max(<span class="number">2</span>)[<span class="number">1</span>].view(batch_size, <span class="number">1</span>)</span><br><span class="line">            preds.append(y)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> torch.cat(preds, <span class="number">1</span>), <span class="literal">None</span></span><br></pre></td></tr></table></figure>

<p>In [145]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">hidden_size = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#传入中文和英文参数</span></span><br><span class="line">encoder = PlainEncoder(vocab_size=en_total_words,</span><br><span class="line">                      hidden_size=hidden_size,</span><br><span class="line">                      dropout=dropout)</span><br><span class="line">decoder = PlainDecoder(vocab_size=cn_total_words,</span><br><span class="line">                      hidden_size=hidden_size,</span><br><span class="line">                      dropout=dropout)</span><br><span class="line">model = PlainSeq2Seq(encoder, decoder)</span><br></pre></td></tr></table></figure>

<p>In [146]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># masked cross entropy loss</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LanguageModelCriterion</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LanguageModelCriterion, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, target, mask)</span>:</span></span><br><span class="line">        <span class="comment">#target=tensor([[5,108,8,4,3,0,0,0,0,0,0,0],....</span></span><br><span class="line">        <span class="comment">#  mask=tensor([[1,1 ,1,1,1,0,0,0,0,0,0,0],.....</span></span><br><span class="line">        <span class="comment">#print(input.shape,target.shape,mask.shape)</span></span><br><span class="line">        <span class="comment">#torch.Size([64, 12, 3195]) torch.Size([64, 12]) torch.Size([64, 12])</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># input: (batch_size * seq_len) * vocab_size</span></span><br><span class="line">        input = input.contiguous().view(<span class="number">-1</span>, input.size(<span class="number">2</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># target: batch_size * 1=768*1</span></span><br><span class="line">        target = target.contiguous().view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        mask = mask.contiguous().view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment">#print(-input.gather(1, target))</span></span><br><span class="line">        output = -input.gather(<span class="number">1</span>, target) * mask</span><br><span class="line"><span class="comment">#这里算得就是交叉熵损失，前面已经算了F.log_softmax</span></span><br><span class="line"><span class="comment">#.gather的作用https://blog.csdn.net/edogawachia/article/details/80515038</span></span><br><span class="line"><span class="comment">#output.shape=torch.Size([768, 1])</span></span><br><span class="line"><span class="comment">#mask作用是把padding为0的地方重置为零，因为input.gather时，为0的地方不是零了</span></span><br><span class="line">        </span><br><span class="line">        output = torch.sum(output) / torch.sum(mask)</span><br><span class="line">        <span class="comment">#均值损失</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>

<p>In [147]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = model.to(device)</span><br><span class="line">loss_fn = LanguageModelCriterion().to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters())</span><br></pre></td></tr></table></figure>

<p>pythonIn [151]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, data, num_epochs=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        total_num_words = total_loss = <span class="number">0.</span></span><br><span class="line">        <span class="keyword">for</span> it, (mb_x, mb_x_len, mb_y, mb_y_len) <span class="keyword">in</span> enumerate(data):</span><br><span class="line">            <span class="comment">#（英文batch，英文长度，中文batch，中文长度）</span></span><br><span class="line">            </span><br><span class="line">            mb_x = torch.from_numpy(mb_x).to(device).long()</span><br><span class="line">            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#前n-1个单词作为输入，后n-1个单词作为输出，因为输入的前一个单词要预测后一个单词</span></span><br><span class="line">            mb_input = torch.from_numpy(mb_y[:, :<span class="number">-1</span>]).to(device).long()</span><br><span class="line">            mb_output = torch.from_numpy(mb_y[:, <span class="number">1</span>:]).to(device).long()</span><br><span class="line">            <span class="comment">#</span></span><br><span class="line">            mb_y_len = torch.from_numpy(mb_y_len<span class="number">-1</span>).to(device).long()</span><br><span class="line">            <span class="comment">#输入输出的长度都减一。</span></span><br><span class="line">            </span><br><span class="line">            mb_y_len[mb_y_len&lt;=<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)</span><br><span class="line">            <span class="comment">#返回的是类PlainSeq2Seq里forward函数的两个返回值</span></span><br><span class="line">            </span><br><span class="line">            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[<span class="literal">None</span>, :] &lt; mb_y_len[:, <span class="literal">None</span>]</span><br><span class="line"><span class="comment">#mb_out_mask=tensor([[1, 1, 1,  ..., 0, 0, 0],[1, 1, 1,  ..., 0, 0, 0],</span></span><br><span class="line"><span class="comment">#mb_out_mask.shape= (64*19),这句代码咱不懂，这个mask就是padding的位置设置为0，其他设置为1</span></span><br><span class="line"><span class="comment">#mb_out_mask就是LanguageModelCriterion的传入参数mask。</span></span><br><span class="line"></span><br><span class="line">            mb_out_mask = mb_out_mask.float()</span><br><span class="line">            </span><br><span class="line">            loss = loss_fn(mb_pred, mb_output, mb_out_mask)</span><br><span class="line">            </span><br><span class="line">            num_words = torch.sum(mb_y_len).item()</span><br><span class="line">            <span class="comment">#一个batch里多少个单词</span></span><br><span class="line">            </span><br><span class="line">            total_loss += loss.item() * num_words</span><br><span class="line">            <span class="comment">#总损失，loss计算的是均值损失，每个单词都是都有损失，所以乘以单词数</span></span><br><span class="line">            </span><br><span class="line">            total_num_words += num_words</span><br><span class="line">            <span class="comment">#总单词数</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 更新模型</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">5.</span>)</span><br><span class="line">            <span class="comment">#为了防止梯度过大，设置梯度的阈值</span></span><br><span class="line">            </span><br><span class="line">            optimizer.step()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> it % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"Epoch"</span>, epoch, <span class="string">"iteration"</span>, it, <span class="string">"loss"</span>, loss.item())</span><br><span class="line"></span><br><span class="line">                </span><br><span class="line">        print(<span class="string">"Epoch"</span>, epoch, <span class="string">"Training loss"</span>, total_loss/total_num_words)</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            evaluate(model, dev_data) <span class="comment">#评估模型</span></span><br><span class="line">train(model, train_data, num_epochs=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0 iteration 0 loss 4.277793884277344</span><br><span class="line">Epoch 0 iteration 100 loss 3.5520756244659424</span><br><span class="line">Epoch 0 iteration 200 loss 3.483494997024536</span><br><span class="line">Epoch 0 Training loss 3.6435126089915557</span><br><span class="line">Evaluation loss 3.698509503997669</span><br><span class="line">Epoch 1 iteration 0 loss 4.158623218536377</span><br><span class="line">Epoch 1 iteration 100 loss 3.412541389465332</span><br><span class="line">Epoch 1 iteration 200 loss 3.3976175785064697</span><br><span class="line">Epoch 1 Training loss 3.5087569079050698</span><br></pre></td></tr></table></figure>

<p>In [135]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, data)</span>:</span></span><br><span class="line">    model.eval()</span><br><span class="line">    total_num_words = total_loss = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():<span class="comment">#不需要更新模型，不需要梯度</span></span><br><span class="line">        <span class="keyword">for</span> it, (mb_x, mb_x_len, mb_y, mb_y_len) <span class="keyword">in</span> enumerate(data):</span><br><span class="line">            mb_x = torch.from_numpy(mb_x).to(device).long()</span><br><span class="line">            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()</span><br><span class="line">            mb_input = torch.from_numpy(mb_y[:, :<span class="number">-1</span>]).to(device).long()</span><br><span class="line">            mb_output = torch.from_numpy(mb_y[:, <span class="number">1</span>:]).to(device).long()</span><br><span class="line">            mb_y_len = torch.from_numpy(mb_y_len<span class="number">-1</span>).to(device).long()</span><br><span class="line">            mb_y_len[mb_y_len&lt;=<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)</span><br><span class="line"></span><br><span class="line">            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[<span class="literal">None</span>, :] &lt; mb_y_len[:, <span class="literal">None</span>]</span><br><span class="line">            mb_out_mask = mb_out_mask.float()</span><br><span class="line"></span><br><span class="line">            loss = loss_fn(mb_pred, mb_output, mb_out_mask)</span><br><span class="line"></span><br><span class="line">            num_words = torch.sum(mb_y_len).item()</span><br><span class="line">            total_loss += loss.item() * num_words</span><br><span class="line">            total_num_words += num_words</span><br><span class="line">    print(<span class="string">"Evaluation loss"</span>, total_loss/total_num_words)</span><br></pre></td></tr></table></figure>

<p>In [ ]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#翻译个句子看看结果咋样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">translate_dev</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="comment">#随便取出句子</span></span><br><span class="line">    en_sent = <span class="string">" "</span>.join([inv_en_dict[w] <span class="keyword">for</span> w <span class="keyword">in</span> dev_en[i]])</span><br><span class="line">    print(en_sent)</span><br><span class="line">    cn_sent = <span class="string">" "</span>.join([inv_cn_dict[w] <span class="keyword">for</span> w <span class="keyword">in</span> dev_cn[i]])</span><br><span class="line">    print(<span class="string">""</span>.join(cn_sent))</span><br><span class="line"></span><br><span class="line">    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(<span class="number">1</span>, <span class="number">-1</span>)).long().to(device)</span><br><span class="line">    <span class="comment">#把句子升维，并转换成tensor</span></span><br><span class="line">    </span><br><span class="line">    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)</span><br><span class="line">    <span class="comment">#取出句子长度，并转换成tensor</span></span><br><span class="line">    </span><br><span class="line">    bos = torch.Tensor([[cn_dict[<span class="string">"BOS"</span>]]]).long().to(device)</span><br><span class="line">    <span class="comment">#bos=tensor([[2]])</span></span><br><span class="line"></span><br><span class="line">    translation, attn = model.translate(mb_x, mb_x_len, bos)</span><br><span class="line">    <span class="comment">#这里传入bos作为首个单词的输入</span></span><br><span class="line">    <span class="comment">#translation=tensor([[ 8,  6, 11, 25, 22, 57, 10,  5,  6,  4]])</span></span><br><span class="line">    </span><br><span class="line">    translation = [inv_cn_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> translation.data.cpu().numpy().reshape(<span class="number">-1</span>)]</span><br><span class="line">    trans = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> translation:</span><br><span class="line">        <span class="keyword">if</span> word != <span class="string">"EOS"</span>: <span class="comment"># 把数值变成单词形式</span></span><br><span class="line">            trans.append(word) <span class="comment">#</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    print(<span class="string">""</span>.join(trans))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>,<span class="number">120</span>):</span><br><span class="line">    translate_dev(i)</span><br><span class="line">    print()</span><br></pre></td></tr></table></figure>

<p>数据全部处理完成，现在我们开始构建seq2seq模型</p>
<h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><ul>
<li>Encoder模型的任务是把输入文字传入embedding层和GRU层，转换成一些hidden states作为后续的context vectors</li>
</ul>
<h2 id="下面的注释我先把原理捋清楚吧"><a href="#下面的注释我先把原理捋清楚吧" class="headerlink" title="下面的注释我先把原理捋清楚吧"></a>下面的注释我先把原理捋清楚吧</h2><p>In [0]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        </span><br><span class="line">        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=<span class="literal">True</span>, bidirectional=<span class="literal">True</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.fc = nn.Linear(enc_hidden_size * <span class="number">2</span>, dec_hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, lengths)</span>:</span></span><br><span class="line">        sorted_len, sorted_idx = lengths.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">        x_sorted = x[sorted_idx.long()]</span><br><span class="line">        embedded = self.dropout(self.embed(x_sorted))</span><br><span class="line">        </span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=<span class="literal">True</span>)</span><br><span class="line">        packed_out, hid = self.rnn(packed_embedded)</span><br><span class="line">        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=<span class="literal">True</span>)</span><br><span class="line">        _, original_idx = sorted_idx.sort(<span class="number">0</span>, descending=<span class="literal">False</span>)</span><br><span class="line">        out = out[original_idx.long()].contiguous()</span><br><span class="line">        hid = hid[:, original_idx.long()].contiguous()</span><br><span class="line">        </span><br><span class="line">        hid = torch.cat([hid[<span class="number">-2</span>], hid[<span class="number">-1</span>]], dim=<span class="number">1</span>)</span><br><span class="line">        hid = torch.tanh(self.fc(hid)).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out, hid</span><br></pre></td></tr></table></figure>

<h4 id="Luong-Attention"><a href="#Luong-Attention" class="headerlink" title="Luong Attention"></a>Luong Attention</h4><ul>
<li>根据context vectors和当前的输出hidden states，计算输出</li>
</ul>
<p>In [0]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, enc_hidden_size, dec_hidden_size)</span>:</span></span><br><span class="line">        super(Attention, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.enc_hidden_size = enc_hidden_size</span><br><span class="line">        self.dec_hidden_size = dec_hidden_size</span><br><span class="line"></span><br><span class="line">        self.linear_in = nn.Linear(enc_hidden_size*<span class="number">2</span>, dec_hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.linear_out = nn.Linear(enc_hidden_size*<span class="number">2</span> + dec_hidden_size, dec_hidden_size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, output, context, mask)</span>:</span></span><br><span class="line">        <span class="comment"># output: batch_size, output_len, dec_hidden_size</span></span><br><span class="line">        <span class="comment"># context: batch_size, context_len, 2*enc_hidden_size</span></span><br><span class="line">    </span><br><span class="line">        batch_size = output.size(<span class="number">0</span>)</span><br><span class="line">        output_len = output.size(<span class="number">1</span>)</span><br><span class="line">        input_len = context.size(<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        context_in = self.linear_in(context.view(batch_size*input_len, <span class="number">-1</span>)).view(                </span><br><span class="line">            batch_size, input_len, <span class="number">-1</span>) <span class="comment"># batch_size, context_len, dec_hidden_size</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># context_in.transpose(1,2): batch_size, dec_hidden_size, context_len </span></span><br><span class="line">        <span class="comment"># output: batch_size, output_len, dec_hidden_size</span></span><br><span class="line">        attn = torch.bmm(output, context_in.transpose(<span class="number">1</span>,<span class="number">2</span>)) </span><br><span class="line">        <span class="comment"># batch_size, output_len, context_len</span></span><br><span class="line"></span><br><span class="line">        attn.data.masked_fill(mask, <span class="number">-1e6</span>)</span><br><span class="line"></span><br><span class="line">        attn = F.softmax(attn, dim=<span class="number">2</span>) </span><br><span class="line">        <span class="comment"># batch_size, output_len, context_len</span></span><br><span class="line"></span><br><span class="line">        context = torch.bmm(attn, context) </span><br><span class="line">        <span class="comment"># batch_size, output_len, enc_hidden_size</span></span><br><span class="line">        </span><br><span class="line">        output = torch.cat((context, output), dim=<span class="number">2</span>) <span class="comment"># batch_size, output_len, hidden_size*2</span></span><br><span class="line"></span><br><span class="line">        output = output.view(batch_size*output_len, <span class="number">-1</span>)</span><br><span class="line">        output = torch.tanh(self.linear_out(output))</span><br><span class="line">        output = output.view(batch_size, output_len, <span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> output, attn</span><br></pre></td></tr></table></figure>

<h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><ul>
<li>decoder会根据已经翻译的句子内容，和context vectors，来决定下一个输出的单词</li>
</ul>
<p>In [0]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.attention = Attention(enc_hidden_size, dec_hidden_size)</span><br><span class="line">        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.out = nn.Linear(dec_hidden_size, vocab_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_mask</span><span class="params">(self, x_len, y_len)</span>:</span></span><br><span class="line">        <span class="comment"># a mask of shape x_len * y_len</span></span><br><span class="line">        device = x_len.device</span><br><span class="line">        max_x_len = x_len.max()</span><br><span class="line">        max_y_len = y_len.max()</span><br><span class="line">        x_mask = torch.arange(max_x_len, device=x_len.device)[<span class="literal">None</span>, :] &lt; x_len[:, <span class="literal">None</span>]</span><br><span class="line">        y_mask = torch.arange(max_y_len, device=x_len.device)[<span class="literal">None</span>, :] &lt; y_len[:, <span class="literal">None</span>]</span><br><span class="line">        mask = (<span class="number">1</span> - x_mask[:, :, <span class="literal">None</span>] * y_mask[:, <span class="literal">None</span>, :]).byte()</span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, ctx, ctx_lengths, y, y_lengths, hid)</span>:</span></span><br><span class="line">        sorted_len, sorted_idx = y_lengths.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">        y_sorted = y[sorted_idx.long()]</span><br><span class="line">        hid = hid[:, sorted_idx.long()]</span><br><span class="line">        </span><br><span class="line">        y_sorted = self.dropout(self.embed(y_sorted)) <span class="comment"># batch_size, output_length, embed_size</span></span><br><span class="line"></span><br><span class="line">        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=<span class="literal">True</span>)</span><br><span class="line">        out, hid = self.rnn(packed_seq, hid)</span><br><span class="line">        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=<span class="literal">True</span>)</span><br><span class="line">        _, original_idx = sorted_idx.sort(<span class="number">0</span>, descending=<span class="literal">False</span>)</span><br><span class="line">        output_seq = unpacked[original_idx.long()].contiguous()</span><br><span class="line">        hid = hid[:, original_idx.long()].contiguous()</span><br><span class="line"></span><br><span class="line">        mask = self.create_mask(y_lengths, ctx_lengths)</span><br><span class="line"></span><br><span class="line">        output, attn = self.attention(output_seq, ctx, mask)</span><br><span class="line">        output = F.log_softmax(self.out(output), <span class="number">-1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, hid, attn</span><br></pre></td></tr></table></figure>

<h4 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h4><ul>
<li>最后我们构建Seq2Seq模型把encoder, attention, decoder串到一起</li>
</ul>
<p>In [0]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2Seq</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder)</span>:</span></span><br><span class="line">        super(Seq2Seq, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, x_lengths, y, y_lengths)</span>:</span></span><br><span class="line">        encoder_out, hid = self.encoder(x, x_lengths)</span><br><span class="line">        output, hid, attn = self.decoder(ctx=encoder_out, </span><br><span class="line">                    ctx_lengths=x_lengths,</span><br><span class="line">                    y=y,</span><br><span class="line">                    y_lengths=y_lengths,</span><br><span class="line">                    hid=hid)</span><br><span class="line">        <span class="keyword">return</span> output, attn</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">translate</span><span class="params">(self, x, x_lengths, y, max_length=<span class="number">100</span>)</span>:</span></span><br><span class="line">        encoder_out, hid = self.encoder(x, x_lengths)</span><br><span class="line">        preds = []</span><br><span class="line">        batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">        attns = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(max_length):</span><br><span class="line">            output, hid, attn = self.decoder(ctx=encoder_out, </span><br><span class="line">                    ctx_lengths=x_lengths,</span><br><span class="line">                    y=y,</span><br><span class="line">                    y_lengths=torch.ones(batch_size).long().to(y.device),</span><br><span class="line">                    hid=hid)</span><br><span class="line">            y = output.max(<span class="number">2</span>)[<span class="number">1</span>].view(batch_size, <span class="number">1</span>)</span><br><span class="line">            preds.append(y)</span><br><span class="line">            attns.append(attn)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(preds, <span class="number">1</span>), torch.cat(attns, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>训练</p>
<p>In [0]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">embed_size = hidden_size = <span class="number">100</span></span><br><span class="line">encoder = Encoder(vocab_size=en_total_words,</span><br><span class="line">                       embed_size=embed_size,</span><br><span class="line">                      enc_hidden_size=hidden_size,</span><br><span class="line">                       dec_hidden_size=hidden_size,</span><br><span class="line">                      dropout=dropout)</span><br><span class="line">decoder = Decoder(vocab_size=cn_total_words,</span><br><span class="line">                      embed_size=embed_size,</span><br><span class="line">                      enc_hidden_size=hidden_size,</span><br><span class="line">                       dec_hidden_size=hidden_size,</span><br><span class="line">                      dropout=dropout)</span><br><span class="line">model = Seq2Seq(encoder, decoder)</span><br><span class="line">model = model.to(device)</span><br><span class="line">loss_fn = LanguageModelCriterion().to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters())</span><br></pre></td></tr></table></figure>

<p>In [2]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(model, train_data, num_epochs=30)</span><br></pre></td></tr></table></figure>

<p>In [0]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for i in range(100,120):</span><br><span class="line">    translate_dev(i)</span><br><span class="line">    print()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">BOS you have nice skin . EOS</span><br><span class="line">BOS 你 的 皮 膚 真 好 。 EOS</span><br><span class="line">你好害怕。</span><br><span class="line"></span><br><span class="line">BOS you &apos;re UNK correct . EOS</span><br><span class="line">BOS 你 部 分 正 确 。 EOS</span><br><span class="line">你是全子的声音。</span><br><span class="line"></span><br><span class="line">BOS everyone admired his courage . EOS</span><br><span class="line">BOS 每 個 人 都 佩 服 他 的 勇 氣 。 EOS</span><br><span class="line">他的袋子是他的勇氣。</span><br><span class="line"></span><br><span class="line">BOS what time is it ? EOS</span><br><span class="line">BOS 几 点 了 ？ EOS</span><br><span class="line">多少时间是什么？</span><br><span class="line"></span><br><span class="line">BOS i &apos;m free tonight . EOS</span><br><span class="line">BOS 我 今 晚 有 空 。 EOS</span><br><span class="line">我今晚有空。</span><br><span class="line"></span><br><span class="line">BOS here is your book . EOS</span><br><span class="line">BOS 這 是 你 的 書 。 EOS</span><br><span class="line">这儿是你的书。</span><br><span class="line"></span><br><span class="line">BOS they are at lunch . EOS</span><br><span class="line">BOS 他 们 在 吃 午 饭 。 EOS</span><br><span class="line">他们在午餐。</span><br><span class="line"></span><br><span class="line">BOS this chair is UNK . EOS</span><br><span class="line">BOS 這 把 椅 子 很 UNK 。 EOS</span><br><span class="line">這些花一下是正在的。</span><br><span class="line"></span><br><span class="line">BOS it &apos;s pretty heavy . EOS</span><br><span class="line">BOS 它 真 重 。 EOS</span><br><span class="line">它很美的脚。</span><br><span class="line"></span><br><span class="line">BOS many attended his funeral . EOS</span><br><span class="line">BOS 很 多 人 都 参 加 了 他 的 葬 礼 。 EOS</span><br><span class="line">多多衛年轻地了他。</span><br><span class="line"></span><br><span class="line">BOS training will be provided . EOS</span><br><span class="line">BOS 会 有 训 练 。 EOS</span><br><span class="line">别将被付錢。</span><br><span class="line"></span><br><span class="line">BOS someone is watching you . EOS</span><br><span class="line">BOS 有 人 在 看 著 你 。 EOS</span><br><span class="line">有人看你。</span><br><span class="line"></span><br><span class="line">BOS i slapped his face . EOS</span><br><span class="line">BOS 我 摑 了 他 的 臉 。 EOS</span><br><span class="line">我把他的臉抱歉。</span><br><span class="line"></span><br><span class="line">BOS i like UNK music . EOS</span><br><span class="line">BOS 我 喜 歡 流 行 音 樂 。 EOS</span><br><span class="line">我喜歡音樂。</span><br><span class="line"></span><br><span class="line">BOS tom had no children . EOS</span><br><span class="line">BOS T o m 沒 有 孩 子 。 EOS</span><br><span class="line">汤姆没有照顧孩子。</span><br><span class="line"></span><br><span class="line">BOS please lock the door . EOS</span><br><span class="line">BOS 請 把 門 鎖 上 。 EOS</span><br><span class="line">请把門開門。</span><br><span class="line"></span><br><span class="line">BOS tom has calmed down . EOS</span><br><span class="line">BOS 汤 姆 冷 静 下 来 了 。 EOS</span><br><span class="line">汤姆在做了。</span><br><span class="line"></span><br><span class="line">BOS please speak more loudly . EOS</span><br><span class="line">BOS 請 說 大 聲 一 點 兒 。 EOS</span><br><span class="line">請說更多。</span><br><span class="line"></span><br><span class="line">BOS keep next sunday free . EOS</span><br><span class="line">BOS 把 下 周 日 空 出 来 。 EOS</span><br><span class="line">繼續下週一下一步。</span><br><span class="line"></span><br><span class="line">BOS i made a mistake . EOS</span><br><span class="line">BOS 我 犯 了 一 個 錯 。 EOS</span><br><span class="line">我做了一件事。</span><br></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Attention/">Attention</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Seq2Seq/">Seq2Seq</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
    <article id="post-机器翻译与文本摘要" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/04/09/机器翻译与文本摘要/" class="article-date">
      <time datetime="2020-04-09T00:37:53.000Z" itemprop="datePublished">2020-04-09</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/09/机器翻译与文本摘要/">机器翻译与文本摘要</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h1><p><img src="https://uploader.shimo.im/f/brdWUlzu4FsL8Owh.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/L7hZdGTE2Rw7LaK7.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/khdZXF2WzdUjtnfh.png!thumbnail" alt="img"></p>
<h1 id><a href="#" class="headerlink" title></a><img src="https://uploader.shimo.im/f/OA9BcNUkI4USlE4s.png!thumbnail" alt="img"></h1><p>现在的<strong>机器翻译模型都是由数据驱动</strong>的。什么数据？</p>
<ul>
<li><p>新闻</p>
</li>
<li><p>公司网页</p>
</li>
<li><p>法律/专利文件，联合国documents</p>
</li>
<li><p>电影/电视字幕</p>
</li>
</ul>
<p>IBM fire a linguist, their machine translation system improves by 1%</p>
<p>Parallel Data</p>
<ul>
<li><p>我们希望使用双语的，有对应关系的数据</p>
</li>
<li><p>大部分数据都是由文档级别的</p>
</li>
</ul>
<p>如何<strong>评估</strong>翻译模型？</p>
<ul>
<li><p><strong>人工评估</strong>最好，但是非常<strong>费时费力</strong></p>
</li>
<li><p>还有哪些问题需要人类评估？</p>
</li>
<li><p>需要一些自动评估的手段</p>
</li>
<li><p><strong>BLUE</strong> (Bilingual Evaluation Understudy), Papineni et al. (2002)</p>
</li>
<li><p>计算系统生成翻译与人类参考翻译之间的n-gram overlap</p>
</li>
<li><p>BLEU score与<strong>人类评测的相关度非常高</strong></p>
</li>
<li><p><a href="https://www.aclweb.org/anthology/P02-1040.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/P02-1040.pdf</a></p>
</li>
<li><p>precision based metric</p>
</li>
<li><p>自动评估依然是一个<strong>有价值的研究问题</strong></p>
</li>
</ul>
<p>precision: 在我翻译的单词当中，有哪些单词是正确的。</p>
<p>unigram, bigram, trigram, 4-gram precision </p>
<p><strong>BLEU-4</strong>: average of the 4 kinds of grams</p>
<p><strong>BLEU-3</strong></p>
<p>统计学翻译模型</p>
<p><img src="https://uploader.shimo.im/f/phryZdcQGH8Z5i5V.png!thumbnail" alt="img"></p>
<p>Encoder-decoder 模型</p>
<p>x：英文</p>
<p><strong>y：中文</strong></p>
<p>P(y|x) x: noisy input</p>
<p><img src="https://uploader.shimo.im/f/1zBNjrukMK8ennr8.png!thumbnail" alt="img"></p>
<p>P(y|x) = P(x, y) / P(x) = P(x|y)P(y) / P(x)</p>
<p>argmax_y P(y|x) = <strong>argmax_y P(x|y)P(y)</strong></p>
<p><strong>P(x|y)</strong> </p>
<p><strong>P(y)</strong></p>
<h2 id="Encoder-Decoder-Model"><a href="#Encoder-Decoder-Model" class="headerlink" title="Encoder-Decoder Model"></a>Encoder-Decoder Model</h2><p><img src="https://uploader.shimo.im/f/fSgtSMHGwlsMwqR8.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/NJXXeu6kA4QJzzj3.png!thumbnail" alt="img"></p>
<p>RNN(x) –&gt; c (<strong>c能够完全包含整个句子的信息?</strong>）</p>
<p>RNN(c) –&gt; y (c作为输入进入每一个decoding step)</p>
<p>训练方式是什么？损失函数是什么？</p>
<ul>
<li><p>cross entropy loss， 作业一中的context模型</p>
</li>
<li><p>SGD, Adam</p>
</li>
</ul>
<p>GRU</p>
<p><a href="https://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1406.1078.pdf</a></p>
<p><img src="https://uploader.shimo.im/f/UBhRdKWsAvEKpbz0.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/eF9pRfBLFSQi5NHd.png!thumbnail" alt="img"></p>
<h2 id="Attention机制"><a href="#Attention机制" class="headerlink" title="Attention机制"></a>Attention机制</h2><p><img src="https://uploader.shimo.im/f/CkL5KNLrUQE2tzH4.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/JQtrWTdEgiURNKTX.png!thumbnail" alt="img"></p>
<p>图片来自 Bahdanau et al., Neural Machine Translation by Jointly Learning to Align and Translate <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1409.0473.pdf</a></p>
<h2 id="-1"><a href="#-1" class="headerlink" title></a><img src="https://uploader.shimo.im/f/dWsWHO9MF20QJ2kI.png!thumbnail" alt="img"></h2><p><img src="https://uploader.shimo.im/f/KRbuH9pTLpoNLHN7.png!thumbnail" alt="img"></p>
<p>图片来自Luong et al., Effective Approaches to Attention-based Neural Machine Translation</p>
<p><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1508.04025.pdf</a></p>
<p>Google Neural Machine Translation</p>
<p><a href="https://arxiv.org/pdf/1609.08144.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1609.08144.pdf</a></p>
<p><img src="https://uploader.shimo.im/f/en9dH9PnTeoPDMMv.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/W8BSXh4U2Kc8ZKjL.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/vpGOGoKHN5AQvKiB.png!thumbnail" alt="img"></p>
<h2 id="Zero-shot-NMT"><a href="#Zero-shot-NMT" class="headerlink" title="Zero-shot NMT"></a>Zero-shot NMT</h2><p><img src="https://uploader.shimo.im/f/I5SzyIfYl6sfFUoA.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/v9nmM5q6jDoyq7ZR.png!thumbnail" alt="img"></p>
<h2 id="Transformer模型"><a href="#Transformer模型" class="headerlink" title="Transformer模型"></a>Transformer模型</h2><p><a href="https://shimo.im/docs/gPwkqCXrkJyRW89V" target="_blank" rel="noopener">https://shimo.im/docs/gPwkqCXrkJyRW89V</a></p>
<p>这个模型非常重要</p>
<p>模型 x –&gt; encoder decoder model –&gt; \hat{y}</p>
<p>cross entropy loss (\hat{y}, y)</p>
<p>训练 P(y_i | x, <strong>y_1, …, y_{i-1}</strong>) 训练的时候，我们知道y_1 … y_{i-1}</p>
<p>在预测的时候，我们不知道y_1 … y_{i-1}</p>
<p>怎么样统一训练和测试</p>
<h2 id="Model-Inference"><a href="#Model-Inference" class="headerlink" title="Model Inference"></a>Model Inference</h2><p>在各类文本生成任务中，其实文本的生成与训练是两种不同的情形。在训练的过程中，我们假设模型在生成下一个单词的时候知道所有之前的单词（groud truth）。然而在真正使用模型生成文本的时候，每一步生成的文本都来自于模型本身。这其中训练和预测的不同导致了模型的效果可能会很差。为了解决这一问题，人们发明了各种提升模型预测水平的方法，例如Beam Search。</p>
<p><strong>Beam Search</strong></p>
<p>Kyunghyun Cho Lecture Notes Page 94-96 <a href="https://arxiv.org/pdf/1511.07916.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1511.07916.pdf</a></p>
<p>Encoder(我喜欢自然语言处理) –&gt; c</p>
<p>Decoder(c) –&gt; y_1</p>
<p>Decoder(c, y_1) –&gt; y_2</p>
<p>Decoder(c, y_1, y_2) –&gt; y_3</p>
<p>…..</p>
<p>EOS</p>
<p>argmax_y P(y|x) </p>
<p>greedy search</p>
<p>argmax y_1</p>
<p>Beam 横梁</p>
<p>————————————————</p>
<p>一种固定宽度的装置</p>
<p>————————————————</p>
<p>在后续的课程中我们还会介绍一些别的方法用于生成文本。</p>
<p>美国总统和中国主席打电话</p>
<p>–&gt; K = 无穷大 |V|^seq_len</p>
<p>American, U.S. , United</p>
<p>….</p>
<p>decoding step: K</p>
<p>K x |V| –&gt; K</p>
<p>K x |V| –&gt; K</p>
<h2 id="开源项目"><a href="#开源项目" class="headerlink" title="开源项目"></a>开源项目</h2><p>FairSeq <a href="https://github.com/pytorch/fairseq" target="_blank" rel="noopener">https://github.com/pytorch/fairseq</a></p>
<p>Tensor2Tensor <a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener">https://github.com/tensorflow/tensor2tensor</a></p>
<p>Trax <a href="https://github.com/google/trax" target="_blank" rel="noopener">https://github.com/google/trax</a></p>
<h1 id="文本摘要"><a href="#文本摘要" class="headerlink" title="文本摘要"></a>文本摘要</h1><p>文本摘要这个任务定义非常简单，给定一段长文章，我们希望生成一段比较精简的文本摘要，可以覆盖整篇文章的信息。</p>
<p>文本摘要按照任务的定义大致可以分为两类。</p>
<ul>
<li><p>抽取式：给定一个包含多个句子的长文本，选择其中的一些句子作为短文本。这本质上是个分类问题，也就是判断哪些句子需要保留，哪些句子需要丢弃。<strong>二分类任务</strong></p>
</li>
<li><p>生成式：与抽取式文本摘要不同，这里我们不仅仅是希望选出一些句子，而是希望能够总结归纳文本的信息，用自己的话复述一遍。<strong>直接上transformer模型</strong></p>
</li>
</ul>
<p>gold standard</p>
<p>评估手段: <strong>ROUGE</strong></p>
<p>ROUGE评估的是系统生成文本和参考文本之间 n-gram overlap 的 recall。</p>
<p><strong>Candidate</strong> Summary</p>
<p>the cat was found under the bed</p>
<p><strong>Reference</strong> Summary</p>
<p>the cat was under the bed</p>
<p>针对这一个例子，ROUGE-1分数为1， ROUGE-2为4/5。</p>
<p>s: the cat was found under the bed</p>
<p>p: <strong>the cat was under the bed</strong></p>
<p>ROUGE-L，基于 longest common subsequence的F1 score</p>
<p>例如上面这个案例 LCS  = 6</p>
<p>P = 6/7 </p>
<p>R = 6/6</p>
<p>F1 = 2 / (6/6 + 7/6 )  = 12/13</p>
<p>harmoic mean</p>
<p><img src="https://uploader.shimo.im/f/oNU6qvIsXX8cK7Ta.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/65Raa96bt7kl0ldv.png!thumbnail" alt="img"></p>
<p><a href="https://arxiv.org/pdf/1908.08345.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1908.08345.pdf</a></p>
<p><img src="https://uploader.shimo.im/f/8YQylm0VFkgRXqzU.png!thumbnail" alt="img"></p>
<p><img src="https://uploader.shimo.im/f/vC7ZiGFfsBInlMtQ.png!thumbnail" alt="img"></p>
<p>上期学员的博客</p>
<p><a href="https://blog.csdn.net/Chen_Meng_/article/details/103756716" target="_blank" rel="noopener">https://blog.csdn.net/Chen_Meng_/article/details/103756716</a></p>
<p>CopyNet</p>
<p><a href="https://arxiv.org/pdf/1603.06393.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.06393.pdf</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BLUE/">BLUE</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GRU/">GRU</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
  
</article>










  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/3/">Next &raquo;</a>
    </nav>
  
</div>
      <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2019-2020 MingmingYe
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>

    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>